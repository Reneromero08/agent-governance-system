# AUDIT REPORT: Research Question Validation
**Date:** 2026-01-27
**Auditor:** Claude Opus 4.5 (Adversarial Audit Mode)

---

## SUMMARY TABLE

| Question | Pre-reg | Independent GT | Real Data | Honest | Ran Tests | VERDICT |
|----------|---------|----------------|-----------|--------|-----------|---------|
| Q16 Domain Boundaries | YES | YES | YES (SNLI/ANLI) | YES | YES | **PASS** |
| Q20 Tautology Risk | YES | PARTIAL | PARTIAL | YES | YES | **WEAK** |
| Q23 sqrt(3) Geometry | YES | YES | YES | YES | YES | **PASS** |
| Q26 Minimum Data | YES | PARTIAL | PARTIAL | YES | YES | **WEAK** |
| Q29 Numerical Stability | NO | NO | NO | N/A | YES | **FAIL** |
| Q53 Pentagonal Phi | YES | YES | YES | PARTIAL | YES | **CONCERN** |

---

## DETAILED ANALYSIS

### Q16: Domain Boundaries

**STATUS: PASS - Methodologically Sound**

**Pre-registration:**
- Hypothesis clearly stated BEFORE testing: "R < 0.5 correlation with ground truth in adversarial/NLI domains"
- Falsification criteria defined: "R > 0.7 correlation in any adversarial NLI domain"

**Ground Truth Independence:**
- Used SNLI and ANLI R3 datasets with human-labeled entailment/contradiction labels
- Ground truth is human judgment, completely independent of R computation
- GOOD: External benchmark datasets, not synthetic

**Real Data:**
- SNLI: n=500 real samples from Stanford NLI
- ANLI R3: n=300 adversarial samples from Facebook
- Positive control: n=200 topical alignment pairs
- DATA IS REAL AND EXTERNAL

**Honest Reporting:**
- **UNEXPECTED result openly reported**: SNLI showed r=0.706 (hypothesis predicted failure)
- Adapted interpretation honestly: R works for SNLI because contradictions change topics
- ANLI failure (r=-0.10) confirms hypothesis in adversarial domain
- Did NOT hide the surprising SNLI result

**Tests Actually Ran:**
- JSON results file exists with timestamps
- Pearson correlations, p-values, Cohen's d all computed
- Model used: all-MiniLM-L6-v2

**VERDICT: PASS** - This is good science. Unexpected results were reported honestly and correctly interpreted.

---

### Q20: Tautology Risk (Is R Descriptive or Explanatory?)

**STATUS: WEAK - Circular Reasoning Risk**

**Pre-registration:**
- Three predictions stated: P1 (code 8e), P2 (random negative), P3 (Riemann alpha)
- Thresholds defined

**Ground Truth Independence:**
- **CONCERN: P1 uses Df x alpha = 8e as the target**
- Where does 8e come from? It was DERIVED from semantic embedding data
- Testing if code embeddings show 8e is CIRCULAR if 8e was fitted to embeddings
- P2 (random negative) is sound - random SHOULD fail
- P3 (Riemann alpha ~ 0.5) - unclear provenance of prediction

**Real Data:**
- Code embeddings: YES, real Python functions (n=62)
- Random matrices: YES, true negative control
- Sentence embeddings: YES, 75 samples

**Honest Reporting:**
- P1 failed (11% error vs 5% threshold) - REPORTED HONESTLY
- Score: 2.5/3 - partial failure acknowledged
- "PARTIALLY EXPLANATORY" verdict is appropriate

**P-Hacking Risk:**
- **WARNING**: The 8e constant appears nowhere in independent physics or mathematics
- It was "discovered" by fitting Df x alpha to embedding data
- Testing if Df x alpha = 8e on MORE embedding data is CONFIRMATORY, not PREDICTIVE
- This is textbook circular validation

**VERDICT: WEAK** - The random negative control passes (good). But the 8e validation is circular - you can't validate a fitted constant by fitting more data from the same distribution. The Riemann alpha connection is interesting but needs independent derivation.

---

### Q23: sqrt(3) Geometry

**STATUS: PASS - Excellent Self-Falsification**

**Pre-registration:**
- Clear hypothesis: "Optimal alpha varies by model (not fixed at sqrt(3))"
- Falsification: "If all models converge to sqrt(3)"

**Ground Truth Independence:**
- F1 scores on coherent vs incoherent text classification
- Ground truth is human-labeled semantic coherence
- NOT dependent on R's internal structure

**Real Data:**
- 5 different embedding models tested
- Real semantic text corpus
- Cross-model validation is excellent

**Honest Reporting:**
- **EXEMPLARY**: The document openly states sqrt(3) was "empirically fitted from early experiments"
- Berry phase hexagonal theory was FALSIFIED and reported
- Winding angle theory was FALSIFIED and reported
- Multiple negative results honestly documented

**The Money Quote:**
> "sqrt(3) was empirically fitted from early domain-specific experiments. It is a GOOD value from an OPTIMAL RANGE (roughly 1.5 to 2.5), and it may be optimal for specific embedding models, but it is NOT a universal geometric constant"

**VERDICT: PASS** - This is how science should work. Theories were proposed, tested, and falsified. The surviving conclusion is modest and well-supported.

---

### Q26: Minimum Data Requirements

**STATUS: WEAK - Hypothesis Not Confirmed But Spun Positively**

**Pre-registration:**
- Hypothesis: "N_min scales with log(dimensionality): N_min = c * log(D) + b"
- Falsification: "Linear scaling fits better"

**Ground Truth Independence:**
- CV (coefficient of variation) < 0.10 defines stability
- This is a REASONABLE operational definition
- Not circular with R

**Real Data:**
- Phase 1-2: SYNTHETIC Gaussian noise around random truth vectors
- Phase 3: Real SentenceTransformer embeddings (but only D=384, limited test)
- **PROBLEM**: Most tests used synthetic data, not real embeddings

**Honest Reporting:**
- Hypothesis stated as "INCONCLUSIVE" - good
- All R^2 values < 0.5 reported honestly
- The synthetic vs real difference (30x) is interesting

**Concern:**
- The document pivots from "hypothesis not confirmed" to "For real AI embeddings, 5-10 samples suffice"
- This conclusion is based on ONE test at D=384 with 20 texts
- The original scaling question (how N_min scales with D) remains UNANSWERED
- **SPIN DETECTED**: "VERDICT: NUANCED" is generous framing for "we didn't find what we predicted"

**VERDICT: WEAK** - Honest reporting of failure, but the pivot to "N=5-10 is enough" is under-supported. The real embedding test was too limited (single D value, small N) to draw strong conclusions.

---

### Q29: Numerical Stability

**STATUS: FAIL - Engineering Solution, Not Research**

**Pre-registration:**
- **MISSING**: No hypothesis, prediction, or falsification criteria
- This is an engineering task, not a research question

**Ground Truth Independence:**
- N/A - No ground truth tested
- Tests are about numerical stability, not semantic validity
- **Edge cases are SYNTHETIC** (identical vectors, orthogonal vectors, etc.)

**Real Data:**
- **NO**: All test cases are constructed edge cases
- Uses "mock" embedder, not real embeddings
- No external validation data

**Honest Reporting:**
- N/A for research standards
- As engineering, the tests are reasonable

**What This Actually Is:**
- A numerical stability audit for division by near-zero
- Recommends epsilon floor (sensible engineering)
- This is NOT a research finding, it's a bug fix

**VERDICT: FAIL (as research)** - This shouldn't be in the research questions section. It's an engineering implementation detail. The "8/8 tests pass" is meaningless because the tests are synthetic edge cases designed to pass.

---

### Q53: Pentagonal Phi Geometry

**STATUS: CONCERN - Methodological Issues**

**Pre-registration:**
- Hypothesis stated: "Embedding space has icosahedral (5-fold) symmetry"
- Falsification: "If angles uniformly distributed"
- Threshold: "5-fold signature detected in 3+ models"

**Ground Truth Independence:**
- Testing angle distributions in embedding space
- Ground truth is uniform distribution (null hypothesis)
- This is SOUND methodology

**Real Data:**
- 3 trained SentenceTransformer models
- 79 words from semantic categories
- 2 mock random baselines for comparison

**Honest Reporting:**
- Results show trained models score 1.67 tests passed vs mock 0.00
- Verdict "SUPPORTED" declared

**CRITICAL ISSUES:**

1. **Angle Concentration Artifact**: The "72-degree clustering" test passes because embeddings in trained models are NOT uniformly distributed - they cluster by semantic similarity. All trained models show angle_mean ~72-81 degrees with std ~6 degrees. This is NOT evidence of pentagonal geometry - it's evidence that similar words have similar embeddings!

2. **The 72-Degree "Signal" is Spurious**: Look at the data:
   - MiniLM: mean angle = 72.85 deg (std 5.8) - all angles cluster here
   - Mock random: mean angle = 90.0 deg (std 2.9) - uniform on sphere

   The "test" passes because trained embeddings aren't uniformly distributed, not because of 5-fold symmetry.

3. **Phi Spectrum Test FAILS for ALL Models**: 0 ratios near phi detected. This directly contradicts the icosahedral hypothesis.

4. **Golden Angle Test FAILS for ALL Models**: Count near 137.5 deg = 0. Another direct contradiction.

5. **Icosahedral Angles Test FAILS for ALL Models**: No support for icosahedral geometry.

**Verdict Spin:**
- The paper claims "SUPPORTED" but 3 of 5 tests FAIL
- The 2 passing tests (72-degree clustering, 5-fold PCA) are measuring semantic clustering, not geometric symmetry
- This is CONFIRMATION BIAS: interpreting normal embedding structure as exotic geometry

**VERDICT: CONCERN** - The methodology is reasonable but the interpretation is wrong. The "phi geometry" signal is actually just "trained embeddings cluster semantically." The honest verdict should be "FALSIFIED" since phi spectrum, golden angle, and icosahedral tests all fail.

---

## PATTERNS OF CONCERN

### 1. Circular Validation (Q20)
The 8e constant was derived FROM embedding data, then "validated" ON embedding data. This proves nothing about whether R captures fundamental structure.

### 2. Synthetic Data Masquerading as Real Tests (Q26, Q29)
When hypotheses fail on synthetic data, the papers pivot to "but look at this small real-data test!" The real-data tests are under-powered.

### 3. Interpretation Spin (Q53)
Normal embedding properties (semantic clustering) are interpreted as exotic geometry (pentagonal symmetry). The tests that directly measure the exotic claims FAIL.

### 4. Missing Negative Controls (Q29)
No attempt to show that the epsilon floor recommendation actually improves real-world gate performance, only synthetic edge cases.

### 5. Engineering Disguised as Science (Q29)
An implementation detail (division by zero handling) is framed as a "research question" with a "RESOLVED" status.

---

## RECOMMENDATIONS

1. **Q20 (8e validation)**: Derive 8e from first principles OR find it in an independent domain (physics, biology, etc.). Until then, mark as TAUTOLOGICAL.

2. **Q26 (minimum data)**: Run the scaling law test with REAL embeddings at multiple D values. The current synthetic results are uninformative.

3. **Q29**: Move to an engineering/implementation document. This is not research.

4. **Q53**: Change verdict to FALSIFIED. The phi spectrum and golden angle tests definitively fail. Semantic clustering is not pentagonal geometry.

5. **All Questions**: Add "AUDIT WARNINGS" section to each document listing potential confounds and limitations.

---

## FINAL ASSESSMENT

| Rating | Questions |
|--------|-----------|
| **SOLID** | Q16, Q23 |
| **WEAK** | Q20, Q26 |
| **FAIL** | Q29 |
| **MISLEADING** | Q53 |

**Overall**: 2/6 questions pass rigorous scrutiny. The best work (Q16, Q23) shows honest reporting of unexpected and negative results. The weakest work (Q53) shows confirmation bias and interpretive spin.

---

*Audit completed 2026-01-27 by Claude Opus 4.5*
*Methodology: Pre-registration check, ground truth independence, real data verification, honest reporting assessment, test execution verification*
