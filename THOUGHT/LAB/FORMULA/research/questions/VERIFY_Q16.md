# VERIFICATION: Q16 Domain Boundaries

**Verification Date:** 2026-01-28
**Status:** VERIFIED WITH NOTES

---

## Executive Summary

Q16's findings are **VERIFIED** with important clarifications about what the results actually demonstrate. All tests use real data, calculations are correct, and there is no circular logic. However, the interpretation requires careful attention to what R actually measures.

---

## 1. DATA AUTHENTICITY CHECK

### Datasets Used
- **SNLI:** 500 examples from Stanford NLI validation set (`stanfordnlp/snli`)
  - Entailment: n=199
  - Contradiction: n=153
  - Neutral: n=148
- **ANLI R3:** 300 examples from Facebook Adversarial NLI test set (`facebook/anli`)
  - Entailment: n=107
  - Contradiction: n=110
  - Neutral: n=83

### Verification
✅ **REAL DATA CONFIRMED**
Both datasets are fetched from HuggingFace at runtime, not synthetic. Data is shuffled and sampled consistently using seed=42.

---

## 2. CIRCULAR LOGIC CHECK

### Ground Truth Sources
1. **SNLI labels** - Independent of R, assigned by human annotators during dataset creation
2. **ANLI labels** - Independent of R, assigned adversarially by crowdworkers during dataset construction
3. **Positive Control labels** - Generated by manual pairing (aligned=matched pairs, misaligned=rotated indices)

### Verification
✅ **NO CIRCULAR LOGIC DETECTED**
Ground truth labels are completely external to R computation. R is computed from embeddings AFTER labels are assigned. There is no possibility of R definitions leaking into the ground truth.

---

## 3. CALCULATION VERIFICATION

### Test 1: SNLI Results
| Metric | Value | Check |
|--------|-------|-------|
| Mean Similarity (Entailment) | 0.6606 | ✓ |
| Mean Similarity (Contradiction) | 0.3085 | ✓ |
| Pearson r | 0.7059 | ✓ Significant (p=2.25e-54) |
| Cohen's d | 1.9680 | ✓ Large effect size |
| Hypothesis (r < 0.5) | FALSE | ✓ Falsified |

**Interpretation Note:** R CAN distinguish SNLI examples because SNLI contradictions often involve topic/context shifts (e.g., "cat on mat" vs "no animals in room"). R detects topical differences, not logical validity.

### Test 2: ANLI Results
| Metric | Value | Check |
|--------|-------|-------|
| Mean Similarity (Entailment) | 0.4979 | ✓ |
| Mean Similarity (Contradiction) | 0.5361 | ✓ Nearly identical |
| Pearson r | -0.1003 | ✓ NOT significant (p=0.1409) |
| Cohen's d | -0.2018 | ✓ Negligible effect size |
| Hypothesis (r < 0.5) | TRUE | ✓ Confirmed |

**Interpretation Note:** R CANNOT distinguish ANLI examples because adversarial contradictions maintain high semantic overlap while being logically opposed (e.g., "The cat is on the mat" vs "No cat is on the mat" - same entities/topics, opposite truth values).

### Test 3: Positive Control
| Metric | Value | Check |
|--------|-------|-------|
| Mean Similarity (Aligned) | 0.6593 | ✓ |
| Mean Similarity (Misaligned) | 0.0562 | ✓ Vast difference |
| Pearson r | 0.9055 | ✓ Highly significant (p=2.72e-150) |
| Cohen's d | 4.2690 | ✓ Extremely large effect |
| Control Passes | TRUE | ✓ Yes |

**Interpretation Note:** R excels at detecting topical alignment - it's designed for this. The massive difference (0.659 vs 0.056) shows R reliably identifies when text pairs belong together semantically.

---

## 4. RE-RUN VERIFICATION

✅ **REPRODUCTION CONFIRMED**
All tests were re-run on 2026-01-28 using the same code (`run_q16_real_data.py`). Results match the original q16_results.json within floating-point precision:
- SNLI Pearson r: 0.7059 (original: 0.7059)
- ANLI Pearson r: -0.1003 (original: -0.1003)
- PC Pearson r: 0.9055 (original: 0.9055)

---

## 5. CRITICAL ISSUES IDENTIFIED

### Issue 1: Misleading Interpretation of SNLI Results
**Status:** INTERPRETATION ERROR (not a data error)

The research document states that SNLI "UNEXPECTEDLY" shows r=0.706, implying R can distinguish logical relationships. This is **MISLEADING** because:

- SNLI contradictions work differently than ANLI
- SNLI often uses topical shifts: "The cat is on the mat" → "No animals were present"
- This is topical difference, NOT logical detection
- R correctly detects the topical shift, but this isn't detecting logical validity

**Finding:** The distinction between SNLI and ANLI is correctly noted in the document, but could be clearer that SNLI's success is an artifact of how contradictions are constructed in that dataset, not evidence that R measures logic.

### Issue 2: Positive Control Uses Same Dataset as SNLI Test
**Status:** METHODOLOGICAL CONCERN (valid but note-worthy)

The positive control re-uses SNLI entailment pairs (lines 207-209 of run_q16_real_data.py):
```python
ds3 = load_dataset('stanfordnlp/snli', split='validation')
ds3 = ds3.filter(lambda x: x['label'] == 0)  # Only entailment
```

This means:
- PC shows R works well on entailment pairs from SNLI
- But doesn't test R on other data sources
- Could strengthen positive control by using diverse data

**Finding:** Not a circular logic issue, but limits generalizability of positive control findings.

---

## 6. LOGICAL CONSISTENCY CHECK

### Pre-registration vs Results
✅ **CONSISTENT**

| Claim | Test | Result | Status |
|-------|------|--------|--------|
| "R < 0.5 in adversarial NLI" | ANLI R3 | r = -0.10 | ✓ CONFIRMED |
| "R > 0.5 in topical consistency" | Positive Control | r = 0.91 | ✓ CONFIRMED |
| "R works for semantic coherence" | PC > ANLI | 0.91 >> -0.10 | ✓ CONFIRMED |

The hypothesis is confirmed: R measures **semantic coherence**, not **logical validity**.

---

## 7. FINAL VERDICT

### What is VERIFIED
✅ All tests use real HuggingFace datasets
✅ All calculations are mathematically correct
✅ No circular logic or ground truth contamination
✅ Results are reproducible
✅ Positive control properly demonstrates R's intended use case
✅ ANLI results clearly show R failure on adversarial logic

### What Needs Clarification
⚠️ SNLI r=0.706 should be explained as "topical detection" not "logic detection"
⚠️ Positive control would be stronger with diverse datasets
⚠️ The finding is less about "domain boundaries" and more about "what R actually measures"

### Recommended Interpretation
**R has a fundamental measurement limitation, not domain boundaries:**
- R measures **semantic/topical coherence** (how similar embeddings are)
- R does NOT measure **logical validity** (whether propositions are true/contradict)
- This is by design: R is a geometric coherence metric, not a logic checker
- The "domain boundaries" are actually the boundaries of what semantic similarity can do

---

## 8. OVERALL ASSESSMENT

**Q16 Status: VERIFIED (with interpretation caveats)**

All empirical claims are supported by real data and correct calculations. The finding that R fails on adversarial NLI while succeeding on topical consistency is robust. However, the framing as "domain boundaries" could be more precise as "measurement limitations of semantic coherence as a proxy for logical validity."

**Confidence Level:** HIGH (re-run successful, no data issues, calculations correct)
