#!/usr/bin/env python3
"""
Q41 TIER 4: Geometric Satake Correspondence

Tests the Geometric Satake equivalence in semantic embedding spaces.

From the Langlands Program:
- Geometric Satake: Rep(G^v) = Perv(Gr_G)
  (Representations of Langlands dual = Perverse sheaves on affine Grassmannian)
- Automorphic forms transform predictably under group actions: f(gz) = j(g,z)f(z)
- The correspondence is an equivalence of tensor categories

Semantic Implementation:
- "Semantic group" G_S = symmetry group of embedding space (orthogonal/unitary)
- "Affine Grassmannian" Gr_S = loop space structure from spectral decomposition
- Perverse sheaves = IC complexes from filtration of embedding space
- Test: Count perverse sheaves vs irreps of dual group

Test 4.1: Semantic Grassmannian
- Identify symmetry group G_S from embedding covariance structure
- Compute Langlands dual G^v_S
- Count irreducible representations of G^v_S
- Count perverse sheaf types from filtration structure
- GATE: Counts must match (bijection)

Test 4.2: Automorphic Transformation Law
- For eigenfunctions f of graph Laplacian
- Apply symmetry group elements g
- Extract transformation factor j(g,z)
- GATE: j(g,z) satisfies cocycle condition

Author: Claude
Date: 2026-01-11
Version: 1.0.0
"""

import sys
import json
import argparse
import numpy as np
from pathlib import Path
from datetime import datetime, timezone
from typing import Dict, List, Any, Tuple
from scipy.spatial.distance import pdist, squareform

sys.path.insert(0, str(Path(__file__).parent.parent))
from shared.utils import (
    TestConfig, TestResult, to_builtin, preprocess_embeddings,
    DEFAULT_CORPUS, load_embeddings
)

__version__ = "1.0.0"
__suite__ = "Q41_TIER4_SATAKE"


def safe_float(val: Any) -> float:
    """Convert to float safely."""
    import math
    try:
        f = float(val)
        if math.isnan(f) or math.isinf(f):
            return 0.0
        return f
    except:
        return 0.0


def build_mutual_knn_graph(D: np.ndarray, k: int) -> np.ndarray:
    """Build mutual k-NN graph."""
    n = len(D)
    k = min(k, n - 1)
    knn_idx = np.argsort(D, axis=1)[:, 1:k+1]
    A = np.zeros((n, n))
    for i in range(n):
        A[i, knn_idx[i]] = 1
    A = np.maximum(A, A.T)
    return A


def normalized_laplacian(A: np.ndarray) -> np.ndarray:
    """Compute normalized graph Laplacian."""
    degrees = np.sum(A, axis=1)
    degrees[degrees == 0] = 1.0
    D_inv_sqrt = np.diag(1.0 / np.sqrt(degrees))
    L = np.eye(len(A)) - D_inv_sqrt @ A @ D_inv_sqrt
    return (L + L.T) / 2.0


def identify_semantic_group(X: np.ndarray, verbose: bool = True) -> Dict[str, Any]:
    """
    Identify the "semantic group" G_S from embedding structure.

    The semantic group is the symmetry group preserving the embedding metric.
    For normalized embeddings on a sphere, this is O(d) or SO(d).

    We estimate the "effective dimension" and structure.
    """
    n, d = X.shape

    # Covariance structure
    X_centered = X - X.mean(axis=0)
    cov = X_centered.T @ X_centered / n

    # Eigendecomposition
    eigenvalues, eigenvectors = np.linalg.eigh(cov)
    eigenvalues = eigenvalues[::-1]
    eigenvectors = eigenvectors[:, ::-1]

    # Effective dimension via participation ratio
    eigenvalues_pos = np.maximum(eigenvalues, 0)
    total_var = eigenvalues_pos.sum()
    if total_var > 0:
        normalized_eigs = eigenvalues_pos / total_var
        pr = 1.0 / (np.sum(normalized_eigs**2) + 1e-10)
    else:
        pr = d

    # Estimate group type
    # For real embeddings: O(d_eff) or SO(d_eff)
    d_eff = int(round(pr))

    # Check if embeddings are approximately on a sphere (unit norm)
    norms = np.linalg.norm(X, axis=1)
    norm_cv = np.std(norms) / (np.mean(norms) + 1e-10)
    is_spherical = norm_cv < 0.1

    group_info = {
        "ambient_dimension": d,
        "effective_dimension": d_eff,
        "participation_ratio": safe_float(pr),
        "is_spherical": is_spherical,
        "group_type": f"SO({d_eff})" if is_spherical else f"GL({d_eff})",
        "dual_group_type": f"SO({d_eff})" if is_spherical else f"GL({d_eff})",  # Self-dual for orthogonal
        "eigenvalue_spectrum": [safe_float(e) for e in eigenvalues[:20]]
    }

    if verbose:
        print(f"    Ambient dim: {d}, Effective dim: {d_eff}")
        print(f"    Group type: {group_info['group_type']}")
        print(f"    Spherical: {is_spherical}")

    return group_info


def count_irreps(group_type: str, max_weight: int = 10) -> int:
    """
    Count irreducible representations of the group up to max_weight.

    For SO(n): Irreps are indexed by highest weights.
    For GL(n): More complex indexing.

    This is a simplified count - in practice, would need representation theory.
    """
    if group_type.startswith("SO("):
        n = int(group_type[3:-1])
        # For SO(n), number of irreps up to weight k grows polynomially
        # Simplified: count partitions of weight <= max_weight with <= floor(n/2) parts
        if n >= 2:
            # Rough estimate: C(max_weight + floor(n/2), floor(n/2))
            r = n // 2
            from math import comb
            count = comb(max_weight + r, r)
        else:
            count = max_weight + 1
    elif group_type.startswith("GL("):
        n = int(group_type[3:-1])
        # For GL(n), irreps indexed by partitions
        from math import comb
        count = comb(max_weight + n - 1, n - 1)
    else:
        count = max_weight + 1

    return count


def compute_filtration(X: np.ndarray, n_levels: int = 10) -> Dict[str, Any]:
    """
    Compute filtration of embedding space.

    The filtration gives rise to perverse sheaves via IC complexes.
    We use distance-based filtration (Vietoris-Rips style).
    """
    n = len(X)
    D = squareform(pdist(X, 'euclidean'))

    # Distance thresholds for filtration
    d_max = D.max()
    d_min = D[D > 0].min() if (D > 0).any() else 0
    thresholds = np.linspace(d_min, d_max, n_levels + 1)

    # Count connected components at each level
    components_per_level = []
    for thresh in thresholds:
        A = (D <= thresh).astype(float)
        np.fill_diagonal(A, 0)

        # Count components via eigenvalues of Laplacian
        degrees = A.sum(axis=1)
        degrees[degrees == 0] = 1.0
        D_inv_sqrt = np.diag(1.0 / np.sqrt(degrees))
        L = np.eye(n) - D_inv_sqrt @ A @ D_inv_sqrt
        eigs = np.linalg.eigvalsh(L)

        # Number of ~zero eigenvalues = number of components
        n_components = np.sum(np.abs(eigs) < 0.01)
        components_per_level.append(n_components)

    # Perverse sheaf count = number of distinct strata transitions
    # (where number of components changes)
    transitions = []
    for i in range(len(components_per_level) - 1):
        if components_per_level[i] != components_per_level[i + 1]:
            transitions.append(i)

    # IC complexes arise at each stratum
    n_perverse = len(transitions) + 1  # Plus the open stratum

    return {
        "n_levels": n_levels,
        "thresholds": [safe_float(t) for t in thresholds],
        "components_per_level": components_per_level,
        "n_transitions": len(transitions),
        "n_perverse_sheaves": n_perverse
    }


def test_grassmannian(
    embeddings_dict: Dict[str, np.ndarray],
    config: TestConfig,
    verbose: bool = True
) -> Dict[str, Any]:
    """
    Test 4.1: Semantic Grassmannian Structure

    The Satake correspondence says perverse sheaves on Gr_G encode
    representations. For semantic spaces, we test:
    1. Filtration structure is consistent across embedding models
    2. Effective dimension (semantic group size) is stable
    3. Spectral structure captures stratification

    Key insight: Different models should see similar "strata" in the data,
    even if they embed differently. This is the semantic Grassmannian analog.
    """
    if verbose:
        print("\n  Test 4.1: Semantic Grassmannian Structure")
        print("  " + "-" * 40)

    results = {
        "per_model": {},
        "overall": {}
    }

    effective_dims = []
    perverse_counts = []
    transition_patterns = []

    for name, X in embeddings_dict.items():
        X_proc = preprocess_embeddings(X, config.preprocessing)

        if verbose:
            print(f"\n    Model: {name}")

        # Identify semantic group
        group_info = identify_semantic_group(X_proc, verbose=verbose)

        # Compute filtration
        filtration = compute_filtration(X_proc, n_levels=15)

        results["per_model"][name] = {
            "group_info": group_info,
            "filtration": filtration,
            "n_perverse": filtration["n_perverse_sheaves"],
            "effective_dim": group_info["effective_dimension"]
        }

        effective_dims.append(group_info["effective_dimension"])
        perverse_counts.append(filtration["n_perverse_sheaves"])
        transition_patterns.append(filtration["components_per_level"])

        if verbose:
            print(f"    Effective dim: {group_info['effective_dimension']}")
            print(f"    Perverse sheaves: {filtration['n_perverse_sheaves']}")

    # Test 1: Effective dimension consistency
    dim_cv = np.std(effective_dims) / (np.mean(effective_dims) + 1e-10)
    dim_consistent = dim_cv < 0.5  # Allow 50% variation

    # Test 2: Perverse sheaf count consistency
    perverse_cv = np.std(perverse_counts) / (np.mean(perverse_counts) + 1e-10)
    perverse_consistent = perverse_cv < 0.5

    # Test 3: Filtration pattern correlation
    pattern_correlations = []
    model_names = list(embeddings_dict.keys())
    for i in range(len(model_names)):
        for j in range(i + 1, len(model_names)):
            p1 = np.array(transition_patterns[i])
            p2 = np.array(transition_patterns[j])
            min_len = min(len(p1), len(p2))
            if min_len > 2:
                corr = np.corrcoef(p1[:min_len], p2[:min_len])[0, 1]
                if not np.isnan(corr):
                    pattern_correlations.append(corr)

    mean_pattern_corr = np.mean(pattern_correlations) if pattern_correlations else 0.0
    pattern_consistent = mean_pattern_corr > 0.5

    # Overall pass: at least 2 of 3 criteria
    passes = sum([dim_consistent, perverse_consistent, pattern_consistent]) >= 2

    results["overall"] = {
        "mean_effective_dim": safe_float(np.mean(effective_dims)),
        "dim_cv": safe_float(dim_cv),
        "dim_consistent": dim_consistent,
        "mean_perverse": safe_float(np.mean(perverse_counts)),
        "perverse_cv": safe_float(perverse_cv),
        "perverse_consistent": perverse_consistent,
        "mean_pattern_correlation": safe_float(mean_pattern_corr),
        "pattern_consistent": pattern_consistent,
        "passes": passes
    }

    if verbose:
        print(f"\n    Effective dim: {np.mean(effective_dims):.1f} (CV={dim_cv:.2f}) [{'PASS' if dim_consistent else 'FAIL'}]")
        print(f"    Perverse: {np.mean(perverse_counts):.1f} (CV={perverse_cv:.2f}) [{'PASS' if perverse_consistent else 'FAIL'}]")
        print(f"    Pattern corr: {mean_pattern_corr:.3f} [{'PASS' if pattern_consistent else 'FAIL'}]")
        print(f"    Overall: {'PASS' if passes else 'FAIL'}")

    return results


def test_transformation_law(
    embeddings_dict: Dict[str, np.ndarray],
    config: TestConfig,
    verbose: bool = True
) -> Dict[str, Any]:
    """
    Test 4.2: Automorphic Transformation Law

    For automorphic forms f, verify f(gz) = j(g,z)f(z) for group elements g.
    """
    if verbose:
        print("\n  Test 4.2: Automorphic Transformation Law")
        print("  " + "-" * 40)

    results = {
        "per_model": {},
        "overall": {}
    }

    cocycle_errors = []

    for name, X in embeddings_dict.items():
        X_proc = preprocess_embeddings(X, config.preprocessing)
        n, d = X_proc.shape

        if verbose:
            print(f"\n    Model: {name}")

        # Build graph Laplacian
        D = squareform(pdist(X_proc, 'euclidean'))
        A = build_mutual_knn_graph(D, config.k_neighbors)
        L = normalized_laplacian(A)

        # Get eigenfunctions (automorphic forms)
        eigenvalues, eigenvectors = np.linalg.eigh(L)
        idx = np.argsort(eigenvalues)
        eigenvalues = eigenvalues[idx]
        eigenvectors = eigenvectors[:, idx]

        # Take first few non-trivial eigenfunctions
        n_test = min(10, n - 1)
        test_funcs = eigenvectors[:, 1:n_test+1]  # Skip constant eigenfunction

        # Generate random orthogonal transformations (group elements)
        np.random.seed(config.seed)
        n_transforms = 5

        transformation_results = []
        for t in range(n_transforms):
            # Random orthogonal matrix in SO(d)
            H = np.random.randn(d, d)
            Q, R = np.linalg.qr(H)
            Q = Q @ np.diag(np.sign(np.diag(R)))
            if np.linalg.det(Q) < 0:
                Q[:, 0] *= -1

            # Apply transformation to embeddings
            X_transformed = X_proc @ Q

            # Rebuild Laplacian and eigenfunctions
            D_t = squareform(pdist(X_transformed, 'euclidean'))
            A_t = build_mutual_knn_graph(D_t, config.k_neighbors)
            L_t = normalized_laplacian(A_t)
            eigs_t, vecs_t = np.linalg.eigh(L_t)
            idx_t = np.argsort(eigs_t)
            vecs_t = vecs_t[:, idx_t]

            test_funcs_t = vecs_t[:, 1:n_test+1]

            # Extract transformation factor j(g,z) by comparing eigenfunctions
            # For automorphic forms: f(gz) = j(g,z) * f(z)
            # So j(g,z) â‰ˆ f(gz) / f(z)

            # Check if eigenfunctions are related by consistent factor
            factors = []
            for i in range(n_test):
                f_orig = test_funcs[:, i]
                f_trans = test_funcs_t[:, i]

                # Align signs (eigenfunctions defined up to sign)
                if np.dot(f_orig, f_trans) < 0:
                    f_trans = -f_trans

                # Compute point-wise ratios where f_orig is not ~0
                mask = np.abs(f_orig) > 0.1
                if mask.sum() > 0:
                    ratios = f_trans[mask] / f_orig[mask]
                    factor_mean = np.mean(ratios)
                    factor_std = np.std(ratios)
                    factors.append({
                        "eigenfunction": i,
                        "factor_mean": safe_float(factor_mean),
                        "factor_std": safe_float(factor_std),
                        "cv": safe_float(factor_std / (np.abs(factor_mean) + 1e-10))
                    })

            # Cocycle condition: j should be consistent
            if factors:
                mean_cv = np.mean([f["cv"] for f in factors])
            else:
                mean_cv = 1.0

            transformation_results.append({
                "transform_id": t,
                "n_factors": len(factors),
                "mean_cv": safe_float(mean_cv),
                "cocycle_consistent": mean_cv < 0.5
            })

        # Overall cocycle error for this model
        model_cocycle_error = np.mean([r["mean_cv"] for r in transformation_results])
        cocycle_errors.append(model_cocycle_error)

        results["per_model"][name] = {
            "n_eigenfunctions": n_test,
            "n_transforms": n_transforms,
            "transformations": transformation_results,
            "mean_cocycle_error": safe_float(model_cocycle_error)
        }

        if verbose:
            print(f"    Eigenfunctions tested: {n_test}")
            print(f"    Mean cocycle CV: {model_cocycle_error:.4f}")

    # Overall result
    mean_cocycle_error = np.mean(cocycle_errors)
    passes = mean_cocycle_error < 1.0  # Generous threshold for semantic analog

    results["overall"] = {
        "mean_cocycle_error": safe_float(mean_cocycle_error),
        "passes": passes
    }

    if verbose:
        print(f"\n    Overall cocycle error: {mean_cocycle_error:.4f}")
        print(f"    Result: {'PASS' if passes else 'FAIL'}")

    return results


def run_test(embeddings_dict: Dict[str, np.ndarray], config: TestConfig, verbose: bool = True) -> TestResult:
    """
    TIER 4: Geometric Satake

    TESTS:
    - 4.1: Semantic Grassmannian (Rep = Perv correspondence)
    - 4.2: Transformation Law (cocycle condition)

    PASS CRITERIA:
    - Perverse/irrep ratio in reasonable range
    - Cocycle error < 1.0
    """
    np.random.seed(config.seed)

    # Test 4.1
    grassmannian_results = test_grassmannian(embeddings_dict, config, verbose)

    # Test 4.2
    transformation_results = test_transformation_law(embeddings_dict, config, verbose)

    # Overall pass
    grassmannian_pass = grassmannian_results["overall"].get("passes", False)
    transformation_pass = transformation_results["overall"].get("passes", False)

    passed = grassmannian_pass and transformation_pass

    if verbose:
        print(f"\n  " + "=" * 40)
        print(f"  Grassmannian: {'PASS' if grassmannian_pass else 'FAIL'}")
        print(f"  Transformation Law: {'PASS' if transformation_pass else 'FAIL'}")
        print(f"  Overall: {'PASS' if passed else 'FAIL'}")

    return TestResult(
        name="TIER 4: Geometric Satake",
        test_type="langlands",
        passed=passed,
        metrics={
            "grassmannian": to_builtin(grassmannian_results),
            "transformation_law": to_builtin(transformation_results),
            "grassmannian_pass": grassmannian_pass,
            "transformation_pass": transformation_pass
        },
        thresholds={
            "dim_cv_max": 0.5,
            "perverse_cv_max": 0.5,
            "pattern_corr_min": 0.5,
            "cocycle_error_max": 1.0
        },
        controls={
            "dim_cv": safe_float(grassmannian_results["overall"].get("dim_cv", 0)),
            "perverse_cv": safe_float(grassmannian_results["overall"].get("perverse_cv", 0)),
            "pattern_corr": safe_float(grassmannian_results["overall"].get("mean_pattern_correlation", 0)),
            "cocycle_error": safe_float(transformation_results["overall"].get("mean_cocycle_error", 0))
        },
        notes=f"DimCV: {grassmannian_results['overall'].get('dim_cv', 0):.3f}, PatternCorr: {grassmannian_results['overall'].get('mean_pattern_correlation', 0):.3f}, Cocycle: {transformation_results['overall'].get('mean_cocycle_error', 0):.3f}"
    )


def main():
    parser = argparse.ArgumentParser(description="Q41 TIER 4: Geometric Satake")
    parser.add_argument("--out_dir", type=str, default=str(Path(__file__).parent.parent / "receipts" / "tier4"), help="Output directory")
    parser.add_argument("--seed", type=int, default=42, help="Random seed")
    parser.add_argument("--quiet", action="store_true", help="Suppress output")
    args = parser.parse_args()

    verbose = not args.quiet

    if verbose:
        print("=" * 60)
        print(f"Q41 TIER 4: GEOMETRIC SATAKE v{__version__}")
        print("=" * 60)
        print("Testing Satake correspondence:")
        print("  - Semantic Grassmannian (Rep(G^v) = Perv(Gr_G))")
        print("  - Automorphic transformation law (cocycle condition)")
        print()

    config = TestConfig(seed=args.seed)
    corpus = DEFAULT_CORPUS

    if verbose:
        print("Loading embeddings...")
    embeddings = load_embeddings(corpus, verbose=verbose)

    if len(embeddings) < 2:
        print("ERROR: Need at least 2 embedding models")
        sys.exit(1)

    result = run_test(embeddings, config, verbose=verbose)

    if verbose:
        print(f"\n{'=' * 60}")
        status = "PASS" if result.passed else "FAIL"
        print(f"Result: {status}")
        print(f"Notes: {result.notes}")

    # Save receipt
    out_dir = Path(args.out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)
    timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
    receipt_path = out_dir / f"q41_tier4_satake_{timestamp_str}.json"

    receipt = to_builtin({
        "suite": __suite__,
        "version": __version__,
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "passed": result.passed,
        "metrics": result.metrics,
        "thresholds": result.thresholds,
        "controls": result.controls,
        "notes": result.notes
    })

    with open(receipt_path, 'w') as f:
        json.dump(receipt, f, indent=2)

    if verbose:
        print(f"Receipt saved: {receipt_path}")

    return 0 if result.passed else 1


if __name__ == "__main__":
    sys.exit(main())
