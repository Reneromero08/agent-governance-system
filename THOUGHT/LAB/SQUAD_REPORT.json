[
  {
    "file": "CAPABILITY/TESTBENCH/phases/phase6_governance/test_ags_phase6_bridge.py",
    "status": "failed",
    "reason": "no_valid_proposals"
  },
  {
    "file": "CAPABILITY/TESTBENCH/phases/phase6_governance/test_ags_phase6_capability_pins.py",
    "status": "failed",
    "reason": "no_valid_proposals"
  },
  {
    "file": "CAPABILITY/TESTBENCH/phases/phase6_governance/test_ags_phase6_mcp_adapter_e2e.py",
    "status": "fixed",
    "winner": "Turing",
    "reason": "vote",
    "diff_adds": 0,
    "diff_dels": 0,
    "candidates": [
      "Neo",
      "Elliot",
      "Turing"
    ],
    "candidate_stats": {
      "Neo": {
        "adds": 0,
        "dels": 0
      },
      "Elliot": {
        "adds": 0,
        "dels": 0
      },
      "Turing": {
        "adds": 0,
        "dels": 0
      }
    },
    "test_command": [
      "python",
      "-m",
      "pytest",
      "CAPABILITY/TESTBENCH/phases/phase6_governance/test_ags_phase6_mcp_adapter_e2e.py"
    ],
    "test_ok": false,
    "test_logs_tail": "\"method\": \"crash\",\n                \"params\": {},\n                \"id\": 1\n            },\n            \"caps\": {\n                \"allowed_exit_codes\": [0]\n            }\n        }\n        res = run_wrapper(config, tmp_path)\n        assert res.returncode != 0\n>       assert \"BAD_EXIT_CODE\" in res.stderr\nE       assert 'BAD_EXIT_CODE' in \"C:\\\\Users\\\\rene_\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python.exe: can't open file 'D:\\\\\\\\CCC 2.0\\\\\\\\AI\\\\\\\\ag...ITY\\\\\\\\SKILLS\\\\\\\\LAW\\\\\\\\CONTRACTS\\\\\\\\CAPABILITY_TOOLS\\\\\\\\scripts\\\\\\\\wrapper.py': [Errno 2] No such file or directory\\n\"\nE        +  where \"C:\\\\Users\\\\rene_\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python.exe: can't open file 'D:\\\\\\\\CCC 2.0\\\\\\\\AI\\\\\\\\ag...ITY\\\\\\\\SKILLS\\\\\\\\LAW\\\\\\\\CONTRACTS\\\\\\\\CAPABILITY_TOOLS\\\\\\\\scripts\\\\\\\\wrapper.py': [Errno 2] No such file or directory\\n\" = CompletedProcess(args=['C:\\\\Users\\\\rene_\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python.exe', 'D:\\\\CCC 2.0\\\\AI\\\\...TY\\\\\\\\SKILLS\\\\\\\\LAW\\\\\\\\CONTRACTS\\\\\\\\CAPABILITY_TOOLS\\\\\\\\scripts\\\\\\\\wrapper.py': [Errno 2] No such file or directory\\n\").stderr\n\nCAPABILITY\\TESTBENCH\\phases\\phase6_governance\\test_ags_phase6_mcp_adapter_e2e.py:138: AssertionError\n=========================== short test summary info ===========================\nFAILED CAPABILITY/TESTBENCH/phases/phase6_governance/test_ags_phase6_mcp_adapter_e2e.py::test_happy_echo\nFAILED CAPABILITY/TESTBENCH/phases/phase6_governance/test_ags_phase6_mcp_adapter_e2e.py::test_fail_stderr\nFAILED CAPABILITY/TESTBENCH/phases/phase6_governance/test_ags_phase6_mcp_adapter_e2e.py::test_fail_timeout\nFAILED CAPABILITY/TESTBENCH/phases/phase6_governance/test_ags_phase6_mcp_adapter_e2e.py::test_fail_bloat\nFAILED CAPABILITY/TESTBENCH/phases/phase6_governance/test_ags_phase6_mcp_adapter_e2e.py::test_fail_malformed_output\nFAILED CAPABILITY/TESTBENCH/phases/phase6_governance/test_ags_phase6_mcp_adapter_e2e.py::test_fail_crash\n============================== 6 failed in 0.42s =============================="
  },
  {
    "file": "CAPABILITY/TESTBENCH/phases/phase8_router/test_ags_phase8_model_binding.py",
    "status": "failed",
    "reason": "no_valid_proposals"
  },
  {
    "file": "CAPABILITY/TESTBENCH/phases/phase8_router/test_phase8_router_receipts.py",
    "status": "failed",
    "reason": "no_valid_proposals"
  },
  {
    "file": "CAPABILITY/TESTBENCH/spectrum/test_spectrum02_emission.py",
    "status": "fixed",
    "winner": "Turing",
    "reason": "tests_passed",
    "diff_adds": 0,
    "diff_dels": 0,
    "candidates": [
      "Neo",
      "Elliot",
      "Turing"
    ],
    "candidate_stats": {
      "Neo": {
        "adds": 1,
        "dels": 0
      },
      "Elliot": {
        "adds": 22,
        "dels": 1
      },
      "Turing": {
        "adds": 0,
        "dels": 0
      }
    },
    "test_command": [
      "python",
      "-m",
      "pytest",
      "CAPABILITY/TESTBENCH/spectrum/test_spectrum02_emission.py"
    ],
    "test_ok": true,
    "test_logs_tail": "============================= test session starts =============================\nplatform win32 -- Python 3.11.6, pytest-9.0.2, pluggy-1.6.0\nrootdir: D:\\CCC 2.0\\AI\\agent-governance-system\nplugins: anyio-4.12.0\ncollected 1 item\n\nCAPABILITY\\TESTBENCH\\spectrum\\test_spectrum02_emission.py .              [100%]\n\n============================== 1 passed in 0.14s =============================="
  },
  {
    "file": "CAPABILITY/TESTBENCH/pipeline/test_restore_proof.py",
    "status": "fixed",
    "winner": "Neo",
    "reason": "vote",
    "diff_adds": 8,
    "diff_dels": 98,
    "candidates": [
      "Turing",
      "Elliot",
      "Neo"
    ],
    "candidate_stats": {
      "Turing": {
        "adds": 4,
        "dels": 8
      },
      "Elliot": {
        "adds": 0,
        "dels": 0
      },
      "Neo": {
        "adds": 8,
        "dels": 98
      }
    },
    "test_command": [
      "python",
      "-m",
      "pytest",
      "CAPABILITY/TESTBENCH/pipeline/test_restore_proof.py"
    ],
    "test_ok": false,
    "test_logs_tail": "rrors=None, newline=None):\n        \"\"\"\n        Open the file pointed by this path and return a file object, as\n        the built-in open() function does.\n        \"\"\"\n        if \"b\" not in mode:\n            encoding = io.text_encoding(encoding)\n>       return io.open(self, mode, buffering, encoding, errors, newline)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       FileNotFoundError: [Errno 2] No such file or directory: 'D:\\\\CCC 2.0\\\\AI\\\\agent-governance-system\\\\schemas\\\\proof_v1.json'\n\nC:\\Users\\rene_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\pathlib.py:1044: FileNotFoundError\n============================== warnings summary ===============================\nCAPABILITY\\PRIMITIVES\\ledger.py:9\nCAPABILITY\\PRIMITIVES\\ledger.py:9\n  D:\\CCC 2.0\\AI\\agent-governance-system\\CAPABILITY\\PRIMITIVES\\ledger.py:9: DeprecationWarning: jsonschema.RefResolver is deprecated as of v4.18.0, in favor of the https://github.com/python-jsonschema/referencing library, which provides more compliant referencing behavior as well as more flexible APIs for customization. A future release will remove RefResolver. Please file a feature request (on referencing) if you are missing an API for the kind of customization you need.\n    from jsonschema import Draft7Validator, RefResolver\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n=========================== short test summary info ===========================\nERROR CAPABILITY/TESTBENCH/pipeline/test_restore_proof.py::test_identical_artifacts_verified_true\nERROR CAPABILITY/TESTBENCH/pipeline/test_restore_proof.py::test_hash_mismatch_verified_false\nERROR CAPABILITY/TESTBENCH/pipeline/test_restore_proof.py::test_missing_files_verified_false\nERROR CAPABILITY/TESTBENCH/pipeline/test_restore_proof.py::test_proof_validates_against_schema\nERROR CAPABILITY/TESTBENCH/pipeline/test_restore_proof.py::test_empty_domains_verified_true\n======================== 2 warnings, 5 errors in 0.32s ========================"
  },
  {
    "file": "CAPABILITY/TESTBENCH/pipeline/test_proof_gating.py",
    "status": "fixed",
    "winner": "Neo",
    "reason": "vote",
    "diff_adds": 1,
    "diff_dels": 1,
    "candidates": [
      "Elliot",
      "Turing",
      "Neo"
    ],
    "candidate_stats": {
      "Elliot": {
        "adds": 0,
        "dels": 0
      },
      "Turing": {
        "adds": 0,
        "dels": 0
      },
      "Neo": {
        "adds": 1,
        "dels": 1
      }
    },
    "test_command": [
      "python",
      "-m",
      "pytest",
      "CAPABILITY/TESTBENCH/pipeline/test_proof_gating.py"
    ],
    "test_ok": false,
    "test_logs_tail": "\"catalytic_domains\": [\"CAPABILITY/PRIMITIVES/_scratch\"],\n                \"determinism\": \"deterministic\",\n            }\n            (ledger_dir / \"JOBSPEC.json\").write_text(json.dumps(jobspec, indent=2))\n    \n            status = {\"status\": \"succeeded\", \"restoration_verified\": True, \"exit_code\": 0, \"validation_passed\": True}\n            (ledger_dir / \"STATUS.json\").write_text(json.dumps(status, indent=2))\n    \n            (ledger_dir / \"INPUT_HASHES.json\").write_text(json.dumps({}, indent=2))\n            (ledger_dir / \"OUTPUT_HASHES.json\").write_text(json.dumps({}, indent=2))\n            (ledger_dir / \"DOMAIN_ROOTS.json\").write_text(json.dumps({}, indent=2))\n            (ledger_dir / \"LEDGER.jsonl\").write_text(\"{}\\n\")\n    \n            validator = mock_validator(ledger_dir)\n            success, report = validator.validate()\n>           assert success is False\nE           assert True is False\n\nCAPABILITY\\TESTBENCH\\pipeline\\test_proof_gating.py:122: AssertionError\n============================== warnings summary ===============================\nCAPABILITY\\PRIMITIVES\\ledger.py:9\nCAPABILITY\\PRIMITIVES\\ledger.py:9\n  D:\\CCC 2.0\\AI\\agent-governance-system\\CAPABILITY\\PRIMITIVES\\ledger.py:9: DeprecationWarning: jsonschema.RefResolver is deprecated as of v4.18.0, in favor of the https://github.com/python-jsonschema/referencing library, which provides more compliant referencing behavior as well as more flexible APIs for customization. A future release will remove RefResolver. Please file a feature request (on referencing) if you are missing an API for the kind of customization you need.\n    from jsonschema import Draft7Validator, RefResolver\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n=========================== short test summary info ===========================\nFAILED CAPABILITY/TESTBENCH/pipeline/test_proof_gating.py::test_proof_is_single_source_of_truth\n=================== 1 failed, 1 passed, 2 warnings in 0.21s ==================="
  }
]