Shell *et al.*: A Sample Article Using IEEEtran.cls for IEEE Journals

# Introduction {#sec:intro}

advancements toward artificial general intelligence (AGI) have been largely driven by the emergence of autoregressive large language models (LLMs) [@brown2020language; @achiam2023gpt; @chowdhery2023palm; @touvron2023llama; @bai2023qwen; @zhao2023survey; @guo2025deepseek] and diffusion models for image and video generation [@rombach2022high; @saharia2022photorealistic; @podellsdxl; @esser2024scaling; @brooks2024video]. These models exhibit remarkable capabilities in both understanding and generation across diverse modalities, achieving levels of performance that were previously unimaginable. The unprecedented scale of these models, reflected in massive parameter counts, vast datasets, substantial efforts in training, and significant computational demands during inference, has pushed AI to new heights, equipping these models with broad general knowledge and a deep understanding of language and the real world.

<figure id="fig:timeline" data-latex-placement="!t">
<embed src="figs/new1204.pdf" />
<figcaption>Timeline of Diffusion Language Models. This figure highlights key milestones in the development of DLMs, categorized into three groups: continuous DLMs, discrete DLMs, and recent multimodal DLMs. We observe that while early research predominantly focused on continuous DLMs, discrete DLMs have gained increasing popularity in more recent years. </figcaption>
</figure>

The rise of the GPT series [@radford2018improving; @radford2019language; @brown2020language], particularly with the public release of ChatGPT [@achiam2023gpt], has propelled autoregressive (AR) language models to a dominant position in natural language processing. By training to predict the next token using causal attention and teacher forcing, AR models [@touvron2023llama; @team2023gemini; @liu2024deepseek] can effectively scale to large datasets and model sizes. Generating text in a sequential, token-by-token fashion, AR models excel at supporting a wide range of tasks, from simple question answering to complex reasoning and creative writing. However, this sequential nature imposes a major bottleneck on inference speed. The autoregressive generation process, which produces one token at a time, inherently limits parallelism and significantly constrains computational efficiency and throughput.

Diffusion models are another highly promising generative paradigm. They are trained to recover data from progressively noised versions through a denoising process, and generate new samples by reversing this stochastic corruption step by step. Excelling at modeling complex data distributions, diffusion models have achieved state-of-the-art results in image and video synthesis [@dhariwal2021diffusion]. Academic breakthroughs in diffusion modeling [@ho2020denoising; @song2020denoising; @songscore; @liuflow] have established a solid theoretical foundation for training and inference. Concurrently, large-scale practical models like Stable Diffusion [@rombach2022high; @podellsdxl; @esser2024scaling], Imagen [@saharia2022photorealistic], and Sora [@brooks2024video] demonstrate the remarkable scalability and generalization of diffusion paradigm, enabling generation of high-fidelity, art-level images and videos from simple text prompts--- often with just a few words. Beyond their strong capacity for modeling complex data distributions, diffusion models offer an inherent advantage in parallelism. Through an iterative denoising process, they can generate multiple tokens or an entire sequence simultaneously, potentially leading to superior inference throughput and better utilization of modern parallel computing hardware. While challenges remain, particularly in modeling discrete data and handling dynamic sequence lengths, Diffusion Language Models (DLMs) have emerged as a compelling alternative to address the trade-off between generation quality and speed.

To adapt diffusion for discrete language data, several key approaches have been proposed. In the early times, the development of DLMs was primarily driven by diffusion models' success in continuous domains like image synthesis. Continuous DLMs map tokens into embeddings and perform denoising in continuous space, as in pioneering works Diffusion-LM [@li2022diffusion] and SED [@strudel2022self]. Discrete DLMs, on the other hand, define the diffusion process directly in token space. Early efforts such as D3PM [@austin2021structured] introduced structured transition matrices with absorbing states, allowing token-level corruption and iterative denoising. Subsequent work like DiffusionBERT [@he2023diffusionbert] integrated pre-trained masked language models (e.g., BERT) to enhance denoising quality, and proposed tailored noise schedules (e.g., the spindle schedule) to better align token corruption with token frequency. These early models demonstrated the feasibility of applying iterative denoising to non-autoregressive text generation, offering controllability and parallelism, though their performance still lagged behind strong autoregressive baselines. As core challenges in DLMs are gradually addressed and the paradigm matures, larger-scale DLMs have been developed. By initializing from autoregressive models, 7B-level models like Dream [@dream2025] and DiffuLLaMA [@gongscaling] have shown that DLMs can be effectively adapted from existing models while achieving competitive performance. LLaDA-8B [@nie2025largelanguagediffusionmodels] further demonstrates the potential of training DLMs from scratch, achieving performance comparable to similarly sized LLaMA3-8B models. Multimodal DLMs, also known as diffusion multimodal large language models (dMLLMs), have also shown promise in modeling hybrid data such as text and images. Built upon open-source DLMs, models like LLaDA-V [@you2025llada], Dimple [@yu2025dimple], and MMaDA [@yang2025mmada] integrate cross-modal reasoning and generation into the diffusion framework. Meanwhile, industry efforts have also shown growing interest in DLMs. The Mercury series [@labs2025mercury], Gemini Diffusion [@deepmind2024geminidiffusion], and Seed Diffusion [@song2025seed] report strong performance while achieving inference speeds of thousands of tokens per second. These developments highlight the growing practicality and commercial potential of DLMs. We provide a timeline of DLMs development in Fig. [1](#fig:timeline){reference-type="ref" reference="fig:timeline"}, ranging from representative models to recent advancements[@xu2024energy; @deschenauxbeyond; @han2024transfer; @sahoo2025diffusion; @zhang2025non; @dang2025inference; @rout2025anchored], followed by a visualization of DLM trends in Fig. [2](#fig:trending){reference-type="ref" reference="fig:trending"}.

Diffusion language models also present unique challenges and opportunities in both training and inference. Pretraining typically follows strategies similar to those used in autoregressive language models or image diffusion models [@dream2025; @yang2025mmada; @yu2025dimple]. To accelerate training and reuse previous training efforts, many DLMs are initialized from pretrained autoregressive model weights [@gongscaling; @dream2025]. Supervised fine-tuning (SFT) in DLMs also mirrors that of autoregressive models: clean prompt data is provided, and the model learns to generate the target completion. Reinforcement learning (RL) is also adopted to DLMs post-training to improve performance on complex tasks. Variants of GRPO [@shao2024deepseekmath] algorithm such as diffu-GRPO [@zhao2025d1] and UniGRPO [@yang2025mmada] have been proposed to enhance the reasoning capabilities and alignment of DLMs at scale. During inference, various strategies and optimizations have been developed to fully utilize the capabilities of DLMs. Continuous DLMs can leverage ODE/SDE solvers or other few-step generation techniques to accelerate the iterative denoising process [@chen2025dlm]. As discrete DLMs face more challenges in parallel generation, specialized parallel decoding strategies [@wu2025fast; @yu2025dimple; @israel2025accelerating] have been proposed to enable acceptance of multiple tokens at a single step and overcome parallel curse. Unmasking and remasking strategies [@nie2025largelanguagediffusionmodels; @wang2025remasking] further improve generation quality by selectively revealing low-confidence tokens, while caching techniques [@liu2025dllm; @ma2025dkv] can significantly reduce computation and enhance inference speed for both paradigms.

<figure id="fig:trending" data-latex-placement="!t">
<embed src="figs/Number_of_Paper.pdf" style="width:49.0%" />
<figcaption>Trend of diffusion language model papers. For discrete DLM, the statistics are drawn from papers citing D3PM <span class="citation" data-cites="austin2021structured"></span>, with a further selection of those whose titles or abstracts include the keyword “language”. For continuous DLM, the statistics are based on the number of related studies documented in the repository associated with this paper. The results reflect a growing research interest in this domain. The statistics are for reference only.</figcaption>
</figure>

Compared to autoregressive models, diffusion language models are widely believed to offer several distinct advantages as follows:

- **Parallel Generation:** DLMs can generate multiple tokens in parallel through an iterative denoising process, significantly improving inference speed and throughput over autoregressive models.

- **Bidirectional Context:** DLMs naturally incorporate bidirectional context, enabling more nuanced language understanding and generation. They also produce richer contextual embeddings, which are beneficial for cross-modal generation tasks. This enables fine-grained control over the generation process as well.

- **Iterative Refinement:** The iterative denoising process allows DLMs to update their perceptions over multiple steps. By accepting high-confidence tokens early and retaining low-confidence regions as masked, Masked DLMs can progressively improve uncertain areas, often resulting in more coherent and higher-quality text generation.

- **Controllability:** DLMs can be conditioned on specific token positions or structures, making them well-suited for tasks like infilling and structured generation. Additionally, guidance techniques (e.g., classifier-free guidance) enable better control over style and semantic relevance.

- **Unified Modeling Across Modalities:** By applying a shared denoising-based modeling framework, DLMs naturally support unified text and vision generation tasks. This makes them particularly promising for multimodal applications that require both generation and understanding within a single model.

Despite the recent rise in popularity of DLMs, there remains a lack of a comprehensive survey that systematically covers the entire DLM ecosystem. We structured our survey as follows: Section [2](#sec:paradigms){reference-type="ref" reference="sec:paradigms"} provides a comprehensive overview of modern language modeling paradigms, including autoregressive, masked, and diffusion-based approaches. Section [3](#sec:training){reference-type="ref" reference="sec:training"} delves into the training methodologies for diffusion language models, covering both pre-training and subsequent fine-tuning techniques such as SFT and RL alignment. Section [4](#sec:inference){reference-type="ref" reference="sec:inference"} details various inference strategies and optimizations, focusing on techniques tailored for continuous and discrete space models. Section [5](#sec:multimodal){reference-type="ref" reference="sec:multimodal"} explores the extension of diffusion models to multimodal contexts, surveying state-of-the-art models and architectures like LLaDA-V [@you2025llada], MMaDA [@yang2025mmada], and Dimple [@yu2025dimple]. Section [6](#sec:performance){reference-type="ref" reference="sec:performance"} presents and visualizes performance comparisons of DLMs. Section [7](#sec:application){reference-type="ref" reference="sec:application"} showcases the diverse applications of DLMs in tasks ranging from text and code generation to computational biology. Section [8](#sec:challenges){reference-type="ref" reference="sec:challenges"} highlights the challenges and limitations of diffusion language models, including issues of efficiency, reasoning, agent capability, and infrastructure, also outlines promising directions for future research. To provide a consolidated overview, a taxonomy of DLMs is presented in Fig. [3](#fig:taxonomy){reference-type="ref" reference="fig:taxonomy"}.

<figure id="fig:taxonomy" data-latex-placement="!t">

<figcaption>A taxonomy of Diffusion Language Models, covering foundations, training and inference strategies, and key applications. The section numbers (§) correspond to the sections in this survey.</figcaption>
</figure>

# Paradigms of Diffusion Language Models {#sec:paradigms}

Diffusion Language Models have emerged as a powerful non-autoregressive paradigm that balances generative quality with inference parallelism. Inspired by principles from non-equilibrium thermodynamics [@sohl2015deep], DLMs learn to reverse a gradual noising process. This iterative refinement approach allows for parallel generation of the entire sequence, offering a potential solution to the inference bottleneck of AR models. DLMs can be broadly categorized based on the space in which the diffusion process operates: either continuous or discrete. Additionally, there are hybrid AR-Diffusion models that combine autoregressive and diffusion in various forms, aiming to leverage the complementary strengths of both paradigms. We present model information from several works in Table [\[tab:diffusion_models_summary\]](#tab:diffusion_models_summary){reference-type="ref" reference="tab:diffusion_models_summary"} and provide a comparison of different paradigms in Fig. [4](#fig:paradigm){reference-type="ref" reference="fig:paradigm"}.

## Preliminary of Modern Language Modeling

The field of language modeling has evolved through several distinct paradigms, each characterized by unique architectural choices, training objectives, and associated trade-offs. In this subsection, we provide a brief overview of recent transformer-based paradigms at scale, highlighting their core principles, mathematical formulations, and representative models. Earlier approaches are not included, as we focus on modern, large-scale designs here. This review serves to establish the conceptual foundation for understanding the emergence of diffusion language models as a novel and promising alternative that addresses key limitations of prior methods.

### Masked Language Models

Masked Language Models (MLMs), popularized by BERT [@devlin2019bert], represent a foundational paradigm that scales pretrained language models using transformer-based encoder-only architectures. Conceptually simple yet empirically powerful, MLMs learn bidirectional contextual representations by predicting randomly masked tokens within an input sequence, leveraging both preceding and succeeding context. This approach follows a denoising autoencoding framework, where a subset of input tokens is masked, and the model is trained to reconstruct them: $$\begin{equation}
\mathcal{L}_{\text{MLM}} = \mathbb{E}_{x \sim \mathcal{D}} \, \mathbb{E}_{\mathcal{M} \sim Mask(x)} \left[ - \sum_{i \in \mathcal{M}} \log P_\theta(x_i \mid x_{\setminus \mathcal{M}}) \right]
\end{equation}$$ Here, $x$ denotes the input sequence, $\mathcal{M}$ is the set of masked positions, and $x_{\setminus \mathcal{M}}$ represents the visible (unmasked) context. BERT also introduces a next sentence prediction (NSP) objective to model inter-sentence relationships: $$\begin{equation}
\mathcal{L}_{\text{NSP}} = \mathbb{E}_{(A,B,y) \sim \mathcal{D}} \left[ - \log P_\theta(y \mid A, B) \right]
\end{equation}$$ where $(A, B)$ is a pair of text segments, and $y \in \{0,1\}$ indicates whether $B$ follows $A$ in the original text.

BERT's effectiveness in language understanding tasks such as sentiment analysis, named entity recognition, and question answering has inspired numerous improved variants. For instance, RoBERTa [@liu2019roberta] removes the NSP objective and adopts more aggressive training strategies, while ALBERT [@lanalbert] introduces parameter sharing and matrix factorization for efficiency. DeBERTa [@hedeberta] further enhances contextual encoding with disentangled attention and improved decoding mechanisms for masked token prediction.

Despite their strengths in understanding tasks, MLMs are not inherently designed for generative tasks, generating text requires specialized fine-tuning strategies or decoding schemes, making them unsuitable for open-ended generation without significant architectural modifications.

### Autoregressive Language Models

Illustrated by GPT series [@radford2018improving; @radford2019language; @brown2020language; @achiam2023gpt] and Transformer-XL [@dai2019transformer], further advanced by subsequent LLMs [@chowdhery2023palm; @touvron2023llama; @bai2023qwen; @zhang2022opt], autoregressive language models have become the backbone of modern generative AI, characterized by their unidirectional, left-to-right token generation process. Unlike bidirectional models, Autoregressive LMs factorize the joint probability of a text sequence into a product of conditional probabilities: $$\begin{equation}
P(x) = \prod_{i=1}^{n} P_\theta(x_i \mid x_1, x_2, \dots, x_{i-1})
\end{equation}$$

Given a token sequence $X = (x_1, x_2, \dots, x_n)$, the training objective is to maximize the log-likelihood of the sequence under this factorization: $$\begin{equation}
\mathcal{L}_{\text{AR}} = \mathbb{E}_{X \sim \mathcal{D}} \left[ - \sum_{i=1}^{n} \log P_\theta(x_i \mid x_1, \dots, x_{i-1}) \right]
\end{equation}$$ This is typically implemented using a decoder-only Transformer architecture with causal attention masking and teacher forcing during training, ensuring that each token prediction is conditioned only on preceding tokens while enabling parallel computation of the loss.

The sequential generation formulation is both a strength and a limitation. On one hand, it aligns with text generation tasks and facilitates straightforward sampling, naturally suits for various applications. On the other hand, it imposes a fundamental bottleneck on inference speed, as token generation is inherently sequential and cannot be parallelized. This trade-off between generation quality and latency has become a central challenge in advancing AR models. Beyond the standard next-token prediction (NTP), recent research has explored multi-token prediction (MTP) [@gloecklebetter; @liu2024deepseek] to accelerate inference by generating multiple tokens per step. These efforts share conceptual similarities with parallel decoding strategies employed in DLMs, while some other works are directly inspired by diffusion process to align LLMs [@chen2025diffpo].

### Other Paradigms

**Sequence-to-Sequence Models.** Sequence-to-sequence (Seq2Seq) models [@sutskever2014sequence], an early yet powerful paradigm, are built on an encoder-decoder architecture and serve as a versatile framework for conditional text generation tasks such as machine translation and summarization. Modern models like T5 [@raffel2020exploring] and BART [@lewis2020bart] are prominent examples.

In this architecture, the encoder processes the source sequence to produce an intermediate representation, which the decoder then uses to generate the target sequence, typically in an autoregressive manner. While standard Seq2Seq decoders are autoregressive, the framework itself is highly flexible. Many DLMs, such as DiffuSeq [@gongdiffuseq] and SeqDiffuSeq [@yuan2022seqdiffuseq], adapt this architecture by replacing the autoregressive decoder with a non-autoregressive diffusion decoder, leveraging the encoder's strong conditioning ability to guide the denoising process in generation.

**Permutation Language Models.** Permutation Language Models (PLM), exemplified by XLNet [@yang2019xlnet], offer an alternative approach to incorporating bidirectional context within a generative framework. PLMs are trained to predict tokens in a sequence, but in a random, permuted order rather than a fixed left-to-right order. The objective is to maximize the expected log-likelihood over all possible permutations of the factorization order: $$\begin{equation}
\mathcal{L}_{\text{PLM}} = \mathbb{E}_{\mathbf{z} \sim \mathcal{Z}_T} \left[- \sum_{t=1}^{T} \log P_\theta(x_{z_t} | \mathbf{x}_{\mathbf{z}_{<t}}) \right]
\end{equation}$$ where $\mathcal{Z}_T$ denotes the set of all possible permutations of a sequence of length $T$, and $\mathbf{z}_t$, $\mathbf{z}_{<t}$ refer to the $t$-th and first $t-1$ elements of a given permutation $\mathbf{z} \in \mathcal{Z}_T$. This formulation allows the model to capture bidirectional context for each token, combining the advantages of bidirectional context (like MLMs) with a coherent autoregressive generation process. This contrasts with DLMs, which achieve bidirectionality through a parallel iterative refinement process.

## Continuous Diffusion Language Models

Continuous-space DLMs model language by first mapping discrete tokens into a continuous embedding space. A diffusion process then models the data distribution in this continuous space [@li2022diffusion; @strudel2022self]. Typically, diffusion models define a generative process by learning to reverse a predefined corruption process that gradually transforms data into noise. This process consists of a **forward (noising) process** and a **reverse (denoising) process**. The forward process gradually transforms a data sample $\mathbf{x}_0$ into noise over $T$ timesteps via a fixed Markov chain: $$\begin{equation}
q(\mathbf{x}_{1:T} \mid \mathbf{x}_0) = \prod_{t=1}^T q(\mathbf{x}_t \mid \mathbf{x}_{t-1})
\end{equation}$$ $$\begin{equation}
\quad q(\mathbf{x}_t \mid \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \mu_t(\mathbf{x}_{t-1}), \Sigma_t),
\end{equation}$$ where $\mu_t$ and $\Sigma_t$ define the noise schedule. In many implementations, such as DDPM [@ho2020denoising] and Rectified Flow [@liuflow], the marginal distribution at each timestep has a closed-form expression: $$\begin{equation}
\mathbf{x}_t = \alpha_t \mathbf{x}_0 + b_t \mathbf{\epsilon}, \quad \mathbf{\epsilon} \sim \mathcal{N}(0, \mathbf{I}),
\end{equation}$$ where $\alpha_t$ and $b_t$ are deterministic functions of time $t$.

The reverse process learns to invert the corruption, starting from noise $\mathbf{x}_T \sim \mathcal{N}(0, \mathbf{I})$ and gradually denoising to recover a sample close to $\mathbf{x}_0$. This is parameterized by a neural network $f_\theta(\mathbf{x}_t, t)$, typically implemented as a Transformer, which predicts a target quantity $\mathbf{z}$ associated with the forward process (e.g., clean data, noise, or velocity). A common training objective takes the form: $$\begin{equation}
\mathcal{L}_{\text{simple}} = \mathbb{E}_{t, \mathbf{x}_0, \mathbf{z}} \left[ \left\| f_\theta(\mathbf{x}_t, t) - \mathbf{z} \right\|^2 \right],
\end{equation}$$ where $\mathbf{x}_t$ is sampled via the forward process given $\mathbf{x}_0$, and $\mathbf{z}$ is the corresponding regression target derived from $\mathbf{x}_0$ and $t$.

After training, generation proceeds by sampling from the learned reverse process, starting from noise $\mathbf{x}_T \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$. At each timestep $t = T, T-1, \ldots, 1$, the model defines a conditional distribution $p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t)$ which aims to approximate the true reverse transition $q(\mathbf{x}_{t-1} \mid \mathbf{x}_t)$. Sampling iteratively from these learned conditionals produces progressively less noisy latent states until an estimate of the original data $\mathbf{x}_0$ is recovered. After generating a denoised embedding $\hat{\mathbf{x}}_0$, a **rounding step** maps it back to a discrete token. This is typically done by nearest-neighbor searching in the embedding space, using a decoder head, or thresholding techniques [@chen2022analog].

Diffusion-LM [@li2022diffusion] firstly introduces a diffusion process in the embedding space to create a non-autoregressive language generation model. By using a classifier-guidance mechanism similar to those in image diffusion models, it achieves highly controllable text generation and infilling. LDEBM [@yu2022latent] presents a novel symbiosis of latent space EBMs and diffusion models in a variational learning framework to address the learning issues of energy-based priors, with a focus on interpretable text modeling. LATENTOPS [@liu2022composable] proposes an efficient framework for composable text operations by working within a compact latent space. It introduces an efficient sampler based on ordinary differential equation (ODE) to generate latent vectors guided by arbitrary plug-in control operators, which are then decoded into the desired text. Later, Diffuseq [@gongdiffuseq], a classifier-free DLM for sequence-to-sequence tasks is proposed, which corrupts only the target sequence embeddings in the forward process to achieve strong and diverse conditional text generation. The Self-conditioned Embedding Diffusion (SED) [@strudel2022self] framework conducts diffusion directly on a fixed, continuous token embedding space. By incorporating a self-conditioning mechanism, it achieves strong performance in both conditional and unconditional text generation, rivaling standard autoregressive models. CDCD [@dieleman2022continuous] applies continuous diffusion to categorical data by embedding tokens into a continuous space. It proposes score interpolation, which uniquely allows the model to be trained with a cross-entropy loss, and time warping, an adaptive strategy to efficiently schedule noise levels during training. To address optimization challenges in the embedding space, Difformer [@gao2022empowering] introduces an anchor loss to prevent embedding collapse and a noise rescaling framework to mitigate model degeneration. LD4LG [@lovelace2023latent] leverages a pretrained language model as a powerful autoencoder to create a compact latent space, where a continuous diffusion model is then trained for high-quality text generation. GENIE [@lin2023text] proposes a large-scale pre-training framework for diffusion language models, introducing a novel continuous paragraph denoise objective to effectively learn from large corpora by reconstructing corrupted text paragraphs. InfoDiffusion [@wang2023infodiffusion] introduces an information entropy-aware noise schedule to guide the model toward a more human-like \"keyinfo-first\" process that prioritizes generating core content. EDDPMs [@liu2024unified] unify generation, reconstruction, and representation by generalizing the diffusion process with a parameterized encoder-decoder, enabling stable, joint training of all components within a single framework. SMOOTHIE [@shabalin2025smoothie] proposes a novel diffusion process that progressively smooths token embeddings based on semantic similarity, combining the advantages of continuous latent spaces and discrete token handling.

Continuous diffusion processes can also be formulated in the logit space rather than the embedding space. TESS [@mahabadi2024tess] introduces a fully non-autoregressive framework that diffuses over a k-logit simplex representation of tokens and employs a novel self-conditioning mechanism tailored to this setting. Extending this, TESS 2 [@tae2025tess] scales the approach by adapting pretrained large autoregressive models into general-purpose diffusion language models through a diffusion-specific pretraining recipe and instruction tuning, enabling strong instruction-following capabilities.

::: table*
  **Model**                                         **Parameters**      **Diffusion type**        **Task**            **Training data**         
  ---------------------------------------------- --------------------- -------------------- -------------------- --------------------------- -- --
  D3PM [@austin2021structured]                            70M                Discrete             Language               65B tokens             
  Diffusion-LM [@li2022diffusion]                     100M & 300M           Continuous            Language                   --                 
  Diffuseq [@gongdiffuseq]                                91M               Continuous            Language           565K sentence pairs        
  SSD-LM [@han2023ssd]                                   400M               Continuous            Language               123B tokens            
  DiffusionBERT [@he2023diffusionbert]                   110M                Discrete             Language               16B tokens             
  CDCD [@dieleman2022continuous]                         1.3B               Continuous            Language               315B tokens            
  LD4LG [@lovelace2023latent]                            188M               Continuous            Language           5.2M sentence pairs        
  SeqDiffuSeq [@yuan2022seqdiffuseq]                  65M & 110M            Continuous            Language               45B tokens             
  TESS [@mahabadi2024tess]                            125M & 355M           Continuous            Language                   --                 
  MDLM [@sahoo2024simple]                                110M                Discrete             Language               622B tokens            
  DFM [@gat2024discrete]                                 1.7B                Discrete         Language & Code            2.5T tokens            
  TESS-2 [@tae2025tess]                                   7B                Continuous            Language               360B tokens            
  LLaDA [@nie2025largelanguagediffusionmodels]          1B & 8B              Discrete         Language & Code            2.3T tokens            
  Mercury [@labs2025mercury]                              --                 Discrete               Code             Trillions of tokens        
  LLaDA-1.5 [@zhu2025llada]                               8B                 Discrete             Language               2.3T tokens            
  MMaDA [@yang2025mmada]                                  8B                 Discrete        Multimodal Unified    900B image-text tokens       
  Dream [@dream2025]                                      7B                 Discrete         Language & Code            580B tokens            
  LLaDA-V [@you2025llada]                                8.4B                Discrete            Multimodal         3M image-text samples       
  LaViDa [@li2025lavida]                                 8.4B                Discrete            Multimodal        1.6M image-text samples      
  Dimple [@yu2025dimple]                                  7B                 Discrete            Multimodal              0.8B tokens            
  LongLLaDA [@liu2025longllada]                           8B                 Discrete         Language & Code            2.3T tokens            
  DiffuCoder [@gong2025diffucoder]                        7B                 Discrete               Code                 130B tokens            
  LaViDa-O [@li2025lavidao]                              10.4B               Discrete        Multimodal Unified   $>$ 200M image-text pairs     
  Lumina-DiMOO [@xin2025lumina]                           8B                 Discrete        Multimodal Unified   $>$ 110M image-text pairs     
  LLaDA-MoE [@zhu2025lladamoe]                    7B (1.4B activated)        Discrete             Language               20T tokens             
  SDAR [@cheng2025sdar]                             1.7B-30B series          Discrete             Language               54B tokens             
  TiDAR [@liu2025tidar]                                1.5B & 8B             Discrete             Language               150B tokens            
  SDLM [@liu2025sequential]                            3B & 32B              Discrete             Language               2.3B tokens            

[]{#tab:diffusion_models_summary label="tab:diffusion_models_summary"}
:::

<figure id="fig:paradigm" data-latex-placement="!t">
<embed src="figs/table_final3.pdf" />
<figcaption>An overview of training and inference procedures across different paradigms of Diffusion Language Models, with autoregressive (AR) models included for comparison. AR models are trained using teacher forcing and causal attention, whereas both discrete and continuous DLMs employ fully bidirectional attention mechanisms. Block-wise diffusion models, exemplified by BD3-LM <span class="citation" data-cites="arriolablock"></span>, integrate autoregressive and diffusion strategies, and are trained using a specially designed block-causal attention mask. </figcaption>
</figure>

## Discrete Diffusion Language Models

Discrete space DLMs define the diffusion process directly on the vocabulary of tokens, avoiding the need for a continuous embedding space during the diffusion itself. D3PM [@austin2021structured] firstly illustrates this by introducing a structured diffusion process over discrete tokens. The **forward process** corrupts a sequence by applying a transition matrix $\mathbf{Q}_t$ at each step. This matrix defines the probability of a token transitioning to any other token in the vocabulary. The probability of a state $\mathbf{x}_t$ given an initial state $\mathbf{x}_0$ is given by a categorical distribution: $$q(\mathbf{x}_t|\mathbf{x}_0) = \text{Cat}(\mathbf{x}_t; \mathbf{p} = \mathbf{x}_0 \bar{\mathbf{Q}}_t), \quad \text{where} \quad \bar{\mathbf{Q}}_t = \prod_{i=1}^t \mathbf{Q}_i$$ A common choice for $\mathbf{Q}_t$ is an absorbing state transition, where each token has a probability of either remaining unchanged or transitioning to a special '\[MASK\]' token. The **reverse process** learns to reverse these transitions, predicting the probability distribution of the original tokens given the corrupted sequence.

Over time, masked DLMs have emerged as a modern and highly effective evolution of discrete diffusion language models, forming the foundation for several recent large-scale efforts [@nie2025largelanguagediffusionmodels; @gongscaling]. We take LLaDA [@nie2025largelanguagediffusionmodels], the most representative model of this kind as an example. Inspired by earlier work on reparameterized and simplified training objectives [@zhengreparameterized; @shi2024simplified; @ou2024your], LLaDA is trained from scratch using a cross-entropy loss that is computed only over masked tokens: $$\begin{equation}
\label{eq:objective}
   \mathcal{L}(\theta)  \triangleq   -  \mathbb{E}_{t, x_0,  x_t} \left[\frac{1}{t} \sum_{ i = 1 }^L \textbf{1}[x_t^i = \textrm{M}] \log p_{\theta}(x_0^i|x_t) \right] ,
\end{equation}$$ where $x_0$ is sampled from the training corpus, $t$ is sampled uniformly from $[0, 1]$, and $x_t$ is obtained by corrupting $x_0$ through the forward process. The indicator function $\textbf{1}[\cdot]$ ensures that the loss is applied only to positions that have been masked. During inference, the generation process starts with a fully masked sequence of desired length. In each iterative step, the model takes the current sequence (containing a mix of generated tokens and '\[MASK\]' tokens) and predicts a complete sequence of tokens. Based on the model's prediction confidence and noise schedule, a certain number of the highest-confidence predictions are unmasked and fixed, while the remaining positions are re-masked. This refinement process continues iteratively until all '\[MASK\]' tokens are resolved. This approach elegantly combines the bidirectional context of MLMs with a controllable, parallel generation process. LLaDA-8B, in particular, exhibits strong scalability and instruction-following ability, achieving performance on par with powerful autoregressive models such as LLaMA3-8B. This challenges the long-standing dominance of autoregressive models in large-scale language generation.

DiffusionBERT [@he2023diffusionbert] combines a pre-trained BERT with a discrete diffusion process, leveraging its powerful denoising capabilities to learn the reverse process from a masked state. The model is further enhanced by a novel spindle noise schedule that considers token informativeness, achieving significant improvements in generation quality compared with previous DLMs. A different approach, Reparameterized Discrete diffusion Models (RDMs) [@zhengreparameterized], establishes an alternative formulation for the reverse process, which simplifies the training objective to a weighted cross-entropy loss. This enables more flexible and adaptive decoding strategies, leading to significant performance gains over previous discrete diffusion models. Similarly, MD4 [@shi2024simplified] derives a simple weighted integral of cross-entropy losses as the continuous-time variational objective of masked diffusion models, providing a simple and generalized framework for training DLMs. Another analogous approach is MDLM [@sahoo2024simple], which introduces a simplified, Rao-Blackwellized objective that takes the form of a weighted average of masked language modeling losses. Diffusion-LLM [@ye2023diffusion] demonstrates the scalability of DLMs by adapting pre-trained masked language models to diffusion paradigm and further task-specific finetuning and instruction finetuning, unlocking their versatility in solving general language tasks. Diffusion-NAT [@zhou2024diffusion] unifies a discrete diffusion model with a PLM by reformulating the denoising process as a non-autoregressive masked token recovery task, allowing BART to act as an effective denoiser. Plaid [@gulrajani2023likelihood] is the first diffusion language model trained to maximize data likelihood, demonstrating through scaling laws that it can outperform autoregressive models like GPT-2 on standard benchmarks. To improve the training objective, SEDD [@lou2024discrete] introduces a score entropy loss to directly learn the ratios of the data distribution, which serves as a discrete extension of score matching. Reparameterized Absorbing Discrete Diffusion (RADD) [@ou2024your] reveals that the concrete score in absorbing diffusion can be expressed as a time-independent conditional probability of the clean data, multiplied by an analytic, time-dependent scalar. It also formally unifies the training objectives of absorbing discrete diffusion and any-order autoregressive models. Discrete Flow Matching (DFM) [@gat2024discrete] introduces a novel generative paradigm for discrete data that is analogous to continuous Flow Matching. The method learns a generating probability velocity to transform samples along a general family of probability paths from a source to a target distribution. By scaling the model architecture, DFM significantly closes the performance gap with autoregressive models on various benchmarks. DDPD [@liu2024think] presents a framework that decouples the generation process into two specialized models: a planner and a denoiser. At each step, the planner identifies the most corrupted token positions needing refinement, after which the denoiser predicts their values. To improve performance on complex reasoning tasks, MGDM [@ye2024beyond] is introduced to address the problem of subgoal imbalance. This approach enhances discrete diffusion by prioritizing more difficult subgoals during the learning process through a token-level reweighting mechanism. To address the challenge of scaling, a continual pre-training approach [@gongscaling] is proposed to adapt existing autoregressive models, such as LLaMA, into diffusion language models. The resulting models, named DiffuGPT and DiffuLLaMA, are competitive with their AR counterparts while gaining diffusion-native capabilities like flexible infilling. Build on this observation, Dream-7B [@dream2025] is initialized from Qwen2.5 7B [@qwen2.5] and further trained with 580B tokens, largely outperforming existing DLMs and matching the performance of top-tier AR models. GIDD [@von2025generalized] is introduced to overcome the limitation that masked diffusion models cannot revise generated tokens. This framework generalizes the noising process by combining masking with uniform noise, which unlocks the model's ability to self-correct mistakes and improves sample quality. Recently, to address long-context capabilities, LongLLaDA [@liu2025longllada] provides the first systematic analysis of DLMs in this domain. It reveals that DLMs can maintain stable perplexity during direct context extrapolation and have better retrieval capabilities. LongLLaDA also introduces a training-free NTK-based RoPE extrapolation method, which significantly improves the extrapolation performance of DLMs, validating that established extrapolation scaling laws remain effective for DLMs. UltraLLaDA [@he2025ultrallada] extends this line of work by introducing a diffusion-aware NTK RoPE scaling and lightweight long-context post-training, enabling diffusion LLMs to reach 128K context windows and achieving substantially better retrieval and perplexity performance than training-free extrapolation methods. LLaDA-MoE [@zhu2025lladamoe] is the first work to integrate a sparse Mixture-of-Experts (MoE) architecture into diffusion language models, training a new MoE-based DLM from scratch on 20T tokens. Despite activating only about 1.4B parameters during inference, it surpasses larger dense diffusion models and achieves performance comparable to Qwen2.5-3B-Instruct across knowledge, coding, and reasoning benchmarks.

## Hybrid AR-Diffusion Language Models

Hybrid AR-Diffusion models aim to strike a balance between the full parallelism of non-autoregressive models and the strong causal dependency modeling of autoregressive models. A prominent strategy for hybrid AR-diffusion modeling adopts a block-wise semi-autoregressive generation process. In this setting, the model generates blocks of tokens autoregressively, while the tokens within each block are generated in parallel using a diffusion-like iterative process. Early efforts such as SSD-LM [@han2023ssd] pioneered hybrid approaches by a block-wise continuous diffusion process on simplex representations, AR-DIFFUSION [@wu2023ar] illustrates a multi-level diffusion process and achieves semi-autoregressive by adjusting timestep according token position. Recent representative model BD3-LM [@arriolablock] further advances this direction on discrete models, demonstrating strong performance compared to pure AR and diffusion models. CtrlDiff [@huang2025ctrldiff] improves this paradigm by introducing dynamic block prediction techniques to enhance block-level efficiency and control. SDAR [@cheng2025sdar] further strengthens this hybrid paradigm by converting a pretrained autoregressive model into a blockwise diffusion model through a lightweight adaptation stage. It preserves AR-level performance while enabling efficient parallel intra-block generation, achieving scalable speedups without sacrificing quality.

The generation process in these models usually consists of two nested loops. In the outer loop, blocks of tokens are generated autoregressively, with each block conditioned on previously generated blocks. Within each block, the inner loop performs parallel token-wise generation through a diffusion-style iterative denoising process. In BD3-LM, the training objective is formalized as: $$\begin{equation}
    \mathcal{L}_\text{BD}(\mathbf{x}, \theta) :=  - \sum_{b=1}^{B} \mathbb{E}_{t \sim [0, 1]} \mathbb{E}_{q} \frac{1}{t} \log p_\theta(\mathbf{x}^b | \mathbf{x}_{t}^b, \mathbf{x}^{<b})
\end{equation}$$

This hybrid strategy enables the model to capture long-range dependencies across blocks via autoregression, while simultaneously accelerating generation within each block through parallel diffusion. The design also supports flexible output lengths and KV-Cache which is widely used in AR models [@arriolablock].

Notably, recent masked diffusion language models [@nie2025largelanguagediffusionmodels; @yang2025mmada] also adopt similar semi-autoregressive block-based decoding strategies, which can be seen as instances of hybrid AR-diffusion modeling.

Beyond block-based approaches that combine AR and diffusion at sequence level, hybridization can also occur at the architectural level, where some part of the neural network, typically the encoder, diffuses the entire sequence altogether to an intermediate representation, then an autoregressive decoder generates the final sequence [@zhu2024segment]. LADIDA [@ladida] is a slightly different approach that diffuses at document level but decodes sentences by an AR decoder. SpecDiff [@christopher2025speculative] proposes a collaborative speculative decoding framework, where a lightweight diffusion model drafts candidate outputs, which are then validated and finalized by a large AR model. TiDAR [@liu2025tidar] proposes a sequence-level hybrid architecture that integrates diffusion-based parallel drafting and autoregressive sampling within a single forward pass through structured causal-bidirectional attention. It effectively unifies the efficiency of diffusion models with the quality of AR decoding, achieving up to $5\times$ throughput improvements while maintaining AR-level performance. SDLM [@liu2025sequential] introduces the Next Sequence Prediction (NSP) paradigm, which unifies next-token and next-block prediction to enable adaptive-length generation. By retrofitting pretrained autoregressive models with parallel block training and confidence-based dynamic decoding, SDLM achieves efficient diffusion-style intra-block generation while remaining KV-cache compatible.

# DLMs: Pre-training and Post-training {#sec:training}

## Pre-training and Supervised Fine-tuning

The pretraining process of DLMs largely follows procedures similar to those used in autoregressive language models (for discrete DLMs) or image diffusion models (for continuous DLMs), with relatively fewer design spaces. This section briefly summarizes existing approaches for DLM pretraining, aiming to bridge the methodological gap between DLMs and AR models.

To accelerate training, particularly for large-scale models, it is common practice to initialize DLMs from pretrained AR language models or image diffusion models [@cetin2025large; @gongscaling]. DiffuGPT and DiffuLLaMA [@gongscaling] try to initialize masked DLMs with open-sourced LLMs from 127M to 7B parameters, found that DLMs can be efficiently adapted from AR models, significantly reducing training time and cost while achieving comparable or even superior performance to their AR counterparts. Building on this insight, Dream-7B is initialized from Qwen 2.5 7B [@qwen2.5], and is reported to outperform both LLaDA-8B and LLaMA3-8B on various benchmarks. Some multimodal DLMs, on the other hand, are initialized from pretrained image diffusion models. D-DiT [@li2025dual] and Muddit [@shi2025muddit] are initialized from pretrained MM-DiT backbones from SD3 [@esser2024scaling] and Meissonic [@bai2024meissonic] respectively. Although these models are not originally designed for text generation, their latent representations contain intrinsic language-aligned knowledge, which can effectively facilitate the training of language modeling while retaining strong visual generation capabilities.

In terms of scaling properties, recent scaling-law analyses [@ni2025training; @ni2025diffusion] reveal that DLMs exhibit distinct compute-data tradeoffs from AR models: they are substantially more data-hungry under compute constraints, yet possess far greater data reuse potential under multi-epoch training, offering a principled foundation for designing optimal DLM training regimes.

Supervised fine-tuning in DLMs generally mirrors that of AR models. For masked DLMs like LLaDA [@nie2025largelanguagediffusionmodels], prompt tokens are left unmasked while response tokens are selectively masked, enabling the model to learn conditional response generation in a manner compatible with pre-training. In continuous DLMs, SFT can also be performed by corrupting only the response segment, as demonstrated in TESS2 [@tae2025tess].

Despite the overall similarity with AR training paradigms, DLMs face several unique challenges due to their diffusion-based formulation. A major issue lies in loss computation efficiency of masked DLMs. In typical masked DLM training, only $\sim$`<!-- -->`{=html}50% of tokens (on average) are involved in the loss computation, if timesteps are sampled uniformly. This reduces data utilization and may lead to suboptimal gradients, particularly if critical answer tokens are excluded from the loss. To address this, LaViDa [@li2025lavida] proposes a complementary masking strategy: each training sample is duplicated with two disjoint masking patterns, ensuring that all tokens are included in the loss computation at least once. Furthermore, due to the train-inference discrepancy, as illustrated in  [@asada2025addressing], the model performs significantly better during training than at inference time. The authors propose a two-step diffusion process and an improved scheduling technique to mitigate this issue.

## Post-training for Reasoning Capabilities

Exploration of reasoning capabilities is becoming increasingly popular in DLMs as their performance on language tasks improves. Typically, reasoning capabilities are gained through fine-tuning on reasoning datasets. For DLMs, this presents a unique and formidable challenge. Traditional Chain-of-Thought (CoT) methods are based on the sequential nature of AR models to reason step-by-step, but DLMs generate tokens in parallel. The most successful post-training techniques in the AR domain, particularly those based on reinforcement learning (RL) and policy gradient methods, are built upon the ability to efficiently compute the log-probability of a generated sequence. This is straightforward in AR models due to their factorizable, sequential nature. In DLM, where generation is an iterative, non-sequential process, the log-likelihood is intractable, creating a significant technical barrier to applying the mature suite of RL algorithms developed for AR models to DLMs. Intuitively, we categorize these works into three main streams, which form the structure of this subsection: (1) Parallelizing the reasoning chain, where CoT in AR models is adapted to DLMs in parallel generation. (2) Adapting policy gradient methods, where variants of popular algorithms like GRPO are introduced to DLMs. (3) Adapting preference optimization methods such as DPO to DLMs.

### DoT and DCoLT: Parallelizing the Reasoning Chain

One of the pioneering works to elicit complex reasoning in DLMs is Diffusion-of-Thought (DoT) [@ye2024diffusion], which adapts the popular Chain-of-Thought paradigm to the diffusion framework. Instead of generating reasoning steps sequentially like autoregressive models, DoT formulates them as intermediate thoughts that are refined in parallel throughout the diffusion denoising process. The approach is implemented by fine-tuning pre-trained DLMs such as Plaid [@gulrajani2023likelihood] and SEDD [@lou2024discrete] on datasets containing problems and their corresponding step-by-step rationales. To enhance the model's ability to recover from its own mistakes, DoT introduces specialized training techniques like scheduled sampling and coupled sampling, which exposes the model to its own generated errors during training to improve its self-correction capabilities. This post-training methodology enables smaller DLMs to achieve impressive reasoning performance, even outperforming significantly larger autoregressive models on certain mathematical and logical reasoning benchmarks.

A more recent approach, Diffusion Chain of Lateral Thought (DCoLT) [@huang2025reinforcing], introduces a distinct RL-based reasoning framework inspired by the cognitive concept of lateral thinking, which contrasts with the step-by-step vertical thinking of traditional CoT methods. Instead of supervising intermediate steps, DCoLT treats each step of reverse diffusion process as a latent thinking action, but optimizes the entire multi-step denoising trajectory with outcome-based RL to maximize a reward on the final answer. When applied to masked DLMs like LLaDA, DCoLT innovatively introduces an Unmasking Policy Module (UPM), which learns the optimal order for revealing tokens as part of the RL action space. This approach significantly boosts the reasoning capabilities of DLMs, with the DCoLT-reinforced LLaDA model achieves gains of +9.8% on GSM8K and +19.5% on HumanEval.

::: table*
:::

### Adapting Policy Gradient Methods to DLMs

Score Entropy Policy Optimization (SEPO) [@zekri2025fine] introduces RLHF to discrete DLMs, proposing a theoretically grounded framework to fine-tune discrete diffusion models using policy gradient methods and non-differentiable rewards. Operating within the score entropy framework, SEPO adapts modern policy gradient methods like PPO and GRPO by using importance sampling to derive a stable and low-variance gradient estimate. This allows the model's policy to be iteratively updated to maximize a reward function, making it a general framework for both conditional and unconditional generation. The objective function of SEPO is defined as follows: $$\begin{equation}
l^{A}(\theta)=\mathbb{E}_{x\sim\pi_{\theta_{old}}}\left[\sum_{\substack{y\in\mathcal{X} \\ y\neq x}}w_{x,y}\log s_{\theta}(x,T-T_{0})_{y}\right]
\end{equation}$$

where the model parameters $\theta$ are optimized to maximize the expected log-likelihood of the score entropy $s_\theta$ weighted by $w_{x,y} = \pi_\theta(y)f(r^{T - T_0}_{x,y})$. The expectation is taken over samples $x$ from the previous policy $\pi_{\theta_{\text{old}}}$. The function $f$ can be selected to recover different policy gradient variants; for example, a clipped function yields PPO, while group-standardized rewards yield GRPO. This formulation enables stable and low-variance gradient estimation, even with non-differentiable rewards, and provides a flexible objective for fine-tuning discrete diffusion models. Numerical experiments across several discrete generative tasks showcase scalability and efficiency of SEPO, demonstrating that policy gradient RL can be soundly applied to discrete diffusion models.

d1 [@zhao2025d1] provides a two-stage post-training framework for masked DLMs that combine supervised finetuning (SFT) with a novel policy gradient algorithm, diffu-GRPO. To adapt GRPO for DLMs, which lack a factorized likelihood, it introduces novel methods for both sequence log-probability and per-token log-probability estimation. d1 uses a simple mean-field decomposition to approximate sequence log-probability by a product of independent per-token probabilities, while per-token log-probability is computed by performing a single forward pass on a fully masked completion, conditioned on a randomly masked prompt during each policy gradient update. Using different random masks for the prompt in each inner gradient update step serves as a form of regularization, improving training efficiency and stability. The full d1 pipeline, leveraging SFT followed by diffu-GRPO, demonstrates significant performance improvements on mathematical and planning reasoning tasks for the LLaDA model.

MMaDA [@yang2025mmada], a unified multimodal diffusion model, presents a three-stage training pipeline. After first-stage pre-training, MMaDA employs a mixed Long chain-of-thought fine-tuning strategy, where reasoning trajectories from diverse tasks are curated into a unified format to align reasoning processes across modalities. This facilitates code start training of the third stage, which introduces UniGRPO, a policy-gradient reinforcement learning algorithm tailored for diffusion language models. UniGRPO overcomes the limitations of baseline methods like d1, by leveraging a structured noising strategy which samples a masking ratio $p_i \in [0, 1]$ uniformly rather than masking all response tokens. This ensures the model is exposed to various stages of multi-step diffusion denoising process, from almost fully masked to nearly unmasked, which is consistent with conventional diffusion training and boosts the utilization of the model's multi-step denoising capabilities. Additionally, the sequence-level log-likelihood is approximated by averaging over masked tokens.

DiffuCoder [@gong2025diffucoder] is a 7B-parameter DLM specifically developed and analyzed for code generation. This work introduces an RL algorithm named coupled-GRPO, which is designed to be diffusion-native by leveraging the unique properties of the DLM generation process. The central innovation of coupled-GRPO is its coupled-sampling scheme for log-likelihood estimation. To obtain a more robust and lower-variance estimate, it constructs paired, complementary masks for each completion sequence in a training batch. For a given sequence, two masks are generated such that every token position is masked in exactly one of the two masks. The log-probability estimate is then derived by averaging the losses from these two complementary forward passes. This ensures that every token is evaluated in a partial-masking context during training, providing full token coverage and a more stable gradient signal compared to methods that use a single random mask or a full mask. Coupled-GRPO is shown to substantially improve DiffuCoder's performance on code generation tasks, while also encouraging more parallel, less autoregressive generation patterns.

Sandwiched Policy Gradient (SPG) [@wang2025spg] leverages both an upper and a lower bound of the true log-likelihood to reduce bias in single-sided approximation policy gradient methods for DLMs. The two bounds of likelihood are estimated via Monte Carlo using a block-wise masking strategy to improve training stability. SPG reports to achieve state-of-the-art performance compared with baseline methods on various reasoning benchmarks when applied to LLaDA.

wd1 [@tang2025wd1] introduces a novel policy optimization approach that reformulates the objective as a weighted likelihood, requiring only a single approximation for the current parametrized policy likelihood. This formulation reduces bias and improves both stability and training efficiency, outperforming prior diffusion-based RL methods by up to 16% accuracy on reasoning tasks.

IGPO [@zhao2025inpainting] leverages the unique inpainting ability of masked diffusion models to guide exploration during reinforcement learning. By partially injecting ground-truth reasoning traces, IGPO alleviates the zero-advantage problem in group-based RL.

SAPO [@xie2025step] proposes a step-aware policy optimization scheme that introduces fine-grained process rewards aligned with the latent reasoning hierarchy, mitigating "unstructured refinement" and yielding more interpretable multi-step reasoning traces.

To improve memory efficiency in ELBO-based RL for DLMs, BGPO [@lin2025boundary] introduces a boundary-guided lower bound that allows large Monte Carlo sample sizes without increased memory usage, achieving stronger reasoning performance under the same hardware limits.

### Adapting Preference Optimization to DLMs

LLaDA 1.5 [@zhu2025llada] proposes a novel framework called Variance-Reduced Preference Optimization (VRPO) to adapt preference optimization methods to discrete DLMs. The work identifies that applying Direct Preference Optimization (DPO) to discrete DLMs is challenging due to the high variance of the Evidence Lower Bound (ELBO) used to approximate log-likelihoods. VRPO addresses this by introducing two key unbiased variance reduction techniques: (1) Optimal allocation of the Monte Carlo sampling budget by sampling more diffusion timesteps rather than multiple masked versions per timestep, i.e. $n_t=n$ and $n_{y_t}=1$ (2) Antithetic sampling, where the same timesteps and masked data are shared between the ELBO estimates of the current policy $\pi_\theta$ and the reference policy $\pi_{\text{ref}}$ for the same input $y_w$ or $y_l$. By applying VRPO to LLaDA, the resulting LLaDA 1.5 model shows significant and consistent improvements across mathematics, code, and alignment benchmarks.

# Inference Strategies {#sec:inference}

Inference strategies for DLMs serve three key goals: (i) boosting generation quality like unmasking and remasking schedules, (ii) enabling finer content control, and (iii) improving efficiency via techniques such as KV/feature cache and step distillation. A brief overview is presented in Fig. [5](#fig:inference){reference-type="ref" reference="fig:inference"}.

<figure id="fig:inference" data-latex-placement="!t">
<embed src="figs/inference.pdf" />
<figcaption>Inference Techniques of Diffusion Language Models. We illustrate six different strategies here, including: (a) Parallel Decoding; (b) Unmasking &amp; Remasking; (c) Classifier-free Guidance; (d) Key-Value Cache; (e) Feature Cache; and (f) Step Distillation.</figcaption>
</figure>

## Parallel Decoding

Parallel decoding naturally aligns with DLMs, leveraging their inherent mask-predict capability to generate multiple tokens simultaneously rather than sequentially. However, naïve parallelization can degrade coherence, motivating a series of adaptive strategies that balance efficiency and quality. Fast-dLLM [@wu2025fast] employs confidence-aware decoding, selectively unmasking tokens whose predicted probabilities exceed a threshold, and realizes up to 27.6× speedups without compromising quality. Adaptive Parallel Decoding (APD) [@israel2025accelerating] modulates the degree of parallelism on-the-fly by consulting a lightweight autoregressive auxiliary model, thus trading throughput for fidelity when necessary. SlowFast Sampling [@wei2025accelerating] introduces a two-stage schedule. Firstly, a cautious "slow" phase to locate stable tokens, then an aggressive \"fast\" phase to finalise them in bulk, achieving up to 34× acceleration when combined with caching. SpecDiff [@christopher2025speculative] further pushes throughput by using a discrete diffusion model as a fully parallel 'drafter' whose output is quickly verified (and corrected if needed) by a larger autoregressive model, yielding up to 7.2× speedups over vanilla AR generation. Dimple [@yu2025dimple] employs confident parallel decoding, dynamically adapting the number of tokens revealed per step and cutting generation iterations by 1.5 - 7$\times$. Recent research has further advanced the efficiency of parallel decoding in diffusion language models by introducing learnable and training-level optimization strategies. Learn2PD [@bao2025learning] introduces a learned adaptive parallel decoding policy, where a lightweight filter model predicts whether each token should be unmasked, replacing fixed confidence thresholds. dParallel [@chen2025dparallel] enhances decoding efficiency through certainty-forcing distillation, training diffusion models to reach high confidence for multiple tokens in parallel. Collectively, these parallel decoding approaches substantially narrow the latency gap between diffusion and autoregressive models while preserving, and in some cases improving, generation quality.

## Unmasking/Remasking

State-of-the-art open-source discrete DLMs such as LLaDA [@nie2025largelanguagediffusionmodels] and Dream [@dream2025] adopt a mask-predict paradigm: at each diffusion step they unmask high-confidence tokens and remask uncertain positions, iteratively refining the sequence. The choice of unmasking/remasking policy, i.e., low-confidence sampling, random selection, or adaptive temperature, therefore dominates both generation quality and convergence speed, making it one of the most critical inference levers. Early work Masked DLM [@sahoo2024simple] formalized two baselines: random remasking and confidence-ranked remasking, showing that prioritizing low-confidence positions yields better quality at no extra cost. Building on this insight, Fast-dLLM [@wu2025fast] introduces confidence-aware parallel decoding: every step it unmasks all positions whose predicted probabilities exceed a global threshold, realizing up to 13× speedups while maintaining accuracy. Most recently, ReMDM [@wang2025remasking] proposes a principled inference-time remasking sampler that can remask already decoded tokens for further refinement; by scaling the remasking budget, it offers a smooth compute--quality trade-off and closes the quality gap with autoregressive models under fixed compute. Collectively, these adaptive unmasking/remasking strategies substantially boost the efficiency and quality of diffusion language models, and they integrate cleanly with orthogonal accelerators that will be discussed later such as caching and step distillation.

## Guidance

Guidance is a pivotal inference technique in diffusion models, steering the generative trajectory toward desired attributes and thereby enhancing output quality. In diffusion models, guidance refers to any technique that modifies the model's denoising trajectory so that samples conform to a desired condition, such as a text prompt, a class label, or a stylistic attribute. The idea was popularized by classifier guidance [@dhariwal2021diffusion], where gradients from an external classifier are added to the score estimate to nudge the sample toward a target class. Soon after, classifier-free guidance [@li2025adaptive] removed the need for an extra classifier: the model is trained once with and without conditioning, and at inference the two score estimates are combined: $$\begin{equation}
s_{guided}=s_{uncond} + \lambda (s_{cond} - s_{uncond}),
\end{equation}$$ where $\lambda$ is the guidance scale that balances fidelity to the condition against sample diversity. This simple formulation now underpins most text-to-image systems (e.g., Stable Diffusion [@rombach2022high]) and has been adopted by DLMs for prompt-controlled generation. Subsequent work refines CFG along several axes: dropout-augmented CFG smooths the quality--diversity curve; particle-based guidance blends multiple conditions; and $p2$-weighting rescales the noise term to stabilize high-$\lambda$ sampling. In the text domain, newer schemes extend guidance to structural and semantic constraints. FreeCache [@hu2025accelerating] couples a lightweight autoregressive verifier with a discrete DLM: the verifier approves (or vetoes) draft tokens before they are committed, simultaneously enforcing coherence and enabling aggressive feature caching. DINGO [@suresh2025dingo] formulates regular-expression control as a dynamic-programming search over a DFA, guaranteeing constraint satisfaction without altering the model distribution. In other discrete DLMs, guidance can also be applied at each diffusion step, optionally combined with masking/remasking or caching, to steer content (e.g., topic, sentiment) while preserving efficiency. Overall, guidance has become a cornerstone of diffusion inference, offering a lightweight, tunable handle for aligning model outputs with user intent.

## Efficient Inference

Recent state-of-the-art diffusion language models [@zhu2025llada; @labs2025mercury; @dream2025] integrate the canonical Transformer architectures [@vaswani2017attention] with the step-wise stochastic inference procedures of diffusion processes. Consequently, efforts to accelerate inference in DLMs have converged on two complementary strategies: (1) lowering the per-step computational overhead of the Transformer backbone, e.g., through Key--Value (KV) Cache or Feature Cache. (2) reducing the total number of diffusion sampling steps, e.g., via Step Distillation.

The conventional KV cache leverages the strictly autoregressive decoding pattern of LLMs and is therefore ill-suited to the bidirectional, multi-step generation paradigm of DLMs [@ma2025dkv]. Recent work, however, shows that carefully redesigning the decoding schedule can recover much of its benefit. Block Diffusion [@arriolablock] introduces Block Discrete Denoising Diffusion Language Models (BD3-LMs), which decode text autoregressively across coarse blocks while running diffusion within each block; once a block is finished, its keys and values are frozen and reused, enabling variable-length generation and measurable speedups. Fast-dLLM [@wu2025fast] keeps the blockwise view but adds a training-free, approximate DualCache that exploits the near-identity of KV activations across successive diffusion steps for both prefix and suffix tokens, delivering up to $27\times$ end-to-end throughput gains on LLaDA and Dream with $<1\%$ accuracy loss. Complementing these block-based schemes, dKV-Cache [@ma2025dkv] observes that token representations stabilize only after a position is decoded and therefore deploys a delayed, conditional cache that stores KVs one step later; this design achieves $2$-$10\times$ speedups on the same models with negligible quality drop. d$^{2}$Cache [@jiang2025d] introduces a fine-grained dual adaptive caching scheme, adaptively refreshes only rapidly changing KV states while reusing stable ones. Elastic-Cache [@nguyen2025attention] proposes an attention/depth-aware adaptive refresh mechanism that selectively updates deeper layers and reuses stable shallow-layer caches. It performs attention-based drift detection to trigger cache refreshes only when the most-attended tokens exhibit significant changes, yielding up to $45\times$ speedup with minimal quality loss. Together, these results show that semi-autoregressive scheduling and delayed caching provide practical bridges between diffusion's bidirectional conditioning and Transformer tricks originally devised for autoregression.

Feature caching was first introduced by DeepCache [@ma2024deepcache], which leverages the strong similarity of intermediate U-Net activations across consecutive diffusion steps to avoid redundant computation. Follow-up work $\Delta$-DiT [@chen2024delta], Learning-to-Cache [@ma2024learning], and FasterCache [@lvfastercache] demonstrate that the same principle transfers cleanly to Transformer-based diffusion models, yielding comparable speedups without retraining. With the rise of diffusion language models, dLLM-Cache [@liu2025dllm] extends feature caching to text by distinguishing two redundancies: prompt tokens remain almost static throughout denoising, whereas response tokens evolve only sparsely. It therefore pairs a long-interval prompt cache with an adaptive short-interval response cache refreshed only when a lightweight value-similarity test ("V-verify") detects substantial change, achieving up to $9\times$ end-to-end speedups on LLaDA-8B and Dream-7B. Most recently, FreeCache [@hu2025accelerating] caches the KV/feature projections of already "clean'' tokens and refreshes only dynamic positions, pushing acceleration further to $34\times$ while preserving fidelity. Collectively, these advances illustrate that feature caching can bring diffusion language models within striking distance of autoregressive LLMs in inference latency without sacrificing output quality.

Step distillation is a widely adopted acceleration technique for diffusion models, collapsing the typical thousand-step denoising process into only a few and sometimes even a single sampling steps, thereby drastically reducing inference time. Unlike the training-free methods discussed earlier, it imposes an offline cost: a compact student network must first be trained to mimic the teacher. Early work such as Progressive Distillation [@salimansprogressive], followed by ADD [@sauer2024adversarial] and LADD [@sauer2024fast], progressively halves the step count or aligns intermediate distributions to preserve fidelity. Di4C [@hayakawadistillation] extends the framework to discrete diffusion by explicitly distilling inter-token correlations, enabling four to ten steps students that match teacher quality while providing $\sim$`<!-- -->`{=html}2× speedups. Most recently, DLM-One [@chen2025dlm] employs score-based distillation with adversarial regularisation to train a continuous diffusion language model that generates an entire sequence in a single forward pass, achieving up to 500× acceleration with near-teacher quality. Collectively, these works establish step distillation as the principal route toward closing the latency gap between diffusion and autoregressive language models.

# Multimodal and Unified Approaches {#sec:multimodal}

This section explores recent developments in extending DLMs to multimodal and unified architectures. Similar to autoregressive LLMs, DLMs can be naturally adapted to handle multimodal inputs and outputs. A straightforward approach is to accept vision inputs through a pretrained vision encoder. Following the success of LLaVA [@liu2023visual] in the AR domain, models such as LLaDA-V [@you2025llada], LaViDa [@li2025lavida], and Dimple [@yu2025dimple] employ vision encoders to extract image features, which are then projected into the same embedding space as text tokens. Beyond simple visual understanding, DLMs offer a promising pathway toward unified multimodal generation and understanding. Thanks to their shared denoising diffusion framework, DLMs naturally support joint modeling of different modalities. Visual inputs can be discretized using VQ-VAE, enabling training on multimodal inputs and outputs in a unified token space. Representative models such as MMaDA [@yang2025mmada], Fudoki [@wang2025fudoki], and Muddit [@shi2025muddit] exemplify this direction.

We begin by introducing the LLaDA [@nie2025largelanguagediffusionmodels] family and its derivatives, which are built on the architecture and pretrained weights of the base LLaDA model. LLaDA-V [@you2025llada] integrates a vision encoder with an MLP-based projector that maps visual features into the language token embedding space, enabling effective visual instruction tuning. Following LLaVA-NeXT [@lillava], LLaDA-V adopts three-stage tuning strategies. In the first stage, they only train the MLP projector to align visual representations with text embeddings using LLaVA's training data. In the second stage, the model is further tuned by large-scale visual instruction data [@guo2024mammoth] using DLM objective. The third stage is to enhance multimodal reasoning capabilities by training on QA pairs with reasoning chains. Although the LLaDA backbone is slightly weaker than LLaMA3-8B [@grattafiori2024llama] on pure text tasks, LLaDA-V achieves strong performance and better scalability across various benchmarks compared with LLaMA3-V trained on the same data. It narrows the performance gap with Qwen2-VL [@wang2024qwen2] and outperforms both hybrid and pure DLM-based models [@li2025dual; @xieshow; @kou2024orthus], demonstrating the effectiveness of diffusion architectures in multimodal understanding.

LaViDa [@li2025lavida] introduces a family of VLM based on LLaDA and Dream-7B [@dream2025]. Also utilizing a pretrained vision encoder, LaViDa uses a two-stage training strategy to train the projector and finetune the model respectively. LaViDa makes notable contributions to address training and inference challenges of multimodal DLMs. Typically, in masked DLMs, only about 50% of the tokens are masked for loss computation on average, which reduces efficiency and may omit critical answer tokens during VLM training, thereby causing gradient misalignment. LaViDa introduces complementary masking for effective training: For each sample, two masked versions with disjoint corrupted spans are generated, ensuring all tokens are eventually used in training and improving sample efficiency and gradient flow. During inference, LaViDa employs Prefix KV-Cache to cache the keys and values of visual and prompt tokens, significantly reducing latency and achieving a maximum speedup of 3.9$\times$ with a marginal performance drop. Additionally, timestep shifting is used to unmask tokens earlier, further boosting generation quality. Empirical results show that LaViDa achieves competitive or superior performance to AR-based VLMs, while enjoying significant inference speedup.

Lavida-O [@li2025lavidao] further extends LaViDa into a full-spectrum unified multimodal model capable of both high-quality image generation and fine-grained understanding tasks. It introduces a novel Elastic Mixture-of-Transformers (Elastic-MoT) architecture that decouples the model into a lightweight generation branch and a more powerful understanding branch, enabling scalable training and inference. Lavida-O uniquely supports localized object-level understanding, instruction-based image editing, high-resolution text-to-image synthesis (1024px), and interleaved reasoning and planning within a single unified diffusion framework.

Building upon LLaDA, MMaDA [@yang2025mmada] further generalizes the architecture to support both multimodal understanding and generation. Unlike prior models, MMaDA eliminates the need for an explicit vision encoder by tokenizing images into discrete codes using VQ-VAE, and modeling all modalities jointly with a modality-agnostic diffusion transformer. This design allows seamless integration across text and image modalities without modality-specific components. MMaDA also implements a mixed long CoT fine-tuning strategy that aligns CoT reasoning format across modalities. Moreover, UniGRPO, a unified policy-gradient based RL algorithm, is tailored specially for diffusion language models, making it possible to reason across modalities. Not only surpass similar-sized models like LLaMA3 for textual reasoning and Show-o [@xieshow] for multimodal understanding, MMaDA even excels professional image generation models like SDXL [@podellsdxl] in image generation.

MMaDA-Parallel [@tian2025mmada] replaces the sequential reasoning-then-generation pipeline in MMaDA with a fully parallel multimodal diffusion framework, enabling text and images to interact bidirectionally at every denoising step. By jointly generating reasoning traces and visual outputs and further optimizing cross-modal consistency via a trajectory-level Parallel RL (ParaRL) algorithm, MMaDA-Parallel substantially improves semantic alignment and thinking-aware image synthesis performance.

Dimple [@yu2025dimple] introduces a large multimodal DLM, combining a vision encoder with a discrete DLM backbone. The authors identify that a pure discrete diffusion training approach suffers from significant instability, poor performance, and severe length bias. To overcome these challenges, Dimple proposes a novel two-phase training paradigm called Autoregressive-then-Diffusion. In the first phase, the model undergoes standard autoregressive training to effectively align the vision and language modalities. In the second phase, it switches to diffusion-based training to restore its parallel decoding capabilities. This hybrid strategy ensures stable and efficient training while achieving performance comparable to or even better than contemporary autoregressive models like LLaVA-NEXT.

For inference, Dimple introduces several techniques to improve efficiency and controllability. Confident Decoding dynamically adjusts the number of tokens generated in each step based on a confidence threshold, which reduces the total number of generation iterations. The model also successfully re-implements the prefilling technique, common in autoregressive models, to cache prompt tokens and achieve a speedup of up to 7$\times$ with minimal performance loss. Furthermore, Dimple explores the use of Structure Priors, allowing for precise, fine-grained control over the response format and length, a feature that is difficult to achieve in autoregressive models.

Dual Diffusion Transformer (D-DiT) [@li2025dual] is a large-scale fully end-to-end unified multimodal diffusion model that supports both text-to-image (T2I) and image-to-text (I2T) tasks. It directly addresses the challenges previous diffusion models faced in visual understanding tasks, which have been largely dominated by autoregressive models. The architecture is inspired by the Multimodal Diffusion Transformer (MM-DiT), featuring a dual-branch transformer that processes image and text tokens, with attention mechanisms allowing interaction between modalities in every layer. The model uses a frozen VAE for image processing and a frozen T5 encoder for text, and the major backbone MM-DiT is initialized from pretrained SD3 [@esser2024scaling] weight.

One core innovation of D-DiT is its joint training objective, which combines continuous latent-space diffusion for images and discrete masked-token diffusion for text by jointly optimizing the sum of both modalities' losses. Unlike prior multimodal diffusion models that required an autoregressive component to decode text latents, D-DiT is fully diffusion-based and demonstrates competitive performance against other unified models.

Unified Multimodal Discrete Diffusion (UniDisc) [@swerdlow2025unified] is proposed as a unified generative model for the joint text and image modeling, building upon discrete diffusion as an alternative to dominant AR approaches. Different from previously discussed D-DiT, UniDisc employs an entire masked diffusion process jointly on text and image tokens with full attention, learning to map a sequence of masked tokens back to a clean sequence from a shared vocabulary. Training is performed using a unified discrete diffusion objective from scratch, where tokens from both modalities are randomly masked and the model is supervised with a re-weighted cross-entropy loss.

A key advantage of UniDisc is its superior performance in conditional generation tasks, which is largely attributed to the effective use of classifier-free guidance. One of the most notable capabilities of UniDisc is its ability to perform joint image and text inpainting in a zero-shot manner, a feature not possible with previous AR or unified generative models. The author performs scaling analysis by scaling up the model up to 1.4B, demonstrating UniDisc outperforms AR models in terms of both performance and inference-time compute, with enhanced controllability and editability. However, UniDisc is found to be less training-efficient than a comparable AR model in terms of achieving the same validation loss.

Fudoki [@wang2025fudoki] is introduced as the first general-purpose unified multimodal model built entirely on the discrete flow matching framework, challenging the dominance of autoregressive (AR) and masking-based diffusion models. Instead of relying on a simple masking corruption process, Fudoki leverages a more general metric-induced probability path with kinetic optimal velocities, which allows for a more semantically meaningful corruption process and enables the model to continuously self-correct its predictions during iterative refinement. This self-correction capability is a key distinction from masked DLMs, where unmasked tokens are typically fixed and cannot be revised.

To reduce the high cost of training from scratch, Fudoki is initialized from a pre-trained AR-based MLLM, Janus-1.5B [@wu2025janus], and is then adapted to the discrete flow matching paradigm in a two-stage process. Its architecture is based on Janus-1.5B but uses a full attention mask to better capture global context and removes time embedding layers, as the model can implicitly infer the timestep from the corrupted input. Fudoki achieves performance comparable to state-of-the-art AR models in both visual understanding and image generation tasks, demonstrating a flexible trade-off between inference speed and quality. The model shows significant performance gains when test-time inference scaling techniques are applied, suggesting the potential of this architecture to be further explored for next-generation unified models.

Muddit [@shi2025muddit] is a pure unified discrete diffusion transformer that integrates a strong text-to-image backbone with a lightweight text decoder, enabling flexible and high-quality multimodal generation under a truly unified architecture. Initialized from pretrained MM-DiT from Meissonoic [@bai2024meissonic], the model is trained using a unified discrete diffusion objective, where text and image tokens are stochastically masked according to a cosine schedule and the model learns to predict the original tokens via a re-weighted cross-entropy loss. By a combination of the strength from a semantically rich visual prior and parallel discrete diffusion, Muddit achieves competitive or superior performance compared to significantly larger AR models across generation and understanding benchmarks. It also demonstrates several times speedup over AR baseline, highlighting the efficiency and scalability of a discrete diffusion approach when properly initialized.

Lumina-DiMOO [@xin2025lumina] is a state-of-the-art open-source unified multimodal diffusion model that achieves fast and high-quality multi-modal generation and understanding through a fully discrete diffusion framework. Built on LLaDA, it expands the vocabulary to include 8,192 visual tokens from aMUSEd-VQ [@patil2024amused] and employs a unified training objective over mixed text-image sequences. Lumina-DiMOO supports a wide range of tasks, including text-to-image generation, image editing, subject-driven and controllable generation, and advanced image understanding. It introduces innovations such as Max Logit-based Cache (ML-Cache) for sampling acceleration, parallel and block-wise sampling for efficient decoding, and an end-of-line special token to support arbitrary image resolutions. The training of Lumina-DiMOO is performed in four stages, culminating with Self-GRPO, a self-improving reinforcement learning algorithm that enhances generation and understanding alignment. Lumina-DiMOO ranks first among open-source models on the UniGenBench [@Pref-GRPO&UniGenBench] leaderboard, offering 32$\times$ speedup over AR baselines while delivering superior generation quality.

# Performance Study {#sec:performance}

In this section, we briefly compare the performance of various DLMs with AR models. We present visualizations based on several widely used benchmarks for evaluating DLMs, including PIQA [@bisk2020piqa] and HellaSwag [@zellers2019hellaswag] for general language understanding, HumanEval [@chen2021evaluating] for code generation, and GenEval [@ghosh2023geneval], MME [@fu2023mme], MMMU [@yue2024mmmu] and GQA [@hudson2019gqa] for multimodal generation and comprehension. We also include GSM8K [@cobbe2021training], a popular benchmark in DLM literature for assessing mathematical reasoning capabilities. The corresponding performance visualizations are shown in Fig. [6](#fig:performance){reference-type="ref" reference="fig:performance"}.

The DLMs surveyed range in size from under 1B to 8B parameters. For comparison, we also report the performance of representative AR models of similar scale. Performance data are primarily taken from original publications. If results were not available in the source papers, we consulted subsequent works that reported comparable evaluations.

Our findings suggest that DLMs generally perform competitively with AR models of comparable size. On general language understanding benchmarks such as PIQA and HellaSwag, models like LLaDA achieve performance that is slightly below or on par with AR models such as LLaMA2 [@touvron2023llama] and Qwen2.5 [@team2024qwen2]. However, DLMs exhibit stronger performance in math and science-related benchmarks, including GSM8K, GPQA [@rein2024gpqa], and MATH [@hendrycks2measuring], where models such as LLaDA and Dream consistently outperform similarly sized AR counterparts. In multimodal tasks, models like MMaDA [@yang2025mmada] and LLaDA-V [@you2025llada] often surpass AR-based multimodal models, highlighting the potential of DLMs in unified and cross-modal reasoning. On code generation tasks, DLMs also demonstrate competitive capabilities. Notably, DiffuCoder [@gong2025diffucoder] achieves competitive HumanEval performance among open-source models, illustrating the potential of DLMs in structured, logic-heavy domains. Furthermore, closed-source DLMs such as Gemini Diffusion [@deepmind2024geminidiffusion] and Mercury [@labs2025mercury] achieve state-of-the-art results among all DLMs, rivaling top-tier AR models like GPT-4o.

Given the relatively limited training data and computational resources used to train most current DLMs, these results suggest that DLMs hold strong potential as viable alternatives to AR models in many real-world applications.

Recent scaling studies further show that DLMs tend to outperform AR models in data-constrained, multi-epoch regimes, likely because their any-order denoising objective enables more effective reuse of limited data [@ni2025diffusion].

<figure id="fig:performance" data-latex-placement="h!">
<embed src="figs/Performance.pdf" />
<figcaption>Performance comparison on eight benchmarks: Overall-GenEval, MME, CQA, Hellaswag, PIQA, HumanEval, GSM8K, and MMMU. The horizontal axis in each subplot represents the model size, measured in the number of parameters. The vertical axis indicates the score under the corresponding benchmark, with higher scores reflecting better performance. Model types are distinguished by color: blue represents AR language models, while orange represents DLMs. </figcaption>
</figure>

# Applications on Downstream Tasks {#sec:application}

## Conventional NLP Tasks

Before the emergence of large-scale DLMs for general-purpose language generation, DLMs have already been applied to various conventional NLP tasks, such as text classification [@yuan2024roic], named entity/scene recognition [@shen2023diffusionner; @yang2025ipad], sentiment analysis [@liu2024let], document summarization [@zhang2023diffusum; @dong2025termdiffusum], style transfer [@horvitz2024paraguide; @lyu2023fine], constrained generation [@zhang2023planner; @liu2023diffucom; @xiang2024diffusiondialog; @zou2024improved; @hu2024poetrydiffusion], and machine translation [@chen2023xdlm; @demirag2024benchmarking], etc.

ROIC-DM [@yuan2024roic] is the first work to adapt diffusion models for robust text classification and inference. It applies the diffusion process directly to the class labels and conditions the denoising process on the input text, which can be further enhanced by incorporating traditional language models as advisors. DiffusionNER [@shen2023diffusionner] formulates Named Entity Recognition as a boundary-denoising task. It applies a diffusion process to the start and end boundaries of entities, generating entity spans from random noise through an iterative refinement process. For scene text recognition, IPAD [@yang2025ipad] introduces a parallel, iterative network that frames the task as conditional text generation, employing discrete diffusion and an easy-first decoding method to effectively balance recognition accuracy and inference speed. For aspect-based sentiment analysis, DiffusionABSA [@liu2024let] employs a diffusion model to progressively extract the aspects step-by-step. DiffuSum [@zhang2023diffusum] proposes a novel paradigm for extractive summarization by using a diffusion model to directly generate desired summary sentence representations. The final summary is then formed by extracting document sentences that best match these generated representations. For legal document summarization, TermDiffuSum [@dong2025termdiffusum] proposes a term-guided diffusion model that prioritizes sentences with legal terminology via a multifactor fusion noise weighting schedule. For keyphrase extraction, Diff-KPE [@luo2024enhancing] enhances phrase representations by guiding a text diffusion process with a Variational Information Bottleneck to generate and inject keyphrase information. IPED [@zhao2024iped] treats relational triple extraction as an implicit block diffusion task. EdiText [@lee2025editext] introduces a controllable coarse-to-fine text editing framework by integrating an SDEdit-based technique with a novel self-conditioning method for precise editing control. To generate more specific empathetic responses, DIFFUSEMP [@bi2023diffusemp] utilizes a conditional diffusion model guided by multi-grained control signals (e.g., intent and semantic frames) that are integrated via a special masking strategy. DiffuDetox [@floto2023diffudetox] utilizes a mixed diffusion approach for text detoxification, combining a conditional model to reduce toxicity with an unconditional model to ensure the fluency of the output text. A finetuned DiffuSeq model is shown to achieve state-of-the-art performance on fine-grained text style transfer tasks [@lyu2023fine], while ParaGuide [@horvitz2024paraguide] introduces a more flexible plug-and-play framework that guides a paraphrase-conditioned diffusion model with off-the-shelf classifiers and style embedders at inference time. To generate fluent and diverse paragraphs while avoiding repetition, PLANNER [@zhang2023planner] combines a latent diffusion planning module to generate semantic paragraph embeddings with an autoregressive decoding module to render the final text. DiffuCom [@liu2023diffucom] presents an efficient diffusion model for comment generation that uses context-aware attention mechanism and self-conditioning technology. DiffusionDialog [@xiang2024diffusiondialog] tackles the one-to-many problem in dialogue generation by performing a diffusion process with continuous latent variables, improving response diversity and inference speed. For paraphrase generation, LDP [@zou2024improved] models diffusion in a pretrained model's latent space, avoiding the typical rounding step to achieve greater efficiency. For the highly constrained task of poetry generation, PoetryDiffusion [@hu2024poetrydiffusion] uniquely separates the task by using the diffusion model to generate semantics while a novel, independently trained metrical controller enforces structural rules like format and rhyme. In machine translation, XDLM [@chen2023xdlm] pioneers a cross-lingual pre-training objective for diffusion models, enabling them to effectively learn the mapping between languages in the pretraining stage. DiffusionRet [@qiao2023diffusionret] proposes a two-stage generative retrieval method that first utilizes a diffusion model to generate a pseudo-document from a query, which then serves as input for an n-gram-based model to retrieve the final document. DIFND [@yan2025debunk] employs a diffusion model to generate debunking evidence and a multi-agent MLLM system for chain-of-debunk reasoning to improve accuracy and interpretability for multimodal fake news detection.

## Code Generation

Although DLMs are rarely explicitly designed for code generation, the global planning and iterative refinement capabilities of them are particularly well-suited for the non-sequential nature of code generation. Foundational models like DiffuCoder [@gong2025diffucoder], a 7B open-source model, have been developed specifically for this domain. DiffuCoder's analysis reveals unique decoding behaviors, such as generation order becoming more flexible at higher temperatures. It also proposes coupled-GRPO, a novel sampling scheme that constructs complementary mask noise for completions used in training, which significantly improves the model's performance on code generation tasks. Building on the reasoning aspect, DCoLT [@huang2025reinforcing] treats the entire reverse diffusion process as a form of non-linear, \"lateral\" thinking. With outcome-based RL and unmasking policy module, it achieves strong results on complex coding tasks. Dilated Unmasking Scheduler (DUS) [@luxembourg2025plan] offers an inference-only, planner-free method that unmasks tokens in a non-adjacent pattern to minimize an upper bound on joint entropy gain at each denoising step, achieving promising results on code generation while improving speed-quality trade-off. Demonstrating the real-world potential of DLMs' speed, Mercury Coder [@labs2025mercury] is a commercial-scale diffusion model that achieves state-of-the-art throughput, outperforming speed-optimized autoregressive models by up to 10$\times$ while maintaining comparable quality on major code benchmarks.

## Biological and Scientific Applications

TransDLM [@xiong2024text] performs molecular optimization guided by a textual description of target properties to avoid the error propagation. Another text-guided approach, TGM-DLM [@gong2024text], focuses on molecular generation by collectively and iteratively updating token embeddings of SMILES strings. Without relying on additional data resources, TGM-DLM surpasses MolT5-Base in generation performance. DRAKES [@wangfine] introduces an RL-based fine-tuning method for discrete diffusion models that backpropagate rewards using the Gumbel-Softmax trick for DNA and protein design. For protein modeling, ForceGen [@ni2024forcegen] enables de novo protein design by using a protein language diffusion model to generate sequences that meet complex, nonlinear mechanical property-design objectives. MeMDLM [@goel2024memdlm] introduces a masked diffusion language model for de novo membrane protein design by fine-tuning the ESM-2 protein language model to generate novel and realistic transmembrane sequences. Inspired by LLaDA, DSM [@hallee2025diffusion] introduces a enabling both high-quality representation learning and effective generative protein design. DPLM [@wangdiffusion] offers a versatile protein language model that exhibits strong generative and predictive capabilities for protein sequences, and demonstrates superior performance in representation learning. DPLM2 [@wang2024dplm] further extends the model into a multimodal protein foundation model that can simultaneously process both sequences and structures. By converting 3D structural coordinates into discrete tokens, DPLM-2 learns the joint distribution of these two modalities. This enables the simultaneous co-generation of compatible protein sequences and their 3D structures, in addition to supporting conditional tasks such as protein folding and inverse folding. CFP-GEN [@yincfp] is a novel diffusion language model designed for Combinatorial Functional Protein Generation. It facilitates de novo protein design by integrating multimodal constraints, including functional, sequence, and structural information. CFP-GEN supports high-throughput generation of novel proteins with functionality comparable to that of natural proteins and achieves a high success rate in the design of multifunctional proteins.

## Robotics

Recently, DLM-based vision-language-action (VLA) models have demonstrated strong potential in unifying perception, reasoning, and control. Built upon LLaDA, LLaDA-VLA [@wen2025dvla] incorporates localized special-token classification alongside hierarchical action-structured decoding, leading to significant improvements over autoregressive VLA baselines in both simulation and real-world evaluations. dVLA [@wen2025dvla] leverages pretrained MMaDA as a diffusion backbone to jointly generate visual subgoal images, textual chain-of-thought, and discretized actions, further introducing prefix-attention masking and KV caching for efficient long-horizon manipulation. Unified Diffusion VLA (UD-VLA) [@chen2025unified] proposes a Joint Discrete Denoising Diffusion Process that synchronously denoises future image and action tokens within a shared token space, achieving state-of-the-art performance on benchmarks with substantially improved inference speed.

# Challenges and Future Directions {#sec:challenges}

While diffusion language models have shown considerable promise across a wide range of tasks, several key challenges still remain and limit their practical deployment and broader application. In this section, we outline and discuss critical areas that require further research and innovation.

## Major Challenges

Diffusion language models are designed to generate multiple tokens in parallel. However, this parallelism often comes at the expense of generation quality and consistency. In discrete DLMs, unmasking multiple tokens simultaneously in a single step increases the denoising burden, which can lead to error accumulation. A central issue is the interdependence between tokens, known as the **Parallel Decoding Curse** [@wu2025fast]. When predicting multiple tokens at once, the model produces a distribution for each position and samples from them independently, failing to account for dependencies among positions. Consider a simple example where the training data consists only of two sequences: "ABABAB" and "BABABA". Statistically, "A" and "B" appear with equal frequency at each position in the training data, leading DLMs to assign them similar probabilities during prediction. In autoregressive models, once the first "A" is generated, the model is likely to predict "B" next, preserving consistency. In contrast, a DLM generating tokens in parallel may independently sample "A" for both the first and second positions, producing a sequence like "AAABBA", which deviates from valid training patterns. Empirical studies show that this issue significantly affects DLM performance, particularly when the number of denoising steps is reduced [@gongscaling]. This phenomenon is illustrated in Fig. [7](#fig:decoding_tradeoff){reference-type="ref" reference="fig:decoding_tradeoff"}. Future work may focus on mitigating this trade-off. Potential directions include introducing structured constraints, modeling inter-token dependencies more explicitly, or refining sampling strategies to improve coherence during parallel generation.

<figure id="fig:decoding_tradeoff" data-latex-placement="!t">
<embed src="figs/new7.pdf" style="width:100.0%" />
<figcaption>Generation results of LLaDA <span class="citation" data-cites="nie2025largelanguagediffusionmodels"></span> and MMaDA <span class="citation" data-cites="yang2025mmada"></span> under different denoising step settings. Note that the generation length is set to 128 tokens and 256 tokens for LLaDA and MMaDA respectively. Both models generate a correct and coherent response only when 1 or 2 tokens are unmasked at each step. With fewer steps and more parallelism, the responses are either incorrect or lack fluency and consistency. This illustrates the trade-off between parallelism and output quality in DLMs. We omit part of the thinking process of MMaDA with 256 steps for simplicity. </figcaption>
</figure>

While the training, fine-tuning, and inference of AR models have been significantly simplified and accelerated by open-source, highly optimized libraries and frameworks (e.g., Hugging Face Transformers [@wolf2020transformers]), DLMs still lag behind in this regard. Currently, major machine learning ecosystems offer little to no native support for DLMs, posing practical challenges for researchers and developers. Furthermore, during inference, DLMs lack mature, open-source deployment infrastructure akin to vLLM [@kwon2023efficient], making efficient serving of DLMs difficult.

DLMs are typically trained to denoise fixed-length sequences under a diffusion-based objective, which makes it challenging to generalize to longer or dynamically sized sequences at inference time. Most existing DLMs are limited to a maximum context length of 4,096 tokens, and widely used extrapolation techniques in AR models for longer sequences remain underexplored in the DLM setting. This limitation hinders the applicability of DLMs in tasks requiring long-context understanding or complex reasoning. In addition, DLMs generally require the generation length to be predetermined during inference, making them ill-suited for dynamic-length generation. Although DLMs can predict an \[EOS\] token and omit displaying tokens generated afterward, the entire sequence is still fully updated throughout the denoising process, regardless of whether the generation has logically ended, which leads to unnecessary computational overhead. Recent work has begun to address this limitation through both training-based [@Dreamon2025; @yang2025diffusion] and training-free [@li2025beyond; @chen2025dpad] approaches. In addition, masked DLMs utilize full bidirectional attention at every denoising step, which incurs a computational cost of $\mathcal{O}(N^2)$ per step, where $N$ is the sequence length. Assuming a fixed number of tokens are unmasked at each step, the total number of denoising steps scales linearly with $N$, leading to an overall inference complexity of $\mathcal{O}(N^3)$. Without architectural optimizations such as KV-Cache, this cubic time complexity severely limits the scalability of DLMs for long-sequence generation in real-world applications.

Scalability remains an underexplored challenge for diffusion language models, particularly in comparison to autoregressive models. Although DLMs have shown promising results on certain metrics and benchmarks, they have yet to be scaled to the same extent as AR counterparts. The largest publicly available DLM currently contains only around 8B parameters, significantly smaller than leading AR models that have been scaled to hundreds of billions or even trillions, such as Llama-3.1-405B [@grattafiori2024llama], DeepSeek-V3-671B-A37B MoE [@liu2024deepseek], Qwen3-235B-A22B MoE [@yang2025qwen3], Kimi-K2-1T-A32B MoE [@team2025kimi], etc. Closed-source DLMs, such as Mercury and Gemini Diffusion, also fall short of state-of-the-art AR models across a wide range of benchmarks. Furthermore, many existing DLMs are trained either from previously pretrained AR models or built upon baseline DLMs (e.g., LLaDA) using limited datasets, which further constrains their scalability and performance. Therefore, the ability to further scale up DLMs still needs to be validated or explored.

## Future Directions

Despite the challenges discussed above, DLMs present many promising directions for future exploration. Below, we briefly outline several under-explored directions and opportunities that could significantly advance the field:

- **Training Efficiency:** Current DLMs generally exhibit lower training efficiency compared to AR models, due to factors such as limited token usage during loss computation. Future research could explore hybrid DLM architectures or improved training schemes that match or exceed AR models in efficiency.

- **Quantization and Binarization (Low-bit DLMs):** While extensively studied in AR models, low-bit quantization and binarization remain largely unexplored in DLMs. Adapting these techniques to the diffusion paradigm could yield faster inference and reduced memory consumption, benefiting deployment in real-world systems.

- **Pruning and Distillation:** Model compression techniques such as pruning and knowledge distillation have been successful in reducing model size and inference cost for AR models. Applying these techniques to DLMs could enhance their deployability, especially in resource-constrained or latency-critical environments.

- **Multimodal Unified Reasoning:** Although recent multimodal DLMs demonstrate impressive capabilities in cross-modal understanding and generation, most models are still limited to reasoning within a single modality at a time. Future efforts can focus on building unified DLMs capable of performing complex reasoning across multiple modalities in a truly integrated manner.

- **DLM-based Agents:** The potential of DLMs in powering intelligent agents remains largely underexplored. Leveraging their bidirectional context modeling, parallel decoding, and iterative refinement capabilities, DLM-based agents could offer greater flexibility and adaptability in dynamic environments, making them a promising alternative to traditional AR-based agent approaches.

# Conclusion

In this survey, we present an in-depth overview of the entire landscape of diffusion language models. We outline the fundamental principles, taxonomy, and modeling paradigms of DLMs, and compare them with mainstream autoregressive models, highlighting their unique characteristics and advantages. We further explore the design space of training and inference, covering various training strategies and inference techniques for both quality and efficiency. Moreover, we highlight recent advances in multimodal diffusion language models, demonstrating their capabilities in handling diverse data modalities. Finally, we discuss the limitations and challenges in this field, and outline promising directions for future research. We hope this survey serves as a comprehensive reference for researchers interested in diffusion-based language modeling, offering valuable insights about the current state of the field and its future prospects. We also encourage further exploration and innovation in this exciting area of research, as diffusion language models continue to evolve and push the boundaries of language understanding and generation.
