# Experiments {#sec-exp}

## Training Dataset and Setup {#exp-training-setup}

We used the Conceptual Caption 12M () dataset, which has been similarly used in previous work [@cyclip; @slip; @supervision-exists; @cc12m], for pre-training the models. We set our setup quite similar to that of the original CLIP with the ViT-B-16 backend, in order to ensure a fair comparison of CLIP with and . The pre-trained semantic encoder utilized in for re-scaling image--image cosine similarities is the SBERT all-mpnet-base-v2 model. In order to fairly compare the effectiveness of each model, we trained all of them from scratch using the dataset and the OpenCLIP implementation [@openclip; @openclip-software]. Each model was trained for 6 days using an NVIDIA H100 GPU with batch size 512 for 30 epochs using AdamW optimization with a starting learning rate of $1 \times 10^{-3}$, cosine scheduler, 10,000 warmup steps, a weight decay of $0.1$, and an initial temperature value of 0.07. The output embedding dimension for both the vision and language modalities is set to 768. We used the checkpoint from the last epoch in our evaluations of downstream experiments. In , we set $\alpha=1$ and $\beta=\frac{1}{2}$. In Section [\[appendix-setup\]](#appendix-setup){reference-type="ref" reference="appendix-setup"}, more details about the training setup is provided.

Since our goal is to shed lights on a specific shortcoming of the original CLIP model, i.e., the modality gap problem, we compare our results to the original CLIP model in this paper. Further comparisons to other state-of-the-art models is not the focus of this study, as the goal is to investigate modifications that reduce the modality gap in CLIP without losing performance in downstream tasks, rather than achieving the state-of-the-art results.

::::: center
:::: small
::: sc
:::
::::
:::::

<figure id="fig-cosine-positives" data-latex-placement="t">
<embed src="figures/cumulative_positives.pdf" />
<figcaption>Cumulative distribution of pairwise cosine similarities of positive samples in MSCOCO.</figcaption>
</figure>

## Cross-Modal Alignment

We start by reporting and comparing the alignment scores when using CLIP, SharedCLIP, and AlignCLIP models on the validation sets from CC3M, MSCOCO as well as the ImageNet-1K, CIFAR-100, and CIFAR-10 test datasets. Table [\[res-table-alignment\]](#res-table-alignment){reference-type="ref" reference="res-table-alignment"} summarizes the corresponding alignment scores. We observe that the original CLIP model has relatively low alignment scores, varying within $[0.38, 0.47]$, across all five datasets. These scores mean that the average angle between the paired image--text embeddings is a value between 61 and 68 degrees. In contrast, sharing the parameter space in results in noticeable improvements of up to 0.17 in the alignment scores. As a result, the average angle between the paired image--text embeddings decreases to about 51 degrees. Furthermore, using yields even better alignment scores, ranging from 0.62 to 0.67, and a decreased average angle of 47 degrees between the multi-modal paired samples. These observations confirm that improves the cross-modal alignment in CLIP and thereby, reduces the modality gap.

We also plot the cumulative distribution of cosine similarities of the positive samples from MSCOCO validation dataset when encoded using the original CLIP, , and in Figure [1](#fig-cosine-positives){reference-type="ref" reference="fig-cosine-positives"}. We find that using noticeably shifts the distribution of the cosine similarity of positive samples towards higher similarity values. A higher similarity of positive samples, i.e., image--text pairs, means achieving greater cross-modal alignment, and thus a lower modality gap. Furthermore, shifts the distribution even more to the right, resulting in higher similarities of positive samples, better cross-modal alignment, and a reduction of the modality gap.

::::: center
:::: small
::: sc
:::
::::
:::::

::::: center
:::: small
::: sc
:::
::::
:::::

## Classification

CLIP's pre-training objective for predicting whether a text is paired with an image has resulted in outstanding image classification performance when tested in a zero-shot setting as well as after linear probing [@clip]. Therefore, we further assess how sharing the learnable parameters and the intra-modality loss affect the classification performance in these settings.

**Zero-Shot Image Classification.** We conduct the zero-shot classification experiments on ImageNet-1K [@imagenet1k], CIFAR-100, CIFAR-10 [@cifar], Flowers-102 [@flowers102], and Stanford Cars [@stanfordcars] with the combination of text prompts used by CLIP [@clip], e.g., "a photo of the {label}". The experimental results summarized in Table [\[res-table-zeroshot-cls\]](#res-table-zeroshot-cls){reference-type="ref" reference="res-table-zeroshot-cls"} show that reduces the accuracy of the zero-shot classification on CIFAR-10 by about $5\%$ and $0.4\%$ when measuring Top-1 and Top-5 accuracy scores, respectively. Similarly, 's results on CIFAR-100 show about $2\%$ and $1\%$ decrease in accuracy. The trend of accuracy reduction is also observed on Flowers-102 and Stanford Cars datasets. However, on ImageNet-1K, evinces up to $1\%$ improvement of accuracy. In contrast, yields the best scores across all five datasets. It achieves $1.4\%$ and $2\%$ improvement of Top1 and Top5 accuracy, respectively, on ImageNet-1K when compared to the original CLIP. On CIFAR-10, achieves up to $8\%$ and $2\%$ improvements in Top-1 and Top-5 accuracy scores. Similarly, using for CIFAR-100 results in about $8\%$ and $11\%$ enhancement in Top-1 and Top-5 accuracy scores in comparison to the original CLIP. The trend of improvement is also observed on the Flowers-102 and Stanford Cars datasets. Our experiments thus evince that via sharing parameters and the additional intra-modality separation, improves the cross-modal alignment of the embeddings while enhancing the performance on the downstream zero-shot image classification task.

**Linear Probing.** We further test the performance of and when performing linear probing for image classification and report the Top1 accuracy results in Table [\[res-table-linear-probing\]](#res-table-linear-probing){reference-type="ref" reference="res-table-linear-probing"}. For all datasets, we train the linear classifier layer with a batch size of 128, for 30 epochs, with AdamW optimizer, and a cosine scheduler with a starting learning rate of 5e-4. Table [\[res-table-linear-probing\]](#res-table-linear-probing){reference-type="ref" reference="res-table-linear-probing"} shows the superiority of in the task of image classification with linear probing across all 5 datasets.

## Robustness to Natural Distribution Shift

In the zero-shot image classification task, CLIP has additionally shown impressive robustness to natural distribution shifts and promising generalizability to out-of-distribution images. Therefore, we expand our evaluations and investigate to what extent and change the performance with natural distribution shifts. We use the ImageNetV2, ImageNet-R, ImageNet-A, and ImageNetSketch datasets for these evaluations and report the corresponding results in Table [\[res-table-distr-shift\]](#res-table-distr-shift){reference-type="ref" reference="res-table-distr-shift"}, in terms of Top-1 and Top-5 accuracy. It is first observed that generally improves the classification accuracy in comparison to the CLIP model. Secondly, achieves the best classification results across all datasets. In summary, when comparing to the CLIP model, achieves about $2\%$ and $1\%$ improvement in Top1 and Top5 accuracy on the ImageNetV2 dataset. On ImageNet-R, improves both Top1 and Top5 scores by about $2\%$. The positive general trend of the accuracy enhancement is also observed on the ImageNet-A and ImageNetSketch datasets. Based on these observations, we conclude that sharing parameters and applying intra-modality separation in and improves the robustness to natural distribution shifts when compared to the original CLIP model and at the same time, improve the modality gap.

::::: center
:::: small
::: sc
:::
::::
:::::

## Multi-Modal Retrieval

**Zero-Shot Transfer.** In addition to classification, we evaluate and in the application of zero-shot image-to-text and text-to-image retrieval using the validation splits from the MSCOCO [@mscoco] and Flickr30K [@flickr] datasets. The results of these evaluations are reported in Table [\[res-table-zeroshot-IR\]](#res-table-zeroshot-IR){reference-type="ref" reference="res-table-zeroshot-IR"}. In all settings, the text prompt "a photo of the {caption}" is used. Our experiments show that both and improve the retrieval results measured by $R@\{1, 5, 10\}$ on both datasets when compared to the original CLIP model. In addition, achieves the best overall results in comparison to . When testing text-to-image retrieval on the MSCOCO dataset, outperforms at R@10. Similarly, for the image-to-text retrieval on the Flickr dataset, achieves the best results. These experiments demonstrate that it is possible to reduce the modality gap in CLIP via parameter sharing while improving the downstream multi-modal retrieval tasks. Furthermore, the addition intra-modality separation improves the alignment noticeably while noticeably enhancing the retrieval performance.

:::::: table*
::::: center
:::: small
::: sc
:::
::::
:::::
::::::

:::::: table*
::::: center
:::: small
::: sc
:::
::::
:::::
::::::

**Fine-tuning Multi-Modal Retrieval.** Table [\[res-table-finetuned-IR\]](#res-table-finetuned-IR){reference-type="ref" reference="res-table-finetuned-IR"} summarizes the result of multi-modal retrieval when each model is fine-tuned using the corresponding training set. We fine-tuned each model for 8 and 20 epochs on MSCOCO and Flickr, respectively. In both cases, the batch size was set to 128 and the AdamW optimizer with the learning rate of 5e-6 and a weight decay of 0.2 was used. Our results show that both and outperform the CLIP model in the fine-tuning scenario. Additionally, generally achieves a slightly better performance in comparison to .

## Ablation Study

This section provides an ablation study on the effectiveness of the re-scaling mechanism proposed in Eq. [\[eq-rescale\]](#eq-rescale){reference-type="ref" reference="eq-rescale"}. In order to see the impacts on the different types of downstream tasks, i.e., image classification, classification with distribution shift and multi-modal retrieval, we compare the performance of with and without the re-scaling mechanism on ImageNet-1K, CIFAR-100, CIFAR-10, ImageNetV2, MSCOCO and Flickr30K datasets and summarize the results in Table [\[res-table-ablation\]](#res-table-ablation){reference-type="ref" reference="res-table-ablation"}. This study substantially concludes that the re-scaling mechanism is effective in controlling the separation of similar image samples in the batch and therefore, increasing the results in the downstream tasks.

::::: center
:::: small
::: sc
:::
::::
:::::

## Results Analysis

**DOSNES Visualization.** For a more comprehensive comparison of the distribution of each modality, we visualize the DOSNES projection of the encoded image--texts from the MSCOCO validation set in Figure [5](#fig:dosnes){reference-type="ref" reference="fig:dosnes"}. As can be seen, the uni-modal embeddings in CLIP are densely located on opposite sides of the hypersphere. In contrast, the embeddings get spread out when using the model. Finally, the embedding space of achieves the best spread of the uni-modal embeddings and substantially greater alignment of image and text embeddings. Thus, the intra-modality separation leads to a better alignment and a substantial reduction of the modality gap. Furthermore, Figure [4](#fig:dosnes-alignclip){reference-type="ref" reference="fig:dosnes-alignclip"} shows that reduces the sparsity of the embeddings on the hypersphere.

<figure id="fig:dosnes" data-latex-placement="t">
<figure id="fig:dosnes-clip">
<embed src="figures/dosnes_orgCLIP_mscoco_3.pdf" />
<figcaption>Original CLIP</figcaption>
</figure>
<figure id="fig:dosnes-sharedclip">
<embed src="figures/dosnes_sharedCLIP_mscoco_3.pdf" />
<figcaption>SharedCLIP</figcaption>
</figure>
<figure id="fig:dosnes-alignclip">
<embed src="figures/dosnes_alignCLIP_mscoco_3.pdf" />
<figcaption>AlignCLIP</figcaption>
</figure>
<figcaption>DOSNES visualization of the multi-modal embeddings using CC3M</figcaption>
</figure>

**Comparison of Qualitative Examples.** In Table [\[res-table-mscoco-example\]](#res-table-mscoco-example){reference-type="ref" reference="res-table-mscoco-example"}, examples from the MSCOCO validation dataset is provided where the images convey the same general semantics but one of the ground truth texts provides more detailed information. On the left side of Table [\[res-table-mscoco-example\]](#res-table-mscoco-example){reference-type="ref" reference="res-table-mscoco-example"}, two images with their corresponding ground truth captions are provided. We use CLIP, and for encoding the images and texts, and provide the cosine similarities on the right side of the table. As can be seen, when querying the first image, the similarity of the first image and the second text using the CLIP embeddings is higher in comparison to the ground truth caption. Suggesting that the second text will get selected as the predicted caption when using CLIP. This flaw still appears when using for encoding the images and texts. However, when using , the cosine similarity of the first image and the first text is higher in comparison to the second text, meaning that when querying the first image, the ground truth caption, which is semantically more correct in comparison to the second text, successfully gets selected. This suggests that the semantic regularization of the Intra-Modality Separation in , which is calculated using the semantics of the text samples, potentially contributes in improving the retrieval performance.

::::: center
:::: small
::: sc
:::
::::
:::::
