# Design {#sec:design}

<figure id="fig:overview" data-latex-placement="!t">
<embed src="Figures/dhnsw.pdf" />
<figcaption>The overview of .</figcaption>
</figure>

We present , an RDMA-based vector similarity search engine on disaggregated memory. exploits the characteristics of RDMA-based memory data accessing and graph-based index HNSW to realize fast and bandwidth-efficient vector query processing. achieves so by representative index caching (§[1.1](#sec:design:cache){reference-type="ref" reference="sec:design:cache"}), RDMA-friendly graph index storage in remote memory (§[1.2](#sec:design:layout){reference-type="ref" reference="sec:design:layout"}), and query-aware batched data loading (§[1.3](#sec:design:load){reference-type="ref" reference="sec:design:load"}). Here, we provide a brief overview of as Fig. [1](#fig:overview){reference-type="ref" reference="fig:overview"} shows, requires tailored coordination between compute instances and memory instances on vector query serving. We assume the client load balancer distributes the workload across multiple CPU instances. The compute and memory pools are interconnected via RDMA, enabling efficient transfer of vector indices and data. **We target the disaggregated scenario where compute pools contain abundant CPU resources across many instances, each with limited DRAM serving as a cache, while memory instances have extremely weak computational power, handling lightweight memory registration tasks.**

## Representative index caching. {#sec:design:cache}

<figure id="fig:cache" data-latex-placement="!t">
<embed src="Figures/cache.pdf" />
<figcaption>Representative index caching in .</figcaption>
</figure>

Graph-based vector search schemes [@nsg; @hnsw] rely on greedy routing to iteratively navigate toward the queried vector. However, the search path can span the entire graph, potentially covering distant vectors. For example, HNSW exhibits small-world properties, allowing long-range connections between vectors that are far apart. However, loading the entire graph index from the memory pool to the computer pool for each query is impractical, because the compute pool has limited storage resources in a disaggregated system. This approach would not only consume excessive bandwidth by transferring a significant portion of untraversed vectors but also introduce additional latency, thereby degrading the overall search efficiency.

We propose partitioning the vector database into multiple subsets, as shown in Fig. [2](#fig:cache){reference-type="ref" reference="fig:cache"}. Inspired by Pyramid [@pyramid], we construct a three-layer representative HNSW, referred to as *meta-HNSW*, by uniformly selecting 500 vectors. This meta-HNSW serves as a lightweight index and a cluster classifier for the entire dataset, and it only costs 0.373 MB for SIFT1M and 1.960 MB for GIST1M datasets from our experiments. The search process starts from a fixed entry point in the top layer $L_2$ of meta-HNSW and applies greedy routing at each layer, traversing downward until reaching a vector in its bottom layer $L_0$. Each vector in $L_0$ defines a partition and serves as an entry point to a corresponding *sub-HNSW*. All vectors assigned to the same partition will be used to construct their respective sub-HNSW. The overall graph index consists of two components: meta-HNSW, which provides coarse-grained classification, and sub-HNSWs, which enable fine-grained search within partitions. **To improve search efficiency in disaggregation, we cache the lightweight meta-HNSW in the compute pool, allowing it to identify the most relevant sub-HNSW clusters for a given query.** Meanwhile, we put all sub-HNSW clusters in the memory pool. For each vector query, only a small subset of sub-HNSW clusters needs to be loaded from the memory pool via the network, reducing both bandwidth usage and search latency.

## RDMA-friendly graph index storage layout in remote memory. {#sec:design:layout}

<figure id="fig:layout" data-latex-placement="!t">
<embed src="Figures/layout.pdf" />
<figcaption>RDMA-friendly sub-HNSW indexing data layout in remote memory.</figcaption>
</figure>

RDMA enables efficient data access to targeted remote memory addresses. To efficiently read and write sub-HNSW cluster data in remote memory, an intuitive approach is to serialize all sub-HNSW clusters in the registered memory. Given that the top-$m$ closest sub-HNSW clusters for a queried vector $q$ are {$S_0,..,S_{m-1}$}, the compute instance can issue RDMA_READ commands to access these serialized clusters and then deserialize them. However, two challenges arise: (1) If the queried clusters {$S_0,..,S_{m-1}$} are not stored contiguously in memory, multiple RDMA round trips are required, increasing latency. (2) When new vectors are inserted, the size of each sub-HNSW cluster may exceed the allocated space. Since shifting all stacked sub-HNSW clusters is impractical, newly inserted vectors and their metadata may be placed in non-contiguous memory regions if they are simply appended at the tail of the available area. This fragmentation increases access latency and reduces query throughput due to the higher cost of scattered index access.

As shown in Fig. [3](#fig:layout){reference-type="ref" reference="fig:layout"}, we allocate and register a continuous memory space in memory instance to store both the serialized HNSW index and floating-point vectors. At the beginning of this memory space, a global metadata block records the offsets of each sub-HNSW cluster, as their sizes vary. The remaining memory space is divided into *groups*, each of which is capable of holding two sub-HNSW clusters. Within each group, the first section stores the first serialized sub-HNSW cluster, which includes its metadata, neighbor array for HNSW, and the associated floating-point vectors. The second sub-HNSW cluster is placed at the end of the group. Between these two clusters, we allocate a 0.75 MB for SIFT1M 3.92 MB for GIST1M shared overflow memory space to accommodate newly inserted vectors for both sub-HNSW clusters. When a vector query requires loading a sub-HNSW cluster, the compute instance issues an RDMA_READ command to retrieve the cluster along with its corresponding shared overflow memory space. This layout ensures that newly inserted vectors are stored continuously with the original sub-HNSW data, enabling them to be read back with a one-time RDMA_READ command. To optimize memory usage, each pair of adjacent sub-HNSW clusters shares a single overflow memory space for accommodating newly inserted vectors rather than allocating a separate one for each cluster.

If multiple sub-HNSW clusters need to be loaded into the compute pool for batched query processing, and they are not stored continuously in memory, we leverage *doorbell batching* to read them in a single network round-trip with RDMA NIC issuing multiple PCIe transactions. However, there is a tradeoff in the number of batched operations within a single RDMA command. If too many operations are included in one round-trip, it can interfere with other RDMA commands and incur long latency due to the scalability of the RDMA NIC. The memory offsets of each sub-HNSW cluster are cached in all compute instances after the sub-HNSW clusters are written to the memory pool, with the latest version stored at the beginning of the memory space in the memory instance.

## Query-aware batched data loading. {#sec:design:load}

<figure id="fig:load" data-latex-placement="!t">
<embed src="Figures/load.pdf" />
<figcaption>Query-aware sub-HNSW clusters loading.</figcaption>
</figure>

To reduce bandwidth usage for transferring graph index and improve query efficiency, we propose merging sub-HNSW index loading for queried vectors in the same batch.

Given a batch of queried vectors {$q_1$,$q_2$, \...,$q_s$} and a total of $m$ sub-HNSW clusters, each queried vector requires searching the top-$k$ closest vectors from the $b$ closest sub-HNSWs. However, the DRAM resources in the compute instance can only accommodate and cache $c$ sub-HNSWs. To optimize loading, we analyze the required $b*s$ sub-HNSWs *online* and ensure that each sub-HNSW is loaded from the memory pool only **once**.

For example, as shown in Fig. [4](#fig:load){reference-type="ref" reference="fig:load"}, queried vector $q_1$'s two closest sub-HNSW clusters are $S_1$ and $S_4$, while $q_3$'s two closest sub-HNSWs are $S_4$ and $S_5$. Similarly, $S_3$ is required for both $q_2$ and $q_4$. Given a doorbell batch size of 2 for accessing sub-HNSWs, the compute instance can issue an RDMA_READ command to fetch index $S_3$ and $S_4$ in one network round-trip, then compute the top-$k$ closest vectors candidates for all queries {$q_1$,$q_2$,$q_3$,$q_4$} first. The results will be temporarily stored for further computation and comparison because each query vector still requires another sub-HNSW to obtain the final answer. Note that $S_3$ and $S_4$ will not be loaded again within the same batch.

Once all required sub-HNSW clusters for the batched queried vectors have been loaded and traversed, the query results will be returned. Additionally, we retain the most recently loaded $c$ sub-HNSWs for the next batch. If the required sub-HNSWs are already in the compute instance, they do not need to be loaded again, further reducing data transfer overhead.
