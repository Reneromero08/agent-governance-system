# LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning

Haoqiang Kang1 â€ƒYizhe Zhang2 â€ƒNikki Lijing Kuang1 â€ƒNicklas Majamaki1
  
â€…Navdeep Jaitly2 â€ƒYi-An Ma1 â€ƒLianhui Qin1
  
1University of California, San Diego â€ƒ2Apple

###### Abstract

Large Language Models (LLMs) demonstrate their reasoning ability through chain-of-thought (CoT) generation. However, LLMâ€™s autoregressive decoding may limit the ability to revisit and refine earlier tokens in a holistic manner, which can also lead to inefficient exploration for diverse solutions. In this paper, we propose LaDiR (Latent Diffusion Reasoner), a novel reasoning framework that unifies the expressiveness of continuous latent representation with the iterative refinement capabilities of latent diffusion models for an existing LLM. We first construct a structured latent reasoning space using a Variational Autoencoder (VAE) that encodes text reasoning steps into blocks of thought tokens, preserving semantic information and interpretability while offering compact but expressive representations. Subsequently, we utilize a latent diffusion model that learns to denoise a block of latent thought tokens with a blockwise bidirectional attention mask, enabling longer horizon and iterative refinement with adaptive test-time compute. This design allows efficient parallel generation of diverse reasoning trajectories, allowing the model to plan and revise the reasoning process holistically. We conduct evaluations on a suite of mathematical reasoning and planning benchmarks. Empirical results show that LaDiR consistently improves accuracy, diversity, and interpretability over existing autoregressive, diffusion-based, and latent reasoning methods, revealing a new paradigm for text reasoning with latent diffusion.111Code is available at <https://github.com/mk322/LaDiR>.

## 1 Introduction

Large language models (LLMs) have demonstrated remarkable reasoning abilities through extensive pretraining on human languages, yet the inherent limitations of the autoregressive (AR) paradigm are becoming increasingly difficult to overlookÂ (Zhou etÂ al., [2024b](#bib.bib96); Bachmann & Nagarajan, [2025](#bib.bib2)). As shown in Fig.Â [1](#S1.F1 "Figure 1 â€£ 1 Introduction â€£ LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning") (top left), their sequential nature prevents revising earlier tokens, making self-refinement inefficient and difficult Â (Chen etÂ al., [2024](#bib.bib9); Huang etÂ al., [2024](#bib.bib31)). Moreover, AR models generate a linear chain of thought (CoT)Â (Dziri etÂ al., [2023](#bib.bib15); Wei etÂ al., [2023](#bib.bib72)), which limits reasoning diversity and restricts exploration of multiple valid solutionsÂ (Naik etÂ al., [2024](#bib.bib46); Yu etÂ al., [2024](#bib.bib84)).

Diffusion modelsÂ (Ho etÂ al., [2020](#bib.bib30)), originally introduced for generation in continuous domains like images, have recently gained attention in text generation for their ability to maintain global coherence and enable iterative refinementÂ (Ye etÂ al., [2024b](#bib.bib80); Nie etÂ al., [2025](#bib.bib47); Lou etÂ al., [2023](#bib.bib42); Yu etÂ al., [2025c](#bib.bib88); Weligalle, [2025](#bib.bib73); Sahoo etÂ al., [2024](#bib.bib53); Gulrajani & Hashimoto, [2023](#bib.bib23); Lovelace etÂ al., [2024](#bib.bib43)).
Existing works largely emphasize the parallelization properties of diffusion modelsÂ (Israel etÂ al., [2025](#bib.bib32); Nie etÂ al., [2025](#bib.bib47); Weligalle, [2025](#bib.bib73)). Arguably, a more important direction is to ask: How can these approaches enhance the reasoning capabilities of LLMs? We focus on one particularly promising capability: the ability to self-correct and refine reasoning chains at semantic levels in latent space. As shown in Fig.Â [1](#S1.F1 "Figure 1 â€£ 1 Introduction â€£ LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning") (top right), such self-refinement cannot be achieved by discrete diffusion language models that merely transit into masked tokens.

![Refer to caption](/html/2510.04573/assets/figs/teaser.png)

Figure 1: Comparison of reasoning paradigms: autoregressive CoT and latent CoT generate discrete or continuous tokens sequentially; diffusion LMs iteratively refine masked tokens into text in parallel; and our proposed method LaDiR reasons via latent diffusion over thought tokens, enabling iterative refinement at semantic level and diverse solution exploration.

To address this limitation, we introduce LaDiR (Latent Diffusion Reasoner), a flexible reasoning framework that encodes high-level semantic representations of reasoning steps into continuous latent tokens via a Variational Autoencoder (VAE) as latent thought tokens, and trains a latent diffusion model over them to perform reasoning. This bridges the gap between surface-level token refinement and deeper semantic reasoning. After the reasoning process, the model generates final answer tokens conditioned on the generated latent thought tokens.

Our proposed paradigm establishes a new reasoning framework as a post-training method, bringing several distinctive advantages. First, the iterative refinement ability of diffusion enables a better trade-off between accuracy and test-time compute, as additional denoising steps can be flexibly allocated to improve performance. Second, unlike autoregressive models, our approach leverages diffusion models for parallel exploration over blocks of latent tokens and applies repulsion to push examples in the batch apart during inference, enabling the generation of multiple diverse reasoning trajectories. Finally, leveraging a VAE-based latent space enhances interpretability over continuous diffusion models, making the reasoning process more transparent and readable.

Experimentally, we demonstrate that diffusion-based latent reasoning is not only more accurate but also qualitatively different from prior approaches. On math reasoning benchmarks, including GSM8KÂ (Cobbe etÂ al., [2021](#bib.bib12)) and MATHÂ (Hendrycks etÂ al., [2021](#bib.bib26)), where CoconutÂ (Hao etÂ al., [2024](#bib.bib24)) fails to surpass AR CoT supervised finetuning (SFT), LaDiR consistently outperforms it on average across 7 benchmarks with the LLaMA 3.1 8B modelÂ (Dubey etÂ al., [2024](#bib.bib14)). This suggests that modeling reasoning at the semantic level, rather than at the token level, may lead to more faithful intermediate steps that accumulate into stronger final answers. Moreover, on the Countdown planning task, LaDiR shows over 30% absolute improvement in both Pass@1 and Pass@100, indicating that latent thought tokens potentially enhance global planning ability, while parallel diversity exploration enables the model to generate diverse reasoning paths. Together, these findings suggest that diffusion-based latent reasoning provides a principled way to balance accuracy and diversityâ€”key ingredients for advancing beyond sequential autoregressive reasoning.

## 2 Preliminaries

This section introduces key concepts and notations in VAE and latent diffusion modelsÂ (Rombach etÂ al., [2022](#bib.bib51)). Detailed formulations and background information are provided in AppendixÂ [B](#A2 "Appendix B Additional Preliminaries and Background â€£ LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning").

### 2.1 Variational Autoencoder

A Variational Autoencoder (VAE)Â (Kingma & Welling, [2013](#bib.bib36)) learns a latent representation of data by balancing reconstruction accuracy and prior regularization.
We adopt the Î²\beta-VAEÂ (Higgins etÂ al., [2017](#bib.bib28)), where a scaling factor Î²\beta controls this trade-off:

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„’Î²â€‹-VAE=ğ”¼qÏ•â€‹(z|x)â€‹[âˆ’logâ¡pÎ¸â€‹(x|z)]+Î²â€‹KLâ€‹(qÏ•â€‹(z|x)âˆ¥pâ€‹(z)).\mathcal{L}\_{\beta\text{-VAE}}=\mathbb{E}\_{q\_{\phi}(z|x)}[-\log p\_{\theta}(x|z)]+\beta\,\mathrm{KL}\!\big(q\_{\phi}(z|x)\,\|\,p(z)\big). |  | (1) |

Larger Î²\beta values encourage disentangled and structured latent spaces, at the cost of reconstruction fidelity. During inference, the encoder of VAE produces a mean/variance pair {(Î¼,Ïƒ)}\{(\mu,\sigma)\}, and a latent token zz is sampled as z=Î¼+ÏƒâŠ™Ïµ,Ïµâˆ¼ğ’©â€‹(0,I)z=\mu+\sigma\odot\epsilon,\;\epsilon\sim\mathcal{N}(0,I).

### 2.2 Latent Diffusion and Flow Matching

Latent diffusion modelsÂ (Ho etÂ al., [2020](#bib.bib30); Rombach etÂ al., [2022](#bib.bib51)) generate data by denoising latent variables from Gaussian noise in the latent space of a VAE, which preserves high-level semantic structure.
Diffusion can also be viewed as a continuous-time generative flow trained via *flow matching*Â (Lipman etÂ al., [2022](#bib.bib38)), which we adopt as our primary framework for its superior performance (see AppendixÂ [C](#A3 "Appendix C Additional Ablation Studies â€£ LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning")). The training and inference processes are as follows:

##### Training

Let {zt}tâˆˆ[0,1]\{z\_{t}\}\_{t\in[0,1]} denote a path interpolating between clean data z0âˆ¼pdataz\_{0}\sim p\_{\text{data}} and noise Ïµâˆ¼ğ’©â€‹(0,I):zt=(1âˆ’t)â€‹z0+tâ€‹Ïµ\epsilon\sim\mathcal{N}(0,I):z\_{t}=(1-t)z\_{0}+t\epsilon. This path is controlled by an ordinary differential equation (ODE) uâ‹†â€‹(zt,t)=dâ€‹ztdâ€‹t=Ïµâˆ’z0u^{\star}(z\_{t},t)=\frac{dz\_{t}}{dt}=\epsilon-z\_{0}, where uâ‹†u^{\star} is the target velocity field. A neural network uÎ¸â€‹(zt,t)u\_{\theta}(z\_{t},t) is trained to approximate uâ‹†u^{\star} by minimizing the flow matching loss:

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„’FM=ğ”¼tâˆ¼ğ’°â€‹(0,1),z0âˆ¼pdata,z1âˆ¼ğ’©â€‹(0,I)â€‹[â€–uÎ¸â€‹(zt,t)âˆ’uâ‹†â€‹(zt,t)â€–2].\mathcal{L}\_{\text{FM}}=\mathbb{E}\_{t\sim\mathcal{U}(0,1),\,z\_{0}\sim p\_{\text{data}},\,z\_{1}\sim\mathcal{N}(0,I)}\Big[\|u\_{\theta}(z\_{t},t)-u^{\star}(z\_{t},t)\|^{2}\Big]. |  | (2) |

##### Inference.

At generation time, the process begins from Gaussian noise z1âˆ¼ğ’©â€‹(0,I)z\_{1}\sim\mathcal{N}(0,I). The learned velocity field uÎ¸â€‹(zt,t)u\_{\theta}(z\_{t},t) is then integrated backward in time using an ODE solver as follows: ztâˆ’Î”â€‹t=ztâˆ’Î”â€‹tâ€‹uÎ¸â€‹(zt,t)z\_{t-\Delta t}=z\_{t}-\Delta t\,u\_{\theta}(z\_{t},t), with steps from t=1t=1 to t=0t=0. The final state z0z\_{0} corresponds to a clean latent representation. This procedure naturally supports *iterative refinement*, as each integration step progressively transforms noise into a coherent latent zz.

### 2.3 Block Diffusion

To support flexible and variable-length sequence generation, we employ a *block diffusion* schemeÂ (Arriola etÂ al., [2025](#bib.bib1)) that integrates autoregressive modeling with diffusion. Instead of applying diffusion to individual latent tokens or full sequence, the sequence is divided into contiguous blocks, and diffusion is performed at the block level. This hybrid design retains the open-ended generation of autoregressive models while introducing global coherence within each block. See AppendixÂ [B.4](#A2.SS4 "B.4 Block Diffusion â€£ Appendix B Additional Preliminaries and Background â€£ LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning") for details.

## 3 Methodology

![Refer to caption](/html/2510.04573/assets/figs/main_figure.png)

Figure 2: Illustration of our block-wise latent reasoning framework. A question QQ is first input as condition to generate latent blocks, each delimited by <BOT> and <EOT>. For each block, the model iteratively denoises latent tokens ğ™^(b)\hat{\mathbf{Z}}^{(b)} across timesteps, with bidirectional attention inside a block and causal attention across blocks. The reasoning process terminates when the model emits the <SOA> token, after which the model generates the answer text autoregressively.

Our approach separates reasoning from answering. A variational autoencoder (VAE) constructs a latent space of intermediate reasoning steps, encoding each step as a block of *thought tokens*. We further utilize a reasoning model that predicts and refines *thought tokens* via latent diffusion, and then generates the final answer tokens conditioned on the denoised latent tokens.

### 3.1 Architecture

We employ a VAE to construct the latent space of intermediate reasoning steps, and a reasoning model that predicts latent tokens via diffusion and generates the final text answer.

##### Blockization.

We separate the chain-of-thought (CoT) reasoning and the final answer in the dataset using the prefix â€˜â€˜The answer isâ€™â€™. The text preceding the prefix is treated as CoT cc, while the text following is treated as the final answer yy. We then split cc into individual sentences, each treated as a *block* of latent tokens with block size LbL\_{b}:

|  |  |  |
| --- | --- | --- |
|  | ğ™(b)={z1(b),â€¦,zLb(b)},b=1,â€¦,N.\mathbf{Z}^{(b)}=\{z^{(b)}\_{1},\ldots,z^{(b)}\_{L\_{b}}\},\quad b=1,\ldots,N. |  |

This one-sentence-per-block design ensures that each reasoning step is localized in latent space.

##### VAE architecture.

As shown in FigureÂ [2](#S3.F2 "Figure 2 â€£ 3 Methodology â€£ LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning") (left), our VAE encoder is initialized from a pretrained LLM and fine-tuned with all parameters, along with LbL\_{b} learnable embeddings. The encoderâ€™s last hidden state is passed through two linear projections to obtain the mean Î¼\mu and variance Ïƒ2\sigma^{2}, from which we sample ğ™(b)âˆ¼ğ’©â€‹(Î¼,Ïƒ2)\mathbf{Z}^{(b)}\sim\mathcal{N}(\mu,\sigma^{2}). The decoder is a frozen pretrained LLM that conditions on the sampled ğ™(b)\mathbf{Z}^{(b)} to reconstruct the corresponding block of text. This design enables the encoder to compress each reasoning step into a structured latent representation aligned with the semantic space of the language model.

##### Reasoning model architecture.

We utilize an existing LLM as our reasoning model. As illustrated in Fig.Â [2](#S3.F2 "Figure 2 â€£ 3 Methodology â€£ LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning"), consider the prediction of the second latent block. After the input question QQ, we insert a special token <BOT> to mark the start of a block, followed by the first block tokens ğ™(ğŸ)\mathbf{Z^{(1)}}, and a token <EOT> to mark its end. For the second block, since it is being predicted, we add a timestep embedding between <BOT> and z1(2)z^{(2)}\_{1} to encode the timestep information. Once the latent reasoning process is complete, we switch to text generation mode by appending a <SOA> token to indicate the start of the answer, which is then generated autoregressively. To balance lookahead and variable-length generation, we adopt a hybrid attention mask â„³\mathcal{M} (Fig.Â [2](#S3.F2 "Figure 2 â€£ 3 Methodology â€£ LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning"), top right). Within each block, tokens attend *bidirectionally*, enabling the model to internally *reason* over a horizon defined by the block size and capture richer local dependencies. Across blocks, attention is strictly *causal*, so later steps depend on earlier ones in an autoregressive manner.

### 3.2 Training

We train the two components separately: the VAE is first trained to learn latent representations of thought tokens, after which the reasoning model is trained to predict these thought tokens. We describe each stage in turn, beginning with VAE training and followed by reasoning model training.

#### 3.2.1 VAE Training

We build on the standard Î²\beta-VAE training and inference framework described in SectionÂ [2.1](#S2.SS1 "2.1 Variational Autoencoder â€£ 2 Preliminaries â€£ LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning"), with the following adaptations tailored to our task.

##### Robustness augmentations.

To improve generalization and make the latent space resilient to noise and input variability, we introduce two augmentation strategies during training:

* â€¢

  Latent Gaussian noise. For each latent token zi(b)z\_{i}^{(b)}, we inject isotropic Gaussian perturbations:

  |  |  |  |
  | --- | --- | --- |
  |  | zâ€²i(b)=zi(b)+Î·i,Î·iâˆ¼ğ’©â€‹(0,k2â€‹I){z^{\prime}}\_{i}^{(b)}=z\_{i}^{(b)}+\eta\_{i},\quad\eta\_{i}\sim\mathcal{N}(0,k^{2}I) |  |

  where we find k=3k=3 achieve the best downstream performance. This enhances robustness by smoothing the latent space and mitigating sensitivity to small semantic variations.
* â€¢

  Input token substitution. For the encoder input sequence, with probability p=0.3p=0.3 we replace a token with another randomly chosen token (sampled uniformly from the LLM vocabulary). This forces the encoder to learn invariances to paraphrasing, typos, or minor corruptions in input text, ensuring that latent representations capture semantic content rather than exact lexical form.

Together, these augmentations encourage the VAE to build a smoother latent space that is both robust to perturbations and expressive enough to encode thought-level reasoning steps. A more detailed diagram of the VAE can be seen in Appendix [F](#A6 "Appendix F Additional Model Details â€£ LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning").

### 3.3 Reasoning Model Training

After constructing a latent reasoning space with the VAE, we train a latent diffusion model fÏˆf\_{\psi} from the same pretrained LLM as in our VAE model to denoise latent blocks, gradually transforming noisy latent representations to coherent reasoning blocks. Empirically, we observe that training with the flow-matching objective yields the best performance (see AppendixÂ [C](#A3 "Appendix C Additional Ablation Studies â€£ LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning")), and therefore adopt it as our default training objective in the paper.

##### Answer Token Loss.

While fÏˆf\_{\psi} learns to predict latent reasoning trajectories, to avoid explicitly decoding these steps through the VAE decoder for efficiency during inference, we use the same model to autoregressively predict answer text tokens conditioned on the latent reasoning blocks. To this end, given the question qq, the reasoning blocks ğ™(â‰¤B)\mathbf{Z}^{(\leq B)}, and the past answer tokens y<wy\_{<w}, the model predicts the next answer token ywy\_{w} with distribution pÏˆâ€‹(ywâˆ£q,ğ™(â‰¤B),y<w).p\_{\psi}(y\_{w}\mid q,\mathbf{Z}^{(\leq B)},y\_{<w}).
The training objective for those answer tokens is the cross-entropy loss:

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„’Ans=âˆ’âˆ‘w=1Wlogâ¡pÏˆâ€‹(ytâˆ£q,ğ™(â‰¤B),y<w).\mathcal{L}\_{\mathrm{Ans}}=-\sum\_{w=1}^{W}\log p\_{\psi}(y\_{t}\mid q,\mathbf{Z}^{(\leq B)},y\_{<w}). |  | (3) |

##### Special Token Loss.

To explicitly control the number of latent blocks, we introduce a special binary classification head on top of the LLM transformer backbone. It predicts whether the next block begins with a <SOA> (start-of-answer) or <BOT> (begin-of-thought) token whenever an <EOT> (end-of-thought) token is generated. Formally, let Ï„\tau index positions of <EOT> tokens in the output.
For each Ï„\tau, the model produces a distribution
pÏˆâ€‹(sÏ„âˆ£q,ğ™(â‰¤B),yâ‰¤Ï„),sÏ„âˆˆ{<SOA>,<BOT>},p\_{\psi}(s\_{\tau}\mid q,\mathbf{Z}^{(\leq B)},y\_{\leq\tau}),s\_{\tau}\in\{\texttt{<SOA>},\texttt{<BOT>}\},
and we minimize the corresponding classification loss:

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„’Spec=âˆ’âˆ‘Ï„âˆˆğ’¯EOTlogâ¡pÏˆâ€‹(sÏ„âˆ£q,ğ™(â‰¤B),yâ‰¤Ï„).\mathcal{L}\_{\mathrm{Spec}}=-\sum\_{\tau\in\mathcal{T}\_{\text{EOT}}}\log p\_{\psi}(s\_{\tau}\mid q,\mathbf{Z}^{(\leq B)},y\_{\leq\tau}). |  | (4) |

#### 3.3.1 Stage 1: Teacher-forcing training

In the first stage, the model is trained under a *teacher-forcing* regime, where it has access to oracle latent blocks produced by the VAE encoder, denoted as ğ™(1:B)\mathbf{Z}^{(1:B)}. At every step, these oracle latents are concatenated between special tokens <BOT> and <EOT> and provided as context to the flow-matching model fÏˆf\_{\psi}. The overall training objective jointly optimizes flow matching on latent blocks and cross-entropy supervision on both final answers and special tokens:

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„’=Î»FMâ€‹â„’FM+Î»Ansâ€‹â„’Ans+Î»Specâ€‹â„’Spec,\mathcal{L}=\lambda\_{\mathrm{FM}}\,\mathcal{L}\_{\mathrm{FM}}+\lambda\_{\mathrm{Ans}}\,\mathcal{L}\_{\mathrm{Ans}}+\lambda\_{\mathrm{Spec}}\,\mathcal{L}\_{\mathrm{Spec}}, |  | (5) |

where â„’FM\mathcal{L}\_{\mathrm{FM}} is defined in Eq.Â [2](#S2.E2 "Eq. 2 â€£ Training â€£ 2.2 Latent Diffusion and Flow Matching â€£ 2 Preliminaries â€£ LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning").

#### 3.3.2 Stage 2: Rollout training

After Stage 1, there is a mismatch between training and inference. During inference, the model must be conditioned on previous self-generated latents without access to oracle latents, suffering from error accumulation issue. To address this issue, StageÂ 2 adopts an *rollout* training. We keep the same number of blocks BB as in the ground truth, but instead of conditioning on oracle latents, the model generates its own latents ğ™~(1:B)\tilde{\mathbf{Z}}^{(1:B)} from random noise using a fewer denoising steps (i.e., 50â†’1050\rightarrow 10, following FlowGRPOÂ (Liu etÂ al., [2025](#bib.bib40))). We keep the gradients on ğ™~(1:B)\tilde{\mathbf{Z}}^{(1:B)} during denoising, allowing answer supervision to backpropagate through the trajectory and directly shape latent predictions. To avoid latent collapse as in Coconut w/o curriculum learningÂ (Hao etÂ al., [2024](#bib.bib24)), we keep the flow matching loss. Therefore, the training objective is same as Eq.Â [5](#S3.E5 "Eq. 5 â€£ 3.3.1 Stage 1: Teacher-forcing training â€£ 3.3 Reasoning Model Training â€£ 3 Methodology â€£ LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning").

### 3.4 Reasoning Model Inference

At inference time, the model generates a chain of latent reasoning blocks and subsequently produces the final answer in text space. The process unfolds in two phases: (i) latent block generation via iterative denoising, and (ii) answer generation via autoregressive decoding.

##### Iterative denoising.

Following the inference process of the standard latent diffusion model as in [2.2](#S2.SS2 "2.2 Latent Diffusion and Flow Matching â€£ 2 Preliminaries â€£ LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning"), for each block, we initialize with a Gaussian noise, and we gradually transforms the noise into a semantically coherent latent reasoning block ğ™^(b)\hat{\mathbf{Z}}^{(b)}.

##### Stopping criterion.

Latent block generation continues until the model explicitly predicts the special token <SOA> (*start of answer*). This token signals that sufficient reasoning has been performed and the model should transition from block diffusion to final answer generation.

##### Answer generation.

Once reasoning terminates, conditioned on the generated latent reasoning sequence ğ™^(1:B^)\hat{\mathbf{Z}}^{(1:\hat{B})} and the input question xx, the model predicts output tokens y=(y1,â€¦,yT)y=(y\_{1},\ldots,y\_{T}) autoregressively.

##### Diversity improvement in parallel.

Unlike AR models that generate a single reasoning trajectory sequentially, our framework can generate multiple diverse reasoning trajectories in parallel within a batch. To encourage exploration of alternative solutions, we incorporate two complementary mechanisms:

1. 1.

   Increased initial noise.
   By sampling with an increased variance Ïƒ~2\tilde{\sigma}^{2} as initial noise scale, we broaden the distribution of starting points for latent trajectories. This enables the same input question to yield diverse reasoning sequences across runs, improving coverage of alternative solution strategies.
2. 2.

   Diversity gradient guidance.
   At each denoising step, we enhance diversity by adding a repulsion term to push the latent tokens in a batch apart.
   First, we compute a bandwidth parameter Ïƒ\sigma as the median pairwise distance between the latent tokens in a batch at the current step Ïƒ=mediani<jâ¡â€–ziâˆ’zjâ€–2.\sigma\;=\;\operatorname{median}\_{i<j}\;\|z\_{i}-z\_{j}\|\_{2}.

   The repulsion force field for a latent token ziz\_{i} is then defined as

   |  |  |  |  |
   | --- | --- | --- | --- |
   |  | ğ…â€‹(zi)=âˆ‘jâ‰ iâ€„2â€‹(1âˆ’â€–ziâˆ’zjâ€–22Ïƒ2)â€‹expâ¡(âˆ’â€–ziâˆ’zjâ€–22Ïƒ2)â€‹(ziâˆ’zj),âˆ€jâ‰¤B,\mathbf{F}(z\_{i})\;=\;\sum\_{j\neq i}\;2\,\bigg(1-\frac{\|z\_{i}-z\_{j}\|\_{2}^{2}}{\sigma^{2}}\bigg)\exp\!\Big(-\tfrac{\|z\_{i}-z\_{j}\|\_{2}^{2}}{\sigma^{2}}\Big)\,(z\_{i}-z\_{j}),\forall j\leq B, |  | (6) |

   where zjz\_{j} is any other latent token in the same batch with batch size BB. We apply strong repulsion at the beginning of inference and gradually decay its effect over time. Specifically, the time-dependent scale is defined as Î³t=Î³maxâ€‹(tT)\gamma\_{t}=\gamma\_{\max}\left(\tfrac{t}{T}\right), where TT is the total number of inference steps, tt decreases from TT to 0, Î³max\gamma\_{\max} is the initial repulsion strength as a hyperparameter.

   Finally, the diversity-guided prediction combines the base model output with the repulsion gradient, in a form analogous to classifier-free guidanceÂ (Ho & Salimans, [2022](#bib.bib29)): z^tâˆ’1=fÏˆâ€‹(ğ±t,t,x)+Î³tâ€‹ğ…â€‹(z)\hat{z}\_{t-1}\;=\;f\_{\psi}(\mathbf{x}\_{t},t,x)\;+\;\gamma\_{t}\,\mathbf{F}(z), where fÏˆâ€‹(ğ±t,t,x)f\_{\psi}(\mathbf{x}\_{t},t,x) is the modelâ€™s prediction at step tt.

Together, these mechanisms enhance the stochasticity and coverage of latent reasoning while preserving convergence to valid solutions.

## 4 Experiments

We evaluate LaDiR across two domains: mathematical reasoning (7 datasets) and puzzle planning (Countdown), comparing to AR, latent, and diffusion baselines. Our experiments demonstrate its effectiveness on benchmark datasets, while ablation studies in SectionÂ [4.3](#S4.SS3 "4.3 Ablation Study â€£ 4 Experiments â€£ LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning") and analysesÂ [4.4](#S4.SS4 "4.4 Analysis â€£ 4 Experiments â€£ LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning") provide further insights into the contributions of individual components. See AppendixÂ [E](#A5 "Appendix E Experimental Details â€£ LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning") for experimental details.

### 4.1 Mathematical Reasoning

We begin by assessing LaDiR on a range of mathematical reasoning benchmarks, covering both in-domain datasets, where training and test distributions are closely aligned, and out-of-domain benchmarks that require generalization to unseen problems.

##### Datasets

We fine-tune pretrained LLMs on the DART-MATH dataset (Tong etÂ al., [2024b](#bib.bib69)), a large-scale dataset synthesized to enhance mathematical reasoning. For evaluation, we adopt two in-domain benchmarks, Math (Hendrycks etÂ al., [2021](#bib.bib26)) and GSM8K (Cobbe etÂ al., [2021](#bib.bib12)), and five out-of-domain benchmarks to assess generalization: College-MathÂ (Tang etÂ al., [2024b](#bib.bib67)), DeepMind-MathÂ (Saxton etÂ al., [2019](#bib.bib56)), OlympiaBench-MathÂ (He etÂ al., [2024](#bib.bib25)), TheoremQAÂ (Chen etÂ al., [2023](#bib.bib7)), and Fresh-Gaokao-Math-2023Â (Tang etÂ al., [2024b](#bib.bib67)). Detailed dataset descriptions are provided in AppendixÂ [E](#A5 "Appendix E Experimental Details â€£ LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning").

##### Baselines

We compare our approach against a diverse set of reasoning methods spanning both autoregressive and diffusion-based paradigms. For autoregressive models, we include Sol-Only, trained only on questionâ€“solution pairs without intermediate reasoning steps, and CoT, trained with full chain-of-thought supervision. We further evaluate several latent reasoning methods: Implicit CoT (iCoT)Â (Deng etÂ al., [2023](#bib.bib13)), which gradually removes explicit CoT tokens through curriculum learning; Pause TokenÂ (Goyal etÂ al., [2023](#bib.bib22)), which introduces a learnable pause token to provide additional computation before answering; CoconutÂ (Hao etÂ al., [2024](#bib.bib24)), which leverages hidden states as latent reasoning tokens through curriculum learning; and Discrete LatentÂ (Su etÂ al., [2025](#bib.bib64)), which compresses a span of text (e.g., 16 tokens) into discrete latent codes via VQ-VAE for reasoning. For diffusion-based language models, we compare with the open-sourced LLaDA 8BÂ (Nie etÂ al., [2025](#bib.bib47)), evaluated both with and without SFT. We utilize LLaMA-3.1 8BÂ (Dubey etÂ al., [2024](#bib.bib14)) as the backbone model for our framework as well as for the AR baselines for fair comparison.

|  |  |  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Method | In-Domain | | Out-of-Domain | | | | | Avg. |
| MATH | GSM8K | Gaokao | DM-Math | College | Olympia | TheoremQA |
| Diffusion Language Models - LLaDA 8B | | | | | | | | |
| Base Model | 36.2 | 77.3 | 10.2 | 29.8 | 30.2 | 3.0 | 16.6 | 29.0 |
| CoT SFT | 39.0 | 82.3 | 20.1 | 43.7 | 38.9 | 5.9 | 20.9 | 35.8 |
| AR Models - LLaMA 3.1 8B | | | | | | | | |
| Sol-Only SFT | 13.3 | 16.4 | 0.0 | 18.2 | 15.9 | 4.7 | 16.9 | 12.2 |
| CoT SFT | 43.1 | 84.5 | 30.7 | 47.8 | 45.7 | 10.1 | 21.2 | 40.4 |
| iCoT | 35.2 | 61.8 | 30.0 | 30.6 | 37.6 | 8.3 | 19.5 | 31.8 |
| Pause Token | 42.1 | 83.9 | 28.3 | 42.4 | 31.5 | 3.5 | 8.3 | 34.2 |
| Coconut | 37.3 | 68.3 | 26.8 | 33.5 | 40.2 | 5.8 | 11.4 | 31.9 |
| Discrete Latent | 43.2 | 83.9 | 33.3 | 44.7 | 47.1 | 13.3 | 20.3 | 40.8 |
| LaDiR  (ours) | 45.2 | 84.2 | 33.4 | 46.3 | 48.6 | 11.9 | 22.9 | 41.8 |
| â€“w/o Stage 2 | 30.7 | 57.8 | 24.7 | 32.0 | 32.8 | 5.9 | 11.9 | 32.6 |

Table 1: Results for pass@1 accuracy across in-domain and out-of-domain math benchmarks.

##### Results

As shown in TableÂ [1](#S4.T1 "Table 1 â€£ Baselines â€£ 4.1 Mathematical Reasoning â€£ 4 Experiments â€£ LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning"), our method achieves the strongest overall performance on average, improving the average pass@1 accuracy by 2% over the best prior latent approach. Compared to text-based CoT baselines, our latent reasoning consistently yields more robust solutions, particularly on harder benchmarks such as DM-Math and College-level datasets, where direct text reasoning often struggles with long-horizon consistency. This suggests that reasoning in a latent space at semantic level learns more abstract reasoning patterns. Compared to prior latent approaches (e.g., Coconut), the latent diffusion objective provides a more principled objective for modeling continuous trajectories, leading to stronger generalization to out-of-domain settings such as TheoremQA. Also, incorporating the stage 2 training notably improves performance across all benchmarks, showing its effectiveness in mitigating error accumulation. Taken together, these results indicate that LaDiR combines the interpretability benefits of CoT-style reasoning (see AppendixÂ [D](#A4.SS0.SSS0.Px1 "Intepretability â€£ Appendix D Additional Analysis â€£ LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning")) and expressiveness of continuous latent space, producing generalizable reasoning traces.

### 4.2 Puzzle Planning â€“ Countdown

We evaluate the planning ability of our method using *Countdown*, a combinatorial arithmetic game. Given a set of input numbers, the goal is to reach a target in [10,100][10,100] by applying basic operations {+,âˆ’,Ã—,Ã·}\{+,-,\times,\div\}. Solving a problem thus demands decomposing the target into intermediate subgoals and chaining them correctly. For example, given input numbers {97,38,3,17}\{97,38,3,17\} and target 14, one valid solution is:
97âˆ’38=59,â€…59âˆ’17=42,â€…42Ã·3=14.97-38=59,\>59-17=42,\>42\div 3=14.
FollowingÂ Gandhi etÂ al. ([2024](#bib.bib17)), we construct a dataset of 500k examples, holding out 10% of target numbers for *out-of-distribution* evaluation as test set. We study two settings of growing complexity: CD-4 and CD-5, which use four and five input numbers, respectively.

![Refer to caption](/html/2510.04573/assets/figs/plot_line.png)

Figure 3: Results for pass@k performance on Countdown-4 with kâˆˆ{1,10,25,50,100}k\in\{1,10,25,50,100\}.

##### Baselines

We compare our method against both autoregressive and diffusion-based approaches. For the autoregressive setting, we include (1) LLaMA 8B SFT, which shares the same base model as ours and is finetuned on the same dataset. For diffusion-based baselines, we consider (2) LLaDA 8B SFTÂ (Nie etÂ al., [2025](#bib.bib47)) and (3) Dream 7B BaseÂ (Ye etÂ al., [2025b](#bib.bib82)), two diffusion-based general-purpose language models (the latter evaluated without finetuning), as well as (4) MGDMÂ (Ye etÂ al., [2024a](#bib.bib79)), a small task-specific multinomial-guided diffusion model trained for Countdown.

##### Metrics.

We report Pass@1 and Pass@100 accuracy, using an exact string match between the generated arithmetic equations and the ground-truth solution. Pass@k reflects the accuracy that at least one valid solution is found among kk samples. In addition, we report Diversity, measured as the number of unique valid solutions discovered among 100 samples. All models are evaluated with a decoding temperature of 1.01.0.

##### Implementation Details

In this setting, we deliberately disable the answer generation and restrict the reasoning process to a single latent block, which is compressed into a fixed-size representation (4 tokens). The model is trained under a teacher-forcing regime and evaluated on decoded text tokens from our VAE, thereby isolating the latent diffusion modelâ€™s capacity to capture planning dynamics without autoregressive supervision. During inference, we set the initial noise scale to 2 and the maximum diversity guidance scale to 0.8.

| Model | CD-4 P@1 | CD-4 P@100 | CD-4 Div. | CD-5 P@1 | CD-5 P@100 | CD-5 Div. |
| --- | --- | --- | --- | --- | --- | --- |
| Dream 7B Baseâˆ— | 16.0 | 24.7 | 4.1 | 4.2 | 10.3 | 5.6 |
| MGDMâ€  | 91.5 | 95.2 | 3.2 | 46.6 | 70.4 | 4.9 |
| LLaDA 8B SFT | 51.2 | 75.2 | 5.4 | 34.4 | 45.2 | 6.2 |
| LLaMA 8B SFT | 46.7 | 65.3 | 3.0 | 8.9 | 15.4 | 3.5 |
| LaDiR | 76.6 | 96.4 | 7.3 | 38.5 | 75.2 | 8.9 |

Table 2: Results on Countdown tasks. We report Pass@1, Pass@100, and Diversity (Div.). Best results are in bold, and second-best are underlined. âˆ—Dream 7B Base refers to the open-sourced base model without finetuning on this task.â€ MGDM is a task-specific small discrete diffusion model rather than a general-purpose language model.

##### Results

On the Countdown tasks, as shown in TableÂ [2](#S4.T2 "Table 2 â€£ Implementation Details â€£ 4.2 Puzzle Planning â€“ Countdown â€£ 4 Experiments â€£ LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning"), our method outperforms autoregressive baselines and remains competitive with specialized diffusion models. In CD-4, it improves Pass@1 by more than 25 points over LLaMA 8B SFT and over 20 points over LLaDA SFT, demonstrating stronger planning ability beyond token-by-token generation, while also delivering the best Pass@100 and over two points higher diversity than any baseline. On the more challenging CD-5 task, our model surpasses AR baselines by nearly 30 points in Pass@1 and over 30 points in Pass@100, again with the highest diversity. In addition, as shown in FigureÂ [3](#S4.F3 "Figure 3 â€£ 4.2 Puzzle Planning â€“ Countdown â€£ 4 Experiments â€£ LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning"), our pass@k curve rises steeply with kk, surpassing MGDM at larger kk. This high pass@k reflects both diverse trajectory exploration and strong potential for reinforcement learning for post-trainingÂ (Yue etÂ al., [2025](#bib.bib89)).

![Refer to caption](/html/2510.04573/assets/figs/noise_plot.png)

(a) Effect of Initial Noise Scale Ïƒ~2\tilde{\sigma}^{2} (Î³mâ€‹aâ€‹x=0.8\gamma\_{max}=0.8).

![Refer to caption](/html/2510.04573/assets/figs/div_plot.png)

(b) Effect of Max Diversity Scale Î³mâ€‹aâ€‹x\gamma\_{max}(Ïƒ~2=2\tilde{\sigma}^{2}=2).

Figure 4: Ablation study on the hyperparameters for diversity during inference on Countdown-4.

### 4.3 Ablation Study

##### Diversity Scale and Initial Noise

We study how inference-time stochasticity and diversity guidance affect solution diversity and accuracy by varying (i) the *initial noise scale*, which controls the variance of Gaussian initialization, and (ii) the *maximum diversity scale* Î³max\gamma\_{\max}, which regulates the repulsion strength among latent tokens (see Sec.Â [3.4](#S3.SS4 "3.4 Reasoning Model Inference â€£ 3 Methodology â€£ LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning")). We evaluate both the average number of unique solutions and best-of-100 accuracy. TableÂ [4](#S4.F4 "Figure 4 â€£ Results â€£ 4.2 Puzzle Planning â€“ Countdown â€£ 4 Experiments â€£ LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning") shows that increasing noise from 1 to 2 improves both diversity and accuracy, but excessive noise (scale 3) harms convergence despite higher diversity. For diversity guidance, removing repulsion (Î³max=0\gamma\_{\max}=0) yields the lowest diversity, while moderate values (0.30.3â€“0.50.5) strike the best trade-off. Stronger repulsion (Î³maxâ‰¥1.0\gamma\_{\max}\geq 1.0) further boosts diversity but causes accuracy to drop, suggesting that over-dispersing latents destabilizes reasoning.
See AppendixÂ [C](#A3 "Appendix C Additional Ablation Studies â€£ LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning") for additional ablation studies.

### 4.4 Analysis

##### Iterative Refinement

TableÂ [3](#S4.T3 "Table 3 â€£ Iterative Refinement â€£ 4.4 Analysis â€£ 4 Experiments â€£ LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning") shows how the our flow-matching model refines its reasoning across denoising steps. From pure noise at T=1T=1, the model quickly produces structured equations, though early steps contain arithmetic errors (e.g., off-by-one mistakes). As denoising progresses, partial results stabilizeâ€”such as 43+9=5243+9=52 appearing consistently from T=0T=0 onwardâ€”and later steps are gradually corrected. By T=0.25T=0.25, the full reasoning matches the ground truth and remains stable through T=0T=0. This demonstrates that our method exhibits the same iterative refinement ability as reasoning modelsÂ (Shao etÂ al., [2024](#bib.bib57)), progressively correcting previously generated steps. See TableÂ [6](#A4.T6 "Table 6 â€£ Reasoning at Semantic Level â€£ Appendix D Additional Analysis â€£ LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning") for an example on GSM8K.

| Input | 43, 9, 54, 25, 81 |
| --- | --- |
| GT Answer | 43+9=52, 54-25=29, 52+29=81 |
| Decode(Z^t=1\hat{Z}\_{t=1}) | â€œI .. ex1 â€¦â€ (random noise) |
| Decode(Z^t=0.8\hat{Z}\_{t=0.8}) | 43+8=51, 54-24=30, 51+30=81 |
| Decode(Z^t=0.7\hat{Z}\_{t=0.7}) | 43+10=53, 54-25=28, 53+28=81 |
| Decode(Z^t=0.6\hat{Z}\_{t=0.6}) | 43+9=52, 54-27=27, 52+27=79 |
| Decode(Z^t=0.5\hat{Z}\_{t=0.5}) | 43+9=52, 54-25=29, 52+28=80 |
| Decode(Z^t=0.4\hat{Z}\_{t=0.4}) | 43+9=52, 54-25=29, 52+30=82 |
| Decode(Z^t=0.25\hat{Z}\_{t=0.25}) | 43+9=52, 54-25=29, 52+29=81 |
| Decode(Z^t=0\hat{Z}\_{t=0}) | 43+9=52, 54-25=29, 52+29=81 |

Table 3: Examples of iterative self-refinement of decoded text from the VAE decoder on the Countdown-4 dataset across different denoising timesteps (tt).

##### Adaptive Test-Time Compute.

As shown in FigureÂ [5](#S4.F5 "Figure 5 â€£ Adaptive Test-Time Compute. â€£ 4.4 Analysis â€£ 4 Experiments â€£ LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning"), using more denoising steps consistently improves accuracy across different math benchmarks. For example, increasing from 5 to 10 steps (a 2Ã—\times compute increase) yields a large jump of +11.7 points in accuracy on average of 7 benchmarks. Starting from 10 steps, tripling the compute to 30 steps provides an additional +4.8 points on average, while a 5Ã—\times compute increase to 50 steps brings a total gain of +9.8 points on average. These results demonstrate that our method can flexibly trade test-time compute for higher performance as an alternative paradigm in reasoning for long CoT of existing reasoning modelsÂ (Jaech etÂ al., [2024](#bib.bib33); Muennighoff etÂ al., [2025](#bib.bib45); Liu etÂ al., [2024a](#bib.bib39); Li etÂ al., [2025](#bib.bib37)). This may motivate adaptive policies that dynamically assign more denoising steps to harder queries, maximizing the overall accuracyâ€“compute trade-off.

We provide additional analyses on interpretability and semantic-level reasoning in AppendixÂ [D](#A4 "Appendix D Additional Analysis â€£ LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning").

![Refer to caption](/html/2510.04573/assets/figs/test-time.png)

Figure 5: Effect of number of denoising steps on downstream reasoning performance on the math reasoning tasks.

## 5 Related Works

##### Latent Reasoning

Latent reasoning methods address token-level limits of chain-of-thought by enabling reasoning in a latent space, yielding more abstract representations through discrete special tokens that expand internal computation or capture unstated stepsÂ (Herel & Mikolov, [2024](#bib.bib27); Pfau etÂ al., [2024](#bib.bib50); Wang etÂ al., [2024](#bib.bib71); Zelikman etÂ al., [2024](#bib.bib90); Jin etÂ al., [2025](#bib.bib34)). Prior works show that reasoning in latent space, rather than discrete tokens, improves performance by allowing LLMs to generate continuous tokens, either self-generated or provided by an auxiliary modelÂ (Cheng & Durme, [2024](#bib.bib11); Hao etÂ al., [2024](#bib.bib24); Liu etÂ al., [2024b](#bib.bib41); Shen etÂ al., [2025](#bib.bib58); Tack etÂ al., [2025](#bib.bib65); Zhu etÂ al., [2025](#bib.bib97); Butt etÂ al., [2025](#bib.bib5); Zhang etÂ al., [2025](#bib.bib92); Wu etÂ al., [2025](#bib.bib74)). This has been further extended to recurrent or looped architectures that induce latent reasoning internally, removing the need to represent reasoning steps explicitly as tokensÂ (Chen etÂ al., [2025c](#bib.bib10); Geiping etÂ al., [2025](#bib.bib20); Mohtashami etÂ al., [2025](#bib.bib44); Saunshi etÂ al., [2025](#bib.bib55); Yu etÂ al., [2025b](#bib.bib87)). However, prior latent reasoning approaches lack interpretability, as their continuous states are opaque and difficult to understand or control. whereas our method structures the latent space with a VAE, making each step explicit and thus more transparent. Due to the page limit, we discuss further related works in AppendixÂ [A](#A1 "Appendix A Additional Related Works â€£ LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning").

## 6 Conclusion

We introduced LaDiR, a latent diffusion reasoner that utilizes the iterative refinement capability of latent diffusion models to perform reasoning at the semantic level, our framework offers three key benefits: (1) better tradeoff between accuracy and test-time compute through iterative denoising steps with self-refinement, (2) parallel and diverse exploration of reasoning trajectories beyond the limitations of sequential autoregression, and (3) enhanced interpretability through semantically meaningful latent representations. Our experiments on mathematical reasoning and planning benchmarks show that LaDiR consistently outperforms AR and diffusion baselines, achieving both higher accuracy and greater diversity in reasoning.

## Acknowlegement

We thank Apple for their support and sponsorship of this project, as well as for the insightful discussions and constructive feedback provided during the meetings.
This work is supported in part by the NSF Award CCF-2112665 (TILOS), the DARPA AIE program, and the CDC-RFA-FT-23-0069. We thank Vincent Tu for early contributions to this project.

## References

* Arriola etÂ al. (2025)

  Marianne Arriola, Aaron Gokaslan, JustinÂ T Chiu, Zhihan Yang, Zhixuan Qi, Jiaqi Han, SubhamÂ Sekhar Sahoo, and Volodymyr Kuleshov.
  Block diffusion: Interpolating between autoregressive and diffusion language models.
  *arXiv preprint arXiv:2503.09573*, 2025.
* Bachmann & Nagarajan (2025)

  Gregor Bachmann and Vaishnavh Nagarajan.
  The pitfalls of next-token prediction, 2025.
  URL <https://arxiv.org/abs/2403.06963>.
* Bi etÂ al. (2025)

  Zhenni Bi, Kai Han, Chuanjian Liu, Yehui Tang, and Yunhe Wang.
  Forest-of-thought: Scaling test-time compute for enhancing llm reasoning, 2025.
  URL <https://arxiv.org/abs/2412.09078>.
* Black etÂ al. (2024)

  Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, LucyÂ Xiaoyang Shi, James Tanner, Quan Vuong, Anna Walling, Haohuan Wang, and Ury Zhilinsky.
  Ï€0\pi\_{0}: A vision-language-action flow model for general robot control, 2024.
  URL <https://arxiv.org/abs/2410.24164>.
* Butt etÂ al. (2025)

  Natasha Butt, Ariel Kwiatkowski, Ismail Labiad, Julia Kempe, and Yann Ollivier.
  Soft tokens, hard truths.
  *arXiv preprint arXiv:2509.19170*, 2025.
* Chen etÂ al. (2025a)

  Shoufa Chen, Chongjian Ge, Yuqi Zhang, Yida Zhang, Fengda Zhu, Hao Yang, Hongxiang Hao, Hui Wu, Zhichao Lai, Yifei Hu, Ting-Che Lin, Shilong Zhang, FuÂ Li, Chuan Li, Xing Wang, Yanghua Peng, Peize Sun, Ping Luo, YiÂ Jiang, Zehuan Yuan, Bingyue Peng, and Xiaobing Liu.
  Goku: Flow based video generative foundation models, 2025a.
  URL <https://arxiv.org/abs/2502.04896>.
* Chen etÂ al. (2023)

  Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and Tony Xia.
  Theoremqa: A theorem-driven question answering dataset.
  *arXiv preprint arXiv:2305.12524*, 2023.
* Chen etÂ al. (2025b)

  Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan.
  Janus-pro: Unified multimodal understanding and generation with data and model scaling, 2025b.
  URL <https://arxiv.org/abs/2501.17811>.
* Chen etÂ al. (2024)

  Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, etÂ al.
  Do not think that much for 2+ 3=? on the overthinking of o1-like llms.
  *arXiv preprint arXiv:2412.21187*, 2024.
* Chen etÂ al. (2025c)

  Yilong Chen, Junyuan Shang, Zhenyu Zhang, Yanxi Xie, Jiawei Sheng, Tingwen Liu, Shuohuan Wang, YuÂ Sun, Hua Wu, and Haifeng Wang.
  Inner thinking transformer: Leveraging dynamic depth scaling to foster adaptive internal thinking, 2025c.
  URL <https://arxiv.org/abs/2502.13842>.
* Cheng & Durme (2024)

  Jeffrey Cheng and BenjaminÂ Van Durme.
  Compressed chain of thought: Efficient reasoning through dense representations, 2024.
  URL <https://arxiv.org/abs/2412.13171>.
* Cobbe etÂ al. (2021)

  Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, etÂ al.
  Training verifiers to solve math word problems.
  *arXiv preprint arXiv:2110.14168*, 2021.
* Deng etÂ al. (2023)

  Yuntian Deng, Kiran Prasad, Roland Fernandez, Paul Smolensky, Vishrav Chaudhary, and Stuart Shieber.
  Implicit chain of thought reasoning via knowledge distillation.
  *arXiv preprint arXiv:2311.01460*, 2023.
* Dubey etÂ al. (2024)

  Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, etÂ al.
  The llama 3 herd of models.
  *arXiv e-prints*, pp. arXivâ€“2407, 2024.
* Dziri etÂ al. (2023)

  Nouha Dziri, Ximing Lu, Melanie Sclar, XiangÂ Lorraine Li, Liwei Jiang, BillÂ Yuchen Lin, Peter West, Chandra Bhagavatula, RonanÂ Le Bras, JenaÂ D. Hwang, Soumya Sanyal, Sean Welleck, Xiang Ren, Allyson Ettinger, Zaid Harchaoui, and Yejin Choi.
  Faith and fate: Limits of transformers on compositionality, 2023.
  URL <https://arxiv.org/abs/2305.18654>.
* Fan etÂ al. (2024)

  Lijie Fan, Tianhong Li, Siyang Qin, Yuanzhen Li, Chen Sun, Michael Rubinstein, Deqing Sun, Kaiming He, and Yonglong Tian.
  Fluid: Scaling autoregressive text-to-image generative models with continuous tokens, 2024.
  URL <https://arxiv.org/abs/2410.13863>.
* Gandhi etÂ al. (2024)

  Kanishk Gandhi, Denise Lee, Gabriel Grand, Muxin Liu, Winson Cheng, Archit Sharma, and NoahÂ D Goodman.
  Stream of search (sos): Learning to search in language.
  *arXiv preprint arXiv:2404.03683*, 2024.
* Gandhi etÂ al. (2025)

  Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, and NoahÂ D. Goodman.
  Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars, 2025.
  URL <https://arxiv.org/abs/2503.01307>.
* Gao etÂ al. (2024)

  Silin Gao, Mete Ismayilzada, Mengjie Zhao, Hiromi Wakaki, Yuki Mitsufuji, and Antoine Bosselut.
  Diffucomet: Contextual commonsense knowledge diffusion, 2024.
  URL <https://arxiv.org/abs/2402.17011>.
* Geiping etÂ al. (2025)

  Jonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, BrianÂ R. Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, and Tom Goldstein.
  Scaling up test-time compute with latent reasoning: A recurrent depth approach, 2025.
  URL <https://arxiv.org/abs/2502.05171>.
* Gong etÂ al. (2025)

  Shansan Gong, Shivam Agarwal, Yizhe Zhang, Jiacheng Ye, Lin Zheng, Mukai Li, Chenxin An, Peilin Zhao, Wei Bi, Jiawei Han, Hao Peng, and Lingpeng Kong.
  Scaling diffusion language models via adaptation from autoregressive models, 2025.
  URL <https://arxiv.org/abs/2410.17891>.
* Goyal etÂ al. (2023)

  Sachin Goyal, Ziwei Ji, AnkitÂ Singh Rawat, AdityaÂ Krishna Menon, Sanjiv Kumar, and Vaishnavh Nagarajan.
  Think before you speak: Training language models with pause tokens.
  *arXiv preprint arXiv:2310.02226*, 2023.
* Gulrajani & Hashimoto (2023)

  Ishaan Gulrajani and TatsunoriÂ B Hashimoto.
  Likelihood-based diffusion language models.
  *Advances in Neural Information Processing Systems*, 36:16693â€“16715, 2023.
* Hao etÂ al. (2024)

  Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian.
  Training large language models to reason in a continuous latent space, 2024.
  URL <https://arxiv.org/abs/2412.06769>.
* He etÂ al. (2024)

  Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, ZhenÂ Leng Thai, Junhao Shen, Jinyi Hu, XuÂ Han, Yujie Huang, Yuxiang Zhang, etÂ al.
  Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems.
  *arXiv preprint arXiv:2402.14008*, 2024.
* Hendrycks etÂ al. (2021)

  Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt.
  Measuring mathematical problem solving with the math dataset.
  *arXiv preprint arXiv:2103.03874*, 2021.
* Herel & Mikolov (2024)

  David Herel and Tomas Mikolov.
  Thinking tokens for language modeling, 2024.
  URL <https://arxiv.org/abs/2405.08644>.
* Higgins etÂ al. (2017)

  Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner.
  beta-vae: Learning basic visual concepts with a constrained variational framework.
  In *International conference on learning representations*, 2017.
* Ho & Salimans (2022)

  Jonathan Ho and Tim Salimans.
  Classifier-free diffusion guidance.
  *arXiv preprint arXiv:2207.12598*, 2022.
* Ho etÂ al. (2020)

  Jonathan Ho, Ajay Jain, and Pieter Abbeel.
  Denoising diffusion probabilistic models.
  *Advances in neural information processing systems*, 33:6840â€“6851, 2020.
* Huang etÂ al. (2024)

  Jie Huang, Xinyun Chen, Swaroop Mishra, HuaixiuÂ Steven Zheng, AdamsÂ Wei Yu, Xinying Song, and Denny Zhou.
  Large language models cannot self-correct reasoning yet, 2024.
  URL <https://arxiv.org/abs/2310.01798>.
* Israel etÂ al. (2025)

  Daniel Israel, GuyÂ Van den Broeck, and Aditya Grover.
  Accelerating diffusion llms via adaptive parallel decoding, 2025.
  URL <https://arxiv.org/abs/2506.00413>.
* Jaech etÂ al. (2024)

  Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, etÂ al.
  Openai o1 system card.
  *arXiv preprint arXiv:2412.16720*, 2024.
* Jin etÂ al. (2025)

  Mingyu Jin, Weidi Luo, Sitao Cheng, Xinyi Wang, Wenyue Hua, Ruixiang Tang, WilliamÂ Yang Wang, and Yongfeng Zhang.
  Disentangling memory and reasoning ability in large language models, 2025.
  URL <https://arxiv.org/abs/2411.13504>.
* Khot etÂ al. (2023)

  Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal.
  Decomposed prompting: A modular approach for solving complex tasks, 2023.
  URL <https://arxiv.org/abs/2210.02406>.
* Kingma & Welling (2013)

  DiederikÂ P Kingma and Max Welling.
  Auto-encoding variational bayes.
  *arXiv preprint arXiv:1312.6114*, 2013.
* Li etÂ al. (2025)

  Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, etÂ al.
  From system 1 to system 2: A survey of reasoning large language models.
  *arXiv preprint arXiv:2502.17419*, 2025.
* Lipman etÂ al. (2022)

  Yaron Lipman, RickyÂ TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le.
  Flow matching for generative modeling.
  *arXiv preprint arXiv:2210.02747*, 2022.
* Liu etÂ al. (2024a)

  Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, etÂ al.
  Deepseek-v3 technical report.
  *arXiv preprint arXiv:2412.19437*, 2024a.
* Liu etÂ al. (2025)

  Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, DiÂ Zhang, and Wanli Ouyang.
  Flow-grpo: Training flow matching models via online rl.
  *arXiv preprint arXiv:2505.05470*, 2025.
* Liu etÂ al. (2024b)

  Tianqiao Liu, Zui Chen, Zitao Liu, MiÂ Tian, and Weiqi Luo.
  Expediting and elevating large language model reasoning via hidden chain-of-thought decoding, 2024b.
  URL <https://arxiv.org/abs/2409.08561>.
* Lou etÂ al. (2023)

  Aaron Lou, Chenlin Meng, and Stefano Ermon.
  Discrete diffusion language modeling by estimating the ratios of the data distribution.
  *arXiv preprint arXiv:2310.16834*, 2023.
* Lovelace etÂ al. (2024)

  Justin Lovelace, Varsha Kishore, Yiwei Chen, and KilianÂ Q Weinberger.
  Diffusion guided language modeling.
  *arXiv preprint arXiv:2408.04220*, 2024.
* Mohtashami etÂ al. (2025)

  Amirkeivan Mohtashami, Matteo Pagliardini, and Martin Jaggi.
  CoTFormer: A chain of thought driven architecture with budget-adaptive computation cost at inference.
  In *The Thirteenth International Conference on Learning Representations*, 2025.
  URL <https://openreview.net/forum?id=7igPXQFupX>.
* Muennighoff etÂ al. (2025)

  Niklas Muennighoff, Zitong Yang, Weijia Shi, XiangÂ Lisa Li, LiÂ Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel CandÃ¨s, and Tatsunori Hashimoto.
  s1: Simple test-time scaling.
  *arXiv preprint arXiv:2501.19393*, 2025.
* Naik etÂ al. (2024)

  Ranjita Naik, Varun Chandrasekaran, Mert Yuksekgonul, Hamid Palangi, and Besmira Nushi.
  Diversity of thought improves reasoning abilities of llms, 2024.
  URL <https://arxiv.org/abs/2310.07088>.
* Nie etÂ al. (2025)

  Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, and Chongxuan Li.
  Large language diffusion models, 2025.
  URL <https://arxiv.org/abs/2502.09992>.
* Nye etÂ al. (2021)

  Maxwell Nye, AndersÂ Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena.
  Show your work: Scratchpads for intermediate computation with language models, 2021.
  URL <https://arxiv.org/abs/2112.00114>.
* Pan etÂ al. (2025)

  Xichen Pan, SatyaÂ Narayan Shukla, Aashu Singh, Zhuokai Zhao, ShlokÂ Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, JiÂ Hou, and Saining Xie.
  Transfer between modalities with metaqueries, 2025.
  URL <https://arxiv.org/abs/2504.06256>.
* Pfau etÂ al. (2024)

  Jacob Pfau, William Merrill, and SamuelÂ R. Bowman.
  Letâ€™s think dot by dot: Hidden computation in transformer language models, 2024.
  URL <https://arxiv.org/abs/2404.15758>.
* Rombach etÂ al. (2022)

  Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer.
  High-resolution image synthesis with latent diffusion models.
  In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pp. 10684â€“10695, 2022.
* Saha etÂ al. (2025)

  Swarnadeep Saha, Xian Li, Marjan Ghazvininejad, Jason Weston, and Tianlu Wang.
  Learning to plan & reason for evaluation with thinking-llm-as-a-judge, 2025.
  URL <https://arxiv.org/abs/2501.18099>.
* Sahoo etÂ al. (2024)

  Subham Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin Chiu, Alexander Rush, and Volodymyr Kuleshov.
  Simple and effective masked diffusion language models.
  *Advances in Neural Information Processing Systems*, 37:130136â€“130184, 2024.
* Salimans & Ho (2022)

  Tim Salimans and Jonathan Ho.
  Progressive distillation for fast sampling of diffusion models.
  *arXiv preprint arXiv:2202.00512*, 2022.
* Saunshi etÂ al. (2025)

  Nikunj Saunshi, Nishanth Dikkala, Zhiyuan Li, Sanjiv Kumar, and SashankÂ J. Reddi.
  Reasoning with latent thoughts: On the power of looped transformers, 2025.
  URL <https://arxiv.org/abs/2502.17416>.
* Saxton etÂ al. (2019)

  David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli.
  Analysing mathematical reasoning abilities of neural models.
  *arXiv preprint arXiv:1904.01557*, 2019.
* Shao etÂ al. (2024)

  Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y.Â K. Li, Y.Â Wu, and Daya Guo.
  Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024.
  URL <https://arxiv.org/abs/2402.03300>.
* Shen etÂ al. (2025)

  Zhenyi Shen, Hanqi Yan, Linhai Zhang, Zhanghao Hu, Yali Du, and Yulan He.
  Codi: Compressing chain-of-thought into continuous space via self-distillation, 2025.
  URL <https://arxiv.org/abs/2502.21074>.
* Shi etÂ al. (2025a)

  Jiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, and MichalisÂ K. Titsias.
  Simplified and generalized masked diffusion for discrete data, 2025a.
  URL <https://arxiv.org/abs/2406.04329>.
* Shi etÂ al. (2025b)

  Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, XiÂ Victoria Lin, Luke Zettlemoyer, and Lili Yu.
  Lmfusion: Adapting pretrained language models for multimodal generation, 2025b.
  URL <https://arxiv.org/abs/2412.15188>.
* Shinn etÂ al. (2023)

  Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao.
  Reflexion: Language agents with verbal reinforcement learning, 2023.
  URL <https://arxiv.org/abs/2303.11366>.
* Song etÂ al. (2020)

  Jiaming Song, Chenlin Meng, and Stefano Ermon.
  Denoising diffusion implicit models.
  *arXiv preprint arXiv:2010.02502*, 2020.
* Song etÂ al. (2025)

  Yuxuan Song, Zheng Zhang, Cheng Luo, Pengyang Gao, Fan Xia, Hao Luo, Zheng Li, Yuehang Yang, Hongli Yu, Xingwei Qu, Yuwei Fu, Jing Su, GeÂ Zhang, Wenhao Huang, Mingxuan Wang, Lin Yan, Xiaoying Jia, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Yonghui Wu, and Hao Zhou.
  Seed diffusion: A large-scale diffusion language model with high-speed inference, 2025.
  URL <https://arxiv.org/abs/2508.02193>.
* Su etÂ al. (2025)

  DiJia Su, Hanlin Zhu, Yingchen Xu, Jiantao Jiao, Yuandong Tian, and Qinqing Zheng.
  Token assorted: Mixing latent and text tokens for improved language model reasoning.
  *arXiv preprint arXiv:2502.03275*, 2025.
* Tack etÂ al. (2025)

  Jihoon Tack, Jack Lanchantin, Jane Yu, Andrew Cohen, Ilia Kulikov, Janice Lan, Shibo Hao, Yuandong Tian, Jason Weston, and Xian Li.
  Llm pretraining with continuous concepts, 2025.
  URL <https://arxiv.org/abs/2502.08524>.
* Tang etÂ al. (2024a)

  Haotian Tang, Yecheng Wu, Shang Yang, Enze Xie, Junsong Chen, Junyu Chen, Zhuoyang Zhang, Han Cai, Yao Lu, and Song Han.
  Hart: Efficient visual generation with hybrid autoregressive transformer, 2024a.
  URL <https://arxiv.org/abs/2410.10812>.
* Tang etÂ al. (2024b)

  Zhengyang Tang, Xingxing Zhang, Benyou Wang, and Furu Wei.
  Mathscale: Scaling instruction tuning for mathematical reasoning.
  *arXiv preprint arXiv:2403.02884*, 2024b.
* Tong etÂ al. (2024a)

  Shengbang Tong, David Fan, Jiachen Zhu, Yunyang Xiong, Xinlei Chen, Koustuv Sinha, Michael Rabbat, Yann LeCun, Saining Xie, and Zhuang Liu.
  Metamorph: Multimodal understanding and generation via instruction tuning, 2024a.
  URL <https://arxiv.org/abs/2412.14164>.
* Tong etÂ al. (2024b)

  Yuxuan Tong, Xiwen Zhang, Rui Wang, Ruidong Wu, and Junxian He.
  Dart-math: Difficulty-aware rejection tuning for mathematical problem-solving.
  *Advances in Neural Information Processing Systems*, 37:7821â€“7846, 2024b.
* van Krieken etÂ al. (2025)

  Emile van Krieken, Pasquale Minervini, Edoardo Ponti, and Antonio Vergari.
  Neurosymbolic diffusion models, 2025.
  URL <https://arxiv.org/abs/2505.13138>.
* Wang etÂ al. (2024)

  Xinyi Wang, Lucas Caccia, Oleksiy Ostapenko, Xingdi Yuan, WilliamÂ Yang Wang, and Alessandro Sordoni.
  Guiding language model reasoning with planning tokens.
  In *First Conference on Language Modeling*, 2024.
  URL <https://openreview.net/forum?id=wi9IffRhVM>.
* Wei etÂ al. (2023)

  Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, EdÂ Chi, Quoc Le, and Denny Zhou.
  Chain-of-thought prompting elicits reasoning in large language models, 2023.
  URL <https://arxiv.org/abs/2201.11903>.
* Weligalle (2025)

  Ashen Weligalle.
  Discrete diffusion models for language generation.
  *arXiv preprint arXiv:2507.07050*, 2025.
* Wu etÂ al. (2025)

  ChÃ¼nhung Wu, Jinliang Lu, Zixuan Ren, Gangqiang Hu, Zhi Wu, Dai Dai, and Hua Wu.
  Llms are single-threaded reasoners: Demystifying the working mechanism of soft thinking.
  *arXiv preprint arXiv:2508.03440*, 2025.
* Xiao etÂ al. (2024)

  Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu.
  Omnigen: Unified image generation, 2024.
  URL <https://arxiv.org/abs/2409.11340>.
* Xie etÂ al. (2025)

  Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo.
  Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning, 2025.
  URL <https://arxiv.org/abs/2502.14768>.
* Xie etÂ al. (2023)

  Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, XuÂ Zhao, Min-Yen Kan, Junxian He, and Qizhe Xie.
  Self-evaluation guided beam search for reasoning, 2023.
  URL <https://arxiv.org/abs/2305.00633>.
* Yao etÂ al. (2023)

  Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, ThomasÂ L. Griffiths, Yuan Cao, and Karthik Narasimhan.
  Tree of thoughts: Deliberate problem solving with large language models, 2023.
  URL <https://arxiv.org/abs/2305.10601>.
* Ye etÂ al. (2024a)

  Jiacheng Ye, Jiahui Gao, Shansan Gong, Lin Zheng, Xin Jiang, Zhenguo Li, and Lingpeng Kong.
  Beyond autoregression: Discrete diffusion for complex reasoning and planning.
  *arXiv preprint arXiv:2410.14157*, 2024a.
* Ye etÂ al. (2024b)

  Jiacheng Ye, Shansan Gong, Liheng Chen, Lin Zheng, Jiahui Gao, Han Shi, Chuan Wu, Xin Jiang, Zhenguo Li, Wei Bi, and Lingpeng Kong.
  Diffusion of thoughts: Chain-of-thought reasoning in diffusion language models, 2024b.
  URL <https://arxiv.org/abs/2402.07754>.
* Ye etÂ al. (2025a)

  Jiacheng Ye, Jiahui Gao, Shansan Gong, Lin Zheng, Xin Jiang, Zhenguo Li, and Lingpeng Kong.
  Beyond autoregression: Discrete diffusion for complex reasoning and planning, 2025a.
  URL <https://arxiv.org/abs/2410.14157>.
* Ye etÂ al. (2025b)

  Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong.
  Dream 7b: Diffusion large language models.
  *arXiv preprint arXiv:2508.15487*, 2025b.
* Ye etÂ al. (2025c)

  Jiasheng Ye, Zaixiang Zheng, YuÂ Bao, Lihua Qian, and Quanquan Gu.
  Diffusion language models can perform many tasks with scaling and instruction-finetuning, 2025c.
  URL <https://arxiv.org/abs/2308.12219>.
* Yu etÂ al. (2024)

  Fangxu Yu, Lai Jiang, Haoqiang Kang, Shibo Hao, and Lianhui Qin.
  Flow of reasoning: Efficient training of llm policy with divergent thinking.
  *arXiv preprint arXiv:2406.05673*, 1(2):6, 2024.
* Yu etÂ al. (2025a)

  Fangxu Yu, Lai Jiang, Haoqiang Kang, Shibo Hao, and Lianhui Qin.
  Flow of reasoning: Training llms for divergent reasoning with minimal examples, 2025a.
  URL <https://arxiv.org/abs/2406.05673>.
* Yu etÂ al. (2023)

  Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, YuÂ Zhang, JamesÂ T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu.
  Metamath: Bootstrap your own mathematical questions for large language models.
  *arXiv preprint arXiv:2309.12284*, 2023.
* Yu etÂ al. (2025b)

  Qifan Yu, Zhenyu He, Sijie Li, Xun Zhou, Jun Zhang, Jingjing Xu, and DiÂ He.
  Enhancing auto-regressive chain-of-thought through loop-aligned reasoning, 2025b.
  URL <https://arxiv.org/abs/2502.08482>.
* Yu etÂ al. (2025c)

  Runpeng Yu, Xinyin Ma, and Xinchao Wang.
  Dimple: Discrete diffusion multimodal large language model with parallel decoding.
  *arXiv preprint arXiv:2505.16990*, 2025c.
* Yue etÂ al. (2025)

  Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang.
  Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model?
  *arXiv preprint arXiv:2504.13837*, 2025.
* Zelikman etÂ al. (2024)

  Eric Zelikman, GeorgesÂ Raif Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah Goodman.
  Quiet-STar: Language models can teach themselves to think before speaking.
  In *First Conference on Language Modeling*, 2024.
  URL <https://openreview.net/forum?id=oRXPiSOGH9>.
* Zhang etÂ al. (2024)

  Dan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, and Jie Tang.
  ReST-MCTS\*: LLM self-training via process reward guided tree search.
  In *The Thirty-eighth Annual Conference on Neural Information Processing Systems*, 2024.
  URL <https://openreview.net/forum?id=8rcFOqEud5>.
* Zhang etÂ al. (2025)

  Zhen Zhang, Xuehai He, Weixiang Yan, AoÂ Shen, Chenyang Zhao, Shuohang Wang, Yelong Shen, and XinÂ Eric Wang.
  Soft thinking: Unlocking the reasoning potential of llms in continuous concept space.
  *arXiv preprint arXiv:2505.15778*, 2025.
* Zheng etÂ al. (2024)

  Lin Zheng, Jianbo Yuan, Lei Yu, and Lingpeng Kong.
  A reparameterized discrete diffusion model for text generation, 2024.
  URL <https://arxiv.org/abs/2302.05737>.
* Zhou etÂ al. (2024a)

  Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy.
  Transfusion: Predict the next token and diffuse images with one multi-modal model, 2024a.
  URL <https://arxiv.org/abs/2408.11039>.
* Zhou etÂ al. (2023)

  Denny Zhou, Nathanael SchÃ¤rli, LeÂ Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, and EdÂ Chi.
  Least-to-most prompting enables complex reasoning in large language models, 2023.
  URL <https://arxiv.org/abs/2205.10625>.
* Zhou etÂ al. (2024b)

  Zixuan Zhou, Xuefei Ning, KeÂ Hong, Tianyu Fu, Jiaming Xu, Shiyao Li, Yuming Lou, Luning Wang, Zhihang Yuan, Xiuhong Li, Shengen Yan, Guohao Dai, Xiao-Ping Zhang, Yuhan Dong, and YuÂ Wang.
  A survey on efficient inference for large language models, 2024b.
  URL <https://arxiv.org/abs/2404.14294>.
* Zhu etÂ al. (2025)

  Hanlin Zhu, Shibo Hao, Zhiting Hu, Jiantao Jiao, Stuart Russell, and Yuandong Tian.
  Reasoning by superposition: A theoretical perspective on chain of continuous thought.
  *arXiv preprint arXiv:2505.12514*, 2025.

## Appendix A Additional Related Works

##### Diffusion Language Models for Text Reasoning

Masked diffusion language models attempt to address some common limitations of autoregressive LLMsâ€”such as rigid left-to-right decoding and inefficiencyâ€”by iteratively denoising masked tokens, enabling parallel and order-agnostic text generation. Prior studies show that these models achieve better inference efficiency compared to AR models while maintaining comparable performance on both general tasksÂ (Zheng etÂ al., [2024](#bib.bib93); Gong etÂ al., [2025](#bib.bib21); Nie etÂ al., [2025](#bib.bib47); Shi etÂ al., [2025a](#bib.bib59); Song etÂ al., [2025](#bib.bib63); Ye etÂ al., [2025c](#bib.bib83); [b](#bib.bib82)) and reasoning benchmarks with chain-of-thoughtÂ (Gao etÂ al., [2024](#bib.bib19); Ye etÂ al., [2024b](#bib.bib80); van Krieken etÂ al., [2025](#bib.bib70); Ye etÂ al., [2025a](#bib.bib81)).
However, these approaches remain constrained to language space, unable to capture reasoning at an abstract semantic level or revise previously generated tokens as continuous diffusion modelsÂ (Ho etÂ al., [2020](#bib.bib30); Song etÂ al., [2020](#bib.bib62)) do, and they require training on massive datasets rather than leveraging a well-trained LLM. In contrast, our method overcomes these limitations by structuring reasoning in an interpretable continuous latent space, producing abstract CoT representations with self-correction ability for an existing LLM, while keeping diffusionâ€™s strengths in parallel generation to enhance exploration and diversity.

##### CoT Reasoning

Chain-of-thought reasoning refers to methods which elicit LLMs to generate intermediate reasoning steps in language prior to outputting a final answer in order to improve performance on reasoning tasks. This can be accomplished via prompting methods Â (Nye etÂ al., [2021](#bib.bib48); Wei etÂ al., [2023](#bib.bib72); Khot etÂ al., [2023](#bib.bib35); Zhou etÂ al., [2023](#bib.bib95)) or through training LLMs (by SFT, RL, or a combination of the two) to output the intermediate reasoning steps Â (Yu etÂ al., [2023](#bib.bib86); Shao etÂ al., [2024](#bib.bib57); Yu etÂ al., [2025a](#bib.bib85)). Works have also extended CoT to allow LLMs to mimic various tree search algorithms such as BFS or MCTS, which especially improves performance on more complex tasks Â (Xie etÂ al., [2023](#bib.bib77); Yao etÂ al., [2023](#bib.bib78); Zhang etÂ al., [2024](#bib.bib91); Bi etÂ al., [2025](#bib.bib3)). Beyond following specific algorithms, works that implement long chain-of-thought (combining extensive reasoning, exploration, and reflection) have also demonstrated improved reasoning performance Â (Shinn etÂ al., [2023](#bib.bib61); Gandhi etÂ al., [2025](#bib.bib18); Saha etÂ al., [2025](#bib.bib52); Xie etÂ al., [2025](#bib.bib76)). One overarching limiting factor with these CoT methods is that they fundamentally work at a next-token-prediction level, constraining the outputs to the token space and limiting the modelâ€™s horizon.

##### Hybrid AR+Diffusion Model Architecture

Other AR-Diffusion hybrid models have shown successful results in rivaling their AR and diffusion counterparts, particularly in multimodal generation and image understanding. The Transfusion Â (Zhou etÂ al., [2024a](#bib.bib94)) architecture demonstrated that hybrid models could outperform standard AR models and compete with state-of-the-art diffusion models in image-generation benchmarks, a phenomenon further reinforced by other studies of hybrid models Â (Fan etÂ al., [2024](#bib.bib16); Tang etÂ al., [2024a](#bib.bib66); Xiao etÂ al., [2024](#bib.bib75)). This extends beyond image generation, with several works demonstrating the effectiveness of hybrid AR-diffusion models in other domains such as image understanding, video generation, and robot control Â (Black etÂ al., [2024](#bib.bib4); Tong etÂ al., [2024a](#bib.bib68); Chen etÂ al., [2025a](#bib.bib6); [b](#bib.bib8)). Furthermoreâ€“similar to our model architectureâ€“works have demonstrated successful adaptations of frozen models for these hybrid AR-diffusion archictures in multimodal domains Â (Pan etÂ al., [2025](#bib.bib49); Shi etÂ al., [2025b](#bib.bib60)). Aside from the difference in domain from these works, many do not use block diffusion for variable-length generations as in LaDiR and we critically introduce CE loss to guide better latent predictions.

## Appendix B Additional Preliminaries and Background

We provide more details about the background information of VAE and Diffusion models in this section.

### B.1 Variational Autoencoder and Î²\beta-VAE

The Variational Autoencoder (VAE)Â (Kingma & Welling, [2013](#bib.bib36)) is a latent-variable model that learns a compressed representation of data xx through an encoderâ€“decoder pair.
The encoder qÏ•â€‹(z|x)q\_{\phi}(z|x) maps inputs into a distribution over latent variables zz, typically parameterized as a diagonal Gaussian.
The decoder pÎ¸â€‹(x|z)p\_{\theta}(x|z) reconstructs the input from zz, enabling generative sampling.
Training maximizes the evidence lower bound (ELBO):

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„’VAE=ğ”¼qÏ•â€‹(z|x)â€‹[âˆ’logâ¡pÎ¸â€‹(x|z)]+KLâ€‹(qÏ•â€‹(z|x)âˆ¥pâ€‹(z)),\mathcal{L}\_{\text{VAE}}=\mathbb{E}\_{q\_{\phi}(z|x)}[-\log p\_{\theta}(x|z)]+\mathrm{KL}\!\big(q\_{\phi}(z|x)\,\|\,p(z)\big), |  | (7) |

where the first term ensures faithful reconstruction and the second term regularizes the posterior toward a simple prior pâ€‹(z)p(z), usually ğ’©â€‹(0,I)\mathcal{N}(0,I).
The reparameterization trick,

|  |  |  |
| --- | --- | --- |
|  | z=Î¼Ï•â€‹(x)+ÏƒÏ•â€‹(x)âŠ™Ïµ,Ïµâˆ¼ğ’©â€‹(0,I),z=\mu\_{\phi}(x)+\sigma\_{\phi}(x)\odot\epsilon,\quad\epsilon\sim\mathcal{N}(0,I), |  |

enables low-variance gradient estimates for stochastic optimization.

##### Î²\beta-VAE.

The Î²\beta-VAEÂ (Higgins etÂ al., [2017](#bib.bib28)) introduces a hyperparameter Î²\beta to control the KL weight:

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„’Î²â€‹-VAE=ğ”¼qÏ•â€‹(z|x)â€‹[âˆ’logâ¡pÎ¸â€‹(x|z)]+Î²â€‹KLâ€‹(qÏ•â€‹(z|x)âˆ¥pâ€‹(z)).\mathcal{L}\_{\beta\text{-VAE}}=\mathbb{E}\_{q\_{\phi}(z|x)}[-\log p\_{\theta}(x|z)]+\beta\,\mathrm{KL}\!\big(q\_{\phi}(z|x)\,\|\,p(z)\big). |  | (8) |

When Î²>1\beta>1, the model enforces stronger alignment to the prior, which encourages disentangled and interpretable latent variables at the expense of reconstruction fidelity.
This property is desirable when latent codes are later used as the substrate for generative modeling.

##### Why VAE for Latent Diffusion.

Latent diffusion models (LDMs)Â (Rombach etÂ al., [2022](#bib.bib51)) operate not on raw high-dimensional inputs (e.g., images or sequences), but in a compressed latent space learned by a VAE.
This design provides three key advantages:

1. 1.

   Efficiency. Operating in latent space reduces dimensionality, leading to faster training and inference while maintaining semantic richness.
2. 2.

   Semantic abstraction. The VAE learns to discard imperceptible details and retain high-level structure, making diffusion steps focus on meaningful features rather than pixel-level noise.
3. 3.

   Flexibility. The decoder pÎ¸â€‹(x|z)p\_{\theta}(x|z) ensures that even when denoising occurs in latent space, the final output remains in the original input domain. This separation enables diffusion to generalize across modalities with a shared latent backbone.

### B.2 Latent Diffusion: Training and Inference

Latent diffusion operates in the compressed latent space z0z\_{0} of a pretrained VAE.

##### Forward process.

Noise is added gradually:

|  |  |  |
| --- | --- | --- |
|  | qâ€‹(zt|z0)=ğ’©â€‹(zt;Î±Â¯tâ€‹z0,(1âˆ’Î±Â¯t)â€‹I),q(z\_{t}|z\_{0})=\mathcal{N}\!\big(z\_{t};\sqrt{\bar{\alpha}\_{t}}\,z\_{0},(1-\bar{\alpha}\_{t})I\big), |  |

with Î±Â¯t=âˆs=1t(1âˆ’Î²s)\bar{\alpha}\_{t}=\prod\_{s=1}^{t}(1-\beta\_{s}).

##### Training objective.

The denoiser ÏµÎ¸â€‹(zt,t)\epsilon\_{\theta}(z\_{t},t) predicts the injected noise:

|  |  |  |
| --- | --- | --- |
|  | â„’LDM=ğ”¼z0,Ïµ,tâ€‹[â€–Ïµâˆ’ÏµÎ¸â€‹(zt,t)â€–2].\mathcal{L}\_{\text{LDM}}=\mathbb{E}\_{z\_{0},\epsilon,t}\big[\|\epsilon-\epsilon\_{\theta}(z\_{t},t)\|^{2}\big]. |  |

##### Inference.

Generation starts from zTâˆ¼ğ’©â€‹(0,I)z\_{T}\sim\mathcal{N}(0,I) and denoises iteratively:

|  |  |  |
| --- | --- | --- |
|  | ztâˆ’1=1Î±tâ€‹(ztâˆ’Î²t1âˆ’Î±Â¯tâ€‹ÏµÎ¸â€‹(zt,t))+Ïƒtâ€‹Ïµ.z\_{t-1}=\frac{1}{\sqrt{\alpha\_{t}}}\Big(z\_{t}-\frac{\beta\_{t}}{\sqrt{1-\bar{\alpha}\_{t}}}\epsilon\_{\theta}(z\_{t},t)\Big)+\sigma\_{t}\epsilon. |  |

### B.3 Comparison of Parameterizations

Diffusion training can be expressed through different target parameterizations, all of which can be interpreted as variants of the same continuous-time flow. Below we summarize the most common forms:

#### B.3.1 Ïµ\epsilon-Prediction

The denoiser directly predicts the added Gaussian noise Ïµ\epsilon:

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„’Ïµ=ğ”¼z0,Ïµ,tâ€‹[â€–Ïµâˆ’ÏµÎ¸â€‹(zt,t)â€–2],\mathcal{L}\_{\epsilon}=\mathbb{E}\_{z\_{0},\epsilon,t}\Big[\|\epsilon-\epsilon\_{\theta}(z\_{t},t)\|^{2}\Big], |  | (9) |

where zt=Î±Â¯tâ€‹z0+1âˆ’Î±Â¯tâ€‹Ïµz\_{t}=\sqrt{\bar{\alpha}\_{t}}z\_{0}+\sqrt{1-\bar{\alpha}\_{t}}\,\epsilon.
This is the standard DDPM formulationÂ (Ho etÂ al., [2020](#bib.bib30)). It is stable but sometimes less efficient for long horizons.

#### B.3.2 x0x\_{0}-Prediction (DDIM-x0x\_{0})

Instead of noise, the model predicts the clean latent z0z\_{0}:

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„’x0=ğ”¼z0,tâ€‹[â€–z0âˆ’x0,Î¸â€‹(zt,t)â€–2].\mathcal{L}\_{x\_{0}}=\mathbb{E}\_{z\_{0},t}\Big[\|z\_{0}-x\_{0,\theta}(z\_{t},t)\|^{2}\Big]. |  | (10) |

This corresponds to the DDIM formulationÂ (Song etÂ al., [2020](#bib.bib62)), enabling deterministic sampling and fewer inference steps, but can overfit to data scale.

#### B.3.3 vv-Prediction

Proposed byÂ Salimans & Ho ([2022](#bib.bib54)), vv is defined as a linear combination of noise and clean latent:

|  |  |  |  |
| --- | --- | --- | --- |
|  | v=Î±Â¯tâ€‹Ïµâˆ’1âˆ’Î±Â¯tâ€‹z0,v=\sqrt{\bar{\alpha}\_{t}}\,\epsilon-\sqrt{1-\bar{\alpha}\_{t}}\,z\_{0}, |  | (11) |

with objective

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„’v=ğ”¼z0,Ïµ,tâ€‹[â€–vâˆ’vÎ¸â€‹(zt,t)â€–2].\mathcal{L}\_{v}=\mathbb{E}\_{z\_{0},\epsilon,t}\Big[\|v-v\_{\theta}(z\_{t},t)\|^{2}\Big]. |  | (12) |

vv-prediction is numerically better conditioned, often improving stability across timesteps.

All four parameterizations can be viewed as different instantiations of the same underlying generative flow. Ïµ\epsilon-prediction, x0x\_{0}-prediction, and vv-prediction specify *which quantity* the denoiser regresses on. Flow matching directly learns the continuous velocity field, avoiding discretization artifacts.

### B.4 Block Diffusion

Suppose we are using Ïµ\epsilon-prediction, and let a sequence be segmented into MM blocks {B1,â€¦,BM}\{B\_{1},\dots,B\_{M}\}, where Bmâˆˆâ„kÃ—dB\_{m}\in\mathbb{R}^{k\times d} contains kk latent tokens of dimension dd. The forward noising process for block BmB\_{m} is

|  |  |  |  |
| --- | --- | --- | --- |
|  | qâ€‹(Bm,tâˆ£Bm,0)=ğ’©â€‹(Î±Â¯tâ€‹Bm,0,(1âˆ’Î±Â¯t)â€‹I),q(B\_{m,t}\mid B\_{m,0})=\mathcal{N}\!\big(\sqrt{\bar{\alpha}\_{t}}\,B\_{m,0},(1-\bar{\alpha}\_{t})I\big), |  | (13) |

and the denoiser fÎ¸f\_{\theta} is trained to predict the noise at the block level:

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„’block=ğ”¼m,t,Ïµâ€‹[â€–Ïµâˆ’fÎ¸â€‹(Bm,t,t)â€–2].\mathcal{L}\_{\text{block}}=\mathbb{E}\_{m,t,\epsilon}\Big[\big\|\epsilon-f\_{\theta}(B\_{m,t},t)\big\|^{2}\Big]. |  | (14) |

Blocks are generated autoregressively, i.e.,

|  |  |  |  |
| --- | --- | --- | --- |
|  | pâ€‹(Bmâˆ£B<m)=âˆ«qâ€‹(Bm,0âˆ£x)â€‹âˆtpÎ¸â€‹(Bm,tâˆ’1âˆ£Bm,t,B<m)â€‹dâ€‹Bm,0,p(B\_{m}\mid B\_{<m})=\int q(B\_{m,0}\mid x)\prod\_{t}p\_{\theta}(B\_{m,t-1}\mid B\_{m,t},B\_{<m})\,dB\_{m,0}, |  | (15) |

so that each block is denoised iteratively while conditioning on all previously generated blocks.

## Appendix C Additional Ablation Studies

| Objective | CD-4 Pass@1 (%) |
| --- | --- |
| MSE Loss | 46.0 |
| x0x\_{0} | 53.0 |
| Ïµ\epsilon | 58.0 |
| vv | 62.0 |
| uu (ours) | 73.5 |

Table 4: Ablation study on latent prediction objectives on the Countdown-4 dataset.

##### Latent Prediction Objective

To assess the impact of different training objectives for latent prediction, we compare several widely used formulations. The first baseline is MSE loss, which directly minimizes the mean-squared error between predicted and ground-truth latents but yields the weakest results. We then adopt three DDIM-basedÂ (Song etÂ al., [2020](#bib.bib62)) objectives: predicting the clean latent state (x0x\_{0}), the added noise vector (Ïµ\epsilon), and the velocity (vv). These diffusion objectives consistently improve accuracy, highlighting the advantage of explicitly modeling the denoising process rather than relying on direct predictions. Moreover, the flow matching (uu) objective achieves the strongest gains, suggesting that learning the latent vector field is more effective for capturing the pattern of reasoning compared to DDIMâ€™s denoising objectives.

![Refer to caption](/html/2510.04573/assets/figs/latent_token.png)

Figure 6: Ablation analysis of block size on the GSM8K benchmark.

##### Effect of the Block Size.

We investigate how the number of latent tokens per block (LbL\_{b}) influences reconstruction quality and downstream reasoning accuracy on GSM8K. As shown in FigureÂ [6](#A3.F6 "Figure 6 â€£ Latent Prediction Objective â€£ Appendix C Additional Ablation Studies â€£ LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning"), too few tokens (i.e., 1 token) limit the modelâ€™s ability to capture necessary information, harming reconstruction. Performance improves as the number of tokens increases, reaching near-perfect reconstruction at n=6n=6. Beyond this point, however, adding more tokens introduces redundancy, which makes the latent diffusion model harder to predict accurately and leads to diminished reasoning accuracy. This reveals a trade-off between compact latent representations and effective downstream reasoning.

## Appendix D Additional Analysis

##### Intepretability

In addition to achieving superior or competitive accuracy across each benchmark, as shown in TableÂ [5](#A4.T5 "Table 5 â€£ Intepretability â€£ Appendix D Additional Analysis â€£ LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning"), LaDiR also benefits from being more interpretable by nature compared to standard diffusion-based methods.

| Block | Text |
| --- | --- |
| Question | *Billy sells DVDs. He has 8 customers on Tuesday. The first 3 buy one DVD each, the next 2 buy two DVDs each, the last 3 buy none. How many DVDs did Billy sell?* |
| Decode(Z(1))(Z^{(1)}): | Billyâ€™s first 3 customers buy one DVD each, so thatâ€™s 3âˆ—1=âŸ¨âŸ¨3âˆ—1=3âŸ©âŸ©â€‹33\*1=\langle\langle 3\*1=3\rangle\rangle 3 DVDs. |
| Decode(Z(2))(Z^{(2)}): | His next 2 customers buy 2 DVDs each, so thatâ€™s 2 Ã—\times 2 = 4 DVDs. |
| Decode(Z(3))(Z^{(3)}): | His last 3 customers donâ€™t buy any DVDs, so thatâ€™s 0 DVDs sold. |
| Decode(Z(4))(Z^{(4)}): | Therefore, Billy sold a total of 3 + 4 + 0 = 7 DVDs on Tuesday. |
| Answer | The answer is: 7. |

Table 5: Example of interpretable continuous thought tokens: each latent block ZiZ^{i} is able to be decoded to human-readable text through the VAE decoder. Each latent block is decoded individually, so the entire latent thought is represented by the block in isolation. This allows for clear interpretibility of each latent thought, while still allowing for a model to reason in a latent space.

##### Reasoning at Semantic Level

TableÂ [6](#A4.T6 "Table 6 â€£ Reasoning at Semantic Level â€£ Appendix D Additional Analysis â€£ LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning") demonstrates that LaDiR refines its reasoning through semantic information rather than lexical connections. The pink segments trace how the model incrementally refines its intermediate reasoning stepsâ€”transforming rough or incomplete arithmetic expressions into logically consistent equations. For example, the model first produces an approximate relation such as 2=1+12=1+1 and later reconstructs the correct multiplicative form 2âˆ—1=22\*1=2, eventually combining these into a complete causal chain that leads to 2âˆ—4=82\*4=8. Meanwhile, the green segments serve as stable anchors capturing the essential reasoning cuesâ€”phrases like â€œtwo times older thanâ€ and â€œfour times as oldâ€â€”which define the causal dependencies between Suzy, Ben, and Brandon. In contrast, the yellow segments correspond to non-reasoning or noisy parts (e.g., â€œAlice,â€ â€œInstagramâ€) that the model learns to â€œignoreâ€ during early denoising, prioritizing semantically coherent and logical grounded reasoning over irrelevant lexical content. In contrast, the SFT baseline remains syntactically fluent but semantically brittleâ€”producing text that appears correct on the surface yet fails to capture the true causal logic. These observations indicate that LaDiRâ€™s denoising process operates on the level of semantics and relational consistency, enabling reasoning that aligns with human-like conceptual understanding.

| Question | Brandonâ€™s iPhone is four times as old as Benâ€™s iPhone. Benâ€™s iPhone is two times older than Suzyâ€™s iPhone. If Suzyâ€™s iPhone is 1 year old, how old is Brandonâ€™s iPhone? | |
| --- | --- | --- |
| SFT (Baseline) | Suzyâ€™s iPhone is 1 year old. Benâ€™s iPhone would be 1+1=21+1=2 years old. Since Brandonâ€™s iPhone is four times as old as Benâ€™s, we compute 4Ã—1=44\times 1=4. Therefore, Brandonâ€™s iPhone is 4 years old. The answer is: 4. | |
| LaDiR (Ours) | Block 1 | Block 2 |
| Decode(Z^t=1.0\hat{Z}\_{t=1.0}) | 10 and 10 10.... | [garbled tokens] Â Â  &Ë†\* \_ |
| Decode(Z^t=0.9\hat{Z}\_{t=0.9}) | If Alice is 1 year old, then Bobâ€™s age is 2 years older than Alice, which means Bob is 2 = 1 + 1 years old. | ://@natechandraâ€™s Instagram is four times as old as Benâ€™s Instagram, so Benâ€™s Instagram is 1 \* 4 = 4 years old. |
| Decode(Z^t=0.8\hat{Z}\_{t=0.8}) | If Suzyâ€™s age is 1 year old, then Benâ€™s age is two times older than Suzyâ€™s age, which is 2 \* 1 = 2 years old. | If Brandonâ€™s phone is four times as old as Benâ€™s phone, then Brandonâ€™s phone is 2 \* 4 = 8 years old.1nbsp;nbsp;nbspâ€¦ |
| Decode(Z^tâ‰¤0.7\hat{Z}\_{t\leq 0.7}) | If Suzyâ€™s iPhone is 1 year old, then Benâ€™s iPhone is two times older than Suzyâ€™s iPhone, so Benâ€™s iPhone is 2 \* 1 = 2 years old. | If Brandonâ€™s iPhone is four times as old as Benâ€™s iPhone, then Brandonâ€™s iPhone is 2 \* 4 = 8 years old. |
| Answer | The answer is: 8. | |

Table 6: An example of self-refinement during inference on the GSM8k dataset, showing how reasoning becomes progressively clearer as tt decreases. Later denoising steps correct arithmetic errors while maintaining earlier structure, demonstrating semantic self-refinement. Pink segments highlight refined reasoning portions, yellow segments indicate non-reasoning or noisy parts that the model gradually corrects, and green segments denote key reasoning cues essential for correct logic.

##### Consistency Across Long Reasoning Blocks

As shown in TableÂ [7](#A4.T7 "Table 7 â€£ Consistency Across Long Reasoning Blocks â€£ Appendix D Additional Analysis â€£ LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning"), LaDiR maintains coherent and semantically consistent reasoning across a long sequence of blocks.
Each block builds logically on the previous oneâ€”starting from variable definition (xx as silver coins), to constraint formulation (x+(x+30)=110x+(x+30)=110), simplification, and finally the correct numerical conclusion (x=40x=40, gold =70=70).
Unlike the SFT baseline, which produces a single-step approximation that conflates intermediate relations, LaDiR preserves arithmetic and causal consistency throughout the reasoning chain, demonstrating stable multi-step inference even with a large number of reasoning blocks.

| Question | Gretchen has 110 coins. There are 30 more gold coins than silver coins. How many gold coins does Gretchen have? |
| --- | --- |
| SFT (Baseline) | Letâ€™s assume Gretchen has 110 coins in total and 30 more gold than silver. Half of the coins plus 30 should be gold, so 110/2+30=85110/2+30=85 gold coins. |
| LaDiR (Ours) |  |
| Decode(Z(1))(Z^{(1)}) | Letâ€™s assume the number of silver coins Gretchen has is xx silver coins. |
| Decode(Z(2))(Z^{(2)}) | We also know that there are 3030 more gold coins than silver coins, so the number of silver coins is x+30x+30 gold coins. |
| Decode(Z(3))(Z^{(3)}) | The total number of coins Gretchen has is x+(x+30)=110x+(x+30)=110. |
| Decode(Z(4))(Z^{(4)}) | Combining like terms, we get 2â€‹x+30=1102x+30=110. |
| Decode(Z(5))(Z^{(5)}) | Subtracting 3030 from both sides, we get 2â€‹x=802x=80. |
| Decode(Z(6))(Z^{(6)}) | Dividing both sides by 22, we get x=40x=40. |
| Decode(Z(7))(Z^{(7)}) | Therefore, Gretchen has 30+40=7030+40=70 gold coins. |
| Answer | The answer is: 7070. |

Table 7: An qualitative example in GSM8K illustrating the long reasoning blocks generated by our method compared to the baseline SFT.

## Appendix E Experimental Details

### E.1 Math Reasoning

##### Implementation Details

The CoT data is segmented into thought-level blocks, where each block corresponds to a single sentence and is represented by 66 latent thought tokens. For VAE training, we set the to Î²=10âˆ’5\beta=10^{-5}, in which the encoder is finetuned from the backbone model while the decoder remains frozen. The flow-matching model is trained with the objective in  [Eq.Â 5](#S3.E5 "In 3.3.1 Stage 1: Teacher-forcing training â€£ 3.3 Reasoning Model Training â€£ 3 Methodology â€£ LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning"), using Î»FM=5,Î»Ans=1,Î»Spec=2\lambda\_{\mathrm{FM}}=5,\lambda\_{\mathrm{Ans}}=1,\lambda\_{\mathrm{Spec}}=2. During inference, we initialize the Gaussian noise of scale 22, and apply diversity guidance with a maximum scale of 0.80.8.

##### Datasets

We train on only the DART-MATH dataset, holding out all other benchmarks for evaluation only. TableÂ [8](#A5.T8 "Table 8 â€£ Datasets â€£ E.1 Math Reasoning â€£ Appendix E Experimental Details â€£ LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning") summarizes the datasets. While training is limited to mathematical reasoning problems, our evaluations also include out-of-domain tasks such as engineering and physics, providing both in-domain and out-of-domain benchmarks to assess the reasoning and generalization capabilities of LaDiR and the baselines.

| Dataset | # Samples | Domain / Level | Notes |
| --- | --- | --- | --- |
| DART-MATH | 585k | Mixed math (train) | Synthesized for reasoning, based on GSM8K/MATH |
| MATH | 500 | High school / competition | In-domain benchmark |
| GSM8K | 1.3k | Grade school arithmetic | In-domain benchmark |
| College-Math | 2.8k | College-level | Linear algebra, differential equations, etc. |
| DM-Math | 1k | Kâ€“12 curriculum | Out-of-domain generalization |
| OlympiaBench-Math | 675 | Olympiad-level | Advanced competition problems |
| TheoremQA | 800 | STEM / theorem-driven | Math, physics, engineering |
| Fresh-Gaokao-Math-2023 | 30 | Gaokao exam | Real-world test distribution |

Table 8: Summary of datasets used in our experiments. We use DART-MATH (Tong etÂ al., [2024b](#bib.bib69)), MATHÂ (Hendrycks etÂ al., [2021](#bib.bib26)), GSM8KÂ (Cobbe etÂ al., [2021](#bib.bib12)), College-MathÂ (Tang etÂ al., [2024b](#bib.bib67)), DeepMind-MathÂ (Saxton etÂ al., [2019](#bib.bib56)), OlympiaBench-MathÂ (He etÂ al., [2024](#bib.bib25)), TheoremQAÂ (Chen etÂ al., [2023](#bib.bib7)), and Fresh-Gaokao-Math-2023Â (Tang etÂ al., [2024b](#bib.bib67)).

## Appendix F Additional Model Details

##### VAE Training Architecture

FigureÂ [7](#A6.F7 "Figure 7 â€£ VAE Training Architecture â€£ Appendix F Additional Model Details â€£ LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning") is a more in-depth diagram of the training of the VAE. The encoder LLM first maps the input sequence of original text embeddings and learnable embeddings into hidden states. These hidden states are then projected through two linear layers to produce the mean and variance of the latent distribution, from which we sample the thought tokens via the reparameterization trick. The sampled latent tokens z~1,z~2,â€¦,z~k{\tilde{z}\_{1},\tilde{z}\_{2},\ldots,\tilde{z}\_{k}} are passed to the decoder LLM, which reconstructs the original text tokens under a teacher-forcing setup. This design enables the model to compress high-dimensional text into a smaller set of semantically meaningful latent variables, while still maintaining faithful reconstruction of the original reasoning process.

![Refer to caption](/html/2510.04573/assets/figs/vae_fig.png)

Figure 7: Detailed architecture of the variational autoencoder for latent reasoning. The encoder is a finetuned LLM that takes both original text embeddings eâ€‹(wi)e(w\_{i}) and learnable embeddings ezâ€‹(zi)e\_{z}(z\_{i}), producing mean and variance vectors through linear projections of the last hidden state hh. Latent thought tokens z~i\tilde{z}\_{i} are then sampled from ğ’©â€‹(Î¼,Ïƒ2)\mathcal{N}(\mu,\sigma^{2}). The decoder is a frozen LLM that reconstructs the original CoT text under teacher forcing, conditioned on both the sampled thought tokens and the original text embeddings.

## Appendix G Use of Large Language Models (LLMs)

Throughout this project, Large Language Models were utilized as coding tools and as grammar checkers to support the writing of the paper. They did not play a significant role in research ideation or writing to the extent of being listed as a contributor. LLMs were used strictly as a general purpose tool.