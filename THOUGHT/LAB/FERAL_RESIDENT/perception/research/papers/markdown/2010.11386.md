# Distilling Dense Representations for Ranking using Tightly-Coupled Teachers

Sheng-Chieh Lin
Â Â Contributed equally.
David R. Cheriton School of Computer Science
  
University of Waterloo

Jheng-Hong Yang00footnotemark: 0
David R. Cheriton School of Computer Science
  
University of Waterloo

Jimmy Lin
David R. Cheriton School of Computer Science
  
University of Waterloo

###### Abstract

We present an approach to ranking with dense representations that applies knowledge distillation to improve the recently proposed late-interaction ColBERT model.
Specifically, we distill the knowledge from ColBERTâ€™s expressive MaxSim operator for computing relevance scores into a simple dot product, thus enabling single-step ANN search.
Our key insight is that during distillation, tight coupling between the teacher model and the student model enables more flexible distillation strategies and yields better learned representations.
We empirically show that our approach improves query latency and greatly reduces the onerous storage requirements of ColBERT, while only making modest sacrifices in terms of effectiveness.
By combining our dense representations with sparse representations derived from document expansion, we are able to approach the effectiveness of a standard cross-encoder reranker using BERT that is orders of magnitude slower.

## 1 Introduction

For well over half a century, solutions to the ad hoc retrieval problemâ€”where the systemâ€™s task is return a list of top kğ‘˜k texts from an arbitrarily large corpus ğ’ğ’\mathcal{C} that maximizes some metric of quality such as average precision or nDCGâ€”has been dominated by sparse vector representations, for example, bag-of-words BM25.
Even in modern multi-stage ranking architectures, which take advantage of large pretrained transformers such as BERTÂ Devlin etÂ al. ([2018](#bib.bib6)), the models are deployed as rerankers over initial candidates retrieved based on sparse vector representations; this is sometimes called â€œfirst-stage retrievalâ€.
One well-known example of this design is the BERT-based reranker ofÂ Nogueira and Cho ([2019](#bib.bib21)).

The standard reranker architecture, while effective, exhibits high query latency, on the order of seconds per queryÂ HofstÃ¤tter and Hanbury ([2019](#bib.bib11)); Khattab and Zaharia ([2020](#bib.bib15)) because expensive neural inference must be applied at query time on queryâ€“document pairs.
This design is known as a cross-encoderÂ Humeau etÂ al. ([2020](#bib.bib12)), and it exploits queryâ€“document attention interactions across all transformer layers.
As an alternative, the field has seen much recent interest in approaches based on representation learning that allow document representations to be precomputed independently of queries and stored.
Efficient libraries then allow large-scale comparisons between query and document vectors.
Overall, such approaches are less effective than cross-encoder reranking models, but far more efficient.

Within this general framework, we describe our low latency end-to-end approach for the ad hoc passage retrieval task that combines dense and sparse representations.
As a starting point, we adopt the â€œlate interactionâ€ ColBERT modelÂ Khattab and Zaharia ([2020](#bib.bib15)) and, via knowledge distillationÂ Hinton etÂ al. ([2015](#bib.bib9)), are able to simplify its MaxSim relevance computation into dot-product similarity over pooled embeddings.
Since lexical signals (e.g., term frequencies) from sparse representations remain essential for ad hoc retrievalÂ Karpukhin etÂ al. ([2020](#bib.bib14)); Luan etÂ al. ([2020](#bib.bib19)), we further demonstrate that our dense representations can simply incorporate sparse signals without a complex joint training strategyÂ Gao etÂ al. ([2020](#bib.bib7)).
In sum, we introduce simple-yet-effective strategies that leverage both dense and sparse representations for the end-to-end ad hoc passage retrieval task.

![Refer to caption](/html/2010.11386/assets/x1.png)

Figure 1: Tight coupling between teacher and student models during distillation of dense representations for ranking.

Our key insight is that during distillation, tight coupling between the teacher model and the student model enables more flexible distillation strategies and yields better learned representations (illustrated in FigureÂ [1](#S1.F1 "Figure 1 â€£ 1 Introduction â€£ Distilling Dense Representations for Ranking using Tightly-Coupled Teachers")).
By tight coupling, we mean that inference using the teacher model is interleaved directly during the distillation process:Â This is a key difference between our approach and previous methods where queryâ€“document scores are precomputedÂ HofstÃ¤tter etÂ al. ([2020](#bib.bib10)).
With this tight coupling, we can also avoid computationally expensive mechanisms such as periodic index refreshes that are necessary during representation learningÂ Guu etÂ al. ([2020](#bib.bib8)); Xiong etÂ al. ([2020](#bib.bib24)).
A practical consequence of this tight coupling is that the teacher model must itself be reasonably efficient (thus, for example, ruling out teacher models based on cross-encoders).
For this role, ColBERTÂ Khattab and Zaharia ([2020](#bib.bib15)) is a good fit.

## 2 Background

We begin by formalizing the representation learning problem for text ranking and review learning approaches.
We represent matrices by uppercase letters Xğ‘‹X, scalars by lowercase italic letters xğ‘¥x, and vectors by lowercase bold letters ğ±ğ±\mathbf{x}.

The ad hoc retrieval task can be viewed as a text ranking problem; here, we adopt the formulation ofÂ Lin etÂ al. ([2020](#bib.bib17)).
Specifically, we aim to learn some transformation Î·(â‹…)subscriptğœ‚â‹…\eta\_{(\cdot)}, called an encoder, that maximize the following probability via surrogate functions given a pair comprising a query ğªâˆˆâ„nğªsuperscriptâ„ğ‘›\mathbf{q}\in\mathbb{R}^{n} and a candidate text (e.g., a passage) ğâˆˆâ„nğsuperscriptâ„ğ‘›\mathbf{d}\in\mathbb{R}^{n}:

|  |  |  |  |
| --- | --- | --- | --- |
|  | P(Relevant|ğª,ğ)â‰œÏ•(Î·q(ğª),Î·d(ğ)),P(\text{Relevant}\lvert\mathbf{q},\mathbf{d})\triangleq\phi(\eta\_{q}(\mathbf{q}),\eta\_{d}(\mathbf{d})), |  | (1) |

where Ï•italic-Ï•\phi is a similarity function and nğ‘›n is an arbitrary natural number.
The systemâ€™s task is to return the top-kğ‘˜k relevant texts for a query via the similarity function Ï•italic-Ï•\phi that takes Î·qâ€‹(ğª)subscriptğœ‚ğ‘ğª\eta\_{q}(\mathbf{q}) and Î·dâ€‹(ğ)subscriptğœ‚ğ‘‘ğ\eta\_{d}(\mathbf{d}).
Depending on the design, Î·qsubscriptğœ‚ğ‘\eta\_{q} and Î·dsubscriptğœ‚ğ‘‘\eta\_{d} can be identical or distinct.

Dot-product similarity.
Since online serving latency is critical in real-world applications, a standard choice of Ï•italic-Ï•\phi in Eq.Â ([1](#S2.E1 "In 2 Background â€£ Distilling Dense Representations for Ranking using Tightly-Coupled Teachers")) is the dot product, âŸ¨â‹…,â‹…âŸ©

â‹…â‹…\langle\cdot,\cdot\rangle.
Given this formulation, finding the top scoring passages that maximize âŸ¨ğª,ğâŸ©

ğªğ\langle\mathbf{q},\mathbf{d}\rangle can be approximated efficiently by Approximated Nearest Neighbor search (ANN)Â Liu etÂ al. ([2004](#bib.bib18)) or Maximum Inner Product Search (MIPS)Â Shrivastava and Li ([2014](#bib.bib23)) (henceforth, ANN), and accomplished by existing off-the-shelf libraries.

Transformer-based bi-encoders.
For large-scale applications, encoders based on pretrained transformersÂ Devlin etÂ al. ([2018](#bib.bib6)) have been widely adopted to map queries and passages into low dimensional vectors independentlyÂ Lee etÂ al. ([2019](#bib.bib16)); Chang etÂ al. ([2020](#bib.bib3)); Khattab and Zaharia ([2020](#bib.bib15)); Guu etÂ al. ([2020](#bib.bib8)); Karpukhin etÂ al. ([2020](#bib.bib14)); Luan etÂ al. ([2020](#bib.bib19)); Xiong etÂ al. ([2020](#bib.bib24)).
Known as a bi-encoder design, a query (or a passage) is first mapped to a contextualized representation Eqâˆˆâ„lqÃ—tsubscriptğ¸ğ‘superscriptâ„subscriptğ‘™ğ‘ğ‘¡E\_{q}\in\mathbb{R}^{l\_{q}\times t} (or Edâˆˆâ„ldÃ—tsubscriptğ¸ğ‘‘superscriptâ„subscriptğ‘™ğ‘‘ğ‘¡E\_{d}\in\mathbb{R}^{l\_{d}\times t}), where lğ‘™l indicates the length of the tokenized query (or the passage) of tğ‘¡t-dimensional vectors.
Given the contextualized representation matrix, there are many choices for Î·:â„lÃ—tâ†’â„h:ğœ‚â†’superscriptâ„ğ‘™ğ‘¡superscriptâ„â„\eta\colon\mathbb{R}^{l\times t}\to\mathbb{R}^{h} that transforms E(â‹…)subscriptğ¸â‹…E\_{(\cdot)} into a lower dimensional vector Î·â€‹(E(â‹…))ğœ‚subscriptğ¸â‹…\eta(E\_{(\cdot)}), where hâ‰ªlâ‹…tmuch-less-thanâ„â‹…ğ‘™ğ‘¡h\ll l\cdot t.

Prior to retrieval, the hâ„h-dimensional representations can be precomputed for each of |ğ’|ğ’|\mathcal{C}| texts in a corpus.
With specialized ANN libraries that take advantage of the parallelism provided by GPUs, even a brute force scan over millions of vectors is feasible.
With index structures, for example, based on small world graphsÂ Malkov and Yashunin ([2020](#bib.bib20)), the ANN search problem can be further accelerated.

Design choices.
In general, compositions of Ï•italic-Ï•\phi and Î·ğœ‚\eta can be designed with different approaches.
For example, given a queryâ€“passage pair, we can define relevance in terms of the dot product between the two pooled embeddings as follows:

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ï•PoolDotâ€‹(ğª,ğ)=âŸ¨Poolâ€‹(Eq),Poolâ€‹(Ed)âŸ©,subscriptitalic-Ï•PoolDotğªğ  Poolsubscriptğ¸ğ‘Poolsubscriptğ¸ğ‘‘\phi\_{\text{PoolDot}}(\mathbf{q},\mathbf{d})=\langle\text{Pool}(E\_{q}),\text{Pool}(E\_{d})\rangle, |  | (2) |

where the Pool operator can be average or maximum pooling over token embeddings, or an indicator to a specific token embedding, e.g., the [CLS] embedding in BERT.
In this study, we adopt average pooling over token embeddings as our baseline, denoted as PoolAvg.

Our work builds on ColBERTÂ Khattab and Zaharia ([2020](#bib.bib15)), who proposed a Ï•italic-Ï•\phi comparison function defined in terms of MaxSim, as follows:

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ï•MaxSimâ€‹(ğª,ğ)=âˆ‘iâˆˆ|Eq|maxjâˆˆ|Ed|â¡âŸ¨Î·qâ€‹(Eqi),Î·dâ€‹(Edj)âŸ©â€‹,subscriptitalic-Ï•MaxSimğªğsubscriptğ‘–subscriptğ¸ğ‘subscriptğ‘—subscriptğ¸ğ‘‘subscriptğœ‚ğ‘subscriptsubscriptğ¸ğ‘ğ‘–subscriptğœ‚ğ‘‘subscriptsubscriptğ¸ğ‘‘ğ‘—,\phi\_{\text{MaxSim}}(\mathbf{q},\mathbf{d})=\sum\_{i\in|E\_{q}|}\max\_{j\in|E\_{d}|}\langle\eta\_{q}({E\_{q}}\_{i}),\eta\_{d}({E\_{d}}\_{j})\rangle\text{,} |  | (3) |

where Î·ğœ‚\eta is composition of functions:

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î·qâ€‹(x)=Normalizeâ€‹(Conv1Dâ€‹(x))subscriptğœ‚ğ‘xNormalizeConv1Dx\displaystyle\eta\_{q}(\textbf{x})=\text{Normalize}(\text{Conv1D}(\textbf{x})) |  | (4) |
|  | Î·d(x)=Filter(Normalize(Conv1D(x)).\displaystyle\eta\_{d}(\textbf{x})=\text{Filter}(\text{Normalize}(\text{Conv1D}(\textbf{x})). |  |

We refer readers toÂ Khattab and Zaharia ([2020](#bib.bib15)) for more details.
While ColBERT represents a design that greatly reduces retrieval latency with only a modest degradation in quality compared to the cross-encoder design, it still has two major limitations:

* â€¢

  The process of computing Eq.Â ([3](#S2.E3 "In 2 Background â€£ Distilling Dense Representations for Ranking using Tightly-Coupled Teachers")) is approximated by a two-stage pipeline:Â retrieving then reranking, since MaxSim over the entire collection is not feasible.
  Thus, despite its aspirations to single-stage ANN search, end-to-end retrieval with ColBERT still requires multi-stage retrieval.
* â€¢

  The technique suffers from unreasonably high storage requirements compared to Ï•PoolDotsubscriptitalic-Ï•PoolDot\phi\_{\text{PoolDot}} because the passages are preprocessed and stored as sequences of token embeddings via Conv1Dâ€‹(Ed)âˆˆâ„lâˆ—Ã—hâˆ—Conv1Dsubscriptğ¸ğ‘‘superscriptâ„superscriptğ‘™superscriptâ„\text{Conv1D}(E\_{d})\in\mathbb{R}^{l^{\*}\times h^{\*}}, where lâˆ—superscriptğ‘™l^{\*} denotes the length of the passage in tokens and hâˆ—superscriptâ„h^{\*} denotes the kernel dimension of Conv1D in Eq.Â ([4](#S2.E4 "In 2 Background â€£ Distilling Dense Representations for Ranking using Tightly-Coupled Teachers")).111Khattab and Zaharia ([2020](#bib.bib15)) append specialized tokens, [Q] and [D], for both queries and passages and set the kernel dimension of Conv1D to 128.

On the other hand, learning well-behaved representations for the pooled embeddings using dot products directly, as in Eq.Â ([2](#S2.E2 "In 2 Background â€£ Distilling Dense Representations for Ranking using Tightly-Coupled Teachers")) is not trivial, since this process involves drastically non-linear dimension reduction.
The recent work ofÂ Guu etÂ al. ([2020](#bib.bib8)) andÂ Xiong etÂ al. ([2020](#bib.bib24)) propose adapting ANN search for mining hard negative examples to fine-tune the pretrained representations E(â‹…)subscriptğ¸â‹…E\_{(\cdot)}, which reduces the gap between training and inference.
However, this process is computationally demanding since it requires periodically refreshing the ANN index of all candidates (i.e., requiring inference over all texts in the corpus) to ensure that the best negative examples are retrieved.
Xiong etÂ al. ([2020](#bib.bib24)) reports that re-encoding the entire corpus takes around 10 hours, and this occurs every 5K steps during training.

In another work, HofstÃ¤tter etÂ al. ([2020](#bib.bib10)) demonstrate that knowledge distillation from precomputed relevance scores of well-behaved cross-encoder rerankers is effective.
While distillation is able to capture reranking effectiveness, computationally expensive cross-encoder teachers limit the flexibility of exploring different combinations of queryâ€“document pairs, as exhaustively precomputing relevance scores using these cross-encoders can be computationally intractable.

## 3 Methodology

In contrast to the methods discussed above, we propose a simple-yet-effective approach:Â knowledge distillationÂ Hinton etÂ al. ([2015](#bib.bib9)) with the novel insight that teacher and student models should be tightly coupled.
During training, in addition to fine-tuning using the contextualized representations E(â‹…)subscriptğ¸â‹…E\_{(\cdot)} with relevance labels, we distill knowledge from ColBERTâ€™s similarity function Ï•MaxSimsubscriptitalic-Ï•MaxSim\phi\_{\text{MaxSim}} into a dot-product bi-encoder.

Although ColBERT has enabled efficient passage retrieval, we seek to simplify it further.
To reduce computation and storage cost, we remove Conv1D and define our own similarity function in terms of average pooling over token embeddings (PoolAvg).
Thus, we precompute and store passage embeddings as Poolâ€‹(Ed)âˆˆâ„hPoolsubscriptğ¸ğ‘‘superscriptâ„â„\text{Pool}(E\_{d})\in\mathbb{R}^{h} using ANN indexing in advance;
during inference, we only have to encode query embeddings as Poolâ€‹(Eq)âˆˆâ„hPoolsubscriptğ¸ğ‘superscriptâ„â„\text{Pool}(E\_{q})\in\mathbb{R}^{h} and then conduct ANN search.222We set h=768â„768h=768 for both queries and passages.

### 3.1 Knowledge Distillation

Formally, given a query ğªğª\mathbf{q}, we first estimate the relevance of a passage ğğ\mathbf{d} using two sets of conditional probabilities:

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | Pâ€‹(ğ|ğª)=expâ€‹(Ï•PoolDotâ€‹(ğª,ğ))âˆ‘ğâ€²âˆˆDexpâ€‹(Ï•PoolDotâ€‹(ğª,ğâ€²))ğ‘ƒconditionalğğªexpsubscriptitalic-Ï•PoolDotğªğsubscriptsuperscriptğâ€²ğ·expsubscriptitalic-Ï•PoolDotğªsuperscriptğâ€²\displaystyle P(\mathbf{d}|\mathbf{q})=\frac{\text{exp}(\phi\_{\text{PoolDot}}(\mathbf{q},\mathbf{d}))}{\sum\_{\mathbf{d^{\prime}}\in D}\text{exp}(\phi\_{\text{PoolDot}}(\mathbf{q},\mathbf{d^{\prime}}))} |  | | (5) |
|  | P^â€‹(ğ|ğª)=expâ€‹(Ï•MaxSimâ€‹(ğª,ğ)/Ï„)âˆ‘ğâ€²âˆˆDexpâ€‹(Ï•MaxSimâ€‹(ğª,ğâ€²)/Ï„)^ğ‘ƒconditionalğğªexpsubscriptitalic-Ï•MaxSimğªğğœsubscriptsuperscriptğâ€²ğ·expsubscriptitalic-Ï•MaxSimğªsuperscriptğâ€²ğœ\displaystyle\hat{P}(\mathbf{d}|\mathbf{q})=\frac{\text{exp}(\phi\_{\text{MaxSim}}(\mathbf{q},\mathbf{d})/\tau)}{\sum\_{\mathbf{d^{\prime}}\in D}\text{exp}(\phi\_{\text{MaxSim}}(\mathbf{q},\mathbf{d^{\prime}})/\tau)} | , |  |

where ğ’Ÿğ’Ÿ\mathcal{D} is the set of all the passages, P^^ğ‘ƒ\hat{P} is the relevance probability estimated by the knowledge source, and Ï„ğœ\tau is the temperature to control the probability distribution.

Note that it is infeasible to enumerate all the passages during each training step; hence, followingÂ Chang etÂ al. ([2020](#bib.bib3)), we replace ğ’Ÿğ’Ÿ\mathcal{D} with a sampled passage set ğ’Ÿâ„¬subscriptğ’Ÿâ„¬\mathcal{D}\_{\mathcal{B}} in the same batch â„¬â„¬\mathcal{B}.
Specifically, we have a batch of triplets (ğªi,ğqi+,ğqiâˆ’)iâˆˆâ„¬subscriptsubscriptğªğ‘–superscriptsubscriptğsubscriptğ‘ğ‘–superscriptsubscriptğsubscriptğ‘ğ‘–ğ‘–â„¬(\mathbf{q}\_{i},\mathbf{d}\_{q\_{i}}^{+},\mathbf{d}\_{q\_{i}}^{-})\_{i\in\mathcal{B}} as follows. For a query ğªisubscriptğªğ‘–\mathbf{q}\_{i}, we have:

1. 1.

   a positive passage ğqi+superscriptsubscriptğsubscriptğ‘ğ‘–\mathbf{d}\_{q\_{i}}^{+} in a positive labeled set ğ’¯qi+superscriptsubscriptğ’¯subscriptğ‘ğ‘–\mathcal{T}\_{q\_{i}}^{+},
2. 2.

   a negative passage ğqiâˆ’superscriptsubscriptğsubscriptğ‘ğ‘–\mathbf{d}\_{q\_{i}}^{-} in a negative set ğ’¯qi;BM25âˆ’superscriptsubscriptğ’¯
   subscriptğ‘ğ‘–BM25\mathcal{T}\_{q\_{i};\text{BM25}}^{-} sampled by BM25 but not in ğ’¯qi+superscriptsubscriptğ’¯subscriptğ‘ğ‘–\mathcal{T}\_{q\_{i}}^{+}, and
3. 3.

   the rest of passages for other queries {ğªj}jâˆˆâ„¬subscriptsubscriptğªğ‘—ğ‘—â„¬\{\mathbf{q}\_{j}\}\_{j\in\mathcal{B}} in the same batch: {ğqj+}jâˆˆâ„¬âˆª{ğqjâˆ’}jâˆˆâ„¬subscriptsuperscriptsubscriptğsubscriptğ‘ğ‘—ğ‘—â„¬subscriptsuperscriptsubscriptğsubscriptğ‘ğ‘—ğ‘—â„¬\{\mathbf{d}\_{q\_{j}}^{+}\}\_{j\in\mathcal{B}}\cup\{\mathbf{d}\_{q\_{j}}^{-}\}\_{j\in\mathcal{B}}.

We denote the negative passage set ğ’¯qi;â„¬âˆ’superscriptsubscriptğ’¯

subscriptğ‘ğ‘–â„¬\mathcal{T}\_{q\_{i};\mathcal{B}}^{-} for a query qisubscriptğ‘ğ‘–q\_{i} as the union of (2) and (3).
We train our model using the following objective function:

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„’=âˆ’âˆ‘i=1|â„¬|{Î³â‹…ğŸ™ğiâˆˆğ’¯qi+â€‹logâ¡(Pâ€‹(ği|ğªi))âˆ’(1âˆ’Î³)â€‹âˆ‘ğâ€²âˆˆğ’Ÿâ„¬KLâ€‹(P^â€‹(ğâ€²|ğªi)â€‹||â€‹Pâ€‹(ğâ€²|ğªi))},â„’superscriptsubscriptğ‘–1â„¬limit-fromâ‹…ğ›¾subscript1subscriptğğ‘–superscriptsubscriptğ’¯subscriptğ‘ğ‘–ğ‘ƒconditionalsubscriptğğ‘–subscriptğªğ‘–1ğ›¾subscriptsuperscriptğâ€²subscriptğ’Ÿâ„¬KL^ğ‘ƒconditionalsuperscriptğâ€²subscriptğªğ‘–   ğ‘ƒconditionalsuperscriptğâ€²subscriptğªğ‘–\displaystyle\mathcal{L}=-\sum\_{i=1}^{|\mathcal{B}|}\left\{\begin{aligned} \gamma\cdot\mathds{1}\_{\mathbf{d}\_{i}\in\mathcal{T}\_{q\_{i}}^{+}}\log(P(\mathbf{d}\_{i}|\mathbf{q}\_{i}))-\\ (1-\gamma)\sum\_{\mathbf{d^{\prime}}\in\mathcal{D}\_{\mathcal{B}}}\text{KL}(\hat{P}(\mathbf{d^{\prime}}|\mathbf{q}\_{i})\lvert\rvert P(\mathbf{d^{\prime}}|\mathbf{q}\_{i}))\end{aligned}\right\}, |  | (6) |

where the first term corresponds to the softmax cross entropy over relevance labels, the second term denotes the KL divergence between the sampled probability distributions from our teacher, the Tightly-Coupled Teacher ColBERT (TCT-ColBERT), and our student, the Siamese NetworkÂ Bromley etÂ al. ([1993](#bib.bib2)) with BERT-base as encoders, denoted as bi-encoder (TCT-ColBERT).
The hyperparameter Î³ğ›¾\gamma controls the loss from hard and soft labels.333The pretrained weights for BERT-base are from <https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip>.
During the fine-tuning of the bi-encoder (TCT-ColBERT), we freeze the weight of ColBERT and set the temperature Ï„ğœ\tau and Î³ğ›¾\gamma to 0.25 and 0.1, respectively.

### 3.2 Hybrid Dense-Sparse Ranking

As shown inÂ Luan etÂ al. ([2020](#bib.bib19)); Gao etÂ al. ([2020](#bib.bib7)), a single dense embedding cannot sufficiently represent passages, especially when the passages are long, and they further demonstrate that sparse retrieval can complement dense retrieval by a linear combination of their scores.
However, it is not practical to compute scores over all query and passage pairs, especially when the corpus is large.
Thus, we propose an alternative approximation, which is easy to implement.
In this work, we conduct end-to-end sparse and dense retrieval using AnseriniÂ Yang etÂ al. ([2018](#bib.bib25))444<https://github.com/castorini/anserini> and FaissÂ Johnson etÂ al. ([2017](#bib.bib13)),555<https://github.com/facebookresearch/faiss> respectively.

For each query ğªğª\mathbf{q}, we use sparse and dense representations to retrieve top 1000 passages, ğ’Ÿsâ€‹psubscriptğ’Ÿğ‘ ğ‘\mathcal{D}\_{sp} and ğ’Ÿdâ€‹ssubscriptğ’Ÿğ‘‘ğ‘ \mathcal{D}\_{ds}, with their relevance scores, Ï•sâ€‹pâ€‹(ğª,ğâˆˆğ’Ÿsâ€‹p)subscriptitalic-Ï•ğ‘ ğ‘

ğªğ
subscriptğ’Ÿğ‘ ğ‘\phi\_{sp}(\mathbf{q},\mathbf{d}\in\mathcal{D}\_{sp}) and Ï•dâ€‹sâ€‹(ğª,ğâˆˆğ’Ÿdâ€‹s)subscriptitalic-Ï•ğ‘‘ğ‘ 

ğªğ
subscriptğ’Ÿğ‘‘ğ‘ \phi\_{ds}(\mathbf{q},\mathbf{d}\in\mathcal{D}\_{ds}), respectively.
Then, we compute the scores for each retrieved passages, ğâˆˆğ’Ÿsâ€‹pâˆªğ’Ÿdâ€‹sğsubscriptğ’Ÿğ‘ ğ‘subscriptğ’Ÿğ‘‘ğ‘ \mathbf{d}\in\mathcal{D}\_{sp}\cup\mathcal{D}\_{ds}, as follows:

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ï•â€‹(ğª,ğ)={Î±â‹…Ï•sâ€‹pâ€‹(ğª,ğ)+minğâˆˆğ’Ÿdâ€‹sâ€‹Ï•dâ€‹sâ€‹(ğª,ğ),ifÂ â€‹ğâˆ‰Ddâ€‹sÎ±â‹…minğâˆˆğ’Ÿsâ€‹pâ€‹Ï•sâ€‹pâ€‹(ğª,ğ)+Ï•dâ€‹sâ€‹(ğª,ğ),ifÂ â€‹ğâˆ‰Dsâ€‹pÎ±â‹…Ï•sâ€‹pâ€‹(ğª,ğ)+Ï•dâ€‹sâ€‹(ğª,ğ),otherwise.italic-Ï•ğªğcasesâ‹…ğ›¼subscriptitalic-Ï•ğ‘ ğ‘ğªğğsubscriptğ’Ÿğ‘‘ğ‘ subscriptitalic-Ï•ğ‘‘ğ‘ ğªğifÂ ğsubscriptğ·ğ‘‘ğ‘ â‹…ğ›¼ğsubscriptğ’Ÿğ‘ ğ‘subscriptitalic-Ï•ğ‘ ğ‘ğªğsubscriptitalic-Ï•ğ‘‘ğ‘ ğªğifÂ ğsubscriptğ·ğ‘ ğ‘â‹…ğ›¼subscriptitalic-Ï•ğ‘ ğ‘ğªğsubscriptitalic-Ï•ğ‘‘ğ‘ ğªğotherwise.\phi(\mathbf{q},\mathbf{d})=\begin{cases}\alpha\cdot\phi\_{sp}(\mathbf{q},\mathbf{d})+\underset{\mathbf{d}\in\mathcal{D}\_{ds}}{\min}\phi\_{ds}(\mathbf{q},\mathbf{d}),&\text{if }\mathbf{d}\notin D\_{ds}\\ \alpha\cdot\underset{\mathbf{d}\in\mathcal{D}\_{sp}}{\min}\phi\_{sp}(\mathbf{q},\mathbf{d})+\phi\_{ds}(\mathbf{q},\mathbf{d}),&\text{if }\mathbf{d}\notin D\_{sp}\\ \alpha\cdot\phi\_{sp}(\mathbf{q},\mathbf{d})+\phi\_{ds}(\mathbf{q},\mathbf{d}),&\text{otherwise.}\\ \end{cases} |  | (7) |

Eq.Â ([7](#S3.E7 "In 3.2 Hybrid Dense-Sparse Ranking â€£ 3 Methodology â€£ Distilling Dense Representations for Ranking using Tightly-Coupled Teachers")) is an approximation of linear combination of sparse and dense relevant scores.
For approximation, if ğâˆ‰ğ’Ÿsâ€‹pâ€‹(orÂ â€‹ğ’Ÿdâ€‹s)ğsubscriptğ’Ÿğ‘ ğ‘orÂ subscriptğ’Ÿğ‘‘ğ‘ \mathbf{d}\notin\mathcal{D}\_{sp}(\text{or }\mathcal{D}\_{ds}), we directly use the minimum score of Ï•sâ€‹pâ€‹(ğª,ğâˆˆğ’Ÿsâ€‹p)subscriptitalic-Ï•ğ‘ ğ‘

ğªğ
subscriptğ’Ÿğ‘ ğ‘\phi\_{sp}(\mathbf{q},\mathbf{d}\in\mathcal{D}\_{sp}), or Ï•dâ€‹sâ€‹(ğª,ğâˆˆğ’Ÿdâ€‹s)subscriptitalic-Ï•ğ‘‘ğ‘ 

ğªğ
subscriptğ’Ÿğ‘‘ğ‘ \phi\_{ds}(\mathbf{q},\mathbf{d}\in\mathcal{D}\_{ds}) as a substitute.

Table 1: Main results on passage retrieval tasks.

|  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- |
|  | MS MARCO dev | | TREC2019 DL | | latency |
|  | MRR@10 | R@1000 | NDCG@10 | R@1000 | (ms/query) |
| Sparse retrieval (Single Stage) |  |  |  |  |  |
| BM25 | 0.184 | 0.853 | 0.506 | 0.738 | 55 |
| DeepCTÂ Dai and Callan ([2020](#bib.bib5)) | 0.243 | 0.913 | 0.551 | 0.756 | 55 |
| doc2query-T5Â Nogueira and Lin ([2019](#bib.bib22)) | 0.277 | 0.947 | 0.642 | 0.802 | 64 |
| Dense retrieval (Single Stage) |  |  |  |  |  |
| ANCEÂ Xiong etÂ al. ([2020](#bib.bib24)) | 0.330 | 0.959 | 0.648 | - | 103 |
| Bi-encoder (PoolAvg) | 0.310 | 0.945 | 0.626 | 0.658 | 103 |
| Bi-encoder (TCT-ColBERT) | 0.335 | 0.964 | 0.670 | 0.720 | 103 |
| Multi-Stage |  |  |  |  |  |
| ColBERTÂ Khattab and Zaharia ([2020](#bib.bib15)) | 0.360 | 0.968 | - | - | 458 |
| BM25 + BERT-largeÂ Nogueira and Cho ([2019](#bib.bib21)) | 0.365 | - | 0.736 | - | 3,500 |
| Hybrid dense + sparse (Single Stage) |  |  |  |  |  |
| CLEARÂ Gao etÂ al. ([2020](#bib.bib7)) | 0.338 | 0.969 | 0.699 | 0.812 | - |
| Bi-encoder (PoolAvg) + BM25 | 0.342 | 0.962 | 0.701 | 0.804 | 106 |
| Bi-encoder (TCT-ColBERT) + BM25 | 0.352 | 0.970 | 0.714 | 0.819 | 106 |
| Bi-encoder (PoolAvg) + doc2query-T5 | 0.354 | 0.970 | 0.719 | 0.818 | 106 |
| Bi-encoder (TCT-ColBERT) + doc2query-T5 | 0.364 | 0.973 | 0.739 | 0.832 | 106 |

## 4 Experimental Setup

To demonstrate the efficiency and effectiveness of our proposed design, we conduct experiments on a large-scale real world dataset.
We first describe the experiment settings and then elaborate on our empirical results in detail.

We conduct ad hoc passage retrieval on the MS MARCO ranking dataset (henceforth, MS MARCO)Â Bajaj etÂ al. ([2016](#bib.bib1)).
It consists a collection of 8.8M passage from web pages and a set of 0.5M relevant (question, passage) pairs as training data, where each query on average has one relevant passage.
We follow two protocols for evaluation aligned with previous workÂ Nogueira and Lin ([2019](#bib.bib22)); Dai and Callan ([2020](#bib.bib5)); Gao etÂ al. ([2020](#bib.bib7)); Luan etÂ al. ([2020](#bib.bib19)); Khattab and Zaharia ([2020](#bib.bib15)):

* (a)

  MS MARCO Dev: 6980 queries comprise the development set for MS MARCO, with on average one relevant passage per query.
  We report MRR@10 and R@1000 as top-kğ‘˜k retrieval measures.
* (b)

  TREC-2019 DLÂ Craswell etÂ al. ([2019](#bib.bib4)):Â the organizers of the 2019 Deep Learning track at the Text REtrieval Conference (TREC) released 43 queries with multiple graded relevance labels, where 9k (query, passage) pairs were annotated by NIST assessors.
  We report NDCG@10 and R@1000 for this evaluation set.

There are two steps in our training procedure:Â (1) fine-tune Ï•MaxSimsubscriptitalic-Ï•MaxSim\phi\_{\text{MaxSim}} as our teacher model, (2) freeze Ï•MaxSimsubscriptitalic-Ï•MaxSim\phi\_{\text{MaxSim}} and distill knowledge into our student model while fine-tuning Ï•Poolsubscriptitalic-Ï•Pool\phi\_{\text{Pool}}.
For both steps, we train models on the MS MARCO â€œsmallâ€ triples training set for 160k iterations with a batch size of 96.
Note that at the second stage, we initialize the student model using the trained weights of the teacher model.
We fix sequence length to 32 and 150 for queries and passages, respectively.
For the sparse and dense retrieval combination, we tune the hyperparameter Î±ğ›¼\alpha on 6000 randomly sampled queries from the 0.5M queries with relevance labels for training (the â€œtrain qrelsâ€).
We conduct denseâ€“sparse hybrid experiment with sparse signals from the original passages (denoted BM25) and docTTTTTqueryÂ Nogueira and Lin ([2019](#bib.bib22)) (denoted doc2query-T5).
The optimal Î±ğ›¼\alpha for BM25 and doc2query-T5 are 0.10 and 0.24 respectively.

## 5 Results

Our main results are shown in TableÂ [1](#S3.T1 "Table 1 â€£ 3.2 Hybrid Dense-Sparse Ranking â€£ 3 Methodology â€£ Distilling Dense Representations for Ranking using Tightly-Coupled Teachers"), which reports effectiveness metrics as well as query latency.
We divide different comparison conditions into four categories:Â sparse retrieval, dense retrieval, multi-stage, denseâ€“sparse hybrid.

The cross-encoder reranker ofÂ Nogueira and Cho ([2019](#bib.bib21)) provides a point of reference for multi-stage designs.
While it is effective, the model is also very slow.
In comparison, ColBERT is much faster, with only a small degradation in effectiveness.
However, it still relies on a two-stage retrieval design, and is about four times slower than other single-stage dense retrieval ANN search methods.

As far as we are aware, ANCEÂ Xiong etÂ al. ([2020](#bib.bib24)) is the current state of the art for single-stage dense retrieval, but as we have explained, its asynchronous training requires re-encoding and re-indexing the whole corpus during training.
Our proposed method, bi-encoder (TCT-ColBERT) slightly outperforms ANCE in terms of ranking accuracy and recall.
To highlight the effectiveness of our training strategy, we report the effectiveness of the bi-encoder design without distillation, denoted bi-encoder (PoolAvg), for a fair comparison.
A sizeable effectiveness increase from bi-encoder (PoolAvg) to bi-encoder (TCT-ColBERT) is observed in both tasks:Â +0.025â€‹(+0.056)0.0250.056+0.025\ (+0.056) in MRR@10 and +0.019â€‹(+0.062)0.0190.062+0.019\ (+0.062) in R@1000 for MS MARCO (TREC2019 DL).

When we further incorporate sparse signals, our proposed method beats the current state of the art in hybrid approaches, CLEARÂ Gao etÂ al. ([2020](#bib.bib7)), in both the MS MARCO and TREC 2019 DL tasks.
Combined with BM25, our model already exhibits better retrieval effectiveness than CLEAR.
In addition, the comparison between bi-encoder (PoolAvg) and bi-encoder (TCT-ColBERT) demonstrates that the gain from distilled dense representation is still present, even with the advanced sparse retrieval method doc2query-T5:Â +0.010â€‹(+0.010)0.0100.010+0.010\ (+0.010) and +0.013â€‹(+0.020)0.0130.020+0.013\ (+0.020) with BM25 (doc2query-T5) in MRR@10 for both the MS MARCO and TREC2019 DL tasks, respectively.
The advanced hybrids (entries with doc2query-T5) reaches effectiveness even better than ColBERT and is almost on par with the cross-encoder reranker.
It is also worth noting that our hybrid end-to-end retrieval method yields state-of-the-art recall in both tasks.
More importantly, our proposed method is four times and thirty times more efficient than the multi-stage methods:Â ColBERT and the cross-encoder reranker, respectively.
These results demonstrate that the denseâ€“sparse hybrid is a promising solution for low latency end-to-end text retrieval.

Table 2: Component latency.

| Stage | latency | device |
| --- | --- | --- |
|  | (ms/query) |  |
| BERT query encoder | 3 | GPU |
| Dot product search | 100 | GPU |
| Score combination | 3 | CPU |

Latency.
TableÂ [2](#S5.T2 "Table 2 â€£ 5 Results â€£ Distilling Dense Representations for Ranking using Tightly-Coupled Teachers") shows the breakdown of end-to-end retrieval latency into individual components.
Specifically, we measure the system overhead of query embedding generation, dense retrieval with top 1000 passages, and denseâ€“sparse score combination.
To obtain the latency for dense retrieval, we run BERT query encoder and dot product search using a 32GB V100 GPU.
Specifically, we conduct brute force dot product search in Faiss (indexing with IndexFlatIP).
As for the denseâ€“sparse hybrid, we assume sparse and dense retrieval can be run in parallel; this is a realistic assumption because sparse retrieval runs on CPUs.
Thus, the total latency of the hybrid model (shown in TableÂ [1](#S3.T1 "Table 1 â€£ 3.2 Hybrid Dense-Sparse Ranking â€£ 3 Methodology â€£ Distilling Dense Representations for Ranking using Tightly-Coupled Teachers")) is bound by dense retrieval with additional 3ms for score combination (since sparse retrieval is faster than dense retrieval).

Ablation study.
Finally, we study the effectiveness of our distilled dense representations on the MS MARCO development set under two settings, reranking and retrieval.
For reranking, we use the public development set retrieved using BM25 for the reranking task (provided by the organizers),
and conduct reranking using dot product scores; for retrieval, we conduct brute force dot product search over the whole corpus.
We split our distillation strategy into two key features of our proposed technique:Â triplet and in-batch subsampling, from which we expect to see the effectiveness of triplet distillation (conditionÂ 2) and in-batch subsampling distillation (conditionÂ 3).
Specifically, by triplet distillation (conditionÂ 2) we mean that for each query ğªisubscriptğªğ‘–\mathbf{q}\_{i}, we only compute soft labels of its triplet (ğªi,ğqi+,ğqiâˆ’

subscriptğªğ‘–superscriptsubscriptğsubscriptğ‘ğ‘–superscriptsubscriptğsubscriptğ‘ğ‘–\mathbf{q}\_{i},\mathbf{d}\_{q\_{i}}^{+},\mathbf{d}\_{q\_{i}}^{-}) for distillation instead of the whole in-batch samples (conditionÂ 3).

Table 3: Ablation study on MS MARCO dev set.

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | Distillation strategy | | MRR@10 | |
| Cond. | Triplet | In-batch | Re-ranking | Retrieval |
| 1 |  |  | 0.319 | 0.310 |
| 2 | âœ“âœ“\checkmark |  | 0.332 | 0.328 |
| 3 | âœ“âœ“\checkmark | âœ“âœ“\checkmark | 0.332 | 0.335 |

TableÂ [3](#S5.T3 "Table 3 â€£ 5 Results â€£ Distilling Dense Representations for Ranking using Tightly-Coupled Teachers") reports the ranking accuracy in terms of MRR@10.
First, we observe reranking yields better effectiveness than retrieval in conditions 1 and 2.
This indicates retrieval is a more challenging task than reranking, and potentially explains the discrepancy between training and inference noted byÂ Xiong etÂ al. ([2020](#bib.bib24)).
That is, in the training phase, the models only learn to discriminate positive passages from BM25-generated negative samples, which is similar to the reranking task; however, when conducting retrieval, models are required to rank documents from the whole corpus.
Despite the discrepancy between training and retrieval, in-batch subsampling (condition 3) shows better retrieval accuracy.
We attribute this to the distilled knowledge from in-batch samples.

Correspondingly, the superior effectiveness from in-batch subsampling showcases a key advantage of our design because the dynamic subsampling is feasible only when using a tightly-coupled teacher.
More advanced sampling methods such as importance sampling beyond uniform in-batch subsampling can be incorporated with our tightly-coupled teacher method, which we leave for future work.

## 6 Conclusions

Learned dense representations for ranking have recently attracted the attention of many researchers.
This approach is exciting because it has the potential to supplement, and perhaps even replace, sparse vector representations using inverted indexes.
There are no doubt many concurrent explorations along these lines, and we add our own contributions to the mix.
Knowledge distillation is a promising approach, and even beyond our specific approach built on ColBERT, we believe that our insight of tighter teacherâ€“student coupling can be applied to other models and contexts as well.

## Acknowledgements

This research was supported in part by the Canada First Research Excellence Fund and the Natural Sciences and Engineering Research Council (NSERC) of Canada.
Additionally, we would like to thank Google for computational resources in the form of Google Cloud credits.

## References

* Bajaj etÂ al. (2016)

  Payal Bajaj, Daniel Campos, Nick Craswell, LiÂ Deng, Jianfeng Gao, Xiaodong Liu,
  Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, etÂ al. 2016.
  MS MARCO: A human generated machine reading comprehension dataset.
  *arXiv:1611.09268*.
* Bromley etÂ al. (1993)

  Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard SÃ¤ckinger, and Roopak
  Shah. 1993.
  Signature verification using a â€Siameseâ€ time delay neural network.
  In *Proc. NeurIPS*, page 737â€“744.
* Chang etÂ al. (2020)

  Wei-Cheng Chang, FelixÂ X. Yu, Yin-Wen Chang, Yiming Yang, and Sanjiv Kumar.
  2020.
  Pre-training tasks for embedding-based large-scale retrieval.
  In *Proc. ICLR*.
* Craswell etÂ al. (2019)

  Nick Craswell, Bhaskar Mitra, and Daniel Campos. 2019.
  Overview of the TREC 2019 deep learning track.
  In *Proc. TREC*.
* Dai and Callan (2020)

  Zhuyun Dai and Jamie Callan. 2020.
  Context-aware term weighting for first stage passage retrieval.
  In *Proc. SIGIR*, page 1533â€“1536.
* Devlin etÂ al. (2018)

  Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.
  BERT: Pre-training of deep bidirectional transformers for language
  understanding.
  *arXiv:1810.04805*.
* Gao etÂ al. (2020)

  Luyu Gao, Zhuyun Dai, Zhen Fan, and Jamie Callan. 2020.
  Complementing lexical retrieval with semantic residual embedding.
  *arXiv:2004.13969*.
* Guu etÂ al. (2020)

  Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020.
  REALM: Retrieval-augmented language model pre-training.
  *arXiv:2002.08909*.
* Hinton etÂ al. (2015)

  Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean. 2015.
  Distilling the knowledge in a neural network.
  In *Proc. NeurIPS: Deep Learning and Representation Learning
  Workshop*.
* HofstÃ¤tter etÂ al. (2020)

  Sebastian HofstÃ¤tter, Sophia Althammer, Michael SchrÃ¶der, Mete
  Sertkan, and Allan Hanbury. 2020.
  Improving efficient neural ranking models with cross-architecture
  knowledge distillation.
  *arXiv:2010.02666*.
* HofstÃ¤tter and Hanbury (2019)

  Sebastian HofstÃ¤tter and Allan Hanbury. 2019.
  Letâ€™s measure run time! extending the IR replicability
  infrastructure to include performance aspects.
  In *Proc. OSIRRC:Â CEUR Workshop*, pages 12â€“16.
* Humeau etÂ al. (2020)

  Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, and Jason Weston. 2020.
  Poly-encoders: Architectures and pre-training strategies for fast
  and accurate multi-sentence scoring.
  In *Proc. ICLR*.
* Johnson etÂ al. (2017)

  Jeff Johnson, Matthijs Douze, and HervÃ© JÃ©gou. 2017.
  Billion-scale similarity search with GPUs.
  *arXiv:1702.08734*.
* Karpukhin etÂ al. (2020)

  Vladimir Karpukhin, Barlas OÄŸuz, Sewon Min, Ledell Wu, Sergey Edunov,
  Danqi Chen, and Wen-tau Yih. 2020.
  Dense passage retrieval for open-domain question answering.
  *arXiv:2004.04906*.
* Khattab and Zaharia (2020)

  Omar Khattab and Matei Zaharia. 2020.
  ColBERT: Efficient and effective passage search via contextualized
  late interaction over BERT.
  In *Proc. SIGIR*, page 39â€“48.
* Lee etÂ al. (2019)

  Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019.
  Latent retrieval for weakly supervised open domain question
  answering.
  In *Proc. ACL*, pages 6086â€“6096.
* Lin etÂ al. (2020)

  Jimmy Lin, Rodrigo Nogueira, and Andrew Yates. 2020.
  Pretrained transformers for text ranking: BERT and beyond.
  *arXiv:2010.06467*.
* Liu etÂ al. (2004)

  Ting Liu, AndrewÂ W. Moore, Alexander Gray, and KeÂ Yang. 2004.
  An investigation of practical approximate nearest neighbor
  algorithms.
  In *Proc. NeurIPS*, page 825â€“832.
* Luan etÂ al. (2020)

  YiÂ Luan, Jacob Eisenstein, Kristina Toutanova, and Michael Collins. 2020.
  Sparse, dense, and attentional representations for text retrieval.
  *arXiv:2005.00181*.
* Malkov and Yashunin (2020)

  YuÂ A. Malkov and D.Â A. Yashunin. 2020.
  Efficient and robust approximate nearest neighbor search using
  hierarchical navigable small world graphs.
  *IEEE Trans. Pattern Anal. Mach. Intell.*, 42(4):824â€“836.
* Nogueira and Cho (2019)

  Rodrigo Nogueira and Kyunghyun Cho. 2019.
  Passage re-ranking with BERT.
  *arXiv:1901.04085*.
* Nogueira and Lin (2019)

  Rodrigo Nogueira and Jimmy Lin. 2019.
  From doc2query to docTTTTTquery.
* Shrivastava and Li (2014)

  Anshumali Shrivastava and Ping Li. 2014.
  Asymmetric LSH (ALSH) for sublinear time maximum inner product
  search (MIPS).
  In *Proc. NeurIPS*, page 2321â€“2329.
* Xiong etÂ al. (2020)

  Lee Xiong, Chenyan Xiong, YeÂ Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett,
  Junaid Ahmed, and Arnold Overwijk. 2020.
  Approximate nearest neighbor negative contrastive learning for dense
  text retrieval.
  *arXiv:2007.00808*.
* Yang etÂ al. (2018)

  Peilin Yang, Hui Fang, and Jimmy Lin. 2018.
  Anserini: Reproducible ranking baselines using Lucene.
  *ACM J. Data. Inf. Qual.*, 10(4):Article 16.