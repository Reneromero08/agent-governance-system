# Introduction

The release of large pre-trained language models like BERT [@bert] has shaken-up Natural Language Processing and Information Retrieval. These models have shown a strong ability to adapt to various tasks by simple fine-tuning. At the beginning of 2019, *Nogueira and Cho* [@passage_ranking] achieved state-of-the-art results -- by a large margin -- on the MS MARCO passage re-ranking task, paving the way for LM-based neural ranking models. Because of strict efficiency requirements, these models have initially been used as re-rankers in a two-stage ranking pipeline, where first-stage retrieval -- or candidate generation -- is conducted with bag-of-words models (e.g. BM25) that rely on inverted indexes. While BOW models remain strong baselines [@neural_hype], they suffer from the long standing vocabulary mismatch problem, where relevant documents might not contain terms that appear in the query. Thus, there have been attempts to substitute standard BOW approaches by learned (neural) rankers. Designing such models poses several challenges regarding efficiency and scalability: therefore there is a need for methods where most of the computation can be done offline and online inference is fast. Dense retrieval with approximate nearest neighbors search has shown impressive results [@xiong2021approximate; @qu-etal-2021-rocketqa; @lin-etal-2021-batch; @Hofstaetter2021_tasb_dense_retrieval], but can still benefit from BOW models (e.g. by combining both types of signals), due to the absence of explicit term matching. Hence, there has recently been a growing interest in learning *sparse representations* for queries and documents [@snrm; @dai2019contextaware; @nogueira2019document; @zhao2020sparta; @sparterm2020; @gao-etal-2021-coil; @10.1145/3404835.3463030; @10.1145/3404835.3463098]. By doing so, models can inherit from the desirable properties of BOW models like exact-match of (possibly latent) terms, efficiency of inverted indexes and interpretability. Additionally, by modeling implicit or explicit (latent, contextualized) *expansion* mechanisms -- similarly to standard expansion models in IR -- these models can reduce the vocabulary mismatch.

In this paper, we build on the SPLADE model [@10.1145/3404835.3463098] and propose several improvements/modifications that bring gains in terms of effectiveness or efficiency:

::: enumerate*
by simply modifying SPLADE pooling mechanism, we are able to increase effectiveness by a large margin;

in the meantime, we propose an extension of the model without query expansion. Such model is inherently more efficient, as everything can be pre-computed and indexed offline, while providing results that are still competitive;

finally, we use distillation techniques [@hofstatter2020improving] to boost SPLADE performance, leading to close to state-of-the-art results on the MS MARCO passage ranking task as well as the BEIR zero-shot evaluation benchmark [@beir_2021].
:::

# Related Works

Dense retrieval based on BERT Siamese models [@sentence_bert] has become the standard approach for candidate generation in Question Answering and IR [@guu2020realm; @karpukhin2020dense; @xiong2021approximate; @qu-etal-2021-rocketqa; @lin-etal-2021-batch; @Hofstaetter2021_tasb_dense_retrieval]. While the backbone of these models remains the same, recent works highlight the critical aspects of the training strategy to obtain state-of-the-art results, ranging from improved negative sampling [@lin-etal-2021-batch; @Hofstaetter2021_tasb_dense_retrieval] to distillation [@hofstatter2020improving; @lin-etal-2021-batch]. ColBERT [@colbert] pushes things further: the postponed token-level interactions allow to efficiently apply the model for first-stage retrieval, benefiting of the effectiveness of modeling fine-grained interactions, at the cost of storing embeddings for each (sub)term -- raising concerns about the actual scalability of the approach for large collections. To the best of our knowledge, very few studies have discussed the impact of using *approximate* nearest neighbors (ANN) search on IR metrics [@boytsov2018efficient; @tu2020approximate]. Due to the moderate size of the MS MARCO collection, results are usually reported with an *exact*, brute-force search, therefore giving no indication on the effective computing cost.

An alternative to dense indexes is term-based ones. Building on standard BOW models, *Zamani et al.* first introduced SNRM [@snrm]: the model embeds documents and queries in a sparse high-dimensional latent space by means of $\ell_1$ regularization on representations. However, SNRM effectiveness remains limited and its efficiency has been questioned [@paria2020minimizing].

Motivated by the success of BERT, there have been attempts to transfer the knowledge from pre-trained LM to sparse approaches. DeepCT [@dai2019contextaware; @10.1145/3366423.3380258; @10.1145/3397271.3401204] focused on learning contextualized term weights in the full vocabulary space -- akin to BOW term weights. However, as the vocabulary associated with a document remains the same, this type of approach does not solve the vocabulary mismatch, as acknowledged by the use of query expansion for retrieval [@dai2019contextaware]. A first solution to this problem consists in expanding documents using generative approaches such as doc2query [@nogueira2019document] and doc2query-T5 [@doct5] to predict expansion words for documents. The document expansion adds new terms to documents -- hence fighting the vocabulary mismatch -- as well as repeats existing terms, implicitly performing re-weighting by boosting important terms.

Recently, DeepImpact [@10.1145/3404835.3463030] combined the expansion from doc2query-T5 with the re-weighting from DeepCT to learn term *impacts*. These expansion techniques are however limited by the way they are trained (predicting queries), which is indirect in nature and limit their progress. A second solution to this problem, that has been chosen by recent works [@sparterm2020; @MacAvaney_2020; @zhao2020sparta; @10.1145/3404835.3463098], is to estimate the importance of each term of the vocabulary *implied by* each term of the document (or query), i.e. to compute an interaction matrix between the document or query tokens and all the tokens from the vocabulary. This is followed by an aggregation mechanism (roughly sum for SparTerm [@sparterm2020] and SPLADE [@10.1145/3404835.3463098], max for EPIC [@MacAvaney_2020] and SPARTA [@zhao2020sparta]), that allows to compute an importance weight for each term of the vocabulary, for the full document or query.

However, EPIC and SPARTA (document) representations are not sparse enough by construction -- unless resorting on top-$k$ pooling -- contrary to SparTerm, for which fast retrieval is thus possible. Furthermore, the latter does not include (like SNRM) an *explicit* sparsity regularization, which hinders its performance. SPLADE however relies on such regularization, as well as other key changes, that boost both the efficiency and the effectiveness of this type of approaches, providing a model that both learns expansion and compression in an end-to-end manner. Furthermore, COIL [@gao-etal-2021-coil] proposed to revisit exact-match mechanisms by learning dense representations *per term* to perform contextualized term matching, at the cost of increased index size.

# Sparse Lexical representations for first-stage ranking

In this section, we first describe in details the SPLADE model recently introduced in [@10.1145/3404835.3463098].

## SPLADE

SPLADE predicts term importance -- in BERT WordPiece vocabulary ($|V|=30522$) -- based on the logits of the Masked Language Model (MLM) layer. More precisely, let us consider an input query or document sequence (after WordPiece tokenization) $t=(t_1,t_2,...,t_N)$, and its corresponding BERT embeddings $(h_1,h_2,...,h_N)$. We consider the importance $w_{ij}$ of the token $j$ (vocabulary) for a token $i$ (of the input sequence): $$\begin{equation}
    w_{ij} = \text{transform}(h_i)^T E_j + b_j \quad j \in \{1,...,|V|\}
    \label{equation_1}
\end{equation}$$ where $E_j$ denotes the BERT input embedding for token $j$, $b_j$ is a token-level bias, and transform$(.)$ is a linear layer with GeLU activation and LayerNorm. Note that Eq. [\[equation_1\]](#equation_1){reference-type="eqref" reference="equation_1"} is equivalent to the MLM prediction, thus it can also be initialized from a pre-trained MLM model. The final representation is then obtained by summing importance predictors over the input sequence tokens, after applying a log-saturation effect [@10.1145/3404835.3463098]: $$\begin{equation}
\label{eq:2}
w_j= \sum_{i \in t} \log \left(1 + \text{ReLU}(w_{ij}) \right)
\end{equation}$$

#### **Ranking loss**

Let $s(q,d)$ denote the ranking score obtained via dot product between $q$ and $d$ representations from Eq. [\[eq:2\]](#eq:2){reference-type="eqref" reference="eq:2"}. Given a query $q_i$ in a batch, a positive document $d_i^+$, a (hard) negative document $d_i^-$ (e.g. coming from BM25 sampling), and a set of negative documents in the batch (positive documents from other queries) $\{d_{i,j}^-\}_j$, we consider a constrastive loss, which can be interpreted as the maximization of the probability of the document $d_i^+$ being relevant among the documents $d_i^+, d_i^-$ and $\{d_{i,j}^-\}$:

$$\begin{equation}
\mathcal{L}_{rank-IBN} = - \log\frac{e^{s(q_i,d_i^+)}}{e^{s(q_i,d_i^+)} + e^{s(q_i,d_i^-)} + \sum_j e^{s(q_i,d_{i,j}^-)}}
\end{equation}$$

The *in-batch negatives* (IBN) sampling strategy is widely used for training image retrieval models, and has shown to be effective in learning first-stage rankers [@karpukhin2020dense; @qu-etal-2021-rocketqa; @lin-etal-2021-batch].

#### **Learning sparse representations**

The idea of learning sparse representations for first-stage retrieval dates back to SNRM [@snrm], via $\ell_1$ regularization. Later, [@paria2020minimizing] pointed-out that minimizing the $\ell_1$ norm of representations does not result in the most efficient index, as nothing ensures that posting lists are evenly distributed. Note that this is even more true for standard indexes due to the Zipfian nature of the term frequency distribution. To obtain a well-balanced index, *Paria et al.* [@paria2020minimizing] introduce the `FLOPS` regularizer, a smooth relaxation of the average number of floating-point operations necessary to compute the score between a query and a document, and hence directly related to the retrieval time. It is defined using $a_j$ as a continuous relaxation of the activation (i.e. the term has a non-zero weight) probability $p_j$ for token $j$, and estimated for documents $d$ in a batch of size $N$ by $\bar{a}_j=\frac{1}{N} \sum_{i=1}^N w^{(d_i)}_{j}$. This gives the following regularization loss $$\begin{equation}
\label{eq:reg}
\ell_{\texttt{FLOPS}} = \sum_{j\in V} {\bar a}_j^2 = \sum_{j \in V} \left( \frac{1}{N} \sum_{i=1}^N  w_j^{(d_i)} \right)^2
\end{equation}$$

#### **Overall loss**

By jointly optimizing the model in Eq. [\[eq:2\]](#eq:2){reference-type="eqref" reference="eq:2"} with ranking and regularization losses, SPLADE combines the best of both worlds for end-to-end training of sparse, expansion-aware representations of documents and queries: $$\begin{equation}
\mathcal{L} = \mathcal{L}_{rank-IBN} + \lambda_q \mathcal{L}^{q}_{\texttt{reg}} + \lambda_d \mathcal{L}^{d}_{\texttt{reg}}
\end{equation}$$ where $\mathcal{L}_{\texttt{reg}}$ is the sparse `FLOPS` regularization from Eq. [\[eq:reg\]](#eq:reg){reference-type="ref" reference="eq:reg"}. We use two distinct regularization weights ($\lambda_d$ and $\lambda_q$) for queries and documents -- allowing to put more pressure on the sparsity for queries, which is critical for fast retrieval.

## Pooling strategy

We propose to change the sum in Eq. [\[eq:2\]](#eq:2){reference-type="eqref" reference="eq:2"} by a $\max$ pooling operation: $$\begin{equation}
w_j= \max_{i \in t} \log \left(1 + \text{ReLU}(w_{ij}) \right)
\end{equation}$$ The model now bears more similarities with SPARTA and EPIC, and to some extent ColBERT. As shown in the experiments section, it considerably improves SPLADE performance. In the following, max pooling is the default configuration for SPLADE, and the corresponding model is referred to as SPLADE-${\max}$.

## SPLADE document encoder

In addition to the max pooling operation, we consider a document-only version of SPLADE. In this case, there are no query expansion nor query term weighting, and the ranking score is simply given by: $$\begin{equation}
 s(q,d) = \sum_{j  \in q}  w_j^d
\end{equation}$$

Such extension offers an interesting efficiency boost: because the ranking score solely depends on the document term weights, everything can be pre-computed offline, and inference cost is consequently reduced, while still offering competitive results as shown in the experiments. We refer to this model as SPLADE-doc.

## Distillation and hard negatives

We also incorporate distillation to our training procedure, following the improvements shown in [@hofstatter2020improving]. The distillation training is done in two steps:

::: enumerate*
we first train both a SPLADE first-stage retriever as well as a cross-encoder reranker [^1] using the triplets generated by [@hofstatter2020improving];

in the second step, we generate triplets using SPLADE trained with distillation (thus providing harder negatives than BM25), and use the aforementioned reranker to generate the scores needed for the Margin-MSE loss. We then train a SPLADE model from scratch using these triplets and scores. The result of the second step is what we call DistilSPLADE-${\max}$.
:::

# Experimental setting and results

We trained and evaluated our models on the MS MARCO passage ranking dataset[^2] in the full ranking setting. The dataset contains approximately $8.8$M passages, and hundreds of thousands training queries with shallow annotation ($\approx 1.1$ relevant passages per query in average). The development set contains $6980$ queries with similar labels, while the TREC DL 2019 evaluation set provides fine-grained annotations from human assessors for a set of $43$ queries [@craswell2020overview].

#### **Training, indexing and retrieval**

We initialized the models with the `DistilBERT-base` checkpoint. Models are trained with the ADAM optimizer, using a learning rate of $2e^{-5}$ with linear scheduling and a warmup of $6000$ steps, and a batch size of $124$. We keep the best checkpoint using MRR@10 on a validation set of $500$ queries, after training for $150$k iterations, using an approximate retrieval validation set similar to [@Hofstaetter2021_tasb_dense_retrieval]. For the SPLADE-doc approach, we simply train for $50$k steps and select the last checkpoint. We consider a maximum length of $256$ for input sequences. In order to mitigate the contribution of the regularizer at the early stages of training, we follow [@paria2020minimizing] and use a scheduler for $\lambda$, quadratically increasing $\lambda$ at each training iteration, until a given step ($50$k in our case), from which it remains constant. Typical values for $\lambda$ fall between $1e^{-1}$ and $1e^{-4}$. For storing the index, we use a custom implementation based on Python arrays, and we rely on Numba [@lam2015numba] to parallelize retrieval. Models[^3] are trained using PyTorch [@paszke2019pytorch] and HuggingFace transformers [@wolf2020huggingfaces], on $4$ Tesla $V100$ GPUs with $32$GB memory.

#### **Evaluation**

We report Recall@1000 for both datasets, as well as the official metrics MRR@10 and NDCG@10 for MS MARCO dev set and TREC DL 2019 respectively. Since we are essentially interested in the first retrieval step, we do not consider re-rankers based on BERT, and we compare our approach to first stage rankers only -- results reported on the MS MARCO leaderboard are thus not comparable to the results presented here. We compare to the following sparse approaches

::: enumerate*
BM25,

DeepCT [@dai2019contextaware],

doc2query-T5 [@doct5],

SparTerm [@sparterm2020],

COIL-tok [@gao-etal-2021-coil] and

DeepImpact [@10.1145/3404835.3463030]
:::

, as well as state-of-the-art dense approaches ANCE [@xiong2020approximate], TCT-ColBERT [@lin-etal-2021-batch] and TAS-B [@Hofstaetter2021_tasb_dense_retrieval], reporting results from corresponding papers.

MS MARCO dev and TREC DL 2019 results are given in Table [1](#table_1){reference-type="ref" reference="table_1"}: as the performance for SPLADE depends on the regularization strength $\lambda$, and as more efficient models are generally less effective, we select in the table the best performing model in our grid of experiments, with reasonable efficiency (in terms of FLOPS). Figure [1](#perf_flops){reference-type="ref" reference="perf_flops"} highlights the actual trade-off between effectiveness and efficiency for SPLADE, by showing the performance (MRR@10 on MS MARCO dev set) vs FLOPS, for SPLADE models trained with different regularization strength. The FLOPS metric is an estimation of the average number of floating-point operations between a query and a document which is defined as the expectation $\mathbb{E}_{q,d} \left[ \sum_{j \in V} p_j^{(q)}p_j^{(d)} \right]$ where $p_j$ is the activation probability for token $j$ in a document $d$ or a query $q$. It is empirically estimated from a set of approximately $100$k development queries, on the MS MARCO collection. Overall, we observe that:

::: enumerate*
*our improved models outperform the other sparse retrieval methods by a large margin on the MS MARCO dev set as well as TREC DL 2019 queries*;

*the results are competitive with state-of-the-art dense retrieval methods.*
:::

#### **BEIR**

Finally, we verify the zero-shot performance of SPLADE using a subset of datasets from the BEIR [@beir_2021] benchmark that encompasses various IR datasets for zero-shot evaluation. We solely use a subset due to the fact that some of the datasets (namely `CQADupstack`, `BioASQ`, `Signal-1M`, `TREC-NEWS`, `Robust04`) are not readily available. Results are displayed in Table [2](#table:beir_ndcg){reference-type="ref" reference="table:beir_ndcg"} (NDCG@10). We compare against the best performing models from the original benchmark paper [@beir_2021] (ColBERT [@colbert]) and the two best performing from the rolling benchmark[^4] (tuned BM25 and TAS-B [@Hofstaetter2021_tasb_dense_retrieval]). We also report the SPLADE evaluation against these baselines.

::: {#table_1}
+:----------------------------------------------+:------:+:------:+:-------:+:------:+
| model                                         | MS MARCO dev    | TREC DL 2019     |
+-----------------------------------------------+--------+--------+---------+--------+
|                                               | MRR@10 | R@1000 | NDCG@10 | R@1000 |
+-----------------------------------------------+--------+--------+---------+--------+
| `Dense retrieval`                             |        |        |         |        |
+-----------------------------------------------+--------+--------+---------+--------+
| Siamese (ours)                                | 0.312  | 0.941  | 0.637   | 0.711  |
+-----------------------------------------------+--------+--------+---------+--------+
| ANCE [@xiong2020approximate]                  | 0.330  | 0.959  | 0.648   | \-     |
+-----------------------------------------------+--------+--------+---------+--------+
| TCT-ColBERT [@lin-etal-2021-batch]            | 0.359  | 0.970  | 0.719   | 0.760  |
+-----------------------------------------------+--------+--------+---------+--------+
| TAS-B [@Hofstaetter2021_tasb_dense_retrieval] | 0.347  | 0.978  | 0.717   | 0.843  |
+-----------------------------------------------+--------+--------+---------+--------+
| RocketQA [@qu-etal-2021-rocketqa]             | 0.370  | 0.979  | \-      | \-     |
+-----------------------------------------------+--------+--------+---------+--------+
| `Sparse retrieval`                            |        |        |         |        |
+-----------------------------------------------+--------+--------+---------+--------+
| BM25                                          | 0.184  | 0.853  | 0.506   | 0.745  |
+-----------------------------------------------+--------+--------+---------+--------+
| DeepCT [@dai2019contextaware]                 | 0.243  | 0.913  | 0.551   | 0.756  |
+-----------------------------------------------+--------+--------+---------+--------+
| doc2query-T5 [@doct5]                         | 0.277  | 0.947  | 0.642   | 0.827  |
+-----------------------------------------------+--------+--------+---------+--------+
| SparTerm [@sparterm2020]                      | 0.279  | 0.925  | \-      | \-     |
+-----------------------------------------------+--------+--------+---------+--------+
| COIL-tok [@gao-etal-2021-coil]                | 0.341  | 0.949  | 0.660   | \-     |
+-----------------------------------------------+--------+--------+---------+--------+
| DeepImpact [@10.1145/3404835.3463030]         | 0.326  | 0.948  | 0.695   | \-     |
+-----------------------------------------------+--------+--------+---------+--------+
| SPLADE [@10.1145/3404835.3463098]             | 0.322  | 0.955  | 0.665   | 0.813  |
+-----------------------------------------------+--------+--------+---------+--------+
| `Our methods`                                 |        |        |         |        |
+-----------------------------------------------+--------+--------+---------+--------+
| SPLADE-${\max}$                               | 0.340  | 0.965  | 0.684   | 0.851  |
+-----------------------------------------------+--------+--------+---------+--------+
| SPLADE-doc                                    | 0.322  | 0.946  | 0.667   | 0.747  |
+-----------------------------------------------+--------+--------+---------+--------+
| DistilSPLADE-$\max$                           | 0.368  | 0.979  | 0.729   | 0.865  |
+-----------------------------------------------+--------+--------+---------+--------+

: Evaluation on MS MARCO passage retrieval (dev set) and TREC DL 2019.
:::

[]{#table_1 label="table_1"}

<figure id="perf_flops" data-latex-placement="b">
<p><embed src="figures/splade_new.pdf" style="width:45.0%" /> <span id="perf_flops" data-label="perf_flops"></span></p>
<figcaption>Performance vs FLOPS for SPLADE models trained with different regularization strength <span class="math inline"><em>λ</em></span> on MS MARCO.</figcaption>
</figure>

<figure id="splade_doc" data-latex-placement="b">
<p><embed src="figures/splade_doc.pdf" style="width:45.0%" /> <span id="splade_doc" data-label="splade_doc"></span></p>
<figcaption>Performance vs average document length (number of non-zero dimensions in document representations) for SPLADE-doc models trained with different regularization strength <span class="math inline"><em>λ</em><sub><em>d</em></sub></span> on MS MARCO.</figcaption>
</figure>

## Impact of max pooling

First, on MS MARCO and TREC, max pooling brings almost $2$ points in MRR@10 and NDCG@10 compared to the SPLADE baseline. It becomes competitive with COIL and DeepImpact. In addition, Figure [1](#perf_flops){reference-type="ref" reference="perf_flops"} shows that SPLADE-$\max$ is consistently better than SPLADE, in terms of effectiveness-efficiency trade-off. SPLADE-$\max$ has also improved performance on the BEIR benchmark (cf Table [2](#table:beir_ndcg){reference-type="ref" reference="table:beir_ndcg"}).

## Document expansion

Our document encoder model with max pooling is able to reach the same performance as the previous SPLADE model, outperforming doc2query-T5 on MS MARCO. As this model has no query encoder, it is more efficient in terms of e.g. latency. Figure [2](#splade_doc){reference-type="ref" reference="splade_doc"} illustrates how we can balance efficiency (in terms of the average size of document representations) with effectiveness. For relatively sparse representations, we are able to obtain performance on par with approaches like doc2query-T5 (e.g. MRR@10=$29.6$ for a model with an average of $19$ non-zero weights per document). In addition, it is straightforward to train and apply to a new document collection: a single forward is required as opposed to multiple inferences with beam search for doc2query-T5.

## Distillation

By training with distillation, we are able to considerably improve the performance of SPLADE, as seen in Table [1](#table_1){reference-type="ref" reference="table_1"}. From Figure [1](#perf_flops){reference-type="ref" reference="perf_flops"}, we observe that distilled models bring huge improvements for higher values of FLOPS ($0.368$ MRR@10 for $\approx$ $4$ FLOPS), but are still very efficient in low regime ($0.35$ MRR for $\approx$ 0.3 FLOPS). Furthermore, DistilSPLADE-$\max$ is able to outperform all other methods in most datasets of the BEIR benchmark (cf Table [2](#table:beir_ndcg){reference-type="ref" reference="table:beir_ndcg"}).

::: {#table:beir_ndcg}
+--------------------+-------------------------------+----------------------------------------------------+
| Corpus             | Baselines                     | SPLADE                                             |
+:===================+:=========:+:=========:+:=====:+:==============================:+:=====:+:=========:+
| 2-7                | ColBERT   | BM25      | TAS-B | sum [@10.1145/3404835.3463098] | max   | distil    |
+--------------------+-----------+-----------+-------+--------------------------------+-------+-----------+
| `MS MARCO`         | 0.425     | 0.228     | 0.408 | 0.387                          | 0.402 | **0.433** |
+--------------------+-----------+-----------+-------+--------------------------------+-------+-----------+
| `ArguAna`          | 0.233     | 0.315     | 0.427 | 0.447                          | 0.439 | **0.479** |
+--------------------+-----------+-----------+-------+--------------------------------+-------+-----------+
| `Climate-FEVER`    | 0.184     | 0.213     | 0.228 | 0.162                          | 0.199 | **0.235** |
+--------------------+-----------+-----------+-------+--------------------------------+-------+-----------+
| `DBPedia`          | 0.392     | 0.273     | 0.384 | 0.343                          | 0.366 | **0.435** |
+--------------------+-----------+-----------+-------+--------------------------------+-------+-----------+
| `FEVER`            | 0.771     | 0.753     | 0.700 | 0.728                          | 0.730 | **0.786** |
+--------------------+-----------+-----------+-------+--------------------------------+-------+-----------+
| `FiQA-2018`        | 0.317     | 0.236     | 0.300 | 0.258                          | 0.287 | **0.336** |
+--------------------+-----------+-----------+-------+--------------------------------+-------+-----------+
| `HotpotQA`         | 0.593     | 0.603     | 0.584 | 0.635                          | 0.636 | **0.684** |
+--------------------+-----------+-----------+-------+--------------------------------+-------+-----------+
| `NFCorpus`         | 0.305     | 0.325     | 0.319 | 0.311                          | 0.313 | **0.334** |
+--------------------+-----------+-----------+-------+--------------------------------+-------+-----------+
| `NQ`               | **0.524** | 0.329     | 0.463 | 0.438                          | 0.469 | 0.521     |
+--------------------+-----------+-----------+-------+--------------------------------+-------+-----------+
| `Quora`            | **0.854** | 0.789     | 0.835 | 0.829                          | 0.835 | 0.838     |
+--------------------+-----------+-----------+-------+--------------------------------+-------+-----------+
| `SCIDOCS`          | 0.145     | **0.158** | 0.149 | 0.141                          | 0.145 | **0.158** |
+--------------------+-----------+-----------+-------+--------------------------------+-------+-----------+
| `SciFact`          | 0.671     | 0.665     | 0.643 | 0.626                          | 0.628 | **0.693** |
+--------------------+-----------+-----------+-------+--------------------------------+-------+-----------+
| `TREC-COVID`       | 0.677     | 0.656     | 0.481 | 0.655                          | 0.673 | **0.710** |
+--------------------+-----------+-----------+-------+--------------------------------+-------+-----------+
| `Touché-2020 (v1)` | 0.275     | **0.614** | 0.173 | 0.289                          | 0.316 | 0.364     |
+--------------------+-----------+-----------+-------+--------------------------------+-------+-----------+
| Avg. all           | 0.455     | 0.440     | 0.435 | 0.446                          | 0.460 | **0.500** |
+--------------------+-----------+-----------+-------+--------------------------------+-------+-----------+
| Avg. zero-shot     | 0.457     | 0.456     | 0.437 | 0.451                          | 0.464 | **0.506** |
+--------------------+-----------+-----------+-------+--------------------------------+-------+-----------+
| Best on dataset    | 2         | 2         | 0     | 0                              | 0     | **11**    |
+--------------------+-----------+-----------+-------+--------------------------------+-------+-----------+

: NDCG@10 results on BEIR (subset containing all the readily available datasets).
:::

# Conclusion

In this paper, we have built on the SPLADE model by reconsidering its pooling mechanism, and by using standard training techniques such as distillation for neural IR models. Our experiments have shown that the max pooling technique indeed provides a substantial improvement. Secondly, the document encoder is an interesting model for faster retrieval conditions. Finally, the distilled SPLADE model leads to close to state-of-the-art models on MS MARCO and TREC DL 2019, while clearly outperforming recent dense models on zero-shot evaluation.

[^1]: Using <https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-12-v2> as pre-trained checkpoint.

[^2]: <https://github.com/microsoft/MSMARCO-Passage-Ranking>

[^3]: We made the code public at <https://github.com/naver/splade>

[^4]: <https://docs.google.com/spreadsheets/d/1L8aACyPaXrL8iEelJLGqlMqXKPX2oSP_R10pZoy77Ns>
