# Text Embeddings Reveal (Almost) As Much As Text

John X. Morris, Volodymyr Kuleshov, Vitaly Shmatikov, Alexander M. Rush
  
Department of Computer Science
  
Cornell University

###### Abstract

How much private information do text embeddings reveal about the original text? We investigate the problem of embedding inversion, reconstructing the full text represented in dense text embeddings. We frame the problem as controlled generation: generating text that, when reembedded, is close to a fixed point in latent space. We find that although a naÃ¯ve model conditioned on the embedding performs poorly, a multi-step method that iteratively corrects and re-embeds text is able to recover 92%percent9292\% of 32â€‹-token32-token32\text{-token} text inputs exactly. We train our model to decode text embeddings from two state-of-the-art embedding models, and also show that our model can recover important personal information (full names) from a dataset of clinical notes. 111Our code is available on Github: [github.com/jxmorris12/vec2text](https://github.com/jxmorris12/vec2text).

## 1 Introduction

![Refer to caption](/html/2310.06816/assets/x1.png)

Figure 1: Overview of our method, Vec2Text. Given access to a target embedding eğ‘’e (blue) and query access to an embedding model Ï•italic-Ï•\phi (blue model), the system aims to iteratively generate (yellow model) hypotheses e^^ğ‘’\hat{e} (pink) to reach the target. Example input is a taken from a recent Wikipedia article (June 2023). Vec2TextÂ perfectly recovers this text from its embedding after 444 rounds of correction.

Systems that utilize large language models (LLMs) often store auxiliary data in a vector database of dense embeddings Borgeaud etÂ al. ([2022](#bib.bib6)); Yao etÂ al. ([2023](#bib.bib53)). Users of these systems infuse knowledge into LLMs by inserting retrieved documents into the language modelâ€™s prompt. Practitioners are turning to hosted vector database services to execute embedding search efficiently at scale [Pinecone](#bib.bib36) ; [Qdrant](#bib.bib37) ; [Vdaas](#bib.bib47) ; [Weaviate](#bib.bib49) ; [LangChain](#bib.bib21) .
In these databases, the data owner only sends embeddings of text data Le and Mikolov ([2014](#bib.bib22)); Kiros etÂ al. ([2015](#bib.bib19)) to the third party service, and never the text itself. The database server returns a search result as the index of the matching document on the client side.

Vector databases are increasingly popular, but privacy threats within them have not been comprehensively explored. Can the third party service to reproduce the initial text, given its embedding? Neural networks are in general non-trivial or even impossible to invert exactly. Furthermore, when querying a neural network through the internet, we may not have access to the model weights or gradients at all.

Still, given input-output pairs from a network, it is often possible to approximate the networkâ€™s inverse. Work on inversion in computer vision Mahendran and Vedaldi ([2014](#bib.bib27)); Dosovitskiy and Brox ([2016](#bib.bib9)) has shown that it is possible to learn to recover the input image (with some loss) given the logits of the final layer. Preliminary work has explored this question for text Song and Raghunathan ([2020](#bib.bib42)), but only been able to recover an approximate bag of words given embeddings from shallow networks.

In this work, we target full reconstruction of input text from its embedding. If text is recoverable, there is a threat to privacy: a malicious user with access to a vector database, and text-embedding pairs from the model used to produce the data, could learn a function that reproduces text from embeddings.

We frame this problem of recovering textual embeddings as a controlled generation problem, where we seek to generate text such that the text is as close as possible to a given embedding. Our method, Vec2Text, uses the difference between a hypothesis embedding and a ground-truth embedding to make discrete updates to the text hypothesis.

When we embed web documents using a state-of-the-art black-box encoder, our method can recover 32-token inputs with a near-perfect BLEU score of 97.397.397.3, and can recover 92%percent9292\% of the examples exactly. We then evaluate on embeddings generated from a variety of common retrieval corpuses from the BEIR benchmark. Even though these texts were not seen during training, our method is able to perfectly recover the inputs for a number of datapoints across a variety of domains. We evaluate on embeddings of clinical notes from MIMIC and are able to recover 89%percent8989\% of full names from embedded notes. These results imply that text embeddings present the same threats to privacy as the text from which they are computed, and embeddings should be treated with the same precautions as raw data.

## 2 Overview: Embedding Inversion

Text embedding models learn to map text sequences to embedding vectors. Embedding vectors are useful because they encode some notion of semantic similarity: inputs that are similar in meaning should have embeddings that are close in vector space Mikolov etÂ al. ([2013](#bib.bib29)). Embeddings are commonly used for many tasks such as search, clustering, and classification Aggarwal and Zhai ([2012](#bib.bib3)); Neelakantan etÂ al. ([2022](#bib.bib33)); Muennighoff etÂ al. ([2023](#bib.bib32)).

Given a text sequence of tokens xâˆˆğ•nğ‘¥superscriptğ•ğ‘›x\in\mathbb{V}^{n}, a text encoder Ï•:ğ•nâ†’â„d:italic-Ï•â†’superscriptğ•ğ‘›superscriptâ„ğ‘‘\phi:\mathbb{V}^{n}\rightarrow\mathbb{R}^{d} maps xğ‘¥x to a fixed-length embedding vector eâˆˆâ„dğ‘’superscriptâ„ğ‘‘e\in\mathbb{R}^{d}.

Now consider the problem of inverting textual embeddings: given some unknown encoder Ï•italic-Ï•\phi, we seek to recover the text xğ‘¥x given its embedding e=Ï•â€‹(x)ğ‘’italic-Ï•ğ‘¥e=\phi(x). Text embedding models are typically trained to encourage similarity between related inputs Karpukhin etÂ al. ([2020](#bib.bib17)). Thus, we can write the problem as recovering text that has a maximally similar embedding to the ground-truth. We can formalize the search for text x^^ğ‘¥\hat{x} with embedding eğ‘’e under encoder Ï•italic-Ï•\phi as optimization:

|  |  |  |  |
| --- | --- | --- | --- |
|  | x^=argâ¡maxxâ¡cosâ€‹(Ï•â€‹(x),e)^ğ‘¥subscriptğ‘¥cositalic-Ï•ğ‘¥ğ‘’\hat{x}=\arg\max\_{x}\text{cos}(\phi(x),e) |  | (1) |

#### Assumptions of our threat model.

In a practical sense, we consider the scenario where an attacker wants to invert a single embedding produced from a black-box embedder Ï•italic-Ï•\phi. We assume that the attacker has access to Ï•italic-Ï•\phi: given hypothesis text x^^ğ‘¥\hat{x}, the attacker can query the model for Ï•â€‹(x^)italic-Ï•^ğ‘¥\phi(\hat{x}) and compute cosâ€‹(Ï•â€‹(x^),e)cositalic-Ï•^ğ‘¥ğ‘’\text{cos}(\phi(\hat{x}),e). When this term is 1 exactly, the attacker can be sure that x^^ğ‘¥\hat{x} was the original text, i.e. collisions are rare and can be ignored.

## 3 Method: Vec2Text

### 3.1 Base Model: Learning to Invert Ï•italic-Ï•\phi

Enumerating all possible sequences to compute [EquationÂ 1](#S2.E1 "In 2 Overview: Embedding Inversion â€£ Text Embeddings Reveal (Almost) As Much As Text") is computationally infeasible. One way to avoid this computational constraint is by learning a distribution of texts given embeddings. Given a dataset of texts ğ’Ÿ={x1,â€¦}ğ’Ÿsubscriptğ‘¥1â€¦\mathcal{D}=\{x\_{1},\ldots\}, we learn to invert encoder Ï•italic-Ï•\phi by learning a distribution of texts given embeddings, pâ€‹(xâˆ£e;Î¸)ğ‘conditionalğ‘¥

ğ‘’ğœƒp(x\mid e;\theta), by learning Î¸ğœƒ\theta via maximum likelihood:

|  |  |  |
| --- | --- | --- |
|  | Î¸=argâ¡maxÎ¸^â¡ğ”¼xâˆ¼ğ’Ÿâ€‹[pâ€‹(xâˆ£Ï•â€‹(x);Î¸^)]ğœƒsubscript^ğœƒsubscriptğ”¼similar-toğ‘¥ğ’Ÿdelimited-[]ğ‘conditionalğ‘¥  italic-Ï•ğ‘¥^ğœƒ\theta=\arg\,\max\_{\hat{\theta}}\mathbb{E}\_{x\sim\mathcal{D}}[p(x\mid\phi(x);\hat{\theta})] |  |

We drop the Î¸ğœƒ\theta hereon for simplicity of notation. In practice, this process involves training a conditional language model to reconstruct unknown text xğ‘¥x given its embedding e=Ï•â€‹(x)ğ‘’italic-Ï•ğ‘¥e=\phi(x). We can view this learning problem as amortizing the combinatorial optimizationÂ ([EquationÂ 1](#S2.E1 "In 2 Overview: Embedding Inversion â€£ Text Embeddings Reveal (Almost) As Much As Text")) into the weights of a neural network.
Directly learning to generate satisfactory text in this manner is well-known in the literature to be a difficult problem.

### 3.2 Controlling Generation for Inversion

To improve upon this model, we propose Vec2Text shown in FigureÂ [1](#S1.F1 "Figure 1 â€£ 1 Introduction â€£ Text Embeddings Reveal (Almost) As Much As Text"). This approach takes inspiration from methods for Controlled Generation, the task of generating text that satisfies a known conditionÂ Hu etÂ al. ([2018](#bib.bib14)); John etÂ al. ([2018](#bib.bib15)); Yang and Klein ([2021](#bib.bib52)). This task is similar to inversion in that there is a observable function Ï•italic-Ï•\phi that determines the level of control.
However, it differs in that approaches to controlled generation Dathathri etÂ al. ([2020](#bib.bib8)); Li etÂ al. ([2022](#bib.bib26)) generally require differentiating through Ï•italic-Ï•\phi to improve the score of some intermediate representation. Textual inversion differs in that we can only make queries to Ï•italic-Ï•\phi, and cannot compute its gradients.

#### Model.

The method guesses an initial hypothesis and iteratively refines this hypothesis by re-embedding and correcting the hypothesis to bring its embedding closer to eğ‘’e. Note that this model requires computing a new embedding e^(t)=Ï•â€‹(x(t))superscript^ğ‘’ğ‘¡italic-Ï•superscriptğ‘¥ğ‘¡\hat{e}^{(t)}=\phi(x^{(t)}) in order to generate each new correction x(t+1)superscriptğ‘¥ğ‘¡1x^{(t+1)}. We define our model recursively by marginalizing over intermediate hypotheses:

|  |  |  |  |
| --- | --- | --- | --- |
|  | pâ€‹(x(t+1)âˆ£e)ğ‘conditionalsuperscriptğ‘¥ğ‘¡1ğ‘’\displaystyle p(x^{(t+1)}\mid e) | =âˆ‘x(t)pâ€‹(x(t)âˆ£e)â€‹pâ€‹(x(t+1)âˆ£e,x(t),e^(t))absentsubscriptsuperscriptğ‘¥ğ‘¡ğ‘conditionalsuperscriptğ‘¥ğ‘¡ğ‘’ğ‘conditionalsuperscriptğ‘¥ğ‘¡1  ğ‘’superscriptğ‘¥ğ‘¡superscript^ğ‘’ğ‘¡\displaystyle=\sum\_{x^{(t)}}p(x^{(t)}\mid e)p(x^{(t+1)}\mid e,x^{(t)},\hat{e}^{(t)}) |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  | e^(t)superscript^ğ‘’ğ‘¡\displaystyle\hat{e}^{(t)} | =Ï•â€‹(x(t))absentitalic-Ï•superscriptğ‘¥ğ‘¡\displaystyle=\phi(x^{(t)}) |  |

with a base case of the simple learned inversion:

|  |  |  |
| --- | --- | --- |
|  | pâ€‹(x(0)âˆ£e)=pâ€‹(x(0)âˆ£e,âˆ…,Ï•â€‹(âˆ…))ğ‘conditionalsuperscriptğ‘¥0ğ‘’ğ‘conditionalsuperscriptğ‘¥0  ğ‘’italic-Ï•p(x^{(0)}\mid e)=p(x^{(0)}\mid e,\varnothing,\phi(\varnothing)) |  |

Here, x(0)superscriptğ‘¥0x^{(0)} represents the initial hypothesis generation, x(1)superscriptğ‘¥1x^{(1)} the correction of x(0)superscriptğ‘¥0x^{(0)}, and so on. We train this model by first generating hypotheses x(0)superscriptğ‘¥0x^{(0)} from the model in SectionÂ [3.1](#S3.SS1 "3.1 Base Model: Learning to Invert Ï• â€£ 3 Method: Vec2Text â€£ Text Embeddings Reveal (Almost) As Much As Text"), computing e^(0)superscript^ğ‘’0\hat{e}^{(0)}, and then training a model on this generated data.

This method relates to other recent work generating text through iterative editing Lee etÂ al. ([2018](#bib.bib23)); Ghazvininejad etÂ al. ([2019](#bib.bib13)). Especially relevant is Welleck etÂ al. ([2022](#bib.bib50)), which proposes to train a text-to-text â€˜self-correctionâ€™ module to improve language model generations with feedback.

#### Parameterization.

The backbone of our model, pâ€‹(x(t+1)âˆ£e,x(t),e^(t))ğ‘conditionalsuperscriptğ‘¥ğ‘¡1

ğ‘’superscriptğ‘¥ğ‘¡superscript^ğ‘’ğ‘¡p(x^{(t+1)}\mid e,x^{(t)},\hat{e}^{(t)}), is parameterized as a standard encoder-decoder transformer Vaswani etÂ al. ([2017](#bib.bib46)); Raffel etÂ al. ([2020](#bib.bib38)) conditioned on the previous output.

One challenge is the need to input conditioning embeddings eğ‘’e and e^(t)superscript^ğ‘’ğ‘¡\hat{e}^{(t)} into a transformer encoder, which requires a sequence of embeddings as input with some dimension dencsubscriptğ‘‘encd\_{\text{enc}} not necessarily equal to the dimension dğ‘‘d of Ï•italic-Ï•\phiâ€™s embeddings.
Similar to Mokady etÂ al. ([2021](#bib.bib30)), we use small MLP to project a single embedding vector to a larger size, and reshape to give it a sequence length as input to the encoder. For embedding eâˆˆâ„dğ‘’superscriptâ„ğ‘‘e\in\mathbb{R}^{d}:

|  |  |  |
| --- | --- | --- |
|  | EmbToSeqâ€‹(e)=W2â€‹Ïƒâ€‹(W1â€‹e)EmbToSeqğ‘’subscriptğ‘Š2ğœsubscriptğ‘Š1ğ‘’\text{EmbToSeq}(e)=W\_{2}\ \sigma(W\_{1}\ e) |  |

where W1âˆˆâ„dÃ—dsubscriptğ‘Š1superscriptâ„ğ‘‘ğ‘‘W\_{1}\in\mathbb{R}^{d\times d} and W2âˆˆâ„(sâ€‹denc)Ã—dsubscriptğ‘Š2superscriptâ„ğ‘ subscriptğ‘‘encğ‘‘W\_{2}\in\mathbb{R}^{(sd\_{\text{enc}})\times d} for some nonlinear activation function Ïƒğœ\sigma and predetermined encoder â€œlengthâ€ sğ‘ s. We use a separate MLP to project three vectors: the ground-truth embedding eğ‘’e, the hypothesis embedding e^(t)superscript^ğ‘’ğ‘¡\hat{e}^{(t)}, and the difference between these vectors eâˆ’e^ğ‘’^ğ‘’e-\hat{e}. Given the word embeddings of the hypothesis x(t)superscriptğ‘¥ğ‘¡x^{(t)} are {w1â€‹â€¦â€‹wn}subscriptğ‘¤1â€¦subscriptğ‘¤ğ‘›\{w\_{1}...w\_{n}\}, the input (length 3â€‹s+n3ğ‘ ğ‘›3s+n) to the encoder is as follows:

|  |  |  |  |
| --- | --- | --- | --- |
|  | concat(\displaystyle\text{concat}( | EmbToSeqâ€‹(e),EmbToSeqğ‘’\displaystyle\text{EmbToSeq}(e), |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | EmbToSeqâ€‹(e^(t)),EmbToSeqsuperscript^ğ‘’ğ‘¡\displaystyle\text{EmbToSeq}(\hat{e}^{(t)}), |  |
|  |  |  |  |
| --- | --- | --- | --- |
|  |  | EmbToSeq(eâˆ’e^(t)),(w1â€¦wn))\displaystyle\text{EmbToSeq}(e-\hat{e}^{(t)}),(w\_{1}...w\_{n})) |  |

We feed the concatenated input to the encoder and train the full encoder-decoder model using standard language modeling loss.

#### Inference.

In practice we cannot tractably sum out intermediate generations x(t)superscriptğ‘¥ğ‘¡x^{(t)}, so we approximate this summation via beam search.
We perform inference from our model greedily at the token level but implement beam search at the sequence-level x(t)superscriptğ‘¥ğ‘¡x^{(t)}. At each step of correction, we consider some number bğ‘b of possible corrections as the next step. For each possible correction, we decode the top bğ‘b possible continuations, and then take the top bğ‘b unique continuations out of bâ‹…bâ‹…ğ‘ğ‘b\cdot b potential continuations by measuring their distance in embedding space to the ground-truth embedding eğ‘’e.

## 4 Experimental Setup

|  |  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- | --- |
|  | method | tokens | pred tokens | bleu | tf1 | exact | cos |
| GTR   Natural Questions | Bag-of-words Song and Raghunathan ([2020](#bib.bib42)) | 32 | 32 | 0.3 | 51 | 0.0 | 0.70 |
| GPT-2 Decoder Li etÂ al. ([2023](#bib.bib25)) | 32 | 32 | 1.0 | 47 | 0.0 | 0.76 |
| Base [0 steps] | 32 | 32 | 31.9 | 67 | 0.0 | 0.91 |
| (+ beam search) | 32 | 32 | 34.5 | 67 | 1.0 | 0.92 |
| (+ nucleus) | 32 | 32 | 25.3 | 60 | 0.0 | 0.88 |
| Vec2TextÂ [1 step] | 32 | 32 | 50.7 | 80 | 0.0 | 0.96 |
| [20 steps] | 32 | 32 | 83.9 | 96 | 40.2 | 0.99 |
| [50 steps] | 32 | 32 | 85.4 | 97 | 40.6 | 0.99 |
| [50 steps + sbeam] | 32 | 32 | 97.3 | 99 | 92.0 | 0.99 |
| OpenAI   MSMARCO | Base [0 steps] | 31.8 | 31.8 | 26.2 | 61 | 0.0 | 0.94 |
| Vec2TextÂ [1 step] | 31.8 | 31.9 | 44.1 | 77 | 5.2 | 0.96 |
| [20 steps] | 31.8 | 31.9 | 61.9 | 87 | 15.0 | 0.98 |
| [50 steps] | 31.8 | 31.9 | 62.3 | 87 | 14.8 | 0.98 |
| [50 steps + sbeam] | 31.8 | 31.8 | 83.4 | 96 | 60.9 | 0.99 |
| OpenAI   MSMARCO | Base [0 steps] | 80.9 | 84.2 | 17.0 | 54 | 0.6 | 0.95 |
| Vec2TextÂ [1 step] | 80.9 | 81.6 | 29.9 | 68 | 1.4 | 0.97 |
| [20 steps] | 80.9 | 79.7 | 43.1 | 78 | 3.2 | 0.99 |
| [50 steps] | 80.9 | 80.5 | 44.4 | 78 | 3.4 | 0.99 |
| [50 steps + sbeam] | 80.9 | 80.6 | 55.0 | 84 | 8.0 | 0.99 |

Table 1: Reconstruction score on in-domain datasets. Top section of results come from models trained to reconstruct 323232 tokens of text from Wikpedia, embedded using GTR-base. Remaining results come from models trained to reconstruct up to 323232 or 128128128 tokens from MSMARCO, embedded using OpenAI text-embeddings-ada-002.

| dataset | tokens | method | bleu | token F1 |
| --- | --- | --- | --- | --- |
| quora | 15.7 | Base | 36.2 | 73.8 |
| Vec2Text | 95.5 | 98.6 |
| signal1m | 23.7 | Base | 13.2 | 49.5 |
| Vec2Text | 80.7 | 92.5 |
| msmarco | 72.1 | Base | 15.5 | 54.1 |
| Vec2Text | 59.6 | 86.1 |
| climate-fever | 73.4 | Base | 12.8 | 49.3 |
| Vec2Text | 44.9 | 82.6 |
| fever | 73.4 | Base | 12.6 | 49.2 |
| Vec2Text | 45.1 | 82.7 |
| dbpedia-entity | 91.3 | Base | 15.4 | 50.3 |
| Vec2Text | 48.0 | 77.9 |
| nq | 94.7 | Base | 11.0 | 47.1 |
| Vec2Text | 32.7 | 72.7 |
| hotpotqa | 94.8 | Base | 15.4 | 50.1 |
| Vec2Text | 46.6 | 78.7 |
| fiqa | 103.8 | Base | 6.6 | 44.1 |
| Vec2Text | 21.5 | 63.6 |
| webis-touche2020 | 105.2 | Base | 6.6 | 41.5 |
| Vec2Text | 19.6 | 69.7 |
| cqadupstack | 106.4 | Base | 7.1 | 41.5 |
| Vec2Text | 23.3 | 64.3 |
| arguana | 113.5 | Base | 6.8 | 44.1 |
| Vec2Text | 23.4 | 66.3 |
| scidocs | 125.3 | Base | 5.9 | 38.5 |
| Vec2Text | 17.7 | 57.6 |
| trec-covid | 125.4 | Base | 5.6 | 36.3 |
| Vec2Text | 19.3 | 58.6 |
| robust04 | 127.3 | Base | 4.9 | 34.4 |
| Vec2Text | 15.5 | 54.5 |
| bioasq | 127.4 | Base | 5.3 | 35.7 |
| Vec2Text | 22.8 | 59.5 |
| scifact | 127.4 | Base | 4.9 | 35.2 |
| Vec2Text | 16.6 | 56.6 |
| nfcorpus | 127.7 | Base | 6.2 | 39.6 |
| Vec2Text | 25.8 | 64.8 |
| trec-news | 128.0 | Base | 4.9 | 34.8 |
| Vec2Text | 14.5 | 51.5 |

Table 2: Out-of-domain reconstruction performance measured on datasets from the BEIR benchmark. We sort datasets in order of average length in order to emphasize the effect of sequence length on task difficulty.

#### Embeddings.

Vec2Text is trained to invert two state-of-the-art embedding models: GTR-base Ni etÂ al. ([2021](#bib.bib34)), a T5-based pre-trained transformer for text retrieval, and text-embeddings-ada-002 available via the OpenAI API. Both model families are among the highest-performing embedders on the MTEB text embeddings benchmark Muennighoff etÂ al. ([2023](#bib.bib32)).

#### Datasets.

We train our GTR model on 5â€‹M5ğ‘€5M passages from Wikipedia articles selected from the Natural Questions corpus Kwiatkowski etÂ al. ([2019](#bib.bib20)) truncated to 323232 tokens. We train our two OpenAI models Bajaj etÂ al. ([2018](#bib.bib4)), both on versions of the MSMARCO corpus with maximum 323232 or 128128128 tokens per example 222By 2023 pricing of $0.0001currency-dollar0.0001\$0.0001 per 100010001000 tokens, embedding 5 million documents of 70 tokens each costs $35currency-dollar35\$35.. For evaluation, we consider the evaluation datasets from Natural Questions and MSMarco, as well as two out-of-domain settings: the MIMIC-III database of clinical notes Johnson etÂ al. ([2016](#bib.bib16)) in addition to the variety of datasets available from the BEIR benchmark Thakur etÂ al. ([2021](#bib.bib45)).

#### Baseline.

As a baseline, we train the base model pâ€‹(x(0)âˆ£e)ğ‘conditionalsuperscriptğ‘¥0ğ‘’p(x^{(0)}\mid e) to recover text with no correction steps. We also evaluate the bag of words model from Song and Raghunathan ([2020](#bib.bib42)). To balance for the increased number of queries allotted to the correction models, we also consider taking the top-N predictions made from the unconditional model via beam search and nucleus sampling (p=0.9)ğ‘0.9(p=0.9) and reranking via cosine similarity.

#### Metrics.

We use two types of metrics to measure the progress and the accuracy of reconstructed text. First we consider our main goal of text reconstruction. To measure this we use word-match metrics including: BLEU score Papineni etÂ al. ([2002](#bib.bib35)), a measure of n-gram similarities between the true and reconstructed text; Token F1, the multi-class F1 score between the set of predicted tokens and the set of true tokens; Exact-match, the percentage of reconstructed outputs that perfectly match the ground-truth.
We also report the similarity on the internal inversion metric in terms of recovering the vector embedding in latent space. We use cosine similarity between the true embedding and the embedding of reconstructed text according to Ï•italic-Ï•\phi.

#### Models and Inference.

We initialize our models from the T5-base checkpoint Raffel etÂ al. ([2020](#bib.bib38)). Including the projection head, each model has approximately 235M parameters. We set the projection sequence length s=16ğ‘ 16s=16 for all experiments, as preliminary experiments show diminishing returns by increasing this number further. We perform inference on all models using greedy token-level decoding. When running multiple steps of sequence-level beam search, we only take a new generation if it is closer than the previous step in cosine similarity to the ground-truth embedding.

We use unconditional models to seed the initial hypothesis for our iterative models. We examine the effect of using a different initial hypothesis in [SectionÂ 7](#S7 "7 Analysis â€£ Text Embeddings Reveal (Almost) As Much As Text").

We use the Adam optimizer and learning rate of 2âˆ—10âˆ’42superscript1042\*10^{-4} with warmup and linear decay. We train models for 100100100 epochs. We use batch size of 128 and train all models on 4 NVIDIA A6000 GPUs. Under these conditions, training our slowest model takes about two days.

## 5 Results

### 5.1 Reconstruction: In-Domain

[TableÂ 1](#S4.T1 "In 4 Experimental Setup â€£ Text Embeddings Reveal (Almost) As Much As Text") contains in-domain results. Our method outperforms the baselines on all metrics. More rounds is monotonically helpful, although we see diminishing returns â€“ we are able to recover 77%percent7777\% of BLEU score in just 5 rounds of correction, although running for 505050 rounds indeed achieves a higher reconstruction performance. We find that running sequence-level beam search (sbeam) over the iterative reconstruction is particularly helpful for finding exact matches of reconstructions, increasing the exact match score by 222 to 666 times across the three settings. In a relative sense, the model has more trouble exactly recovering longer texts, but still is able to get many of the words.

### 5.2 Reconstruction: Out-of-Domain

We evaluate our model on 151515 datasets from the BEIR benchmark and display results in [TableÂ 2](#S4.T2 "In 4 Experimental Setup â€£ Text Embeddings Reveal (Almost) As Much As Text"). Quora, the shortest dataset in BEIR, is the easiest to reconstruct, and our model is able to exactly recover 66%percent6666\% of examples. Our model adapts well to different-length inputs, generally producing reconstructions with average length error of fewer than 333 tokens. In general, reconstruction accuracy inversely correlates with example length (discussed more in [SectionÂ 7](#S7 "7 Analysis â€£ Text Embeddings Reveal (Almost) As Much As Text")). On all datasets, we are able to recover sequences with Token F1 of at least 414141 and cosine similarity to the true embedding of at least 0.950.950.95.

### 5.3 Case study: MIMIC

As a specific threat domain, we consider MIMIC-III clinical notes Johnson etÂ al. ([2016](#bib.bib16)). Because the original release of MIMIC is completely deidentified, we instead use the â€œpseudo re-identifiedâ€ version from Lehman etÂ al. ([2021](#bib.bib24)) where fake names have been inserted in the place of the deidentified ones.

Each note is truncated to 32 tokens and the notes are filtered so that they each contain at least one name. We measure the typical statistics of our method as well as three new ones: the percentage of first names, last names, and complete names that are recovered. Results are shown in [TableÂ 3](#S5.T3 "In 5.3 Case study: MIMIC â€£ 5 Results â€£ Text Embeddings Reveal (Almost) As Much As Text"). Vec2TextÂ is able to recover 94%percent9494\% of first names, 95%percent9595\% of last names, and 89%percent8989\% of full names (first, last format) while recovering 26%percent2626\% of the documents exactly.

For the recovered clinical notes from [SectionÂ 5.3](#S5.SS3 "5.3 Case study: MIMIC â€£ 5 Results â€£ Text Embeddings Reveal (Almost) As Much As Text"), we extract entities from each true and recovered note using a clinical entity extractor Raza etÂ al. ([2022](#bib.bib40)). We plot the recovery percentage in [3](#S5.T3 "Table 3 â€£ 5.3 Case study: MIMIC â€£ 5 Results â€£ Text Embeddings Reveal (Almost) As Much As Text") (bottom) with the average entity recovery shown as a dashed line. Our model is most accurate at reconstructing entities of the type â€œClinical Eventâ€, which include generic medical words like â€˜arrivedâ€™, â€˜progressâ€™, and â€˜transferredâ€™. Our model is least accurate in the â€œDetailed Descriptionâ€ category, which includes specific medical terminology like â€˜posteriorâ€™ and â€˜hypoxicâ€™, as well as multi-word events like â€˜invasive ventilation - stop 4:00 pmâ€™.

Although we are able to recover 26%percent2626\% of 323232-token notes exactly, the notes that were not exactly recovered are semantically close to the original. Our model generally matches the syntax of notes, even when some entities are slightly garbled; for example, given the following sentence from a doctorâ€™s note â€œRhona Arntson npn/- # resp: infant remains orally intubated on imv / r fiâ€ our model predicts â€œRhona Arpson nrft:# infant remains intubated orally on resp. imv. m/n fiâ€.

| method | first | last | full | bleu | tf1 | exact | cos |
| --- | --- | --- | --- | --- | --- | --- | --- |
| Base | 40.0 | 27.8 | 10.8 | 4.9 | 33.1 | 0. | 0.78 |
| Vec2Text | 94.2 | 95.3 | 89.2 | 55.6 | 80.8 | 26.0 | 0.98 |

![[Uncaptioned image]](/html/2310.06816/assets/x2.png)

Table 3: Performance of our method on reconstructing GTR-embedded clinical notes from MIMIC III Johnson etÂ al. ([2016](#bib.bib16)).

## 6 Defending against inversion attacks

Is it easy for users of text embedding models protect their embeddings from inversion attacks? We consider a basic defense scenario as a sanity check. To implement our defense, the user addes a level of Gaussian noise directly to each embedding with the goal of effectively defending against inversion attacks while preserving utility in the nearest-neightbor retrieval setting. We analyze the trade-off between retrieval performance and reconstruction accuracy under varying levels of noise.

Formally, we define a new embedding model as:

|  |  |  |
| --- | --- | --- |
|  | Ï•noisyâ€‹(x)=Ï•â€‹(x)+Î»â‹…Ïµ,Ïµâˆ¼Nâ€‹(0,1)formulae-sequencesubscriptitalic-Ï•noisyğ‘¥italic-Ï•ğ‘¥â‹…ğœ†italic-Ïµsimilar-toitalic-Ïµğ‘01\phi\_{\text{noisy}}(x)=\phi(x)+\lambda\cdot\epsilon,\epsilon\sim N(0,1) |  |

where Î»ğœ†\lambda is a hyperparameter controlling the amount of noise injected.

We simulate this scenario with Ï•italic-Ï•\phi as GTR-base using our self-corrective model with 101010 steps of correction, given the noisy embedder Ï•noisysubscriptitalic-Ï•noisy\phi\_{\text{noisy}}. To measure retrieval performance, we take the mean NDCG@10 (a metric of retrieval performance; higher is better) across 151515 different retrieval tasks from the BEIR benchmark, evaluated across varying levels of noise.

We graph the average retrieval performance in [FigureÂ 2](#S6.F2 "In 6 Defending against inversion attacks â€£ Text Embeddings Reveal (Almost) As Much As Text") (see [A.2](#A1.SS2 "A.2 Full defense results â€£ Appendix A Appendix â€£ Text Embeddings Reveal (Almost) As Much As Text") for complete tables of results).
At a noise level of Î»=10âˆ’1ğœ†superscript101\lambda=10^{-1}, we see retrieval performance is preserved, while BLEU score drops by 10%percent1010\%. At a noise level of 0.010.010.01, retrieval performance is barely degraded (2%percent22\%) while reconstruction performance plummets to 13%percent1313\% of the original BLEU. Adding any additional noise severely impacts both retrieval performance and reconstruction accuracy. These results indicate that adding a small amount of Gaussian noise may be a straightforward way to defend against naive inversion attacks, although it is possible that training with noise could in theory help Vec2TextÂ recover more accurately from Ï•nâ€‹oâ€‹iâ€‹sâ€‹ysubscriptitalic-Ï•ğ‘›ğ‘œğ‘–ğ‘ ğ‘¦\phi\_{noisy}. Note that low reconstruction BLEU score is not necessarily indicative that coarser inferences, such as clinical area or treatment regimen, cannot be made from embeddings.

![Refer to caption](/html/2310.06816/assets/x3.png)

Figure 2: Retrieval performance and reconstruction accuracy across varying levels of noise injection.

## 7 Analysis

#### How much does the model rely on feedback from Ï•italic-Ï•\phi?

[FigureÂ 3](#S7.F3 "In How much does the model rely on feedback from Ï•? â€£ 7 Analysis â€£ Text Embeddings Reveal (Almost) As Much As Text") shows an ablation study of the importance of feedback, i.e. performing corrections with and without embedding the most recent hypothesis. The model trained with feedback (i.e. additional conditioning on Ï•â€‹(x(t))italic-Ï•superscriptğ‘¥ğ‘¡\phi(x^{(t)}) is able to make a more accurate first correction and gets better BLEU score with more rounds. The model trained with no feedback can still edit the text but does not receive more information about the geometry of the embedding space, and quickly plateaus. The most startling comparison is in terms of the number of exact matches: after 50 rounds of greedy self-correction, our model with feedback gets 52.0%percent52.052.0\% of examples correct (after only 1.5%percent1.51.5\% initially); the model trained without feedback only perfectly matches 4.2%percent4.24.2\% of examples after 505050 rounds.

![Refer to caption](/html/2310.06816/assets/x4.png)

Figure 3: 
Recovery performance across multiple rounds of self-correction comparing models with access to Ï•italic-Ï•\phi vs text-only (32 tokens per sequence).

During training, the model only learns to correct a single hypothesis to the ground-truth sample. Given new text at test time, our model is able to correct the same text multiple times, â€œpushingâ€ the text from 0.90.90.9 embedding similarity to 1.01.01.0. We plot the closeness of the first hypothesis to the ground-truth in the training data for the length-32 model in [FigureÂ 4](#S7.F4 "In How much does the model rely on feedback from Ï•? â€£ 7 Analysis â€£ Text Embeddings Reveal (Almost) As Much As Text"). We see that during training the model learns to correct hypotheses across a wide range of closenesses, implying that corrections should not go â€˜out-of-distributionâ€™ as they approach the ground-truth.

![Refer to caption](/html/2310.06816/assets/x5.png)

Figure 4: 
Distribution of cosâ¡(e,Ï•â€‹(x(0)))ğ‘’italic-Ï•superscriptğ‘¥0\cos(e,\phi(x^{(0)})) over training data. The mean training output from the GTR base model has a cosine similarity of 0.9240.9240.924 with the true embedding.

|  |  |  |
| --- | --- | --- |
| Input | Nabo Gass (25 August, 1954 in Ebingen, Germany) is a German painter and glass artist. |  |
| Round 1 (0.85): | Nabo Gass (11 August 1974 in Erlangen, Germany) is an artist. | âœ— |
| Round 2 (0.99): | Nabo Gass (b. 18 August 1954 in Egeland, Germany) is a German painter and glass artist. | âœ— |
| Round 3 (0.99): | Nabo Gass (25 August 1954 in Ebingen, Germany) is a German painter and glass artist. | âœ— |
| Round 4 (1.00): | Nabo Gass (25 August, 1954 in Ebingen, Germany) is a German painter and glass artist. | âœ“ |

Table 4: Example of our corrective model working in multiple rounds. Left column shows the correction number, from Round 1 (initial hypothesis) to Round 4 (correct guess). The number in parenthesis is the cosine similarity between the guessâ€™s embedding and the embedding of the ground-truth sequence (first row).

#### How informative are embeddings for textual recovery?

We graph BLEU score vs. cosine similarity from a selection of of reconstructed text inputs in [FigureÂ 5](#S7.F5 "In Does having a strong base model matter? â€£ 7 Analysis â€£ Text Embeddings Reveal (Almost) As Much As Text"). We observe a strong correlation between the two metrics. Notably, there are very few generated samples with high cosine similarity but low BLEU score. This implies that better following embedding geometry will further improves systems.
Theoretically some embeddings might be impossible to recover. Prior work Song etÂ al. ([2020](#bib.bib43)); Morris ([2020](#bib.bib31)) has shown that two different sequences can â€˜collideâ€™ in text embedding space, having similar embeddings even without any word overlap.
However, our experiments found no evidence that collisions are a problem; they either do not exist or our model learns during training to avoid outputting them. Improved systems should be able to recover longer text.

#### Does having a strong base model matter?

We ablate the impact of initialization by evaluating our 323232-token Wikipedia model at different initializations of x(0)superscriptğ‘¥0x^{(0)}, as shown in [TableÂ 5](#S7.T5 "In Does having a strong base model matter? â€£ 7 Analysis â€£ Text Embeddings Reveal (Almost) As Much As Text"). After running for 202020 steps of correction, our model is able to recover from an unhelpful initialization, even when the initialization is a random sequence of tokens. This suggests that the model is able to ignore bad hypotheses and focus on the true embedding when the hypothesis is not helpful.

| Initialization | token f1 | cos | exact |
| --- | --- | --- | --- |
| Random tokens | 0.95 | 0.99 | 50.0 |
| "the " \* 32 | 0.95 | 0.99 | 49.8 |
| "thereâ€™s no reverse on a motorcycle, |  |  |  |
| as my friend found out quite |  |  |  |
| dramatically the other day" | 0.96 | 0.99 | 52.0 |
| Base model pâ€‹(x(0)âˆ£e)ğ‘conditionalsuperscriptğ‘¥0ğ‘’p(x^{(0)}\mid e) | 0.96 | 0.99 | 51.6 |

Table 5: Ablation: Reconstruction score on Wikipedia data (323232 tokens) given various initializations. Our self-correction model is able to faithfully recover the original text with greater than 808080 BLEU score, even with a poor initialization. Models run for 202020 steps of correction.

![Refer to caption](/html/2310.06816/assets/x6.png)

Figure 5: Cosine similarity vs BLEU score on 100010001000 reconstructed embeddings from Natural Questions text.

## 8 Related work

#### Inverting deep embeddings.

The task of inverting textual embeddings is closely related to research on inverting deep visual representations in computer vision Mahendran and Vedaldi ([2014](#bib.bib27)); Dosovitskiy and Brox ([2016](#bib.bib9)); Teterwak etÂ al. ([2021](#bib.bib44)); Bordes etÂ al. ([2021](#bib.bib5)), which show that a high amount of visual detail remains in the logit vector of an image classifier, and attempt to reconstruct input images from this vector. There is also a line of work reverse-engineering the content of certain text embeddings: Ram etÂ al. ([2023](#bib.bib39)) analyze the contents of text embeddings by projecting embeddings into the modelâ€™s vocabulary space to produce a distribution of relevant tokens. Adolphs etÂ al. ([2022](#bib.bib2)) train a single-step query decoder to predict the text of queries from their embeddings and use the decoder to produce more data to train a new retrieval model. We focus directly on text reconstruction and its implications for privacy, and propose an iterative method that works for paragraph-length documents, not just sentence-length queries.

#### Privacy leakage from embeddings.

Research has raised the question of information leakage from dense embeddings. In vision, Vec2Face Duong etÂ al. ([2020](#bib.bib10)) shows that faces can be reconstructed from their deep embeddings. Similar questions have been asked about text data:
Lehman etÂ al. ([2021](#bib.bib24)) attempt to recover sensitive information such as names from representations obtained from a model pre-trained on clinical notes, but fail to recover exact text. Kim etÂ al. ([2022](#bib.bib18)) propose a privacy-preserving similarity mechanism for text embeddings and consider a shallow bag-of-words inversion model. Abdalla etÂ al. ([2020](#bib.bib1)) analyze the privacy leaks in training word embeddings on medical data and are able to recover full names in the training data from learned word embeddings. Dziedzic etÂ al. ([2023](#bib.bib11)) note that stealing sentence encoders by distilling through API queries works well and is difficult for API providers to prevent. Song and Raghunathan ([2020](#bib.bib42)) considered the problem of recovering text sequences from embeddings, but only attempted to recover bags of words from the embeddings of a shallow encoder model. Li etÂ al. ([2023](#bib.bib25)) investigate the privacy leakage of embeddings by training a decoder with a text embedding as the first embedding fed to the decoder. Compared to these works, we consider the significantly more involved problem of developing a method to recover the full ordered text sequence from more realistic state-of-the-art text retrieval models.

#### Gradient leakage.

There are parallels between the use of vector databases to store embeddings and the practice of federated learning, where users share gradients with one another in order to jointly train a model. Our work on analyzing the privacy leakage of text embeddings is analogous to research on gradient leakage, which has shown that certain input data can be reverse-engineered from the modelâ€™s gradients during training Melis etÂ al. ([2018](#bib.bib28)); Zhu etÂ al. ([2019](#bib.bib55)); Zhao etÂ al. ([2020](#bib.bib54)); Geiping etÂ al. ([2020](#bib.bib12)).
Zhu etÂ al. ([2019](#bib.bib55)) even shows that they can recover text inputs of a masked language model by backpropagating to the input layer to match the gradient. However, such techniques do not apply to textual inversion: the gradient of the model is relatively high-resolution; we consider the more difficult problem of recovering the full input text given only a single dense embedding vector.

#### Text autoencoders.

Past research has explored natural language processing learning models that map vectors to sentences Bowman etÂ al. ([2016](#bib.bib7)). These include some retrieval models that are trained with a shallow decoder to reconstruct the text or bag-of-words from the encoder-outputted embedding Xiao etÂ al. ([2022](#bib.bib51)); Shen etÂ al. ([2023](#bib.bib41)); Wang etÂ al. ([2023](#bib.bib48)). Unlike these, we invert embeddings from a frozen, pre-trained encoder.

## 9 Conclusion

We propose Vec2Text, a multi-step method that iteratively corrects and re-embeds text based on a fixed point in latent space. Our approach can recover 92%percent9292\% of 323232-token text inputs from their embeddings exactly, demonstrating that text embeddings reveal much of the original text. The model also demonstrates the ability to extract critical clinical information from clinical notes, highlighting its implications for data privacy in sensitive domains like medicine.

Our findings indicate a sort of equivalence between embeddings and raw data, in that both leak similar amounts of sensitive information. This equivalence puts a heavy burden on anonymization requirements for dense embeddings: embeddings should be treated as highly sensitive private data and protected, technically and perhaps legally, in the same way as one would protect raw text.

## 10 Limitations

#### Adaptive attacks and defenses.

We consider the setting where an adversary applies noise to newly generated embeddings, but the reconstruction modules were trained from un-noised embeddings. Future work might consider reconstruction attacks or defenses that are adaptive to the type of attack or defense being used.

#### Search thoroughness.

Our search is limited; in this work we do not test beyond searching for 505050 rounds or with a sequence beam width higher than 888. However,Â Vec2TextÂ gets monotonically better with more searching. Future work could find even more exact matches by searching for more rounds with a higher beam width, or by implementing more sophisticated search algorithms on top of our corrective module.

#### Scalability to long text.

Our method is shown to recover most sequences exactly up to 323232 tokens and some information up to 128128128 tokens, but we have not investigated the limits of inversion beyond embeddings of this length. Popular embedding models support embedding text content on the order of thousands of tokens, and embedding longer texts is common practice Thakur etÂ al. ([2021](#bib.bib45)). Future work might explore the potential and difficulties of inverting embeddings of these longer texts.

#### Access to embedding model.

Our threat model assumes that an adversary has black-box access to the model used to generate the embeddings in the compromised database. In the real world, this is realistic because practitioners so often rely on the same few large models. However, Vec2TextÂ requires making a query to the black-box embedding model for each step of refinement. Future work might explore training an imitation embedding model which could be queried at inference time to save queries to the true embedder.

## References

* Abdalla etÂ al. (2020)

  Mohamed Abdalla, Moustafa Abdalla, Graeme Hirst, and Frank Rudzicz. 2020.
  [Exploring the
  privacy-preserving properties of word embeddings: Algorithmic validation
  study](https://doi.org/10.2196/18055).
  *J Med Internet Res*, 22(7):e18055.
* Adolphs etÂ al. (2022)

  Leonard Adolphs, MichelleÂ Chen Huebscher, Christian Buck, Sertan Girgin,
  Olivier Bachem, Massimiliano Ciaramita, and Thomas Hofmann. 2022.
  [Decoding a neural
  retrieverâ€™s latent space for query suggestion](http://arxiv.org/abs/2210.12084).
* Aggarwal and Zhai (2012)

  CharuÂ C. Aggarwal and ChengXiang Zhai. 2012.
  [*A Survey
  of Text Clustering Algorithms*](https://doi.org/10.1007/978-1-4614-3223-4_4), pages 77â€“128. Springer US, Boston, MA.
* Bajaj etÂ al. (2018)

  Payal Bajaj, Daniel Campos, Nick Craswell, LiÂ Deng, Jianfeng Gao, Xiaodong Liu,
  Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg,
  Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. 2018.
  [Ms marco: A human generated
  machine reading comprehension dataset](http://arxiv.org/abs/1611.09268).
* Bordes etÂ al. (2021)

  Florian Bordes, Randall Balestriero, and Pascal Vincent. 2021.
  High fidelity visualization of what your self-supervised
  representation knows about.
  *Trans. Mach. Learn. Res.*, 2022.
* Borgeaud etÂ al. (2022)

  Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza
  Rutherford, Katie Millican, George vanÂ den Driessche, Jean-Baptiste Lespiau,
  Bogdan Damoc, Aidan Clark, Diego deÂ LasÂ Casas, Aurelia Guy, Jacob Menick,
  Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin
  Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon
  Osindero, Karen Simonyan, JackÂ W. Rae, Erich Elsen, and Laurent Sifre. 2022.
  [Improving language models by
  retrieving from trillions of tokens](http://arxiv.org/abs/2112.04426).
* Bowman etÂ al. (2016)

  SamuelÂ R. Bowman, Luke Vilnis, Oriol Vinyals, AndrewÂ M. Dai, Rafal Jozefowicz,
  and Samy Bengio. 2016.
  [Generating sentences from a
  continuous space](http://arxiv.org/abs/1511.06349).
* Dathathri etÂ al. (2020)

  Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero
  Molino, Jason Yosinski, and Rosanne Liu. 2020.
  [Plug and play
  language models: A simple approach to controlled text generation](https://openreview.net/forum?id=H1edEyBKDS).
  In *International Conference on Learning Representations*.
* Dosovitskiy and Brox (2016)

  Alexey Dosovitskiy and Thomas Brox. 2016.
  [Inverting visual
  representations with convolutional networks](http://arxiv.org/abs/1506.02753).
* Duong etÂ al. (2020)

  ChiÂ Nhan Duong, Thanh-Dat Truong, KhaÂ Gia Quach, Hung Bui, Kaushik Roy, and
  Khoa Luu. 2020.
  [Vec2face: Unveil human faces
  from their blackbox features in face recognition](http://arxiv.org/abs/2003.06958).
* Dziedzic etÂ al. (2023)

  Adam Dziedzic, Franziska Boenisch, Mingjian Jiang, Haonan Duan, and Nicolas
  Papernot. 2023.
  [Sentence
  embedding encoders are easy to steal but hard to defend](https://openreview.net/forum?id=XN5qOxI8gkz).
  In *ICLR 2023 Workshop on Pitfalls of limited data and
  computation for Trustworthy ML*.
* Geiping etÂ al. (2020)

  Jonas Geiping, Hartmut Bauermeister, Hannah DrÃ¶ge, and Michael Moeller. 2020.
  [Inverting gradients â€“ how
  easy is it to break privacy in federated learning?](http://arxiv.org/abs/2003.14053)
* Ghazvininejad etÂ al. (2019)

  Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer. 2019.
  [Mask-predict: Parallel
  decoding of conditional masked language models](http://arxiv.org/abs/1904.09324).
* Hu etÂ al. (2018)

  Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, and EricÂ P. Xing.
  2018.
  [Toward controlled generation
  of text](http://arxiv.org/abs/1703.00955).
* John etÂ al. (2018)

  Vineet John, Lili Mou, Hareesh Bahuleyan, and Olga Vechtomova. 2018.
  [Disentangled representation
  learning for non-parallel text style transfer](http://arxiv.org/abs/1808.04339).
* Johnson etÂ al. (2016)

  AlistairÂ E.W. Johnson, TomÂ J. Pollard, LuÂ Shen, Li-weiÂ H. Lehman, Mengling
  Feng, Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo AnthonyÂ Celi,
  and RogerÂ G. Mark. 2016.
  [Mimic-iii, a freely
  accessible critical care database](https://doi.org/10.1038/sdata.2016.35).
  *Scientific Data*, 3(1):160035.
* Karpukhin etÂ al. (2020)

  Vladimir Karpukhin, Barlas OÄŸuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey
  Edunov, Danqi Chen, and Wen tau Yih. 2020.
  [Dense passage retrieval for
  open-domain question answering](http://arxiv.org/abs/2004.04906).
* Kim etÂ al. (2022)

  Donggyu Kim, Garam Lee, and Sungwoo Oh. 2022.
  [Toward
  privacy-preserving text embedding similarity with homomorphic encryption](https://aclanthology.org/2022.finnlp-1.4).
  In *Proceedings of the Fourth Workshop on Financial Technology
  and Natural Language Processing (FinNLP)*, pages 25â€“36, Abu Dhabi, United
  Arab Emirates (Hybrid). Association for Computational Linguistics.
* Kiros etÂ al. (2015)

  Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, RichardÂ S. Zemel, Antonio
  Torralba, Raquel Urtasun, and Sanja Fidler. 2015.
  [Skip-thought vectors](http://arxiv.org/abs/1506.06726).
* Kwiatkowski etÂ al. (2019)

  Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur
  Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin,
  Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang,
  AndrewÂ M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.
  [Natural questions: A
  benchmark for question answering research](https://doi.org/10.1162/tacl_a_00276).
  *Transactions of the Association for Computational Linguistics*,
  7:452â€“466.
* (21)

  LangChain. 2023.
  [Hwchase17/langchain:
  building applications with llms through composability](https://github.com/hwchase17/langchain).
* Le and Mikolov (2014)

  QuocÂ V. Le and Tomas Mikolov. 2014.
  [Distributed representations
  of sentences and documents](http://arxiv.org/abs/1405.4053).
* Lee etÂ al. (2018)

  Jason Lee, Elman Mansimov, and Kyunghyun Cho. 2018.
  [Deterministic
  non-autoregressive neural sequence modeling by iterative refinement](http://arxiv.org/abs/1802.06901).
* Lehman etÂ al. (2021)

  Eric Lehman, Sarthak Jain, Karl Pichotta, Yoav Goldberg, and ByronÂ C. Wallace.
  2021.
  [Does bert pretrained on
  clinical notes reveal sensitive data?](http://arxiv.org/abs/2104.07762)
* Li etÂ al. (2023)

  Haoran Li, Mingshi Xu, and Yangqiu Song. 2023.
  [Sentence embedding leaks
  more information than you expect: Generative embedding inversion attack to
  recover the whole sentence](http://arxiv.org/abs/2305.03010).
* Li etÂ al. (2022)

  XiangÂ Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and TatsunoriÂ B.
  Hashimoto. 2022.
  [Diffusion-lm improves
  controllable text generation](http://arxiv.org/abs/2205.14217).
* Mahendran and Vedaldi (2014)

  Aravindh Mahendran and Andrea Vedaldi. 2014.
  Understanding deep image representations by inverting them.
  *2015 IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)*, pages 5188â€“5196.
* Melis etÂ al. (2018)

  Luca Melis, Congzheng Song, EmilianoÂ De Cristofaro, and Vitaly Shmatikov. 2018.
  [Exploiting unintended
  feature leakage in collaborative learning](http://arxiv.org/abs/1805.04049).
* Mikolov etÂ al. (2013)

  Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013.
  [Efficient estimation of word
  representations in vector space](http://arxiv.org/abs/1301.3781).
* Mokady etÂ al. (2021)

  Ron Mokady, Amir Hertz, and AmitÂ H. Bermano. 2021.
  [Clipcap: Clip prefix for
  image captioning](http://arxiv.org/abs/2111.09734).
* Morris (2020)

  JohnÂ X. Morris. 2020.
  [Second-order nlp adversarial
  examples](http://arxiv.org/abs/2010.01770).
* Muennighoff etÂ al. (2023)

  Niklas Muennighoff, Nouamane Tazi, LoÃ¯c Magne, and Nils Reimers. 2023.
  [Mteb: Massive text embedding
  benchmark](http://arxiv.org/abs/2210.07316).
* Neelakantan etÂ al. (2022)

  Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, JesseÂ Michael Han, Jerry
  Tworek, Qiming Yuan, Nikolas Tezak, JongÂ Wook Kim, Chris Hallacy, Johannes
  Heidecke, Pranav Shyam, Boris Power, TynaÂ Eloundou Nekoul, Girish Sastry,
  Gretchen Krueger, David Schnurr, FelipeÂ Petroski Such, Kenny Hsu, Madeleine
  Thompson, Tabarak Khan, Toki Sherbakov, Joanne Jang, Peter Welinder, and
  Lilian Weng. 2022.
  [Text and code embeddings by
  contrastive pre-training](http://arxiv.org/abs/2201.10005).
* Ni etÂ al. (2021)

  Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, GustavoÂ HernÃ¡ndez Ãbrego, JiÂ Ma,
  VincentÂ Y. Zhao, YiÂ Luan, KeithÂ B. Hall, Ming-Wei Chang, and Yinfei Yang.
  2021.
  [Large dual encoders are
  generalizable retrievers](http://arxiv.org/abs/2112.07899).
* Papineni etÂ al. (2002)

  Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.
  [Bleu: a method for
  automatic evaluation of machine translation](https://doi.org/10.3115/1073083.1073135).
  In *Proceedings of the 40th Annual Meeting of the Association
  for Computational Linguistics*, pages 311â€“318, Philadelphia, Pennsylvania,
  USA. Association for Computational Linguistics.
* (36)

  Pinecone. 2023.
  [Pinecone](https://www.pinecone.io/).
* (37)

  Qdrant. 2023.
  [Qdrant - vector database](https://qdrant.tech/).
* Raffel etÂ al. (2020)

  Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and PeterÂ J. Liu. 2020.
  [Exploring the limits of
  transfer learning with a unified text-to-text transformer](http://arxiv.org/abs/1910.10683).
* Ram etÂ al. (2023)

  Ori Ram, Liat Bezalel, Adi Zicher, Yonatan Belinkov, Jonathan Berant, and Amir
  Globerson. 2023.
  [What are you token about?
  dense retrieval as distributions over the vocabulary](http://arxiv.org/abs/2212.10380).
* Raza etÂ al. (2022)

  Shaina Raza, DeepakÂ John Reji, Femi Shajan, and SyedÂ Raza Bashir. 2022.
  Large-scale application of named entity recognition to biomedicine
  and epidemiology.
  *PLOS Digital Health*, 1(12):e0000152.
* Shen etÂ al. (2023)

  Tao Shen, Xiubo Geng, Chongyang Tao, Can Xu, Xiaolong Huang, Binxing Jiao,
  Linjun Yang, and Daxin Jiang. 2023.
  [Lexmae: Lexicon-bottlenecked
  pretraining for large-scale retrieval](http://arxiv.org/abs/2208.14754).
* Song and Raghunathan (2020)

  Congzheng Song and Ananth Raghunathan. 2020.
  Information leakage in embedding models.
  *Proceedings of the 2020 ACM SIGSAC Conference on Computer and
  Communications Security*.
* Song etÂ al. (2020)

  Congzheng Song, AlexanderÂ M. Rush, and Vitaly Shmatikov. 2020.
  [Adversarial semantic
  collisions](http://arxiv.org/abs/2011.04743).
* Teterwak etÂ al. (2021)

  Piotr Teterwak, Chiyuan Zhang, Dilip Krishnan, and MichaelÂ C. Mozer. 2021.
  [Understanding invariance via
  feedforward inversion of discriminatively trained classifiers](http://arxiv.org/abs/2103.07470).
* Thakur etÂ al. (2021)

  Nandan Thakur, Nils Reimers, Andreas RÃ¼cklÃ©, Abhishek Srivastava, and Iryna
  Gurevych. 2021.
  [Beir: A heterogenous
  benchmark for zero-shot evaluation of information retrieval models](http://arxiv.org/abs/2104.08663).
* Vaswani etÂ al. (2017)

  Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  AidanÂ N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.
  [Attention is all you need](http://arxiv.org/abs/1706.03762).
* (47)

  Vdaas. 2023.
  [Vdaas/vald: Vald. a highly
  scalable distributed vector search engine](https://github.com/vdaas/vald).
* Wang etÂ al. (2023)

  Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang,
  Rangan Majumder, and Furu Wei. 2023.
  [Simlm: Pre-training with
  representation bottleneck for dense passage retrieval](http://arxiv.org/abs/2207.02578).
* (49)

  Weaviate. 2023.
  [Weaviate - vector database](https://weaviate.io/).
* Welleck etÂ al. (2022)

  Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel
  Khashabi, and Yejin Choi. 2022.
  [Generating sequences by
  learning to self-correct](http://arxiv.org/abs/2211.00053).
* Xiao etÂ al. (2022)

  Shitao Xiao, Zheng Liu, Yingxia Shao, and Zhao Cao. 2022.
  [Retromae: Pre-training
  retrieval-oriented language models via masked auto-encoder](http://arxiv.org/abs/2205.12035).
* Yang and Klein (2021)

  Kevin Yang and Dan Klein. 2021.
  [FUDGE:
  Controlled text generation with future discriminators](https://doi.org/10.18653/v1/2021.naacl-main.276).
  In *Proceedings of the 2021 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies*. Association for Computational Linguistics.
* Yao etÂ al. (2023)

  Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan,
  and Yuan Cao. 2023.
  [React: Synergizing reasoning
  and acting in language models](http://arxiv.org/abs/2210.03629).
* Zhao etÂ al. (2020)

  BoÂ Zhao, KondaÂ Reddy Mopuri, and Hakan Bilen. 2020.
  [idlg: Improved deep leakage
  from gradients](http://arxiv.org/abs/2001.02610).
* Zhu etÂ al. (2019)

  Ligeng Zhu, Zhijian Liu, and Song Han. 2019.
  [Deep leakage from
  gradients](http://arxiv.org/abs/1906.08935).

## Appendix A Appendix

### A.1 Additional analysis

#### How does word frequency affect model correctness?

[fig.Â 6](#A1.F6 "In How does word frequency affect model correctness? â€£ A.1 Additional analysis â€£ Appendix A Appendix â€£ Text Embeddings Reveal (Almost) As Much As Text") shows the number of correct predictions (orange) and incorrect predictions (blue) for ground-truth words, plotted across word frequency in the training data. Our model generally predicts words better that are more frequent in the training data, although it is still able to predict correctly a number of words that were not seen during training333We hypothesize this is because all test tokens were present in the training data, and the model is able to reconstruct unseen words from seen tokens.. Peaks between 104superscript10410^{4} and 105superscript10510^{5} come from the characters (((, âˆ’-, and ))), which appear frequently in the training data, but are still often guessed incorrectly in the reconstructions.

![Refer to caption](/html/2310.06816/assets/x7.png)

Figure 6: Correctness on evaluation samples from ArXiv data.

### A.2 Full defense results

Results on each dataset from BEIR under varying levels of Gaussian noise are shown in [TableÂ 6](#A1.T6 "In A.2 Full defense results â€£ Appendix A Appendix â€£ Text Embeddings Reveal (Almost) As Much As Text"). The model is GTR-base. Note that the inputs are limited to 32â€‹tâ€‹oâ€‹kâ€‹eâ€‹nâ€‹s32ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›ğ‘ 32tokens, far shorter than the average length for some corpuses, which is why baseline (Î»=0ğœ†0\lambda=0) NDCG@10 numbers are lower than typically reported. We included the full results (visualized in [FigureÂ 2](#S6.F2 "In 6 Defending against inversion attacks â€£ Text Embeddings Reveal (Almost) As Much As Text")) as [TableÂ 7](#A1.T7 "In A.2 Full defense results â€£ Appendix A Appendix â€£ Text Embeddings Reveal (Almost) As Much As Text").

| Î»ğœ†\lambda | arguana | bioasq | climate-fever | dbpedia-entity | fiqa | msmarco | nfcorpus | nq | quora | robust04 | scidocs | scifact | signal1m | trec-covid | trec-news | webis-touche2020 |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 0 | 0.328 | 0.115 | 0.136 | 0.306 | 0.208 | 0.647 | 0.239 | 0.306 | 0.879 | 0.205 | 0.095 | 0.247 | 0.261 | 0.376 | 0.245 | 0.233 |
| 0.001 | 0.329 | 0.115 | 0.135 | 0.307 | 0.208 | 0.647 | 0.239 | 0.306 | 0.879 | 0.204 | 0.096 | 0.246 | 0.261 | 0.381 | 0.246 | 0.233 |
| 0.01 | 0.324 | 0.113 | 0.132 | 0.301 | 0.205 | 0.633 | 0.234 | 0.298 | 0.875 | 0.192 | 0.092 | 0.235 | 0.259 | 0.378 | 0.234 | 0.225 |
| 0.1 | 0.005 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.017 | 0.000 | 0.003 | 0.000 | 0.002 | 0.006 | 0.001 | 0.005 | 0.001 | 0.000 |
| 1.0 | 0.001 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.008 | 0.000 | 0.000 | 0.000 | 0.000 | 0.001 | 0.000 | 0.000 | 0.000 | 0.000 |

Table 6: BEIR performance (NDCG@10) for GTR-base at varying levels of noise (32 tokens).

| Î»ğœ†\lambda | NDCG@10 | BLEU |
| --- | --- | --- |
| 0.000 | 0.302 | 80.372 |
| 0.001 | 0.302 | 72.347 |
| 0.010 | 0.296 | 10.334 |
| 0.100 | 0.002 | 0.148 |
| 1.000 | 0.001 | 0.080 |

Table 7: Retrieval performance and reconstruction performance across varying noise levels Î»ğœ†\lambda.