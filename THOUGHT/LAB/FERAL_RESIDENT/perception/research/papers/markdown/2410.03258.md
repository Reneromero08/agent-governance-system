# Introduction

<figure id="fig:BPE-IllSplit" data-latex-placement="!ht">
<img src="figures_final/Fig-Intro.png" />
<figcaption> <span class="smallcaps">AdaptBPE</span> modifies the initialization step of standard <span class="smallcaps">BPE</span> by merging the characters that match with the extended vocabulary (V<sub>DOMAIN</sub>). The incorrect merge step of <span class="smallcaps">BPE</span> for tokenizing the word <em>hypercholesterolemia</em> is highlighted by a red dashed box.</figcaption>
</figure>

Vocabulary adaptation-based fine-tuning has proved successful in domain adaptation to expert domains, characterized by high vocabulary mismatch. Here, the PLM vocabulary is further extended by adding a target domain-specific vocabulary (V~DOMAIN~) during fine-tuning. To identify V~DOMAIN~ works like VOLT [@xu-etal-2021-vocabulary] and [AVocaDo]{.smallcaps} [@hong-etal-2021-avocado] focus on optimizing the model's vocabulary by adding subwords based on utility scoring functions that are based on variants of fragment score [@rust-etal-2021-good] or optimize Pointwise Mutual Information [@diao-etal-2021-taming] or by measuring domain shift of token sequence distribution [@sachidananda-etal-2021-efficient]. [MEDVOC]{.smallcaps} [@balde2024medvoc], is the first work in a summarization setting that uses fragment score as the utility function. **In this work, we establish the need also to adapt the tokenization scheme**; Figure [1](#fig:BPE-IllSplit){reference-type="ref" reference="fig:BPE-IllSplit"} provides an example of ill-tokenization due to the limitations of the standard BPE tokenization scheme.

Prior vocabulary adaptation studies [@hong-etal-2021-avocado; @balde2024medvoc] append added vocabulary and corresponding merge rules towards the end of existing PLM vocabulary (V~PLM~). **This approach does not guarantee that the Byte-Pair Encoding ([BPE]{.smallcaps}) tokenizer will use the added target domain vocabulary**. We believe this is because the merge rules are trivially appended to the end, automatically implying a lower priority (of V~DOMAIN~) over existing PLM vocabulary (V~PLM~).

Our main contribution is to propose the [AdaptBPE]{.smallcaps} tokenization scheme that mitigates the above-mentioned ill-tokenization issue of [BPE]{.smallcaps} when applied to vocabulary adaptation strategies. Our proposed [AdaptBPE]{.smallcaps} algorithm is independent of the target domain-specific vocabulary construction algorithm and only modifies the underlying BPE tokenization phase. **[AdaptBPE]{.smallcaps} modifies the initialization stage of a standard [BPE]{.smallcaps} tokenization** as explained in detail in Algorithm [\[algo:bpe_tokenization\]](#algo:bpe_tokenization){reference-type="ref" reference="algo:bpe_tokenization"}. Instead of starting tokenization by splitting the input token to character level, [AdaptBPE]{.smallcaps} performs the longest substring match in the added vocabulary (V~DOMAIN~) iteratively and preserves the matched substring from splitting into characters further. This modified [BPE]{.smallcaps} algorithm, [AdaptBPE]{.smallcaps}, mitigates the ill-tokenization issues completely, as we observe a significant drop in fragment score (average number of subwords a given word across the entire corpus) of $39.16\%$ and $13.96\%$ in case of [AVocaDo]{.smallcaps} and [MEDVOC]{.smallcaps} respectively.

[AdaptBPE]{.smallcaps} shows improvements of $3.57\%$ and $1.87\%$ over the standard [BPE]{.smallcaps} algorithm in the case of [AVocaDo]{.smallcaps} and [MEDVOC]{.smallcaps} respectively for eight datasets (4 classification and 4 summarization tasks). In the case of [MEDVOC]{.smallcaps} for difficult generation scenarios such as high OOV (out-of-vocabulary) concentration and longer reference summaries, [AdaptBPE]{.smallcaps} consistently improves by $10.41\%$ and $3.30\%$ in terms of Rouge-L. We further perform a human evaluation using medical experts where we observe that [AdaptBPE]{.smallcaps} produces more relevant and faithful summaries in the case of [MEDVOC]{.smallcaps}. We make our codebase publicly available at <https://github.com/gb-kgp/adaptbpe>.

# Background

**Vocabulary Adaptation Strategies for Classification --[AVocaDo]{.smallcaps}.** [AVocaDo]{.smallcaps} [@hong-etal-2021-avocado] propose a vocabulary adaptation strategy for classification tasks. [AVocaDo]{.smallcaps} iteratively adds task-specific vocabulary (V~DOMAIN~) constructed from source documents of target tasks to existing PLM vocabulary (V~PLM~). The amount of vocabulary to be added is decided using fragment score, which is defined as the average number of subwords tokenized per word given a vocabulary. [AVocaDo]{.smallcaps} starts on a set of words that are split into more than two subwords ($W_{s>2}$) and constructs task-specific vocabulary on this set of words. It then keeps on adding the vocabulary from this task-specific vocabulary till the fragment score of words in set ($W_{s>2}$) stays above a fixed threshold, $\gamma$. [AVocaDo]{.smallcaps} initialized the embeddings of the newly added subwords with the average of embeddings of the subwords they were previously split into. [AVocaDo]{.smallcaps} uses contrastive loss framework [@chen2020simple] as a regularization loss along with the standard cross-entropy loss for classification to tune the model with the added embeddings of the newly added subwords.

**Vocabulary Adaptation Strategies for Summarization --[MEDVOC]{.smallcaps}.** [MEDVOC]{.smallcaps} [@balde2024medvoc] proposes a vocabulary adaptation framework for summarization tasks in the medical domain for three models -- BERT, BART, and PEGASUS. First, [MEDVOC]{.smallcaps} identifies vocabulary to be added (V~DOMAIN~) as an optimizable parameter. It constructs vocabulary on candidate set of medical OOV (Out-Of-Vocabulary) words (words that are medical, and split into more than one word using existing PLM vocabulary) identified from combination of PAC (PubMed Abstract Collection) dataset (V~PAC~) and target-task specific datasets (V~TGT~). It then performs a hyperparameter search using fragment score as the metric, over different vocabulary sizes and identifies the optimal vocabulary to be added to existing PLM vocabulary (V~PLM~). The embeddings are initialized randomly and are tuned by performing an intermediate fine-tuning step on PAC dataset comprising PubMed abstract as source document and the title as reference summary.

# Proposed Methodology {#sec:methods}

**Working of the standard [BPE]{.smallcaps} Tokenization.** [BPE]{.smallcaps} is the most common tokenization scheme that is found to be most effective among various tokenization strategies [@galle-2019-investigating; @zouhar-etal-2023-formal; @schmidt2024tokenization], and is used in the majority of recent Large Language Models (LLMs) like LLaMa [@touvron2023llama1; @touvron2023llama2] and Mistral [@jiang2023mistral]. BPE tokenization scheme takes as input two files: (i) vocabulary file, which contains the vocabulary, and (ii) merge rules file, which contains merge rules for the terms present in the vocabulary required for its construction (e.g., *th e* merge rule for the word *'the'* in vocabulary). The standard BPE tokenizer starts by splitting the input word into the character level. Then, following a bottom-up strategy, it iteratively merges adjacent characters following the ordered merge rules from the merge rule file taken as input. For instance, consider the word *happy*. BPE starts by converting this word into a list of characters: *\[h, a, p, p, y\]*. Then, it checks for possible merges on adjacent characters and selects the one with the least rank. Here, it chooses resulting in *\[h, a, pp, y\]*. It then iteratively keeps checking and ends with \[*'happy'*\] as the final output for BPE tokenization.

**[AdaptBPE]{.smallcaps} Tokenization Scheme (Algorithm [\[algo:bpe_tokenization\]](#algo:bpe_tokenization){reference-type="ref" reference="algo:bpe_tokenization"}).** We observe that the main reason for ill-tokenization (See Figure [1](#fig:BPE-IllSplit){reference-type="ref" reference="fig:BPE-IllSplit"}) is certain merge rules that hinder the formation of added vocabulary. Therefore, instead of splitting at the character level at the initialization stage, we first check for the longest substring match [@hofmann-etal-2022-embarrassingly] only in the added vocabulary (V~DOMAIN~) and prevent the match from splitting into the character level. This step is iterated till we cannot find any substring match. Figure [1](#fig:BPE-IllSplit){reference-type="ref" reference="fig:BPE-IllSplit"} shows an example: the word *hypercholesterolemia* is initialized as *\[h,y,p,e,r,cholesterol,e,m,i,a\]* as opposed to standard BPE tokenization which starts entirely at character level: *\[h,y,p,e,r,c,h,o,l,e,s,t,e,r,o,l,e,m,i,a\]*.

::: algorithm
$\text{pre\_tokenized} \gets \text{tokenizer.pre\_tokenize\_str(text)}$

$\text{pre\_tokenized\_text} \gets [\text{word} \text{ for } \text{word} \text{ in } \text{pre\_tokenized}]$

$\mathcal{T} \gets []$
:::

# Experimental Setup {#sec:expt-setup}

We use the same experimental setup as the state-of-the-art vocabulary adaptation works of [AVocaDo]{.smallcaps} and [MEDVOC]{.smallcaps} for the classification and summarization tasks, respectively. Appendix [9](#appendix:expt-setup){reference-type="ref" reference="appendix:expt-setup"} provides all the necessary implementation details.

#### Datasets.

We use the same datasets as used in [AVocaDo]{.smallcaps} and [MEDVOC]{.smallcaps} (see Appendix [9.2](#appendix:datasets){reference-type="ref" reference="appendix:datasets"} for further details) --- (i) four classification tasks: [CHEMPROT]{.smallcaps} [@kringelum2016chemprot] from the biomedical domain, [ACL-ARC]{.smallcaps} [@jurgens2018measuring] from the computer science domain, [HYPERPARTISAN]{.smallcaps} (HYP) [@kiesel2019semeval] from the news domain, and [AMAZON]{.smallcaps} [@amazon-mcauley-2015] from the customer reviews domain, and (ii) two query-focused document summarization datasets: EBM [@DBLP:conf/acl-alta/MollaS11a] and BioASQ [@cite:BioASQ], and two question summarization datasets: MeQSum [@ben-abacha-demner-fushman-2019-summarization] and CHQSum [@yadav2022chq].

**Evaluation Metrics.** We report classification accuracy and Macro-F1 scores for the classification task. For summarization, we report Rouge-L [@lin-2004-rouge] and Concept Score  [@zhang2023famesumm], which measures the overlap of UMLS medical concepts between the generated and reference summaries. See Appendix [9.3](#appendix:eval-metrics){reference-type="ref" reference="appendix:eval-metrics"} for additional details.

# Performance Evaluation of [AdaptBPE]{.smallcaps} {#sec:result_discussion}

We show the performance comparison results of [AdaptBPE]{.smallcaps} in Table [\[tab:avocado\]](#tab:avocado){reference-type="ref" reference="tab:avocado"} and [\[tab:medvoc\]](#tab:medvoc){reference-type="ref" reference="tab:medvoc"} for the vocabulary adaptation strategies for the classification and summarization setting, respectively. Please refer Appendix [\[appendix:added-vocab-sizes\]](#appendix:added-vocab-sizes){reference-type="ref" reference="appendix:added-vocab-sizes"} for the sizes of added vocabularies in both the settings for all the datasets and Appendix [9.5](#appendix:hyperparameters){reference-type="ref" reference="appendix:hyperparameters"} for relevant hyperparameter details.

**Performance Evaluation in Classification Datasets.** We show the performance comparison of BPE versus [AdaptBPE]{.smallcaps} in Table [\[tab:avocado\]](#tab:avocado){reference-type="ref" reference="tab:avocado"}. We observe gains of $3.57\%$ in accuracy and 3.18% in the case of the Macro-F1 score. We further observe huge drops in fragment scores of 39.16% across four datasets from four domains. Thus, [AdaptBPE]{.smallcaps} helps to correctly tokenize the domain words, which leads to better performance.

**Performance Evaluation in Medical Summarization Datasets.** We show the performance comparison of BPE versus [AdaptBPE]{.smallcaps}in Table [\[tab:medvoc\]](#tab:medvoc){reference-type="ref" reference="tab:medvoc"}. We observe gains of $1.87\%$ in Rouge-L (R-L) and $0.9\%$ in the case of ConceptScore (CSr). We further observe huge drops in fragment scores of $13.20\%$ across four datasets. This indicates the efficacy of [AdaptBPE]{.smallcaps}, as we are now correctly tokenizing the words and thus the downstream task improvement. We further investigate how [AdaptBPE]{.smallcaps} performed compared to BPE when reference summaries had high OOV concentration and were long in length following evaluation as performed in [MEDVOC]{.smallcaps}. These points represent the most difficult data points in terms of vocabulary mismatch. [AdaptBPE]{.smallcaps} shows a big improvement of $10.40\%$ on average across four datasets in high OOV settings and $3.41\%$ on average across BioASQ and EBM datasets for a long-form generation.

We randomly select $40$ test data points sampled uniformly from four summarization datasets and follow the annotation procedure as described in  [@fabbri-etal-2021-summeval; @balde2024medvoc] to get annotations of summaries across the dimensions of *relevance*, *coherence* (on a Likert scale of $1$ to $5$), and *faithfulness* (binary). Each annotator was given $30$ minutes to evaluate $10$ summaries and was compensated at a rate of $8$ UK pounds per hour (see Appendix [10](#appendix:human-eval-app){reference-type="ref" reference="appendix:human-eval-app"} for more details). Figure [2](#fig:human-evaluation){reference-type="ref" reference="fig:human-evaluation"} shows the human evaluation results where [AdaptBPE]{.smallcaps} generates more faithful summaries ($97.5\%$ vs. $77.5\%$ of summaries are faithful), and more relevant summaries, where $82.5\%$ of data points get a positive score ($\ge 4$) in Likert scale, as compared to $65\%$ in case of BPE for [MEDVOC]{.smallcaps}.

<figure id="fig:human-evaluation" data-latex-placement="t">
<embed src="figures_final/EMNLP_Annotations_40.pdf" />
<figcaption>Human evaluation scores comparison over 40 randomly selected test data points. <span class="smallcaps">AdaptBPE</span> produces more relevant, coherent, and faithful summaries during human evaluation with medical experts.</figcaption>
</figure>

# Conclusion {#sec:conclusion}

We are the first to show the incorrect BPE tokenization issue present in vocabulary adaptation techniques for fine-tuning PLMs to the target (expert) domain, designed for both classification and summarization tasks from various domains. The newly added target domain vocabulary is trivially added at low priority, causing BPE tokenizers to ignore them. Therefore, we propose a novel BPE tokenization scheme, [AdaptBPE]{.smallcaps}, that modifies the BPE initialization step by searching through V~DOMAIN~ to find the longest substring match. Our proposed [AdaptBPE]{.smallcaps} algorithm is independent of the target domain-specific vocabulary construction algorithm and focus only on improving the tokenization part. [AdaptBPE]{.smallcaps}-enabled models outperform the competing baselines by $3.57\%$ and $1.87\%$ on average over classification and summarization tasks, respectively. Human evaluation using medical experts rate [AdaptBPE]{.smallcaps}-based summaries to be more relevant and faithful than standard [BPE]{.smallcaps}.

# Limitations

We limit our evaluation to only pretrained language models and do not show results on large language models that also utilize BPE, such as LLaMa or Mistral, which uses Sentencepiece [@kudo-richardson-2018-sentencepiece] Byte-level BPE Tokenization with fallback. We observe that $27.76$% of target domain-specific vocabulary terms are still tokenized into more than one subword (i.e., the ill-tokenization issue persists) for MEDVOC in the case of the LlaMa-2-7B model. However, the models considered in this study (BART and RoBERTa) use *huggingface tokenizers* library [@wolf-etal-2020-transformers] and we observed ill-tokenization in $64.13$% of target domain-specific vocabulary terms. Thus, some efforts are needed to make [AdaptBPE]{.smallcaps} work for LLMs. Second, the issue of ill-tokenization is mostly prevalent in the case of [BPE]{.smallcaps} but less prevalent in the case of WordPiece tokenization, which is used by BERT and does not exist for Unigram tokenization scheme, which is used by PEGASUS and FLAN-T5 models.

# Ethics Statement and Broader Impact {#sec:ethics}

Summarization and other text generation systems powered by large language models can suffer from hallucinations, producing outputs that deviate from the source material and are unfaithful summaries. While the proposed [AdaptBPE]{.smallcaps} tokenization scheme generates more faithful summaries compared to existing baselines based on human evaluation, the summaries from such AI models are not yet reliable enough for high-stakes applications like medical contexts involving professionals and clinicians. Substantially more research is still needed to understand better the types of faithfulness and relevance errors made by these AI systems and to ultimately develop methods to mitigate or prevent such errors before these technologies can be safely deployed in sensitive real-world settings.

# Experimental Setup {#appendix:expt-setup}

## Pre-trained Language Models {#appendix:model-type-desc}

To test the generalizability of our method described in Section [3](#sec:methods){reference-type="ref" reference="sec:methods"}, we evaluate the efficacy of [MEDVOC]{.smallcaps} on BART in case of summarization and RoBERTa in case of classification.

- **RoBERTa** [@liu2019roberta]: RoBERTa (Robustly Optimized BERT Approach) is a transformer-based model, enhancing the original BERT model by training with more data and improved training techniques. It eliminates the Next Sentence Prediction (NSP) task used in BERT and employs dynamic masking during pre-training to increase robustness. RoBERTa is trained on a diverse corpus, including the Common Crawl dataset, to better capture nuanced language patterns. This model achieves state-of-the-art performance on various natural language processing (NLP) benchmarks. We use RoBERTa-base[^1] which is a $125$ Million parameter model and uses Byte-pair Encoding tokenization case with a vocabulary ($|\text{V\textsubscript{PLM}}|$) of size $50265$.

- **BART** [@lewis2020bart]: BART is a denoising autoencoder, implemented as a sequence-to-sequence model with a bidirectional encoder over corrupted text and a left-to-right auto-regressive decoder to generate the original document it was derived from. We use the BART-LARGE[^2] model available from the *huggingface* library. BART has $406$ Million parameters, uses *Byte-Pair Encoding* tokenization, and its pretraining objective is a combination of *Text Infilling* and *Sentence Shuffling*. The vocabulary size of this PLM ($|\text{V\textsubscript{PLM}}|$) is $50265$.

## Datasets {#appendix:datasets}

We describe here the details on the target task dataset mentioned briefly in Section [4](#sec:expt-setup){reference-type="ref" reference="sec:expt-setup"}.

#### Classification

We use four target task datasets for classification that were used in [AVocaDo]{.smallcaps}. The dataset stats are described in Table [\[tab:dataset-stats-classification\]](#tab:dataset-stats-classification){reference-type="ref" reference="tab:dataset-stats-classification"}.

- **CHEMPROT** [@kringelum2016chemprot]. Chemprot dataset is a corpus used for the task of chemical-protein relation extraction. It consists of scientific abstracts annotated with various types of interactions between chemical compounds and proteins, such as inhibition, activation, and binding in total 13 classes. The dataset is commonly used to train and evaluate models in the domain of biomedical natural language processing, particularly for the extraction and classification of biochemical relationships.

- **ACL-ARC** [@jurgens2018measuring]. The ACL-ARC dataset is designed to classify the intent behind citations in academic papers. It consists of annotated citations from research papers in the ACL Anthology, categorizing each citation based on its purpose, such as background, use, or comparison in total 6 classes. This dataset aids in understanding the functional and rhetorical roles of citations in scholarly communication.

- **HYPERPARTISAN** [@kiesel2019semeval]. The hyperpartisan dataset consists of news articles labeled for hyperpartisanship, indicating whether they exhibit extreme bias. It was created to support research in detecting biased or partisan news content --a two-class classification, providing annotations on article-level and publisher-level partisanship. This dataset is used in natural language processing tasks to develop and benchmark models for identifying and understanding media bias.

- **AMAZON** [@amazon-mcauley-2015]. The amazon dataset is a comprehensive collection of customer reviews and ratings from Amazon, covering a wide range of products. It includes detailed reviews, ratings, product metadata, and user information, providing a rich resource for sentiment analysis, recommendation systems, and other NLP tasks. The task is to identify whether a given review as input is actually helpful or not.

#### Medical Summarization

We use four target task datasets in this study: two query-focussed summarization datasets, EBM and BioASQ, and two recent benchmark medical question summarization datasets, MeQSum and CHQSum, each of which we describe below.

- **EBM** [@DBLP:conf/acl-alta/MollaS11a]. Here input to the system is a query along with a PubMed abstract, and the expected output is the summary answering the question with the PubMed Abstract as the context.

- **BioASQ** [@cite:BioASQ]. We use the dataset from BioASQ-9B Phase-B summarization task. The input to the system is a question followed by relevant snippets from a collection of PubMed Abstracts. There are two kinds of outputs an exact answer and an ideal answer associated with the input. For the summarization task, we consider the ideal answer as the Reference summary.

- **MeQSum** [@ben-abacha-demner-fushman-2019-summarization]. The dataset is created for better medical question summarization because the original patients' questions are verbose. The dataset contains $1000$ patients' health questions selected from a collection distributed by the U.S. National Library of Medicine. Each question is annotated with a summarized question by medical experts.

- **CHQSum** [@yadav2022chq]. CHQSum consists of $1507$ domain-expert annotated question-summary pairs from the Yahoo community question answering forum[^3] which provides community question answering threads containing users' questions on multiple diverse topics and the answers submitted by other users. The authors with the help of $6$ domain experts identified valid medical question from the forum and asked the experts to formulate an abstractive summary for the questions.

## Evaluation Metrics {#appendix:eval-metrics}

We first describe the implementation details for computing Rouge scores discussed in Section [4](#sec:expt-setup){reference-type="ref" reference="sec:expt-setup"}, where we use the official Rouge [@lin-2004-rouge] script[^4]. The following parameters: *-c 95 -2 -1 -U -r 1000 -n 4 -w 1.2 -a*, are used and we report the median at a $95$% confidence interval. Additionaly, we also use Concept Score which identifies the medical conocept overlaps between generated and reference summary. To identify concepts, we use matcher.match utility of QuickUMLS [@soldaini2016quickumls] tool in default setting.

## Added Vocabulary Sizes

 []{#appendix:added-vocab-sizes label="appendix:added-vocab-sizes"} We mention the size of added vocabulary obtained by [AVocaDo]{.smallcaps} and [MEDVOC]{.smallcaps} on classification and summarization datasets in Table [1](#tab:added-vocab-stats){reference-type="ref" reference="tab:added-vocab-stats"}.

::: {#tab:added-vocab-stats}
+-----------------------------------+--------------------------------------+
| **Dataset**                       | $|\text{V\textsubscript{DOMAIN}{}}|$ |
+:==================================+=====================================:+
| **[AVocaDo]{.smallcaps}** ($|\text{V\textsubscript{PLM}}|$: 50265)       |
+-----------------------------------+--------------------------------------+
| CHEMPROT                          | 5103                                 |
+-----------------------------------+--------------------------------------+
| ACL-ARC                           | 3419                                 |
+-----------------------------------+--------------------------------------+
| AMAZON                            | 1168                                 |
+-----------------------------------+--------------------------------------+
| HYPERPARTISAN                     | 743                                  |
+-----------------------------------+--------------------------------------+
| **[MEDVOC]{.smallcaps}** ($|\text{V\textsubscript{PLM}}|$: 50265)        |
+-----------------------------------+--------------------------------------+
| EBM                               | 11061                                |
+-----------------------------------+--------------------------------------+
| BioASQ                            | 6462                                 |
+-----------------------------------+--------------------------------------+
| MeQSum                            | 747                                  |
+-----------------------------------+--------------------------------------+
| CHQSum                            | 680                                  |
+-----------------------------------+--------------------------------------+

: Size of added vocabulary ($|\text{V\textsubscript{DOMAIN}{}}|$) for [AVocaDo]{.smallcaps}(RoBERTa) and [MEDVOC]{.smallcaps}(BART) on classification and summarization datasets respectively.
:::

## Hyperparameters {#appendix:hyperparameters}

We discuss the following hyperparameters: (i) the training hyperparameters, (iii) inference hyperparameters for MEDVOC.

### Training Hyperparameters

#### [AVocaDo]{.smallcaps}.

All [AVocaDo]{.smallcaps} related experiments were run on one V100 32 GB graphic card. We kept the training hyperparameters same as that of what authors follow in the study. In brief, we tune learning rate : $\in \{1e-5, 2e-5, 5e-5\}$ and temperature: from $1.5$ to $3.5$ in steps of $0.5$.

#### MEDVOC.

All the experiments are run on one A100 40 GB GPU. We use the fine-tuning summarization scripts for BART provided in MEDVOC's codebase. We used the following hyperparameters to train BART model. learning rate: 5e-5, batch size: 32, and gradient accumulation steps: 8, rest all the hyperparameters takes its default values. We checkpoint at every 500 steps and train the model for a total of 5 epochs (approx 15K steps). The training times for IFT-PAC for [MEDVOC]{.smallcaps} is mentioned in Table [2](#tab:runtime-IFTS){reference-type="ref" reference="tab:runtime-IFTS"}.

::: {#tab:runtime-IFTS}
   **Dataset**      **BART**
  ------------- ----------------
       EBM       27 hrs 51 mins
     BioASQ      28 hrs 25 mins
     MeQSum      28 hrs 49 mins
     CHQSum      28 hrs 38 mins

  : Time required in hours for intermediate fine-tuning using PAC for each target task dataset using BART with [AdaptBPE]{.smallcaps}.
:::

### Inference Hyperparameters

We used beam search to run the **inference** on the test set. We tuned the following hyperparameters of beam search: beam size (B $\in [2,10]$) and length-penalty [@wu2016googles] ($lp \in [0.1,3]$) on the validation split of the target task dataset. The best values of hyperparameters thus obtained are mentioned in Table [3](#tab:generation_hyperparameters){reference-type="ref" reference="tab:generation_hyperparameters"}.

::: {#tab:generation_hyperparameters}
   **Dataset**   $B$   $lp$
  ------------- ----- ------
       EBM        3    0.8
     BioASQ       6    0.8
     MeQSum       8    0.1
     CHQSum       6    0.5

  : Optimal values for inference hyperparameters - beam size ($B$) and Length Penalty ($lp$) used for beam-search generation for each of the datasets using BART with [AdaptBPE]{.smallcaps}.
:::

# Human Evaluation {#appendix:human-eval-app}

Twelve individuals took part in an annotation task on the Prolific platform. Each person was asked to annotate ten random pairs of summaries from a pool of forty, with the order and source of the summaries concealed. Participants had thirty minutes to finish the task and were paid 8 UK Pounds per hour for their time. They also provided feedback on the experience and demographic information, excluding any personal details beyond what is made available by the platform. The task was conducted using Google Forms, with participants being shown a consent notice beforehand.

#### Participation Criteria.

The filtering criteria for participants were kept same as that of MEDVOC [@balde2024medvoc]:

- **Age:** $\ge 25$,

- **Primary Language:** English,

- **Highest education level completed:** Graduate degree (MA/MSc/MPhil/other), Doctorate degree (PhD/other)

- **Subject:** Medicine, Health and Medicine, Biomedical Sciences.

#### Annotation Guidelines.

The annotations were carried across three dimensions [@fabbri-etal-2021-summeval] of coherence, relevance, and factual consistency. **Coherence** judges how well formed the summaries are and whether the sentences in the summaries are actually related to each other or not. **Relevance** judges how informative the summaries are considering the input as the context for evaluating relevance. **Factual Consistency** judges whether the facts, figures, numbers stated in the generated summary ca be verified from source input or not. Even if the generated text contains correct fact, but cannot be verified by only looking at input it is deemed as factually incosistent.

For each of these dimensions, we show one positive (high rating) and one negative example (low rating) along with an explanation as a part of our annotation guideline (Table [\[tab:sample-annot-guideline\]](#tab:sample-annot-guideline){reference-type="ref" reference="tab:sample-annot-guideline"}).

#### Demographic analysis of participants.

The average age of participants was $29$ years. Out of $12$ participants, $10$ were female and $2$% were male. All the participants are Graduate studtents. The participants were recruited by platfrom from 3 countries: South Africa(3), Sweden(2), and UK(7).

#### Instruction on platform.

Prolific begins the user study with a clear instruction window describing what the task is about and what the participant is expected to do in the study. We attach the screenshot of that window which is shown to the participants in Figure [3](#fig:instruction-snap){reference-type="ref" reference="fig:instruction-snap"}.

<figure id="fig:instruction-snap" data-latex-placement="!ht">
<img src="figures_final/InstructionAnnotation.png" />
<figcaption>Instruction window as seen by an annotator participating in the study.</figcaption>
</figure>

[^1]: <https://huggingface.co/FacebookAI/roberta-base>

[^2]: <https://huggingface.co/facebook/bart-large>

[^3]: <https://webscope.sandbox.yahoo.com/catalog.php?datatype=l&did=11>

[^4]: <https://github.com/bheinzerling/pyrouge/tree/master>
