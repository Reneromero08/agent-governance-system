# Pretraining and Finetuning {#sec:pretrain}

Current state-of-the-art systems for many NLP tasks finetune a pretrained model with task supervision (e.g. BERT). One of our main motivations is to develop such a model suitable for long document tasks. To do so, we pretrained on a document corpus and finetune it for six tasks, including classification, QA and coreference resolution. The resulting model can process sequences up to 4,096 tokens long (8 times longer than BERT)[^1].

We pretrain with masked language modeling (MLM), where the goal is to recover randomly masked tokens in a sequence. Since MLM pretraining is expensive, we continue pretraining from the RoBERTa [@roberta] released checkpoint, while only making the minimal changes necessary to support 's attention mechanism. Note that our attention pattern can be plugged into any pretrained transformer model without the need to change the model architecture.

#### Attention Pattern

We use sliding window attention with window size of 512, therefore using the same amount of computation as RoBERTa.[^2]

#### Position Embeddings

RoBERTa uses learned absolute position embeddings with the maximum position being 512. To support longer documents, we add extra position embeddings to support up to position 4,096. To leverage RoBERTa's pretrained weights, instead of randomly initializing the new position embeddings, we initialize them by copying the 512 position embeddings from RoBERTa multiple times as analysis of BERT's attention heads shows a strong learned bias to attending to local context, including the previous or next token [@Clark2019WhatDB]. Using the copy initialization preserves this local structure everywhere except at the partition boundaries. Despite its simplicity, we found this to be a very effective (see Tab. [1](#tab:roberta){reference-type="ref" reference="tab:roberta"}), allowing pretraining to rapidly converge with a small number of gradient updates.

::: {#tab:roberta}
  Model                                base   large
  -------------------------------- -------- -------
  RoBERTa (seqlen: 512)               1.846   1.496
  (seqlen: )                         10.299   8.738
      + copy position embeddings      1.957   1.597
         + 2K gradient updates        1.753   1.414
         + 65K gradient updates       1.705   1.358
  (train extra pos. embed. only)      1.850   1.504

  : MLM BPC for RoBERTa and various pretrained configurations.
:::

#### Continued MLM Pretraining

We pretrain using fairseq [@ott2019fairseq] on a corpus of long documents that we compiled (see Appendix [\[sec:mlm_data\]](#sec:mlm_data){reference-type="ref" reference="sec:mlm_data"} for corpus details). We train two model sizes, a base model and a large model. Both models are trained for 65K gradient updates with sequences length , batch size 64 ($2^{18}$ tokens), maximum learning rate of 3e-5, linear warmup of 500 steps, followed by a power 3 polynomial decay. The rest of the hyperparameters are the same as RoBERTa.

::: {#tab:doc-len}
  Wordpieces        WH      TQA     HQA      ON   IMDB      HY
  ------------ ------- -------- ------- ------- ------ -------
  avg.           1,535    6,589   1,316     506    300     705
  95th pctl.     3,627   17,126   1,889   1,147    705   1,975

  : Average and 95th percentile of context length of datasets in wordpieces. WH: WikiHop, TQA: TriviaQA, HQA: HotpotQA, ON: OntoNotes, HY: Hyperpartisan news
:::

::: table*
+--------------------------+--------------------------------+-----------+--------------------------+
|                          | QA                             | Coref.    | Classification           |
+:=========================+=========:+=========:+=========:+==========:+=========:+==============:+
| 2-4 (lr)5-5 (l)6-7 Model | WikiHop  | TriviaQA | HotpotQA | OntoNotes | IMDB     | Hyperpartisan |
+--------------------------+----------+----------+----------+-----------+----------+---------------+
| RoBERTa-base             | 72.4     | 74.3     | 63.5     | 78.4      | 95.3     | 87.4          |
+--------------------------+----------+----------+----------+-----------+----------+---------------+
| -base                    | **75.0** | **75.2** | **64.4** | **78.6**  | **95.7** | **94.8**      |
+--------------------------+----------+----------+----------+-----------+----------+---------------+
:::

Tab. [1](#tab:roberta){reference-type="ref" reference="tab:roberta"} shows the BPC on the development set of our training corpus. The first row shows a 1.846 BPC using RoBERTa-base, which is comparable to the 1.880 BPC reported on the RoBERTa paper on their corpus. This indicates our training corpus is from a distribution close to that used to train RoBERTa. The following two rows show the performance of before pretraining with randomly initialized position embeddings and with copied position embeddings. The significant difference indicates the importance of the copy initialization, and the relative small difference between the RoBERTa BPC and the initialized BPC indicates that our sliding window attention is working well with the RoBERTa weights. The following two rows show the impact of continuing pretraining. Traininig for 2K steps improves BPC from 1.957 to 1.753, which further decreases to 1.705 after 65K steps, demonstrating the model is learning to better utilize the sliding window attention and longer context. Similar patterns are observed with RoBERTa-large and -large.

#### Frozen RoBERTa Weights

We also pretrained while freezing all RoBERTa weights, and only training the new position embeddings. The motivation for this configuration is to perfectly preserve the RoBERTa performance on short documents. This configuration has a BPC of 1.850 (down from 1.957 at initialization), but higher than 1.705 where all the weights are trainable.

# Tasks

We apply to multiple long document tasks, including QA, coreference resolution and classification. Tab. [2](#tab:doc-len){reference-type="ref" reference="tab:doc-len"} shows the evaluation datasets have contexts significantly longer than 512 wordpieces. Our primary goal is to evaluate whether our attention mechanism can act as a replacement for the standard self-attention mechanism in BERT style models, and to perform controlled trials against a strong baseline. We are also interested in evaluating whether we can replace complicated task specific models necessitated by BERT's limited context with simpler models that just concatenate all available context into a single sequence.

Our baseline is a RoBERTa based model that breaks the context into the longest possible segment, passes each individually through RoBERTa, and concatenates the activations for further processing. For QA tasks, we also concatenate the question to each segment so that RoBERTa can condition it's contextual representations of the context on the question. The variant replaces the RoBERTa self-attention mechanism with our windowed attention used during pretraining, plus a task motivated global attention. The global attention uses additional linear projections (§[\[sec:attn\]](#sec:attn){reference-type="ref" reference="sec:attn"}).

## Question answering

We used three datasets: WikiHop [@Welbl2018ConstructingDF-Wikihop], TriviaQA [@Joshi2017TriviaQAAL Wikipedia setting], and HotpotQA, [@Yang2018-HotpotQAAD distractor setting].[^3]

For WikiHop and TriviaQA we follow the simple QA model of BERT [@bert], and concatenate question and documents into one long sequence, run it through , then have a dataset-specific prediction layer. WikiHop uses a classification layer for the candidate while TriviaQA uses the loss function of @Clark2017SimpleAE to predict answer span. We include global attention to question tokens and answer candidates for WikiHop and to question tokens for TriviaQA.

HotpotQA is a multihop QA dataset that involves extracting answer spans and evidence sentences from 10 Wikipedia paragraphs, 2 of which are relevant and the rest are distractors. We use a two-stage model that first selects the most relevant paragraphs then passes them to a second stage for answer extraction. Both stages concatenate question and context into one sequence, run it through , then use task-specific prediction layers. We train the models in a multi-task way to predict relevant paragraphs, evidence sentences, answer spans and question types (yes/no/span) jointly. Note that this model is simpler than recent SOTA models that include complex task-specific architectures (e.g., [@Tu2019SelectAA; @Chen2019MultihopQA; @Tu2020GraphSN; @quark2020]). See Appendix [\[sec:taskdetails\]](#sec:taskdetails){reference-type="ref" reference="sec:taskdetails"} for further details about the models and hyperparameters.

## Coreference Resolution

We use OntoNotes [@pradhan-etal-2012-conll], and the model from @joshi-etal-2019-bert, a modification of the system from @lee-etal-2018-higher to replace ELMo with BERT. The system is a straightforward adaption of the baseline model by replacing RoBERTa with and extending the sequence length. We didn't use global attention for this task.

## Document Classification 

We evaluate on IMDB [@imdb] and Hyperpartisan news detection [@hyperpartisan] datasets.[^4] IMDB is a standard sentiment classification datasets consisting of movie reviews. While most documents in this dataset are short, about 13.6% of them are larger than 512 wordpieces (Tab. [2](#tab:doc-len){reference-type="ref" reference="tab:doc-len"}). Documents in Hyperpartisan are relatively long, and it is small with only 645 documents making it a good test for 's ability to adapt to limited data. We use global attention on the `[CLS]` token.

## Results

#### Main Result

Tab. [\[tab:finetune\]](#tab:finetune){reference-type="ref" reference="tab:finetune"} summarizes the results of all our finetuning experiments. We observe that consistently outperforms the RoBERTa baseline. Its performance gain is especially obvious for tasks that require long context such as WikiHop and Hyperpartisan. For TriviaQA, the improvement is more modest as the local context is often sufficient to answer the question. In the case of HotpotQA, the supporting fact auxiliary supervision allows models to easily find relevant contexts and then focus on local context, leading to smaller gains. This is contrasted with WikiHop that only includes distant supervision of intermediate reasoning chains, where our approach excels by reasoning over the entire context. On the IMDB and OntoNotes datasets the performance gains are smaller. For IMDB, the majority of the dataset consists of short documents and thus it is expected to see smaller improvements. For OntoNotes, we found that the distance between any two mentions is typically quite small so that a baseline that processes smaller chunks separately is able to stitch together mentions into coreference chains without considering cross chunk interactions.

#### -large for QA

We also evaluate the performance of -large on long context QA tasks. Tab. [3](#tab:leaderboard){reference-type="ref" reference="tab:leaderboard"} shows that our -large achieves new state-of-the-art results[^5] on WikiHop and TriviaQA by large margins (3.6 and 4 points respectively), and for HotpotQA, it underperforms the current state-of-the-art [@hotpotqasota] by a point. Tab. [4](#tab:hotpotqa){reference-type="ref" reference="tab:hotpotqa"} shows the detailed results of HotpotQA compared with published and unpublished concurrent models. places second on the published leaderboard, outperforming all other published results except for HGN [@hotpotqasota]. All published top performing models in this task [@Tu2019SelectAA; @hotpotqasota; @Shao2020IsGS] use GNNs [@kipf2017semi] or graph network of entities, which seem to encode an important inductive bias for the task and can potentially improve our results further. Nevertheless, performs strongly outperforming all other methods including the recent non-GNN methods [@Gla2019SpanSP; @Shao2020IsGS; @quark2020].

::: {#tab:leaderboard}
  Model                 WikiHop   TriviaQA   HotpotQA
  ------------------ ---------- ---------- ----------
  Current$^*$ SOTA         78.3       73.3   **74.2**
  -large               **81.9**   **77.3**       73.2

  : Leaderboard results of -large at time of submission (May 2020). All numbers are F1 scores.
:::

::: {#tab:hotpotqa}
  Model                                         ans.   supp.   joint
  ------------------------------------------- ------ ------- -------
  TAP 2 (ensemble) [@Gla2019SpanSP]             79.8    86.7    70.7
  SAE [@Tu2019SelectAA]                         79.6    86.7    71.4
  Quark (dev) [@quark2020]                      81.2    87.0    72.3
  C2F Reader [@Shao2020IsGS]                    81.2    87.6    72.8
  -large                                        81.3    88.3    73.2
  ETC-large$^\dag$ [@ainslie-etal-2020-etc]     81.2    89.1    73.6
  GSAN-large$^\dag$                             81.6    88.7    73.9
  HGN-large [@hotpotqasota]                     82.2    88.5    74.2

  :  HotpotQA results in distractor setting test set. Quark's test results are not available. All numbers are F1 scores. $^\dag$ shows contemporaneous leaderboard submissions.
:::

## Ablations on WikiHop

::: {#tab:finetune_ablation}
  Model                                     Accuracy / $\Delta$
  --------------------------------------- ---------------------
  (seqlen: 4,096)                                          73.8
  RoBERTa-base (seqlen: 512)                        72.4 / -1.4
  (seqlen: 4,096, 15 epochs)                        75.0 / +1.2
  (seqlen: 512, attention: $n^2$)                   71.7 / -2.1
  (seqlen: 2,048)                                   73.1 / -0.7
  (no MLM pretraining)                              73.2 / -0.6
  (no linear proj.)                                 72.2 / -1.6
  (no linear proj. no global atten.)                65.5 / -8.3
  (pretrain extra position embed. only)             73.5 / -0.3

  : WikiHop development set ablations
:::

Tab. [5](#tab:finetune_ablation){reference-type="ref" reference="tab:finetune_ablation"} presents an ablation study for WikiHop on the development set. All results use -base, fine-tuned for five epochs with identical hyperparameters except where noted. benefits from longer sequences, global attention, separate projection matrices for global attention, MLM pretraining, and longer training. In addition, when configured as in RoBERTa-base (seqlen: 512, and $n^2$ attention) performs slightly worse then RoBERTa-base, confirming that performance gains are not due to additional pretraining. Performance drops slightly when using the RoBERTa model pretrained when only unfreezing the additional position embeddings, showing that can learn to use long range context in task specific fine-tuning with large training datasets such as WikiHop.

[^1]: Sequences up to 16K are possible on current GPUs.

[^2]: Adding dilation on a few heads as in §[\[sec:charlm_attn\]](#sec:charlm_attn){reference-type="ref" reference="sec:charlm_attn"} hurt performance, likely because it is not compatible with the pretrained RoBERTa weights. Retraining such model from scratch might be needed to improve performance.

[^3]: We use the full version of TriviaQA and HotpotQA, not the simplified versions in MRQA [@mrqa].

[^4]: For Hyperpartisan we split the training data into 80/10/10 train/dev/test sets, and report mean F1 across five seeds.

[^5]: At submission time, May 2020. Later, BigBird [@Zaheer2020BigBT] improved leaderboard results on these datasets. There are confounding factors such as using 16X more compute in BigBird's pretraining compared with Longformer, potentially affecting the performance.
