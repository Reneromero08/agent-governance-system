\[ orcid=0000-0002-7643-912X \]

\[ \]

\[\]

\[ \]

::: keywords
convolutional neural network , neural network acceleration , neural network quantization , neural network pruning , low-bit mathematics
:::

# Introduction {#sec:intro}

Deep Neural Networks (DNNs) have shown extraordinary abilities in complicated applications such as image classification, object detection, voice synthesis, and semantic segmentation [@Lecun2015]. Recent neural network designs with billions of parameters have demonstrated human-level capabilities but at the cost of significant computational complexity. DNNs with many parameters are also time-consuming to train [@Brown2020]. These large networks are also difficult to deploy in embedded environments. Bandwidth becomes a limiting factor when moving weights and data between Compute Units (CUs) and memory. Over-parameterization is the property of a neural network where redundant neurons do not improve the accuracy of results. This redundancy can often be removed with little or no accuracy loss [@Sze2017].

<figure id="fig:cnn_acc">

<figcaption>CNN Acceleration Approaches: Follow the sense from designing to implementing, CNN acceleration could fall into three categories, structure design (or generation), further optimization, and specialized hardware.</figcaption>
</figure>

[1](#fig:cnn_acc){reference-type="ref+label" reference="fig:cnn_acc"} shows three design considerations that may contribute to over-parameterization: 1) network structure, 2) network optimization, and 3) hardware accelerator design. These design considerations are specific to Convolutional Neural Networks (CNNs) but also generally relevant to DNNs.

Network structure encompasses three parts: 1) novel components, 2) network architecture search, and 3) knowledge distillation. Novel components is the design of efficient blocks such as separable convolution, inception blocks, and residual blocks. They are discussed in [2.4](#sec:cnn-novel){reference-type="ref+label" reference="sec:cnn-novel"}. Network components also encompasses the types of connections within layers. Fully connected deep neural networks require $N^2$ connections between neurons. Feed forward layers reduce connections by considering only connections in the forward path. This reduces the number of connections to $N$. Other types of components such as dropout layers can reduce the number of connections even further.

Network Architecture Search (NAS) [@Wistuba2019], also known as network auto search, programmatically searches for a highly efficient network structure from a large predefined search space. An estimator is applied to each produced architecture. While time-consuming to compute, the final architecture often outperforms manually designed networks.

Knowledge Distillation (KD) [@Gou2020; @Ruffy2019] evolved from knowledge transfer [@Bucilua2006]. The goal is to generate a simpler compressed model that functions as well as a larger model. KD trains a student network that tries to imitate a teacher network. The student network is usually but not always smaller and shallower than the teacher. The trained student model should be less computationally complex than the teacher.

Network optimization [@Lebedev2018] includes: 1) computational convolution optimization, 2) parameter factorization, 3) network pruning, and 4) network quantization. Convolution operations are more efficient than fully connected computations because they keep high dimensional information as a 3D tensor rather than flattening the tensors into vectors that removes the original spatial information. This feature helps CNNs to fit the underlying structure of image data in particular. Convolution layers also require significantly less coefficients compared to Fully Connected Layers (FCLs). Computational convolution optimizations include Fast Fourier Transform (FFT) based convolution [@Mathieu2013], Winograd convolution [@Lavin2016], and the popular image to column (im2col) [@Chellapilla2006] approach. We discuss im2col in detail in [2.3](#sec:cnn-ops){reference-type="ref+label" reference="sec:cnn-ops"} since it is directly related to general pruning techniques.

Parameter factorization is a technique that decomposes higher-rank tensors into lower-rank tensors simplifying memory access and compressing model size. It works by breaking large layers into many smaller ones, thereby reducing the number of computations. It can be applied to both convolutional and fully connected layers. This technique can also be applied with pruning and quantization.

Network pruning [@Reed1993; @Blalock2020; @Augasta2013; @Xu2020a] involves removing parameters that don't impact network accuracy. Pruning can be performed in many ways and is described extensively in [3](#sec:pruning){reference-type="ref+label" reference="sec:pruning"}.

Network quantization [@Krishnamoorthi2018; @Guo2018a] involves replacing datatypes with reduced width datatypes. For example, replacing 32-bit Floating Point (FP32) with 8-bit Integers (INT8). The values can often be encoded to preserve more information than simple conversion. Quantization is described extensively in [4](#sec:quantization){reference-type="ref+label" reference="sec:quantization"}.

Hardware accelerators [@Li2017a; @Reuther2019] are designed primarily for network acceleration. At a high level they encompass entire processor platforms and often include hardware optimized for neural networks. Processor platforms include specialized Central Processing Unit (CPU) instructions, Graphics Processing Units (GPUs), Application Specific Integrated Circuits (ASICs), and Field Programmable Gate Arrays (FPGAs).

CPUs have been optimized with specialized Artificial Intelligence (AI) instructions usually within specialized Single Instruction Multiple Data (SIMD) units [@Cornea2015; @ARM2008]. While CPUs can be used for training, they have primarily been used for inference in systems that do not have specialized inference accelerators.

GPUs have been used for both training and inference. nVidia has specialized tensor units incorporated into their GPUs that are optimized for neural network acceleration [@NVIDIACorporation2017a]. AMD [@AMD], ARM [@Arm2020], and Imagination [@Imagination] also have GPUs with instructions for neural network acceleration.

Specialized ASICs have also been designed for neural network acceleration. They typically target inference at the edge, in security cameras, or on mobile devices. Examples include: General Processor Technologies (GPT) [@Moudgill2020], ARM, nVidia, and 60+ others [@Reuther2019] all have processors targeting this space. ASICs may also target both training and inference in datacenters. Tensor processing units (TPU) from Google [@Jouppi2017], Habana from Intel [@Medina2019], Kunlun from Baidu [@Ouyang2020], Hanguang from Alibaba [@Jiao2020], and Intelligence Processing Unit (IPU) from Graphcore [@Jia2019].

Programmable reconfigurable FPGAs have been used for neural network acceleration [@Guo2017; @Abdelouahab2018; @Venieris2018; @Li2020a]. FPGAs are widely used by researchers due to long ASIC design cycles. Neural network libraries are available from Xilinx [@Kathail2020] and Intel [@FPGA]. Specific neural network accelerators are also being integrated into FPGA fabrics [@Xilinx2018; @Achronix2020; @RichardChuang2020]. Because FPGAs operate at the gate level, they are often used in low-bit width and binary neural networks [@Moss2017; @Zhao2017; @Prost-Boucle2017].

Neural network specific optimizations are typically incorporated into custom ASIC hardware. Lookup tables can be used to accelerate trigonometric activation functions [@Choi2017a] or directly generate results for low bit-width arithmetic [@Esser2016], partial products can be stored in special registers and reused [@Chen2016b], and memory access ordering with specialized addressing hardware can all reduce the number of cycles to compute a neural network output [@Judd2017]. Hardware accelerators are not the primary focus of this paper. However, we do note hardware implementations that incorporate specific acceleration techniques. Further background information on efficient processing and hardware implementations of DNNs can be found in [@Sze2017].

We summarize our main contributions as follows:

- We provide a review of two network compression techniques: pruning and quantization. We discuss methods of compression, mathematical formulations, and compare current State-Of-The-Art (SOTA) compression methods.

- We classify pruning techniques into static and dynamic methods, depending if they are done offline or at runtime, respectively.

- We analyze and quantitatively compare quantization techniques and frameworks.

- We provide practical guidance on quantization and pruning.

This paper focuses primarily on network optimization for convolutional neural networks. It is organized as follows: In [2](#sec:background){reference-type="ref+label" reference="sec:background"} we give an introduction to neural networks and specifically convolutional neural networks. We also describe some of the network optimizations of convolutions. In [3](#sec:pruning){reference-type="ref+label" reference="sec:pruning"} we describe both static and dynamic pruning techniques. In [4](#sec:quantization){reference-type="ref+label" reference="sec:quantization"} we discuss quantization and its effect on accuracy. We also compare quantization libraries and frameworks. We then present quantized accuracy results for a number of common networks. We present conclusions and provide guidance on appropriate application use in [5](#sec:summary){reference-type="ref+label" reference="sec:summary"}. Finally, we present concluding comments in [7](#sec:discussion){reference-type="ref+label" reference="sec:discussion"}.

# Convolutional Neural Network {#sec:background}

Convolutional neural networks are a class of feed-forward DNNs that use convolution operations to extract features from a data source. CNNs have been most successfully applied to visual-related tasks however they have found use in natural language processing [@Hannun2014], speech recognition [@Abdel-hamid2014], recommendation systems [@Shen2016], malware detection [@Sun2020], and industrial sensors time series prediction [@Yuan2020]. To provide a better understanding of optimization techniques, in this section, we introduce the two phases of CNN deployment - training and inference, discuss types of convolution operations, describe Batch Normalization (BN) as an acceleration technique for training, describe pooling as a technique to reduce complexity, and describe the exponential growth in parameters deployed in modern network structures.

## Definitions {#sec:cnn-pre}

This section summarizes terms and definitions used to describe neural networks as well as acronyms collected in [\[tab:acronym\]](#tab:acronym){reference-type="ref+label" reference="tab:acronym"}.

- Coefficient - A constant by which an algebraic term is multiplied. Typically, a coefficient is multiplied by the data in a CNN filter.

- Parameter - All the factors of a layer, including coefficients and biases.

- Hyperparameter - A predefined parameter before network training, or fine-tunning (re-training).

- Activation ($\mathbf{A}\in\mathbb{R}^{h \times w \times c}$) - The activated (e.g., ReLu, Leaky, Tanh, etc.) output of one layer in a multi-layer network architecture, typically in height $h$, width $w$, and channel $c$. The $h \times w$ matrix is sometimes called an activation map. We also denote activation as output ($\mathbf{O}$) when the activation function does not matter.

- Feature ($\mathbf{F}\in\mathbb{R}^{h \times w \times c}$) - The input data of one layer, to distinguish the output $\mathbf{A}$. Generally the feature for the current layer is the activation of the previous layer.

- Kernel ($\mathbf{k} \in \mathbb{R}^{k_1 \times k_2}$) - Convolutional coefficients for a channel, excluding biases. Typically they are square (e.g. $k_1=k_2$) and sized 1, 3, 7.

- Filter ($\mathbf{w}\in\mathbb{R}^{k_1 \times k_2 \times c \times n}$) - Comprises all of the kernels corresponding to the $c$ channels of input features. The filter's number, $n$, results in different output channels.

- Weights - Two common uses: 1) kernel coefficients when describing part of a network, and 2) all the trained parameters in a neural network model when discussing the entire network.

[]{#tab:acronym label="tab:acronym"}

## Training and Inference {#sec:cnn-tran-infer}

CNNs are deployed as a two step process: 1) training and 2) inference. Training is performed first with the result being either a continuous numerical value (regression) or a discrete class label (classification). Classification training involves applying a given annotated dataset as an input to the CNN, propagating it through the network, and comparing the output classification to the ground-truth label. The network weights are then updated typically using a backpropagation strategy such as Stochastic Gradient Descent (SGD) to reduce classification errors. This performs a search for the best weight values. Backpropogation is performed iteratively until a minimum acceptable error is reached or no further reduction in error is achieved. Backpropagation is compute intensive and traditionally performed in data centers that take advantage of dedicated GPUs or specialized training accelerators such as TPUs.

Fine-tuning is defined as retraining a previously trained model. It is easier to recover the accuracy of a quantized or pruned model with fine-tuning versus training from scratch.

CNN inference classification takes a previously trained classification model and predicts the class from input data not in the training dataset. Inference is not as computationally intensive as training and can be executed on edge, mobile, and embedded devices. The size of the inference network executing on mobile devices may be limited due to memory, bandwidth, or processing constraints [@Gordon2018]. Pruning discussed in [3](#sec:pruning){reference-type="ref+label" reference="sec:pruning"} and quantization discussed in [4](#sec:quantization){reference-type="ref+label" reference="sec:quantization"} are two techniques that can alleviate these constraints.

In this paper, we focus on the acceleration of CNN inference classification. We compare techniques using standard benchmarks such as ImageNet [@JiaDeng2009], CIFAR [@Krizhevsky2009], and MNIST [@LeCun1998]. The compression techniques are general and the choice of application domain doesn't restrict its use in object detection, natural language processing, etc.

## Convolution Operations {#sec:cnn-ops}

<figure id="fig:separable-conv" data-latex-placement="htb">
<embed src="8.pdf" style="width:95.0%" />
<figcaption>Separable Convolution: A standard convolution is decomposed into depth-wise convolution and point-wise convolution to reduce both the model size and computations.</figcaption>
</figure>

The top of [2](#fig:separable-conv){reference-type="ref+label" reference="fig:separable-conv"} shows a 3-channel image (e.g., RGB) as input to a convolutional layer. Because the input image has 3 channels, the convolution kernel must also have 3 channels. In this figure four $2\times2\times3$ convolution filters are shown, each consisting of three $2\times2$ kernels. Data is received from all 3 channels simultaneously. 12 image values are multiplied with the kernel weights producing a single output. The kernel is moved across the 3-channel image sharing the 12 weights. If the input image is $12\times12\times3$ the resulting output will be $11\times11\times1$ (using a stride of 1 and no padding). The filters work by extracting multiple smaller bit maps known as feature maps. If more filters are desired to learn different features they can be easily added. In this case 4 filters are shown resulting in 4 feature maps.

The standard convolution operation can be computed in parallel using a GEneral Matrix Multiply (GEMM) library [@Dongarra1990]. [3](#fig:im2col_and_gemm){reference-type="ref+label" reference="fig:im2col_and_gemm"} shows a parallel column approach. The 3D tensors are first flattened into 2D matrices. The resulting matrices are multiplied by the convolutional kernel which takes each input neuron (features), multiplies it, and generates output neurons (activations) for the next layer [@Lecun2015].

<figure id="fig:im2col_and_gemm" data-latex-placement="htb">
<embed src="8.pdf" style="width:95.0%" />
<figcaption>Convolution Performance Optimization: From traditional convolution (dot squared) to image to column (im2col) - GEMM approach, adopted from <span class="citation" data-cites="Chellapilla2006"></span>. The red and green boxes indicate filter-wise and shape-wise elements, respectively.</figcaption>
</figure>

$$\begin{equation}
    \label{eq:convolution}
    \begin{split}
        \mathbf{F}^{l+1}_{n} = \mathbf{A}^{l}_{n} = \operatorname{activate}\left\{\sum_{m=1}^{M}\left(\mathbf{W}^{l}_{mn} \ast \mathbf{F}^{l}_{m}\right) + \mathbf{b}^{l}_{n}\right\}
    \end{split}
\end{equation}$$

[\[eq:convolution\]](#eq:convolution){reference-type="ref+label" reference="eq:convolution"} shows the layer-wise mathematical representation of the convolution layer where $\mathbf{W}$ represents the weights (filters) of the tensor with $m$ input channels and $n$ output channels, $\mathbf{b}$ represents the bias vector, and $\mathbf{F}^{l}$ represents the input feature tensor (typically from the activation of previous layer $\mathbf{A}^{l-1}$). $\mathbf{A}^l$ is the activated convolutional output. The goal of compression is to reduce the size of the $\mathbf{W}$ and $\mathbf{F}$ (or $\mathbf{A}$) without affecting accuracy.

<figure id="fig:conn" data-latex-placement="htb">
<embed src="8.pdf" style="width:95.0%" />
<figcaption>Fully Connected Layer: Each node in a layer connects to all the nodes in the next layer, and every line corresponds to a weight value</figcaption>
</figure>

[4](#fig:conn){reference-type="ref+label" reference="fig:conn"} shows a FCL - also called dense layer or dense connect. Every neuron is connected to each other neuron in a crossbar configuration requiring many weights. As an example, if the input and output channel are 1024 and 1000, respectively, the number of parameters in the filter will be a million by $1024 \times 1000$. As the image size grows or the number of features increase, the number of weights grows rapidly.

## Efficient Structure {#sec:cnn-novel}

The bottom of [2](#fig:separable-conv){reference-type="ref+label" reference="fig:separable-conv"} shows separable convolution implemented in MobileNet [@Howard2017]. Separable convolution assembles a depth-wise convolution followed by a point-wise convolution. A depth-wise convolution groups the input feature by channel, and treats each channel as a single input tensor generating activations with the same number of channels. Point-wise convolution is a standard convolution with $1\times1$ kernels. It extracts mutual information across the channels with minimum computation overhead. For the $12\times12\times3$ image previously discussed, a standard convolution needs $2\times2\times3\times4$ multiplies to generate $1\times1\time4$ outputs. Separable convolution needs only $2\times2\times3$ for depth-wise convolution and $1\times1\times3\times4$ for point-wise convolution. This reduces computations by half from 48 to 24. The number of weights is also reduced from 48 to 24.

<figure id="fig:inception-block" data-latex-placement="htb">
<embed src="8.pdf" style="width:95.0%" />
<figcaption>Inception Block: The inception block computes multiple convolutions with one input tensor in parallel, which extends the receptive field by mixing the size of kernels. The yellow - brown coloured cubes are convolutional kernels sized 1, 3, and 5. The blue cube corresponds to a <span class="math inline">3 × 3</span> pooling operation.</figcaption>
</figure>

The receptive field is the size of a feature map used in a convolutional kernel. To extract data with a large receptive filed and high precision, cascaded layers should be applied as in the top of [5](#fig:inception-block){reference-type="ref+label" reference="fig:inception-block"}. However, the number of computations can be reduced by expanding the network width with four types of filters as shown in [5](#fig:inception-block){reference-type="ref+label" reference="fig:inception-block"}. The concatenated result performs better than one convolutional layer with same computation workloads [@Szegedy2015].

<figure id="fig:residual-and-densenet" data-latex-placement="htb">
<embed src="8.pdf" style="width:95.0%" />
<figcaption>Conventional Network Block (top), Residual Network Block (middle), and Densely Connected Network Block (bottom)</figcaption>
</figure>

A residual network architecture block [@He2015] is a feed forward layer with a short circuit between layers as shown in the middle of [6](#fig:residual-and-densenet){reference-type="ref+label" reference="fig:residual-and-densenet"}. The short circuit keeps information from the previous block to increase accuracy and avoid vanishing gradients during training. Residual networks help deep networks grow in depth by directly transferring information between deeper and shallower layers.

The bottom of [6](#fig:residual-and-densenet){reference-type="ref+label" reference="fig:residual-and-densenet"} shows the densely connected convolutional block from DenseNets [@Huang2017a], this block extends both the network depth and the receptive field by delivering the feature of former layers to all the later layers in a dense block using concatenation. ResNets transfer outputs from a single previous layer. DenseNets build connections across layers to fully utilize previous features. This provides weight efficiencies.

## Batch Normalization {#sec:cnn-bn}

BN was introduced in 2015 to speed up the training phase, and to improve the neural network performance [@Ioffe2015]. Most SOTA neural networks apply BN after a convolutional layer. BN addresses internal covariate shift (an altering of the network activation distribution caused by modifications to parameters during training) by normalizing layer inputs. This has been shown to reduce training time up to $14\times$. Santurkar [@Santurkar2018] argues that the efficiency of BN is from its ability to smooth values during optimization.

$$\begin{equation}
    \begin{split}
        \mathbf{y}=\gamma \cdot \frac{\mathbf{x}-\mu}{\sqrt{\sigma^{2}+\epsilon}}+\beta
    \end{split}
\label{eq:batch-normalization}
\end{equation}$$

[\[eq:batch-normalization\]](#eq:batch-normalization){reference-type="ref+label" reference="eq:batch-normalization"} gives the formula for computing inference BN, where $\mathbf{x} \text{ and } \mathbf{y}$ are the input feature and the output of BN, $\gamma$ and $\beta$ are learned parameters, $\mu$ and $\sigma$ are the mean value and standard deviation calculated from the training set, and $\epsilon$ is the additional small value (e.g., 1e-6) to prevent the denominator from being 0. The variables of [\[eq:batch-normalization\]](#eq:batch-normalization){reference-type="ref+label" reference="eq:batch-normalization"} are determined in the training pass and integrated into the trained weights. If the features in one channel share the same parameters, then it turns to a linear transform on each output channel. Channel-wise BN parameters potentially helps channel-wise pruning. BN could also raise the performance of the cluster-based quantize technique by reducing parameter dependency [@Choudhary2020].

Since the parameters of the BN operation are not modified in the inference phase, they may be combined with the trained weights and biases. This is called BN folding or BN merging. [\[eq:bn-folding\]](#eq:bn-folding){reference-type="ref+label" reference="eq:bn-folding"} show an example of BN folding. The new weight $\mathbf{W'}$ and bias $\mathbf{b'}$ are calculated using the pretrained weights $\mathbf{W}$ and BN parameters from [\[eq:batch-normalization\]](#eq:batch-normalization){reference-type="ref+label" reference="eq:batch-normalization"}. Since the new weight is computed after training and prior to inference, the number of multiplies are reduced and therefore BN folding decreases inference latency and computational complexity. $$\begin{equation}
    % \begin{aligned}
        \mathbf{W'} =\gamma \cdot \frac{\mathbf{W}}{\sqrt{\sigma^{2}+\epsilon}}
        ,\quad
        \mathbf{b'} =\gamma \cdot \frac{\mathbf{b}-\mu}{\sqrt{\sigma^{2}+\epsilon}}+\beta
    % \end{aligned}
\label{eq:bn-folding}
\end{equation}$$

## Pooling {#sec:cnn-pool}

Pooling was first published in the 1980s with neocognitron [@Fukushima1988]. The technique takes a group of values and reduces them to a single value. The selection of the single replacement value can be computed as an average of the values (average pooling) or simply selecting the maximum value (max pooling).

Pooling destroys spatial information as it is a form of down-sampling. The window size defines the area of values to be pooled. For image processing it is usually a square window with typical sizes being $2\times2$, $3\times3$ or $4\times4$. Small windows allow enough information to be propagated to successive layers while reducing the total number of computations [@Sun2017].

Global pooling is a technique where, instead of reducing a neighborhood of values, an entire feature map is reduced to a single value [@Lin2014]. Global Average Pooling (GAP) extracts information from multi-channel features and can be used with dynamic pruning [@Lin2017; @Zhang2019d].

Capsule structures have been proposed as an alternative to pooling. Capsule networks replace the scalar neuron with vectors. The vectors represent a specific entity with more detailed information, such as position and size of an object. Capsule networks void loss of spatial information by capturing it in the vector representation. Rather than reducing a neighborhood of values to a single value, capsule networks perform a dynamic routing algorithm to remove connections [@Sabour2017].

## Parameters {#sec:cnn-param}

![Popular CNN Models: Top-1 accuracy vs GFLOPs and model size, adopted from [@Bianco2018]](FLOPS1.png){#fig:model-size-acc width="98%"}

[7](#fig:model-size-acc){reference-type="ref+label" reference="fig:model-size-acc"} show top-1 accuracy percent verses the number of operations needed for a number of popular neural networks [@Bianco2018]. The number of parameters in each network is represented by the size of the circle. A trend (not shown in the figure) is a yearly increase in parameter complexity. In 2012, AlexNet [@Krizhevsky2012] was published with 60 million parameters. In 2013, VGG [@Simonyan2014] was introduced with 133 million parameters and achieved 71.1% top-1 accuracy. These were part of the ImageNet large scale visual recognition challenge (ILSVRC) [@Russakovsky2015]. The competition's metric was top-1 absolute accuracy. Execution time was not a factor. This incentivized neural network designs with significant redundancy. As of 2020, models with more than 175 billion parameters have been published [@Brown2020].

Networks that execute in data centers can accommodate models with a large number of parameters. In resource constrained environments such as edge and mobile deployments, reduced parameter models have been designed. For example, GoogLeNet [@Szegedy2015] achieves similar top-1 accuracy of 69.78% as VGG-16 but with only 7 million parameters. MobileNet [@Howard2017] has 70% top-1 accuracy with only 4.2 million parameters and only 1.14 Giga FLoating-point OPerations (GFLOPs). A more detailed network comparison can be found in [@Albanie2020].

# Pruning {#sec:pruning}

Network pruning is an important technique for both memory size and bandwidth reduction. In the early 1990s, pruning techniques were developed to reduce a trained large network into a smaller network without requiring retraining [@Reed1993]. This allowed neural networks to be deployed in constrained environments such as embedded systems. Pruning removes redundant parameters or neurons that do not significantly contribute to the accuracy of results. This condition may arise when the weight coefficients are zero, close to zero, or are replicated. Pruning consequently reduces the computational complexity. If pruned networks are retrained it provides the possibility of escaping a previous local minima [@Choi2008] and further improve accuracy.

Research on network pruning can roughly be categorized as sensitivity calculation and penalty-term methods [@Reed1993]. Significant recent research interest has continued showing improvements for both network pruning categories or a further combination of them.

![Pruning Categories: *Static pruning* is performed offline prior to inference while *Dynamic pruning* is performed at runtime.](8.pdf){#fig:prune-class width="95%"}

Recently, new network pruning techniques have been created. Modern pruning techniques may be classified by various aspects including: 1) *structured* and *unstructured pruning* depending if the pruned network is symmetric or not, 2) *neuron* and *connection pruning* depending on the pruned element type, or 3) *static* and *dynamic pruning*. [8](#fig:prune-class){reference-type="ref+label" reference="fig:prune-class"} shows the processing differences between static and dynamic pruning. *Static pruning* has all pruning steps performed offline prior to inference while *dynamic pruning* is performed during runtime. While there is overlap between the categories, in this paper we will use *static pruning* and *dynamic pruning* for classification of network pruning techniques.

[9](#fig:pruning-shapes){reference-type="ref+label" reference="fig:pruning-shapes"} shows a granularity of pruning opportunities. The four rectangles on the right side correspond to the four brown filters in the top of [2](#fig:separable-conv){reference-type="ref+label" reference="fig:separable-conv"}. Pruning can occur on an element-by-element, row-by-row, column-by-column, filter-by-filter, or layer-by-layer basis. Typically element-by-element has the smallest sparsity impact, and results in a unstructured model. Sparsity decreases from left-to-right in [9](#fig:pruning-shapes){reference-type="ref+label" reference="fig:pruning-shapes"}.

<figure id="fig:pruning-shapes" data-latex-placement="htb">
<embed src="8.pdf" style="width:95.0%" />
<figcaption>Pruning Opportunities: Different network sparsity results from the granularity of pruned structures. Shape-wise pruning was proposed by Wen <span class="citation" data-cites="Wen2016a"></span>.</figcaption>
</figure>

$$\begin{equation}
\label{eq:pruning}
\begin{split}
\arg\min_{p} \  {L}={N(x;\mathbf{W}) - N_p(x;\mathbf{W}_p)} \\
\text{where} \  N_p(x;\mathbf{W}_p) = P\left(N\left(x;\mathbf{W}\right)\right) \\
% N(x; \mathbf{W}) = \sum_{i=0}^{n} \left({W}_i \ast {F}_i\right),\ \mathbf{W}=\{W_0, W_1, ..., W_n\}
% , \  N_p = P(N) \\
% \arg\min_{p} \quad &\mathbf{L}=(\mathbf{N}_o - \mathbf{N}_p) \\
% \text{where} \quad &\mathbf{N} = \mathbf{W} \ast \mathbf{F} + \mathbf{B}, \  \mathbf{W}_p = P(\mathbf{W}_o) \\
% \text{subject to}
\end{split}
\end{equation}$$

Independent of categorization, pruning can be described mathematically as [\[eq:pruning\]](#eq:pruning){reference-type="ref+label" reference="eq:pruning"}. ${N}$ represents the entire neural network which contains a series of layers (e.g., convolutional layer, pooling layer, etc.) with $x$ as input. ${L}$ represents the pruned network with $N_p$ performance loss compared to the unpruned network. Network performance is typically defined as accuracy in classification. The pruning function, $P(\cdot)$, results in a different network configuration ${N}_p$ along with the pruned weights $\mathbf{W}_p$. The following sections are primarily concerned with the influence of $P(\cdot)$ on ${N}_p$. We also consider how to obtain $\mathbf{W}_p$.

## Static Pruning {#sec:pruning-static}

Static pruning is a network optimization technique that removes neurons offline from the network after training and before inference. During inference, no additional pruning of the network is performed. Static pruning commonly has three parts: 1) selection of parameters to prune, 2) the method of pruning the neurons, and 3) optionally fine-tuning or re-training [@Han2015]. Retraining may improve the performance of the pruned network to achieve comparable accuracy to the unpruned network but may require significant offline computation time and energy.

### Pruning Criteria {#sec:pruning-static-criteria}

As a result of network redundancy, neurons or connections can often be removed without significant loss of accuracy. As shown in [\[eq:convolution\]](#eq:convolution){reference-type="ref+label" reference="eq:convolution"}, the core operation of a network is a convolution operation. It involves three parts: 1) input features as produced by the previous layer, 2) weights produced from the training phase, and 3) bias values produced from the training phase. The output of the convolution operation may result in either zero valued weights or features that lead to a zero output. Another possibility is that similar weights or features may be produced. These may be merged for distributive convolutions.

An early method to prune networks is brute-force pruning. In this method the entire network is traversed element-wise and weights that do not affect accuracy are removed. A disadvantage of this approach is the large solution space to traverse. A typical metric to determine which values to prune is given by the $l_p\text{-norm, s.t. } p\in\{N, \infty\}$, where $N$ is natural number. The $l_p$-norm of a vector $\mathbf{x}$ which consists of $n$ elements is mathematically described by [\[eq:lp-norm\]](#eq:lp-norm){reference-type="ref+label" reference="eq:lp-norm"}. $$\begin{equation}
\label{eq:lp-norm}
\|\mathbf{x}\|_{p}=\left(\sum_{i=1}^{n}\left|x_{i}\right|^{p}\right)^\frac{1}{p}
\end{equation}$$

Among the widely applied measurements, the $l_1$-norm is also known as the *Manhattan norm* and the $l_2$-norm is also known as the *Euclidean norm*. The corresponding $l_1$ and $l_2$ regularization have the names *LASSO* (least absolute shrinkage and selection operator) and *Ridge*, respectively [@Tishbirani1996]. The difference between the $l_2$-norm pruned tensor and an unpruned tensor is called the $l_2$-distance. Sometimes researchers also use the term $l_0$-norm defined as the total number of nonzero elements in a vector.

$$\begin{equation}
\label{eq:lasso-ori}
\begin{split}
    % (\hat{\alpha}, \hat{\beta})=
    &\arg\,\min_{\alpha, \beta} \left\{\sum_{i=1}^{N}\left(y_{i}-\alpha-\sum_{j=1}^{p} \beta_{j} \mathbf{x}_{i j}\right)^{2}\right\} \\
    &\text {subject to} \sum_{j}^{p}\left|\beta_{j}\right| \leqslant t
\end{split}
\end{equation}$$

Equation [\[eq:lasso-ori\]](#eq:lasso-ori){reference-type="ref+label" reference="eq:lasso-ori"} mathematically describes $l_2$ LASSO regularization. Consider a sample consisting of $N$ cases, each of which consists of $p$ covariates and a single outcome $y_i$. Let $x^i=(x_{i1},...,x_{ip})^T$ be the standardized covariate vector for the $i$-th case (input feature in DNNs), so we have $\sum_ix_{ij}/N=0,\ \sum_ix_{ij}^2/N=1$. $\beta$ represents the coefficients ${\beta}=({\beta_1},...,{\beta_p})^T$ (weights) and $t$ is a predefined tunning parameter that determines the sparsity. The LASSO estimate $\alpha$ is 0 when the average of $y_i$ is 0 because for all $t$, the solution for $\alpha$ is $\alpha = \overline{y}$. If the constraint is $\sum_{j}^{p} \beta_j^2 \leqslant t$ then the [\[eq:lasso-ori\]](#eq:lasso-ori){reference-type="ref+label" reference="eq:lasso-ori"} becomes Ridge regression. Removing the constraint will results in the Ordinary Least Squares (OLS) solution.

$$\begin{equation}
\label{eq:lasso-simp}
\begin{split}
    %LASSO(\hat{\alpha}, \hat{\beta}) = \\
    arg\,\min_{\beta \in \mathbb{R}}\left\{\frac{1}{N}\left\|y - \mathbf{X}\beta\right\|_2^2+\lambda\left\|\beta\right\|_1\right\}
\end{split}
\end{equation}$$

[\[eq:lasso-ori\]](#eq:lasso-ori){reference-type="ref+label" reference="eq:lasso-ori"} can be simplified into the so-called Lagrangian form shown in [\[eq:lasso-simp\]](#eq:lasso-simp){reference-type="ref+label" reference="eq:lasso-simp"}. The Lagrangian multiplier translates the objective function $f(x)$ and constraint $g(x)=0$ into the format of $\mathcal{L}(x, \lambda)=f(x)-\lambda g(x)$, Where the $\|\cdot\|_p$ is the standard $l_p\text{-norm}$, the $\mathbf{X}$ is the covariate matrix that contains $x_{ij}$, and $\lambda$ is the data dependent parameter related to $t$ from [\[eq:lasso-ori\]](#eq:lasso-ori){reference-type="ref+label" reference="eq:lasso-ori"}.

Both magnitude-based pruning and penalty based pruning may generate zero values or near-zero values for the weights. In this section we discuss both methods and their impact.

#### Magnitude-based pruning:

It has been proposed and is widely accepted that trained weights with large values are more important than trained weights with smaller values [@Lei2017]. This observation is the key to magnitude-based methods. Magnitude-based pruning methods seek to identify unneeded weights or features to remove them from runtime evaluation. Unneeded values may be pruned either in the kernel or at the activation map. The most intuitive magnitude-based pruning methods is to prune all zero-valued weights or all weights within an absolute value threshold.

LeCun as far back as 1990 proposed Optimal Brain Damage (OBD) to prune single non-essential weights [@LeCun1990]. By using the second derivative (Hessian matrix) of the loss function, this static pruning technique reduced network parameters by a quarter. For a simplified derivative computation, OBD functions under three assumptions: 1) *quadratic* - the cost function is near-quadratic, 2) *extremal* - the pruning is done after the network converged, and 3) *diagonal* - sums up the error of individual weights by pruning the result of the error caused by their co-consequence. This research also suggested that the sparsity of DNNs could provide opportunities to accelerate network performance. Later Optimal Brain Surgeon (OBS) [@Hassibi1993] extended OBD with a similar second-order method but removed the *diagonal* assumption in OBD. OBS considers the Hessian matrix is usually non-diagonal for most applications. OBS improved the neuron removal precision with up to a 90% reduction in weights for XOR networks.

These early methods reduced the number of connections based on the second derivative of the loss function. The training procedure did not consider future pruning but still resulted in networks that were amenable to pruning. They also suggested that methods based on Hessian pruning would exhibit higher accuracy than those pruned with only magnitude-based algorithms [@Hassibi1993]. More recent DNNs exhibit larger weight values when compared to early DNNs. Early DNNs were also much shallower with orders of magnitude less neurons. GPT-3 [@Brown2020], for example, contains 175-billion parameters while VGG-16 [@Simonyan2014] contains just 133-million parameters. Calculating the Hessian matrix during training for networks with the complexity of GPT-3 is not currently feasible as it has the complexity of $O(W^2)$. Because of this simpler magnitude-based algorithms have been developed [@Molchanov2017; @Lee2019a].

Filter-wise pruning [@Li2017] uses the $l_1$-norm to remove filters that do not affect the accuracy of the classification. Pruning entire filters and their related feature maps resulted in a reduced inference cost of 34% for VGG-16 and 38% for ResNet-110 on the CIFAR-10 dataset with improved accuracy 0.75% and 0.02%, respectively.

Most network pruning methods choose to measure weights rather than activations when rating the effectiveness of pruning [@Guo2016]. However, activations may also be an indicator to prune corresponding weights. Average Percentage Of Zeros (APoZ) [@Hu2016] was introduced to judge if one output activation map is contributing to the result. Certain activation functions, particularly rectification such as Rectified Linear Unit (ReLU), may result in a high percentage of zeros in activations and thus be amenable to pruning. [\[eq:apoz\]](#eq:apoz){reference-type="ref+label" reference="eq:apoz"} shows the definition of $\operatorname{APoZ}_{c}^{(i)}$ of the $c$-th neuron in the $i$-th layer, where $\mathbf{O}_{c}^{(i)}$ denotes the activation, $N$ is the number of calibration (validation) images, and $M$ is the dimension of activation map. $f(\text{true})=1$ and $f(\text{false})=0$. $$\begin{equation}
\label{eq:apoz}
\operatorname{APoZ}_{c}^{(i)}=
\operatorname{APoZ}\left(\mathbf{O}_{c}^{(i)}\right)=\frac{\sum\limits_{k=0}^{N} \sum\limits_{j=0}^{M} f\left(\mathbf{O}_{c, j}^{(i)}(k)=0\right)}{N \times M}
\end{equation}$$ Similarly, inbound pruning [@Polyak2015], also an activation technique, considers channels that do not contribute to the result. If the top activation channel in the standard convolution of [2](#fig:separable-conv){reference-type="ref+label" reference="fig:separable-conv"} are determined to be less contributing, the corresponding channel of the filter in the bottom of the figure will be removed. After pruning this technique achieved about $1.5\times$ compression.

Filter-wise pruning using a threshold from the sum of filters' absolute values can directly take advantage of the structure in the network. In this way, the ratio of pruned to unpruned neurons (i.e. the pruning ratio) is positively correlated to the percentage of kernel weights with zero values, which can be further improved by penalty-based methods.

#### Penalty-based pruning:

In penalty-based pruning, the goal is to modify an error function or add other constraints, known as bias terms, in the training process. A penalty value is used to update some weights to zero or near zero values. These values are then pruned.

Hanson [@HANSON1989] explored hyperbolic and exponential bias terms for pruning in the late 80s. This method uses weight decay in backpropagation to determine if a neuron should be pruned. Low-valued weights are replaced by zeros. Residual zero valued weights after training are then used to prune unneeded neurons.

Feature selection [@Dash1997] is a technique that selects a subset of relevant features that contribute to the result. It is also known as attribute selection or variable selection. Feature selection helps algorithms avoiding over-fitting and accelerates both training and inference by removing features and/or connections that don't contribute to the results. Feature selection also aids model understanding by simplifying them to the most important features. Pruning in DNNs can be considered to be a kind of feature selection [@JianchangMao1994].

LASSO was previously introduced as a penalty term. LASSO shrinks the least absolute valued feature's corresponding weights. This increases weight sparsity. This operation is also referred to as LASSO feature selection and has been shown to perform better than traditional procedures such as OLS by selecting the most significantly contributed variables instead of using all the variables. This lead to approximately 60% more sparsity than OLS [@Muthukrishnan2016].

Element-wise pruning may result in an unstructured network organizations. This leads to sparse weight matrices that are not efficiently executed on instruction set processors. In addition they are usually hard to compress or accelerate without specialized hardware support [@Han2016]. Group LASSO [@Yuan2006] mitigates these inefficiencies by using a structured pruning method that removes entire groups of neurons while maintaining structure in the network organization [@Foroosh2015].

Group LASSO is designed to ensure that all the variables sorted into one group could be either included or excluded as a whole. [\[eq:group-lasso\]](#eq:group-lasso){reference-type="ref+label" reference="eq:group-lasso"} gives the pruning constraint where $\mathbf{X}$ and $\beta$ in [\[eq:lasso-simp\]](#eq:lasso-simp){reference-type="ref+label" reference="eq:lasso-simp"} are replaced by the higher dimensional $\mathbf{X_j}$ and $\beta_j$ for the $j$ groups.

$$\begin{equation}
\label{eq:group-lasso}
\begin{split}
    &arg\,\min_{\beta \in \mathbb{R}^{p}}\left\{\left\|y-\sum_{j=1}^{J} \mathbf{X}_{j} \beta_{j}\right\|_{2}^{2}+\lambda \sum_{j=1}^{J}\left\|\beta_{j}\right\|_{K_{j}}\right\} \\
    %&\text{subject to } \|z\|_{K_{j}}=\left(z^{t} K_{j} z\right)^{1 / 2}
\end{split}
\end{equation}$$

![Types of Sparsity Geometry, adopted from [@Wen2016a]](ssl.pdf){#fig:ssl width=".95\\linewidth"}

[10](#fig:ssl){reference-type="ref+label" reference="fig:ssl"} shows Group LASSO with group shapes used in Structured Sparsity Learning (SSL) [@Wen2016a]. Weights are split into multiple groups. Unneeded groups of weights are removed using LASSO feature selection. Groups may be determined based on geometry, computational complexity, group sparsity, etc. SSL describes an example where group sparsity in row and column directions may be used to reduce the execution time of GEMM. SSL has shown improved inference times on AlexNet with both CPUs and GPUs by $5.1\times$ and $3.1\times$, respectively.

Group-wise brain damage [@Lebedev2016] also introduced the group LASSO constraint but applied it to filters. This simulates brain damage and introduces sparsity. It achieved $2\times$ speedup with 0.7% ILSVRC-2012 accuracy loss on the VGG Network.

Sparse Convolutional Neural Networks (SCNN) [@Foroosh2015] take advantage of two-stage tensor decomposition. By decomposing the input feature map and convolutional kernels, the tensors are transformed into two tensor multiplications. Group LASSO is then applied. SCNN also proposed a hardware friendly algorithm to further accelerate sparse matrix computations. They achieved $2.47\times$ to $6.88\times$ speed-up on various types of convolution.

Network slimming [@Liu2017a] applies LASSO on the scaling factors of BN. BN normalizes the activation by statistical parameters which are obtained during the training phase. Network slimming has the effect of introducing forward invisible additional parameters without additional overhead. Specifically, by setting the BN scaler parameter to zero, channel-wise pruning is enabled. They achieved 82.5% size reduction with VGG and 30.4% computation compression without loss of accuracy on ILSVRC-2012.

Sparse structure selection [@Huang2018a] is a generalized network slimming method. It prunes by applying LASSO to sparse scaling factors in neurons, groups, or residual blocks. Using an improved gradient method, Accelerated Proximal Gradient (APG), the proposed method shows better performance without fine-tunning achieving $4\times$ speed-up on VGG-16 with 3.93% ILSVRC-2012 top-1 accuracy loss.

#### Dropout:

While not specifically a technique to prune networks, dropout does reduce the number of parameters [@Srivastava2014]. It was originally designed as a stochastic regularizer to avoid over-fitting of data [@Hinton2012a]. The technique randomly omits a percentage of neurons typically up to 50%, This *dropout* operation breaks off part of the connections between neurons to avoid co-adaptations. Dropout could also be regarded as an operation that separately trains many sub-networks and takes the average of them during the inference phase. Dropout increases training overhead but it does not affect the inference time.

Sparse variational dropout [@Molchanov2017a] added a dropout hyperparameter called the dropout rate to reduce the weights of VGG-like networks by $68\times$. During training the dropout rate can be used to identify single weights to prune. This can also be applied with other compression approaches for further reduction in weights.

#### Redundancies:

The goal of norm-based pruning algorithms is to remove zeros. This implies that the distribution of values should wide enough to retain some values but contain enough values close to zero such that a smaller network organization is still accurate. This does not hold in some circumstances. For example, filters that have small norm deviations or a large minimum norm have small search spaces making it difficult to prune based on a threshold [@He2018]. Even when parameter values are wide enough, in some networks smaller values may still play an important role in producing results. One example of this is when large valued parameters saturate [@Engelbrecht2001]. In these cases magnitude-based pruning of zero values may decrease result accuracy.

Similarly, penalty-based pruning may cause network accuracy loss. In this case, the filters identified as unneeded due to similar coefficient values in other filters may actually be required. Removing them may significantly decrease network accuracy [@Guo2016]. [3.1.2](#sec:prunning-static-tunning){reference-type="ref+label" reference="sec:prunning-static-tunning"} describes techniques to undo pruning by tuning the weights to minimize network loss while this section describes redundancy based pruning.

Using BN parameters, feature map channel distances can be computed by layer [@Zhang2019]. Using a clustering approach for distance, nearby features can be tuned. An advantage of clustering is that redundancy is not measured with an absolute distance but a relative value. With about 60 epochs of training they were able to prune the network resulting in a 50% reduction in FLOPs (including non-convolutional operations) with a reduction in accuracy of only 1% for both top-1 and top-5 on the ImageNet dataset.

Filter pruning via geometric median (FPGM) [@He2018] identifies filters to prune by measuring the $l_2$-distance using the geometric median. FPGM found 42% FLOPs reduction with 0.05% top-1 accuracy drop on ILSVRC-2012 with ResNet-101.

The reduce and reused (also described as outbound) method [@Polyak2015] prunes entire filters by computing the statistical variance of each filter's output using a calibration set. Filters with low variance are pruned. The outbound method obtained $2.37\times$ acceleration with 1.52% accuracy loss on Labeled Faces in the Wild (LFW) dataset [@Huang2014] in the filed of face recognition.

A method that iteratively removes redundant neurons for FCLs without requiring special validation data is proposed in [@Srinivas2015]. This approach measures the similarity of weight groups after a normalization. It removes redundant weights and merges the weights into a single value. This lead to a 34.89% reduction of FCL weights on AlexNet with 2.24% top-1 accuracy loss on ILSVRC-2012.

Comparing with the similarity based approach above, DIVersity NETworks (DIVNET) [@Mariet2016] considers the calculation redundancy based on the activations. DIVNET introduces Determinantal Point Process (DPP) [@Macchi1975] as a pruning tool. DPP sorts neurons into categories including dropped and retained. Instead of forcing the removal of elements with low contribution factors, they fuse the neurons by a process named re-weighting. Re-weighting works by minimizing the impact of neuron removal. This minimizes pruning influence and mitigates network information loss. They found 3% loss on CIFAR-10 dataset when compressing the network into half weight.

ThiNet [@Luo2017] adopts statistics information from the next layer to determine the importance of filters. It uses a greedy search to prune the channel that has the smallest reconstruction cost in the next layer. ThiNet prunes layer-by-layer instead of globally to minimize large errors in classification accuracy. It also prunes less during each training epoch to allow for coefficient stability. The pruning ratio is a predefined hyper-parameter and the runtime complexity is directly related to the pruning ratio. ThiNet compressed ResNet-50 FLOPs to 44.17% with a top-1 accuracy reduction of 1.87%.

He [@He2017] adopts LASSO regression instead of a greedy algorithm to estimate the channels. Specifically, in one iteration, the first step is to evaluate the most important channel using the $l_1$-norm. The next step is to prune the corresponding channel that has the smallest Mean Square Error (MSE). Compared to an unpruned network, this approach obtained $2\times$ acceleration of ResNet-50 on ILSVRC-2012 with about 1.4% accuracy loss on top-5, and a $4\times$ reduction in execution time with top-5 accuracy loss of 1.0% for VGG-16. The authors categorize their approach as dynamic inference-time channel pruning. However it requires 5000 images for calibration with 10 samples per image and more importantly results in a statically pruned network. Thus we have placed it under static pruning.

### Pruning combined with Tuning or Retraining {#sec:prunning-static-tunning}

Pruning removes network redundancies and has the benefit of reducing the number of computations without significant impact on accuracy for some network architectures. However, as the estimation criterion is not always accurate, some important elements may be eliminated resulting in a decrease in accuracy. Because of the loss of accuracy, time-consuming fine-tuning or re-training may be employed to increase accuracy [@Yu2017a].

Deep compression [@Han2015], for example, describes a static method to prune connections that don't contribute to classification accuracy. In addition to feature map pruning they also remove weights with small values. After pruning they re-train the network to improve accuracy. This process is performed iteratively three times resulting in a $9\times$ to $13\times$ reduction in total parameters with no loss of accuracy. Most of the removed parameters were from FCLs.

#### Recoverable Pruning:

Pruned elements usually cannot be recovered. This may result in reduced network capability. Recovering lost network capability requires significant re-training. Deep compression required millions of iterations to retrain the network [@Han2015]. To avoid this shortcoming, many approaches adopt recoverable pruning algorithms. The pruned elements may also be involved in the subsequent training process and adjust themselves to fit the pruned network.

Guo [@Guo2016] describes a recoverable pruning method using binary mask matrices to indicate whether a single weight value is pruned or not. The $l_1$-norm pruned weights can be stochastically spliced back into the network. Using this approach AlexNet was able to be reduced by a factor of $17.7\times$ with no accuracy loss. Re-training iterations were significantly reduced to 14.58% of Deep compression [@Han2015]. However this type of pruning still results in an asymmetric network complicating hardware implementation.

Soft Filter Pruning (SFP) [@He2017a] further extended recoverable pruning using a dimension of filter. SFP obtained structured compression results with an additional benefit or reduced inference time. Furthermore, SFP can be used on difficult to compress networks achieving a 29.8% speed-up on ResNet-50 with 1.54% ILSVRC-2012 top-1 accuracy loss. Comparing with Guo's recoverable weight [@Guo2016] technique, SFP achieves inference speed-ups closer to theoretical results on general purpose hardware by taking advantage of the structure of the filter.

#### Increasing Sparsity:

Another motivation to apply fine-tuning is to increase network sparsity. Sparse constraints [@Zhou2016] applied low rank tensor constraints [@Liu2013] and group sparsity [@Deng2013] achieving a 70% reduction of neurons with a 0.57% drop of AlexNet in ILSVRC-2012 top-1 accuracy.

#### Adaptive Sparsity:

No matter what kind of pruning criteria is applied, a layer-wise pruning ratio usually requires a human decision. Too high a ratio resulting in very high sparsity may cause the network to diverge requiring heavy re-tuning.

Network slimming [@Liu2017a], previously discussed, addresses this problem by automatically computing layer-wise sparsity. This achieved a $20\times$ model size compression, $5\times$ computing reduction, and less than 0.1% accuracy loss on the VGG network.

Pruning can also be performed using a min-max optimization module [@Singh2019] that maintains network accuracy during tuning by keeping a pruning ratio. This technique compressed the VGG network by a factor of $17.5\times$ and resulted in a theoretical execution time (FLOPs) of $15.56\%$ of the unpruned network. A similar approach was proposed with an estimation of weights sets [@Carreira-Perpinan2018]. By avoiding the use of a greedy search to keep the best pruning ratio, they achieved the same ResNet classification accuracy with only 5% to 10% size of original weights.

AutoPruner [@Luo2020] integrated the pruning and fine-tuning of a three-stage pipeline as an independent training-friendly layer. The layer helped gradually prune during training eventually resulting in a less complex network. AutoPruner pruned 73.59% of compute operations on VGG-16 with 2.39% ILSVRC-2012 top-1 loss. ResNet-50 resulted in a 65.80% of compute operations with 3.10% loss of accuracy.

#### Training from Scratch:

Observation shows that network training efficiency and accuracy is inversely proportional to structure sparsity. The more dense the network, the less training time [@Han2015a; @Li2017; @Frankle2019]. This is one reason that current pruning techniques tend to follow a train-prune-tune pipeline rather than training a pruned structure from scratch.

However, the lottery ticket hypothesis [@Frankle2019] shows that it is not of primary importance to preserve the original weights but the initialization. Experiments show that dense, randomly-initialized pruned sub-networks can be trained effectively and reach comparable accuracy to the original network with the same number of training iterations. Furthermore, standard pruning techniques can uncover the aforementioned sub-networks from a large oversized network - the *Winning Tickets*. In contrast with current static pruning techniques, the lottery ticket hypothesis after a period of time drops all well-trained weights and resets them to an initial random state. This technique found that ResNet-18 could maintain comparable performance with a pruning ratio up to 88.2% on the CIFAR-10 dataset.

#### Towards Better Accuracy:

By reducing the number of network parameters, pruning techniques can also help to reduce over-fitting. Dense-Sparse-Dense (DSD) training [@Han2016d] helps various network improve classification accuracy by 1.1% to 4.3%. DSD uses a three stage pipeline: 1) dense training to identify important connections, 2) prune insignificant weights and sparse training with a sparsity constraint to take reduce the number of parameters, and 3) re-dense the structure to recover the original symmetric structure, this also increase the model capacity. The DSD approach has also shown impressive performance on the other type of deep networks such as Recurrent Neural Networks (RNNs) and Long Short Term Memory networks (LSTMs).

## Dynamic Pruning {#sec:pruning-dynamic}

Except for recoverable techniques, static pruning permanently destroys the original network structure which may lead to a decrease in model capability. Techniques have been researched to recover lost network capabilities but once pruned and re-trained, the static pruning approach can't recover destroyed information. Additionally, observations shows that the importance of neuron binding is input-independent [@Gao2019].

![Dynamic Pruning System Considerations](8.pdf){#fig:dynamic-pruning-tips width="\\textwidth"}

Dynamic pruning determines at runtime which layers, channels, or neurons will not participate in further activity. Dynamic pruning can overcome limitations of static pruning by taking advantage of changing input data potentially reducing computation, bandwidth, and power dissipation. Dynamic pruning typically doesn't perform runtime fine-tuning or re-training. In [11](#fig:dynamic-pruning-tips){reference-type="ref+label" reference="fig:dynamic-pruning-tips"}, we show an overview of dynamic pruning systems. The most important consideration is the decision system that decides what to prune. The related issues are:

1.  The type of the decision components: a) additional connections attached to the original network used during the inference phase and/or the training phase, b) characteristics of the connections that can be learned by standard backpropagation algorithms [@Gao2019], and c) a side decision network which tends to perform well but is often difficult to train [@Lin2017].

2.  The pruning level (shape): a) channel-wise [@Lin2017; @Gao2019; @Zhang2019d], b) layer-wise [@Leroux2017], c) block-wise [@Wu2018a], or d) network-wise [@Bolukbasi2017]. The pruning level chosen influences hardware design.

3.  Input data: a) one-shot information feeding [@Wu2018a] feeds the entire input to the decision system, and b) layer-wise information feeding [@Bolukbasi2017; @Figurnov2017] where a window of data is iteratively fed to the decision system along with the forwarding.

4.  Computing a decision score: $l_p$-norm [@Gao2019], or b) other approaches [@Huang2018b].

5.  Score comparison: a) human experience/experiment results [@Leroux2017] or b) automatic threshold or dynamic mechanisms [@Huang2018b].

6.  Stopping criteria: a) in the case of layer-wise and network-wise pruning, some pruning algorithms skip the pruned layer/network [@Bengio2015; @Wu2018a], b) some algorithms dynamically choose the data path [@Odena2017; @Yu2018a], and c) ending the computation and outputing the predicting results [@Figurnov2017; @Leroux2017; @Li2019]. In this case the remaining layers are considered to be pruned.

7.  Training the decision component: a) attached connections can be trained along with the original network [@Leroux2017; @Li2019; @Gao2019], b) side networks are typically trained using reinforcement learning (RL) algorithms [@Bengio2015; @Lin2017; @Odena2017; @Wu2018a].

For instruction set processors, feature maps or the number of filters used to identify objects is a large portion of bandwidth usage [@Sze2017] - especially for depth-wise or point-wise convolutions where features consume a larger portion of the bandwidth [@Chollet2017]. Dynamic tuning may also be applied to statically pruned networks potentially further reducing compute and bandwidth requirements.

A drawback of dynamic pruning is that the criteria to determine which elements to prune must be computed at runtime. This adds overhead to the system requiring additional compute, bandwidth, and power. A trade-off between dynamic pruning overhead, reduced network computation, and accuracy loss, should be considered. One method to mitigate power consumption inhibits computations from 0-valued parameters within a Processing Element (PE) [@Lin2017].

### Conditional Computing {#sec:pruning-dynamic-conditional}

Conditional computing involves activating an optimal part of a network without activating the entire network. Non-activated neurons are considered to be pruned. They do not participate in the result thereby reducing the number of computations required. Conditional computing applies to training and inference [@Bengio2013; @Davis2013].

Conditional computing has a similarity with RL in that they both learn a pattern to achieve a reward. Bengio [@Bengio2015] split the network into several blocks and formulates the block chosen policies as an RL problem. This approach consists of only fully connected neural networks and achieved a $5.3\times$ speed-up on CIFAR-10 dataset without loss of accuracy.

### Reinforcement Learning Adaptive Networks {#sec:pruning-dynamic-adaptive}

Adaptive networks aim to accelerating network inference by conditionally determining early exits. A trade-off between network accuracy and computation can be applied using thresholds. Adaptive networks have multiple intermediate classifiers to provide the ability of an early exit. A cascade network is a type of adaptive network. Cascade networks are the combinations of serial networks which all have output layers rather than per-layer outputs. Cascade networks have a natural advantage of an early exit by not requiring all output layers to be computed. If the early accuracy of a cascade network is not sufficient, inference could potentially be dispatched to a cloud device [@Leroux2017; @Bolukbasi2017]. A disadvantage of adaptive networks is that they usually need hyper-parameters optimized manually (e.g., confidence score [@Leroux2017]). This introduces automation challenges as well as classification accuracy loss. They found 28.75% test error on CIFAR-10 when setting the threshold to 0.5. A threshold of 0.99 lowered the error to 15.74% at a cost of 3x to inference time.

A cascading network [@Odena2017] is an adaptive network with an RL trained *Composer* that can determine a reasonable computation graph for each input. An adaptive controller *Policy Preferences* is used to intelligently enhance the *Composer* allowing an adjustment of the network computation graph from sub-graphs. The *Composer* performs much better in terms of accuracy than the baseline network with the same number of computation-involved parameters on a modified dataset, namely Wide-MNIST. For example, when invoking 1k parameters, the baseline achieves 72% accuracy while the *Composer* obtained 85%.

BlockDrop [@Wu2018a] introduced a *policy network* that trained using RL to make an image-specific determination whether a residual network block should participate in the following computation. While the other approaches compute an exit confidence score per layer, the *policy network* runs only once when an image is loaded. It generates a boolean vector that indicates which residual blocks are activate or inactive. BlockDrop adds more flexibility to the early exit mechanism by allowing a decision to be made on any block and not just early blocks in Spatially Adaptive Computation Time (SACT) [@Figurnov2017]. This is discussed further in [3.2.3](#sec:pruning-dynamic-wo-RL){reference-type="ref+label" reference="sec:pruning-dynamic-wo-RL"}. BlockDrop achieves an average speed-up of 20% on ResNet-101 for ILSVRC-2012 without accuracy loss. Experiments using the CIFAR dataset showed better performance than other SOTA counterparts at that time [@Figurnov2017; @Graves2016; @Li2017].

Runtime Neural Pruning (RNP) [@Lin2017] is a framework that prunes neural networks dynamically. RNP formulates the feature selection problem as a Markov Decision Process (MDP) and then trains an RNN-based decision network by RL. The MDP reward function in the state-action-reward sequence is computation efficiency. Rather than removing layers, a side network of RNP predicts which feature maps are not needed. They found $2.3\times$ to $5.9\times$ reduction in execution time with top-5 accuracy loss from 2.32% to 4.89% for VGG-16.

### Differentiable Adaptive Networks {#sec:pruning-dynamic-wo-RL}

Most of the aforementioned decision components are non-differential, thus computationally expensive RL is adopted for training. A number of techniques have been developed to reduce training complexity by using differentiable methods.

Dynamic channel pruning [@Gao2019] proposes a method to dynamically select which channel to skip or to process using Feature Boosting and Suppression (FBS). FBS is a side network that guides channel amplification and omission. FBS is trained along with convolutional networks using SGD with LASSO constraints. The selecting indicator can be merged into BN parameters. FBS achieved $5\times$ acceleration on VGG-16 with 0.59% ILSVRC-2012 top-5 accuracy loss, and $2\times$ acceleration on ResNet-18 with 2.54% top-1, 1.46% top-5 accuracy loss.

Another approach, Dynamic Channel Pruning (DCP) [@Zhang2019d] dynamically prunes channels using a *channel threshold weighting (T-Weighting)* decision. Specifically, this module prunes the channels whose score is lower than a given threshold. The score is calculated by a T-sigmoid activation function, which is mathematically described in [\[eq:t-sigmoid\]](#eq:t-sigmoid){reference-type="ref+label" reference="eq:t-sigmoid"}, where $\sigma(x)={1}/(1+e^{-x})$ is the sigmoid function. The input to the T-sigmoid activation function is down sampled by a FCL from the feature maps. The threshold is found using iterative training which can be a computationally expensive process. DCP increased VGG-16 top-5 error by 4.77% on ILSVRC-2012 for $5\times$ computation speed-up. By comparison, RNP increased VGG-16 top-5 error by 4.89% [@Lin2017]. $$\begin{equation}
\label{eq:t-sigmoid}
    h(x)=\begin{cases}
    \sigma(x), &\text{if } x > T \\
    0, &\text{otherwise}
    \end{cases}
\end{equation}$$

The cascading neural network by Leroux [@Leroux2017] reduced the average inference time of overfeat network [@Sermanet2013] by 40% with a 2% ILSVRC-2012 top-1 accuracy loss. Their criteria for early exit is based on the confidence score generated by an output layer. The auxiliary layers were trained with general backpropagation. The adjustable score threshold provides a trade-off between accuracy and efficiency.

Bolukbasi [@Bolukbasi2017] reports a system that contains a combination of other SOTA networks (e.g., AlexNet, ResNet, GoogLeNet, etc.). A policy adaptively chooses a point to exit early. This policy can be trained by minimizing its cost function. They format the system as a directed acyclic graph with various pre-trained networks as basic components. They evaluate this graph to determine leaf nodes for early exit. The cascade of acyclic graphs with a combination of various networks reduces computations while maintaining prediction accuracy. ILSVRC-2012 experiments show ResNet-50 acceleration of $2.8\times$ with 1% top-5 accuracy loss and $1.9\times$ speed-up with no accuracy loss.

Considering the similarity of RNNs and residual networks [@Greff2016], Spatially Adaptive Computation Time (SACT) [@Figurnov2017] explored an early stop mechanism of residual networks in the spatial domain. SACT can be applied to various tasks including image classification, object detection, and image segmentation. SACT achieved about 20% acceleration with no accuracy loss for ResNet-101 on ILSVRC-2012.

To meet the computation constraints, Multi-Scale Dense Networks (MSDNets) [@Huang2018b] designed an adaptive network using two techniques: 1) an *anytime-prediction* to generate prediction results at many nodes to facilitate the network's early exit and 2) *batch computational budget* to enforce a simpler exit criteria such as a computation limit. MSDNets combine multi-scale feature maps [@Zhang2016b] and dense connectivity [@Huang2017a] to enable accurate early exit while maintaining higher accuracy. The classifiers are differentiable so that MSDNets can be trained using stochastic gradient descent. MSDNets achieve $2.2\times$ speed-up at the same accuracy for ResNet-50 on ILSVRC-2012 dataset.

To address the training complexity of adaptive networks, Li [@Li2019] proposed two methods. The first method is gradient equilibrium (GE). This technique helps backbone networks converge by using multiple intermediate classifiers across multiple different network layers. This improves the gradient imbalance issue found in MSDNets [@Huang2018b]. The second method is an Inline Subnetwork Collaboration (ISC) and a One-For-All knowledge distillation (OFA). Instead of independently training different exits, ISC takes early predictions into later predictors to enhance their input information. OFA supervises all the intermediate exits using a final classifier. At a same ILSVRC-2012 top-1 accuracy of 73.1%, their network takes only one-third the computational budget of ResNet.

Slimmable Neural Networks (SNN) [@Yu2018a] are a type of networks that can be executed at different widths. Also known as switchable networks, the network enables dynamically selecting network architectures (width) without much computation overhead. Switchable networks are designed to adaptively and efficiently make trade-offs between accuracy and on-device inference latency across different hardware platforms. SNN found that the difference of feature mean and variance may lead to training faults. SNN solves this issue with a novel switchable BN technique and then trains a wide enough network. Unlike cascade networks which primarily benefit from specific blocks, SNN can be applied with many more types of operations. As BN already has two parameters as mentioned in [2](#sec:background){reference-type="ref+label" reference="sec:background"}, the network switch that controls the network width comes with little additional cost. SNN increased top-1 error by 1.4% on ILSVRC-2012 while achieving about $2\times$ speed-up.

## Comparisons {#sec:pruning-comparison}

Pruning techniques are diverse and difficult to compare. Shrinkbench [@Blalock2020] is a unified benchmark framework aiming to provide pruning performance comparisons.

There exist ambiguities about the value of the pre-trained weights. Liu [@Liu2019Rethingking] argues that the pruned model could be trained from scratch using a random weight initialization. This implies the pruned architecture itself is crucial to success. By this observation, the pruning algorithms could be seen as a type of NAS. Liu concluded that because the weight values can be re-trained, by themselves they are not efficacious. However, the lottery ticket hypothesis [@Frankle2019] achieved comparable accuracy only when the weight initialization was exactly the same as the unpruned model. Glae [@Gale2019] resolved the discrepancy by showing that what really matters is the pruning form. Specifically, unstructured pruning can only be fine-tuned to restore accuracy but structured pruning can be trained from scratch. In addition, they explored the performance of dropout and $l_0$ regularization. The results showed that simple magnitude based pruning can perform better. They developed a magnitude based pruning algorithm and showed the pruned ResNet-50 obtained higher accuracy than SOTA at the same computational complexity.

# Quantization {#sec:quantization}

Quantization is known as *the process of approximating a continuous signal by a set of discrete symbols or integer values*. Clustering and parameter sharing also fall within this definition [@Han2015]. Partial quantization uses clustering algorithms such as k-means to quantize weight states and then store the parameters in a compressed file. The weights can be decompressed using either a lookup table or a linear transformation. This is typically performed during runtime inference. This scheme only reduces the storage cost of a model. This is discussed in [4.2.4](#sec:quant-method-other){reference-type="ref+label" reference="sec:quant-method-other"}. In this section we focus on numerical low-bit quantization.

Compressing CNNs by reducing precision values has been previously proposed. Converting floating-point parameters into low numerical precision datatypes for quantizing neural networks was proposed as far back as the 1990s [@Fiesler1990; @balzer1991weight]. Renewed interest in quantization began in the 2010s when 8-bit weight values were shown to accelerate inference without a significant drop in accuracy [@Vanhoucke2011].

![Quantization Evolution: The development of quantization techniques, from left to right. Purple rectangles indicated quantized data while blue rectangles represent full precision 32-bit floating point format.](8.pdf){#fig:quantize-roadmap width="95%"}

Historically most networks are trained using FP32 numbers [@Sze2017]. For many networks an FP32 representation has greater precision than needed. Converting FP32 parameters to lower bit representations can significantly reduce bandwidth, energy, and on-chip area.

[12](#fig:quantize-roadmap){reference-type="ref+label" reference="fig:quantize-roadmap"} shows the evolution of quantization techniques. Initially, only weights were quantized. By quantizing, clustering, and sharing, weight storage requirements can be reduced by nearly $4\times$. Han [@Han2015] combined these techniques to reduce weight storage requirements from 27MB to 6.9MB. Post training quantization involves taking a trained model, quantizing the weights, and then re-optimizing the model to generate a quantized model with scales [@Banner2019]. Quantization-aware training involves fine-tuning a stable full precision model or re-training the quantized model. During this process real-valued weights are often down-scaled to integer values - typically 8-bits [@Jacob2017]. Saturated quantization can be used to generate feature scales using a calibratation algorithm with a calibration set. Quantized activations show similar distributions with previous real-valued data [@Migacz2017]. Kullback-Leibler divergence (KL-divergence, also known as relative entropy or information divergence) calibrated quantization is typically applied and can accelerate the network without accuracy loss for many well known models [@Migacz2017]. Fine-tuning can also be applied with this approach.

KL-divergence is a measure to show the relative entropy of probability distributions between two sets. [\[eq:kl-divergence\]](#eq:kl-divergence){reference-type="ref+label" reference="eq:kl-divergence"} gives the equation for KL-divergence. $P$ and $Q$ are defined as discrete probability distributions on the same probability space. Specifically, $P$ is the original data (floating-point) distribution that falls in several bins. $Q$ is the quantized data histogram. $$\begin{equation}
\label{eq:kl-divergence}
    D_{ \mathrm{KL} }( P \| Q ) = \sum_{i = 0} ^{N} P ( x_i ) \log \left( \frac { P ( x_i ) } { Q ( x_i ) } \right)
\end{equation}$$

Depending upon the processor and execution environment, quantized parameters can often accelerate neural network inference.

Quantization research can be categorized into two focus areas: 1) quantization aware training (QAT) and 2) post training quantization (PTQ). The difference depends on whether training progress is is taken into account during training. Alternatively, we could also categorize quantization by where data is grouped for quantization: 1) layer-wise and 2) channel-wise. Further, while evaluating parameter widths, we could further classify by length: N-bit quantization.

Reduced precision techniques do not always achieve the expected speedup. For example, INT8 inference doesn't achieve exactly $4\times$ speedup over 32-bit floating point due to the additional operations of quantization and dequantization. For instance, Google's TensorFlow-Lite [@tflite] and nVidia's Tensor RT [@Migacz2017] INT8 inference speedup is about $2\text{-}3 \times$. Batch size is the capability to process more than one image in the forward pass. Using larger batch sizes, Tensor RT does achieve $3\text{-}4\times$ acceleration with INT8 [@Migacz2017].

[8](#sec:quant-perf-clct){reference-type="ref+label" reference="sec:quant-perf-clct"} summarizes current quantization techniques used on the ILSVRC-2012 dataset along with their bit-widths for weights and activation.

## Quantization Algebra {#quant-algo}

$$\begin{equation}
    \mathbf{X}_q = f(s \times g(\mathbf{X}_r) + z)
\label{eq:quantization}
\end{equation}$$ There are many methods to quantize a given network. Generally, they are formulated as [\[eq:quantization\]](#eq:quantization){reference-type="ref+label" reference="eq:quantization"} where $s$ is a scalar that can be calculated using various methods. $g(\cdot)$ is the clamp function applied to floating-point values $\mathbf{X}_r$ performing the quantization. $z$ is the zero-point to adjust the true zero in some asymmetrical quantization approaches. $f(\cdot)$ is the rounding function. This section introduces quantization using the mathematical framework of [\[eq:quantization\]](#eq:quantization){reference-type="ref+label" reference="eq:quantization"}.

$$\begin{equation}
\label{eq:quantization-clamp}
    clamp(x, \alpha, \beta) = max(min(x, \beta), \alpha)
\end{equation}$$

[\[eq:quantization-clamp\]](#eq:quantization-clamp){reference-type="ref+label" reference="eq:quantization-clamp"} defines a clamp function. The *min-max* method is given by [\[eq:quantization-min-max\]](#eq:quantization-min-max){reference-type="ref+label" reference="eq:quantization-min-max"} where $[m, M]$ are the bounds for the minimum and maximum values of the parameters, respectively. $n$ is the maximum representable number derived from the bit-width (e.g., $256 = 2 ^ 8$ in case of 8-bit), and $z, s$ are the same as in [\[eq:quantization\]](#eq:quantization){reference-type="ref+label" reference="eq:quantization"}. $z$ is typically non-zero in the *min-max* method [@Jacob2017]. $$\begin{equation}
\label{eq:quantization-min-max}
    \begin{split}
         &g(x) = clamp(x, m, M) \\
         &s = \frac {n - 1}{M - m}, \ z = \frac{m \times (1 - n)}{M - m} \\
         &\text{where} \ m = \min\{\mathbf{X}_i\}, \ M = \max\{\mathbf{X}_i\} \\
    \end{split}
\end{equation}$$

The *max-abs* method uses a symmetry bound shown in [\[eq:quantization-max-abs\]](#eq:quantization-max-abs){reference-type="ref+label" reference="eq:quantization-max-abs"}. The quantization scale $s$ is calculated from the largest one $R$ among the data to be quantized. Since the bound is symmetrical, the zero point $z$ will be zero. In such a situation, the overhead of computing an offset-involved convolution will be reduced but the dynamic range is reduced since the valid range is narrower. This is especially noticeable for ReLU activated data where all of which values fall on the positive axis. $$\begin{equation}
\label{eq:quantization-max-abs}
    \begin{split}
        &g(x) = clamp(x, -M, M)      \\
        & s = \frac{n - 1}{R}, \ z = 0 \\
        &\text{where} \ R = \max\{abs\{\mathbf{X}_i\}\} \\
    \end{split}
\end{equation}$$

Quantization can be applied on input features $\mathbf{F}$, weights $\mathbf{W}$, and biases $\mathbf{b}$. Taking feature $\mathbf{F}$ and weights $\mathbf{W}$ as an example (ignoring the biases) and using the *min-max* method gives [\[eq:quantize-quantize-weights-and-feature\]](#eq:quantize-quantize-weights-and-feature){reference-type="ref+label" reference="eq:quantize-quantize-weights-and-feature"}. The subscripts $r \text{ and } q$ denote the real-valued and quantized data, respectively. The $max$ suffix is from $R$ in [\[eq:quantization-max-abs\]](#eq:quantization-max-abs){reference-type="ref+label" reference="eq:quantization-max-abs"}, while $s_f=(n-1)/F_{max},\ s_w=(n-1)/W_{max}$. $$\begin{equation}
\label{eq:quantize-quantize-weights-and-feature}
    % \begin{split}
        \mathbf{F}_q = \frac{n - 1}{F_{max}} \times \mathbf{F}_{r}
        \ ,\quad
        \mathbf{W}_q = \frac{n - 1}{W_{max}} \times \mathbf{W}_{r} 
    % \end{split}
\end{equation}$$

Integer quantized convolution is shown in [\[eq:quantize-quantized-convolution\]](#eq:quantize-quantized-convolution){reference-type="ref+label" reference="eq:quantize-quantized-convolution"} and follows the same form as convolution with real values. In [\[eq:quantize-quantized-convolution\]](#eq:quantize-quantized-convolution){reference-type="ref+label" reference="eq:quantize-quantized-convolution"}, the $*$ denotes the convolution operation, $\mathbf{F}$ the feature, $\mathbf{W}$ the weights, and $\mathbf{O}_q$, the quantized convolution result. Numerous third party libraries support this type of integer quantized convolution acceleration. They are discussed in [4.3.2](#sec:quant-deploy-framework){reference-type="ref+label" reference="sec:quant-deploy-framework"}. $$\begin{equation}
    \label{eq:quantize-quantized-convolution}
    \mathbf{O}_q = \mathbf{F}_q \ast \mathbf{W}_q \quad \text{s.t.} \ \mathbf{F}, \mathbf{W} \in \mathbb{Z}
\end{equation}$$

De-quantizing converts the quantized value $\mathbf{O}_{q}$ back to floating-point $\mathbf{O}_{r}$ using the feature scales $s_f$ and weights scales $s_w$. A symmetric example with $z=0$ is shown in [\[eq:quantize-dequantize\]](#eq:quantize-dequantize){reference-type="ref+label" reference="eq:quantize-dequantize"}. This is useful for layers that process floating-point tensors. Quantization libraries are discussed in [4.3.2](#sec:quant-deploy-framework){reference-type="ref+label" reference="sec:quant-deploy-framework"}. $$\begin{equation}
\label{eq:quantize-dequantize}
    \mathbf{O}_{r} = \frac{\mathbf{O}_q}{s_f \times s_w} = \mathbf{O}_q \times \frac{{F}_{max}}{(n - 1)} \times \frac{{W}_{max}}{(n - 1)}
\end{equation}$$

In most circumstances, consecutive layers can compute with quantized parameters. This allows dequantization to be merged in one operation as in [\[eq:quantize-merged\]](#eq:quantize-merged){reference-type="ref+label" reference="eq:quantize-merged"}. $\mathbf{F}_{q}^{l+1}$ is the quantized feature for next layer and $s_{f}^{l+1}$ is the feature scale for next layer. $$\begin{equation}
\label{eq:quantize-merged}
    \mathbf{F}_{q}^{l+1} = \frac{\mathbf{O}_q \times s_{f}^{l+1}}{s_f \times s_w}
\end{equation}$$

The activation function can be placed following either the quantized output $\mathbf{O}_q$, the de-quantized output $\mathbf{O}_r$, or after a re-quantized output $\mathbf{F}_{q}^{l+1}$. The different locations may lead to different numerical outcomes since they typically have different precision.

Similar to convolutional layers, FCLs can also be quantized. K-means clustering can be used to aid in the compression of weights. In 2014 Gong [@Gong2014] used k-means clustering on FCLs and achieved a compression ratio of more than $20 \times$ with 1% top-5 accuracy loss.

Bias terms in neural networks introduce intercepts in linear equations. They are typically regarded as constants that help the network to train and best fit given data. Bias quantization is not widely mentioned in the literature. [@Jacob2017] maintained 32-bit biases while quantizing weights to 8-bit. Since biases account for minimal memory usage (e.g. 12 values for a 10-in/12-out FCL vs 120 weight values) it is recommended to leave biases in full precision. If bias quantization is performed it can be a multiplication by both the feature scale and weight scale [@Jacob2017], as shown in [\[eq:quant-bias-scale\]](#eq:quant-bias-scale){reference-type="ref+label" reference="eq:quant-bias-scale"}. However, in some circumstances they may have their own scale factor. For example, when the bit-lengths are limited to be shorter than the multiplication results. $$\begin{equation}
    s_b = s_w \times s_f
    ,\quad
    \mathbf{b}_q = \mathbf{b}_r \times s_b
\label{eq:quant-bias-scale}
\end{equation}$$

## Quantization Methodology {#quant-method}

We describe PTQ and QAT quantization approaches based on back-propagation use. We can also categorize them based on bit-width. In the following subsections, we introduce common quantization methods. In [4.2.1](#sec:quant-method-low-num-precision){reference-type="ref+label" reference="sec:quant-method-low-num-precision"} low bit-width quantization is discussed. In [4.2.2](#sec:quant-method-log){reference-type="ref+label" reference="sec:quant-method-log"} and [4.2.3](#sec:quant-method-plus){reference-type="ref+label" reference="sec:quant-method-plus"} special cases of low bit-width quantization is discussed. In [4.2.5](#sec:quant-method-training){reference-type="ref+label" reference="sec:quant-method-training"} difficulties with training quantized networks are discussed. Finally, in [4.2.4](#sec:quant-method-other){reference-type="ref+label" reference="sec:quant-method-other"}, alternate approached to quantization are discussed.

### Lower Numerical Precision {#sec:quant-method-low-num-precision}

Half precision floating point (16-bit floating-point, FP16) has been widely used in nVidia GPUs and ASIC accelerators with minimal accuracy loss [@Das2018]. Mixed precision training with weights, activations, and gradients using FP16 while the accumulated error for updating weights remains in FP32 has shown SOTA performance - sometimes even improved performance [@Micikevicius2017].

Researchers [@Ma2016; @He2015; @Vanhoucke2011] have shown that FP32 parameters produced during training can be reduced to 8-bit integers for inference without significant loss of accuracy. Jacob [@Jacob2017] applied 8-bit integers for both training and inference, with an accuracy loss of 1.5% on ResNet-50. Xilinx [@Settle2018] showed that 8-bit numerical precision could also achieve lossless performance with only one batch inference to adjust quantization parameters and without retraining.

Quantization can be considered an exhaustive search optimizing the scale found to reduce an error term. Given a floating-point network, the quantizer will take an initial scale, typically calculated by minimizing the $l_2$-error, and use it to quantize the first layer weights. Then the quantizer will adjust the scale to find the lowest output error. It performans this operation on every layer.

Integer Arithmetic-only Inference (IAI) [@Jacob2017] proposed a practical quantization scheme able to be adopted by industry using standard datatypes. IAI trades off accuracy and inference latency by compressing compact networks into integers. Previous techniques only compressed the weights of redundant networks resulting in better storage efficiency. IAI quantizes $z \not= 0$ in [\[eq:quantization\]](#eq:quantization){reference-type="ref+label" reference="eq:quantization"} requiring additional zero-point handling but resulting in higher efficiency by making use of unsigned 8-bit integers. The data-flow is described in [13](#fig:quantize-interger-arithmetic-only-inference){reference-type="ref+label" reference="fig:quantize-interger-arithmetic-only-inference"}. TensorFlow-Lite [@Jacob2017; @Krishnamoorthi2018] deployed IAI with an accuracy loss of 2.1% using ResNet-150 on the ImageNet dataset. This is described in more detail in [4.3.2](#sec:quant-deploy-framework){reference-type="ref+label" reference="sec:quant-deploy-framework"}.

<figure id="fig:quantize-interger-arithmetic-only-inference" data-latex-placement="!tbh">
<embed src="8.pdf" style="width:45.0%" />
<figcaption>Integer Arithmetic-only Inference: The convolution operation takes unsigned int8 weights and inputs, accumulates them to unsigned int32, and then performs a 32-bit addition with biases. The ReLU6 operation outputs 8-bit integers. Adopted from <span class="citation" data-cites="Jacob2017"></span></figcaption>
</figure>

Datatypes other than INT8 have been used to quantize parameters. Fixed point, where the radix point is not at the right-most binary digit, is one format that has been found to be useful. It provides little loss or even higher accuracy but with a lower computation budget. Dynamic scaled fixed-point representation [@Vanhoucke2011] obtained a $4\times$ acceleration on CPUs. However, it requires specialized hardware including 16-bit fixed-point [@Gupta2015], 16-bit flex point [@Koster2017], and 12-bit operations using dynamic fixed-point format (DFXP) [@Courbariaux2014]. The specialized hardware is mentioned in [4.3.3](#sec:quant-deploy-hw){reference-type="ref+label" reference="sec:quant-deploy-hw"}.

### Logarithmic Quantization {#sec:quant-method-log}

Bit-shift operations are inexpensive to implement in hardware compared to multiplication operations. FPGA implementations [@Alemdar2017] specifically benefit by converting floating-point multiplication into bit shifts. Network inference can be further optimized if weights are also constrained to be power-of-two with variable-length encoding. Logarithmic quantization takes advantage of this by being able to express a larger dynamic range compared to linear quantization.

Inspired by binarized networks [@Courbariaux2015], introduced in [4.2.3](#sec:quant-method-plus){reference-type="ref+label" reference="sec:quant-method-plus"}, Lin [@Lin2015] forced the neuron output into a power-of-two value. This converts multiplications into bit-shift operations by quantizing the representations at each layer of the binarized network. Both training and inference time are thus reduced by eliminating multiplications.

Incremental Network Quantization (INQ) [@Zhou2017a] replaces weights with power-of-two values. This reduces computation time by converting multiplies into shifts. INQ weight quantization is performed iteratively. In one iteration, weight pruning-inspired weight partitioning is performed using group-wise quantization. These weights are then fine-tuned by using a pruning-like measurement [@Han2015; @Guo2016]. Group-wise retraining fine-tunes a subset of weights in full precision to preserve ensemble accuracy. The other weights are converted into power-of-two format. After multiple iterations most of the full precision weights are converted to power-of-two. The final networks have weights from 2 (ternary) to 5 bits with values near zero set to zero. Results of group-wise iterative quantization show lower error rates than a random power-of-two strategy. Specifically, INQ obtained $71\times$ compression with 0.52% top-1 accuracy loss on the ILSVRC-2012 with AlexNet.

Logarithmic Neural Networks (LogNN) [@Miyashita2016] quantize weights and features into a log-based representation. Logarithmic backpropagation during training is performed using shift operations. Bases other than $log_{2}$ can be used. $log_{\sqrt{2}}$ based arithmetic is described as a trade-off between dynamic range and representation precision. $log_{2}$ showed $7\times$ compression with 6.2% top-5 accuracy loss on AlexNet, while $log_{\sqrt{2}}$ showed 1.7% top-5 accuracy loss.

Shift convolutional neural networks (ShiftCNN) [@Gudovskiy2017] improve efficiency by quantizing and decomposing the real-valued weights matrix into an $N$ times $B$ ranged bit-shift, and encoding them with code-books $\mathbf{C}$ as shown in [\[eq:shiftcnn\]](#eq:shiftcnn){reference-type="ref+label" reference="eq:shiftcnn"}. $idx_i(n)$ is the index for the $i^{th}$ weights in the $n^{th}$ code-book. Each coded weight ${w}_{i}$ can be indexed by the NB-bit expression. $$\begin{equation}
    \begin{split}
        &{w}_{i}=\sum_{n=1}^{N} \mathbf{C}_{n}\left[\operatorname{idx}_{i}(n)\right] \\
        &\mathbf{C}_{n}=\left\{0, \pm 2^{-n+1}, \pm 2^{-n}, \pm 2^{-n-1}, \ldots, \pm 2^{-n-\lfloor M / 2\rfloor+2}\right\} \\
        &\text{where}\ M = 2^B - 1 
    \end{split}
\label{eq:shiftcnn}
\end{equation}$$ Note that the number of code-books $C_n$ can be greater than one. This means the encoded weight might be a combination of multiple shift operations. This property allows ShiftCNN to expand to a relatively large-scale quantization or to shrink to binarized or ternary weights. We discuss ternary weights in [4.2.3](#sec:quant-method-plus){reference-type="ref+label" reference="sec:quant-method-plus"}. ShiftCNN was deployed on an FPGA platform and achieved comparable accuracy on the ImageNet dataset with 75% power saving and up to $1090\times$ clock cycle speed-up. ShiftCNN achieves this impressive result without requiring retraining. With $N=2$ and $B=4$ encoding, SqueezeNet [@Iandola2016] has only 1.01% top-1 accuracy loss. The loss for GoogLeNet, ResNet-18, and ResNet-50 is 0.39%, 0.54%, and 0.67%, respectively, While compressing the weights into 7/32 of the original size. This implies that the weights have significant redundancy.

Based on LogNN, Cai [@Cai2018] proposed improvements by disabling activation quantization to reduce overhead during inference. This also reduced the clamp bound hyperparameter tuning during training. These changes resulted in many low-valued weights that are rounded to the nearest value during encoding. As $2^n\  \text{s.t.}\ n\in N$ increases quantized weights sparsity as $n$ increases. In this research, $n$ is allowed to be real-valued numbers as $n \in R$ to quantize the weights. This makes weight quantization more complex. However, a code-book helps to reduce the complexity.

In 2019, Huawei proposed DeepShift, a method of saving computing power by shift convolution [@Elhoushi2019]. DeepShift removed all floating-point multiply operations and replaced them with bit reverse and bit shift. The quantized weight $W_q$ transformation is shown mathematically in [\[eq:deepshift\]](#eq:deepshift){reference-type="ref+label" reference="eq:deepshift"}, where $S$ is a sign matrix, $P$ is a shift matrix, and $Z$ is the set of integers. $$\begin{equation}
    W_q = S \times 2^P,\ \text{s.t.} \ P \in \mathbb{Z}, S \in \{-1, 0, +1\}
    \label{eq:deepshift}
\end{equation}$$ Results indicate that DeepShift networks cannot be easily trained from scratch. They also show that shift-format networks do not directly learn for lager datasets such as Imagenet. Similar to INQ, they show that fine-tuning a pre-trained network can improve performance. For example, with the same configuration of 32-bit activations and 6-bit shift-format weights, the top-1 ILSVRC-2012 accuracy loss on ResNet-18 for trained from scratch and tuned from a pre-trained model are 4.48% and 1.09%, respectively.

DeepShift proposes models with differential backpropagation for generating shift coefficients during the retraining process. DeepShift-Q [@Elhoushi2019] is trained with floating-point parameters in backpropagation with values rounded to a suitable format during inference. DeepShift-PS directly adopts the shift $P$ and sign $S$ parameters as trainable parameters.

Since logarithmic encoding has larger dynamic range, redundant networks particularly benefit. However, less redundant networks show significant accuracy loss. For example, VGG-16 which is a redundant network shows 1.31% accuracy loss on top-1 while DenseNet-121 shows 4.02% loss.

### Plus-minus Quantization {#sec:quant-method-plus}

Plus-minus quantization was in 1990 [@Saad1990]. This technique reduces all weights to 1-bit representations. Similar to logarithmic quantization, expensive multiplications are removed. In this section, we provide an overview of significant binarized network results. Simons [@Simons2019] and Qin [@Qin2020] provide an in-depth review of BNNs.

Binarized neural networks (BNN) have only 1-bit weights and often 1-bit activations. 0 and 1 are encoded to represent -1 and +1, respectively. Convolutions can be separated into multiplies and additions. In binary arithmetic, single bit operations can be performed using *and*, *xnor*, and *bit-count*. We follow the introduction from [@Zhou2017c] to explain bit-wise operation. Single bit fixed point dot products are calculated as in [\[eq:bitcount-bit_prod\]](#eq:bitcount-bit_prod){reference-type="ref+label" reference="eq:bitcount-bit_prod"}, where *and* is a bit-wise AND operation and *bitcount* counts the number of 1's in the bit string. $$\begin{equation}
    \label{eq:bitcount-bit_prod}
    \boldsymbol{x} \cdot \boldsymbol{y} = \operatorname{bitcount}(\operatorname{and}(\boldsymbol{x}, \boldsymbol{y})), \text{ s.t. }\forall i, x_{i}, y_{i} \in\{0,1\}
\end{equation}$$ This can be extended into multi-bit computations as in [\[eq:bitcount-multi_bit_prod\]](#eq:bitcount-multi_bit_prod){reference-type="ref+label" reference="eq:bitcount-multi_bit_prod"} [@Courbariaux2016]. $\boldsymbol{x} \text{ and } \boldsymbol{y}$ are M-bit and K-bit fixed point integers, subject to $\boldsymbol{x}=\sum_{m=0}^{M-1}c_m(\boldsymbol{x})2^m$ and $\boldsymbol{y}=\sum_{k=0}^{K-1}c_k(\boldsymbol{y})2^k$ , where $(c_m({\boldsymbol{x}}))_{m=0}^{M-1}$ and $(c_k({\boldsymbol{y}}))_{k=0}^{K-1}$ are bit vectors. $$\begin{equation}
    \label{eq:bitcount-multi_bit_prod}
    \begin{split}
        \mathrm{x} \cdot \mathrm{y}=\sum_{m=0}^{M-1} \sum_{k=0}^{K-1} 2^{m+k} \operatorname{bitcount}\left[\operatorname{and}\left(c_{m}(\mathrm{x}), c_{k}(\mathrm{y})\right)\right],\\
        \text{ s.t. } c_{m}(\mathrm{x})_{i}, c_{k}(\mathrm{y})_{i} \in\{0,1\} \forall i, m, k.
    \end{split}
\end{equation}$$

By removing complicated floating-point multiplications, networks are dramatically simplified with simple accumulation hardware. Binarization not only reduces the network size by up-to 32$\times$, but also drastically reduces memory usage resulting in significantly lower energy consumption [@Mishra2018; @Hubara2016b]. However, reducing 32-bit parameters into a single bit results in a significant loss of information, which decreases prediction accuracy. Most quantized binary networks significantly under-perform compared to 32-bit competitors.

There are two primary methods to reduce floating-point values into a single bit: 1) stochastic and 2) deterministic [@Courbariaux2015]. Stochastic methods consider global statistics or the value of input data to determine the probability of some parameter to be -1 or +1. Deterministic binarization directly computes the bit value based on a threshold, usually 0, resulting in a sign function. Deterministic binarization is much simpler to implement in hardware.

Binary Connect (BC), proposed by Courbariaux [@Courbariaux2015], is an early stochastic approach to binarize neural networks. They binarized the weights both in forward and backward propagation. [\[eq:bnn-stochastic-binarization\]](#eq:bnn-stochastic-binarization){reference-type="ref+label" reference="eq:bnn-stochastic-binarization"} shows the stochastic binarization $x^b$ with a *hard sigmoid* probability $\sigma(x)$. Both the activations and the gradients use 32-bit single precision floating point. The trained BC network shows 1.18% classification error on the small MNIST dataset but 8.27% classification on the larger CIFAR-10 dataset. $$\begin{equation}
\label{eq:bnn-stochastic-binarization}
    \begin{split}
        x^{b}=
        \begin{cases}
            +1, & \text{with probability } p=\sigma(x) \\
            -1, & \text{with probability } 1-p 
        \end{cases} \\
        \text{where } \sigma(x)=\operatorname{clamp}\left(\frac{x+1}{2}, 0,1\right)
    \end{split}
\end{equation}$$

Courbariaux extended BC networks by binarizing the activations. He named them BinaryNets [@Courbariaux2016], which is recognized as the first BNN. They also report a customized binary matrix multiplication GPU kernel that accelerates the calculation by $7\times$. BNN is considered the first binarized neural network where both weights and activations are quantized to binary values [@Simons2019]. Considering the hardware cost of stochastic binarization, they made a trade-off to apply deterministic binarization in most circumstances. BNN reported 0.86% error on MNIST, 2.53% error on SVHN, and 10.15% error on CIFAR-10. The ILSVRC-2012 dataset accuracy results for binarized AlexNet and GoogleNet are 36.1% top-1 and 47.1%, respectively while the FP32 original networks achieve 57% and 68%, respectively [@Hubara2016b].

Rastegari [@Rastegari2016] explored binary weight networks (BWN) on the ILSVRC dataset with AlexNet and achieved the same classification accuracy as the single precision version. The key is a scaling factor $\alpha \in \mathbb{R}^{+}$ applied to an entire layer of binarized weights $\mathbf{B}$. This results in similar weights values as if they were computed using FP32 $\mathbf{W} \approx \alpha \mathbf{B}$. They also applied weight binarization on ResNet-18 and GoogLeNet, resulting in 9.5% and 5.8% top-1 accuracy loss compared to the FP32 version, respectively. They also extended binarization to activations called XNOR-Net and evaluated it on the large ILSVRC-2012 dataset. Compared to BNN, XNOR-Net also applied a scaling factor on the input feature and a rearrangement of the network structure (swapping the convolution, activation, and BN). Finally, XNOR-Net achieved 44.2% top-1 classification accuracy on ILSVRC-2012 with AlexNet, while accelerating execution time $58\times$ on CPUs. The attached scaling factor extended the binarized value expression, which reduced the network distortion and lead to better ImageNet accuracy.

DoReFa-Net [@Zhou2016a] also adopts plus-minus arithmetic for quantized network. DoReFa additionally quantizes gradients to low-bit widths within 8-bit expressions during the backward pass. The gradients are quantized stochastically in back propagation. For example, it takes 1 bit to represent weights layer-wise, 2-bit activations, and 6-bits for gradients. We describe training details in [4.2.5](#sec:quant-method-training){reference-type="ref+label" reference="sec:quant-method-training"}. They found 9.8% top-1 accuracy loss on AlexNet with ILSVRC-2012 using the 1-2-6 combination. The result for the 1-4-32 combination is 2.9%.

Li [@Li2016] and Leng [@Leng2017] showed that for ternary weights ($-1, 0,\text{ and }+1$), in Ternary Weight Networks (TWN), only a slight accuracy loss was realized. Compared to BNN, TWN has an additional value to reduce information loss while still keeping computational complexity similar to BNN's. Ternary logic may be implemented very efficiently in hardware, as the additional value (zero) do not actually participate in computations [@Cotofana1997]. TWN adopts the $l_2$-distance to find the scale and formats the weights into $-1, 0,\text{ and }+1$ with a threshold generated by an assumption that the weighs are uniformly distributed such as in $[-a, a]$. This resulted in up to $16 \times$ model compression with 3.6% ResNet-18 top-1 accuracy loss on ILSVRC-2012.

Trained Ternary Quantization (TTQ) [@Zhu2016] extended TWN by introducing two dynamic constraints to adjust the quantization threshold. TTQ outperformed the full precision AlexNet on the ILSVRC-2012 top-1 classification accuracy by 0.3%. It also outperformed TWN by 3%.

Ternary Neural Networks (TNN) [@Alemdar2017] extend TWN by quantizing the activations into ternary values. A teacher network is trained with full precision and then using transfer learning the same structure is used but replacing the full precision values with a ternarized student in a layer-wise greedy method. A small difference between the real-valued teacher network and the ternarized student network is that they activate the output with a ternary output activation function to simulate the real TNN output. TNN achieves 1.67% MNIST classification error and 12.11% classification error on CIFAR10. TNN has slightly lower accuracy compared to TWN (an additional 1.02% MNIST error).

Intel proposed Fine-Grained Quantization (FGQ) [@Mellempudi2017] to generalize ternary weights by splitting them into several groups and with independent ternary values. The FGQ quantized ResNet-101 network achieved 73.85% top-1 accuracy on the ImageNet dataset (compared with 77.5% for the baseline) using four groups weights and without re-training. FGQ also showed improvements in (re)training demonstrating a top-1 accuracy improvement from 48% on non-trained to 71.1% top-1 on ResNet-50. ResNet-50's baseline accuracy is 75%. Four groups FGQ with ternary weights and low bit-width activations achieves about $9\times$ acceleration.

MeliusNet [@Bethge2020] is a binary neural network that consist of two types of binary blocks. To mitigate drawbacks of low bit width networks, reduced information quality, and reduced network capacity, MeliusNet used a combination of *dense block* [@Bethge2019] which increases network channels by concatenating derived channels from the input to improve capacity and *improvement block* [@Liu2018c] which improves the quality of features by adding additional convolutional activations onto existing extra channels from *dense block*. They achieved accuracy results comparable to MobileNet on the ImageNet dataset with MeliusNet-59 reporting 70.7% top-1 accuracy while requiring only 0.532 BFLOPs. A similar sized 17MB MobileNet required 0.569 BFLOPs achieving 70.6% accuracy.

AdderNet [@Chen2019b] is another technique that replaces multiply arithmetic but allows larger than 1-bit parameters. It replaces all convolutions with addition. [\[eq:addernet-conv\]](#eq:addernet-conv){reference-type="ref+label" reference="eq:addernet-conv"} shows that for a standard convolution, AdderNet formulates it as a similarity measure problem $$\begin{equation}
\label{eq:addernet-conv}
    Y(m, n, t)=\sum_{i=0}^{d} \sum_{j=0}^{d} \sum_{k=0}^{c_{i n}} S(\mathbf{X}(m+i, n+j, k), \mathbf{F}(i, j, k, t))
\end{equation}$$ where $\mathbf{F} \in \mathbb{R}^{d \times d \times c_{i n} \times c_{\text {out}}}$ is a filter, $d$ is the kernel size, $c_{in}$ is an input channel and $c_{\text {out}}$ is an output channel. $\mathbf{X} \in \mathbb{R}^{h \times w \times c_{i n}}$ stands for the input feature height $h$ and width $w$. With this formulation, the output $Y$ is calculated with the similarity $S(\cdot,\cdot)$, i.e., $S(x, y) =x \times y$ for conventional convolution where the similarity measure is calculated by cross correlation. [\[eq:addernet-add\]](#eq:addernet-add){reference-type="ref+label" reference="eq:addernet-add"} mathematically describes AdderNet, which replaces the multiply with subtraction. The $l_1$-distance is applied to calculate the distance between the filter and the input feature. By replacing multiplications with subtractions, AdderNet speeds up inference by transforming 3.9 billion multiplications into subtractions with a loss in ResNet-50 accuracy of 1.3%. $$\begin{equation}
\label{eq:addernet-add}
Y(m, n, t)=-\sum_{i=0}^{d} \sum_{j=0}^{d} \sum_{k=0}^{c_{i n}}|\mathbf{X}(m+i, n+j, k)-\mathbf{F}(i, j, k, t)|
\end{equation}$$

NAS can be applied to BNN construction. Shen [@Shen2019] adopted evolutionary algorithms to find compact but accurate models achieving 69.65% top-1 accuracy on ResNet-18 with ImageNet at $2.8\times$ speed-up. This is better performance than the 32-bit single precision baseline ResNet-18 accuracy of 69.6%. However, the search approach is time consuming taking 1440 hours on an nVidia V100 GPU to search 50k ImageNet images to process an initial network.

### Other Approaches to Quantization {#sec:quant-method-other}

Weight sharing by vector quantization can also be considered a type of quantization. In order to compress parameters to reduce memory space usage, parameters can be clustered and shared. K-means is a widely used clustering algorithm and has been successfully applied to DNNs with minimal loss of accuracy [@Gong2014; @Wu2016; @Lei2017] achieving 16-24 times compression with 1% accuracy loss on the ILSVRC-2012 dataset [@Gong2014; @Wu2016].

HashNet [@Chen2015] uses a hash to cluster weights. Each hash group is replaced with a single floating-point weight value. This was applied to FCLs and shallow CNN models. They found a compression factor of $64\times$ outperforms equivalent-sized networks on MNIST and seven other datasets they evaluated.

In 2016 Han applied Huffman coding with Deep Compression [@Han2015]. The combination of weight sharing, pruning, and huffman coding achieved $49\times$ compression on VGG-16 with no loss of accuracy on ILSVRC-2012, which was SOTA at the time.

The Hessian method was applied to measure the importance of network parameters and therefore improve weight quantization [@Choi2017]. They minimized the average Hessian weighted quantization errors to cluster parameters. They found compression ratios of 40.65 on AlexNet with 0.94% accuracy loss on ILSVRC-2012. Weight regularization can slightly improve the accuracy of quantized networks by penalizing weights with large magnitudes [@Sheng2018]. Experiments showed that $l_2$ regularization improved 8-bit quantized MobileNet top-1 accuracy by 0.23% on ILSVRC-2012.

BN has proved to have many advantages including addressing the internal covariate shift issue [@Ioffe2015]. It can also be considered a type of quantization. However, quantization performed with BN may have numerical instabilities. The BN layer has nonlinear square and square root operations. Low bit representations may be problematic when using non-linear operations. To solve this, $l_1$-norm BN [@Wu2019] has only linear operations in both forward and backward training. It provided $1.5\times$ speedup at half the power on FPGA platforms and can be used with both training and inference.

### Quantization-aware Training {#sec:quant-method-training}

Most quantization methods use a global (layer-wise) quantization to reduce the full precision model into a reduced bit model. Thus can result in non-negligible accuracy loss. A significant drawback of quantization is information loss caused by the irreversible precision reducing transform. Accuracy loss is particularly visible in binary networks and shallow networks. Applying binary weights and activations to ResNet-34 or GoogLeNet resulted in 29.10% and 24.20% accuracy loss, respectively [@Courbariaux2016]. It has been shown that backward propagation fine-tunes (retrains) a quantized network and can recover losses in accuracy caused by the quantization process [@Merolla2016]. The retraining is even resilient to binarization information distortions. Thus training algorithms play a crucial role when using quantization. In this section, we introduce (re)training of quantized networks.

#### BNN Training:

For a binarized network that has binary valued weights it is not effective to update the weights using gradient decent methods due to typically small derivatives. Early quantized networks were trained with a variation of Bayesian inference named Expectation Back Propagation (EBP) [@Soudry2014; @Cheng2015]. This method assigns limited parameter precision (e.g., binarized) weights and activations. EBP infers networks with quantized weights by updating the posterior distributions over the weights. The posterior distributions are updated by differentiating the parameters of the backpropagation.

BinaryConnect [@Courbariaux2015] adopted the probabilistic idea of EBP but instead of optimizing the weights posterior distribution, BC preserved floating-point weights for updates and then quantized them into binary values. The real-valued weights update using the back propagated error by simply ignoring the binarization in the update.

A binarized Network has only 1-bit parameters - $\pm 1$ quantized from a sign function. Single bit parameters are non-differentiable and therefore it is not possible to calculate gradients needed for parameter updating [@Saad1990]. SGD algorithms have been shown to need 6 to 8 bits to be effective [@Muller2015]. To work around these limitations the Straight-Through Estimator (STE), previously introduced by Hinton [@Hinton2012], was applied for propagating gradients by using discretization [@Hubara2016b]. [\[eq:STE\]](#eq:STE){reference-type="ref+label" reference="eq:STE"} show the STE for sign binarization, where $c$ denotes the cost function, $w_r$ is the real-valued weights, and $w_b$ is the binarized weight produced by the *sign* function. STE bypasses the binarization function to directly calculate real-valued gradients. The floating-point weights are then updated using methods like SGD. To avoid real-valued weights approaching infinity, BNNs typically clamp floating-point weights to the desired range of $\pm 1$ [@Hubara2016b]. $$\begin{equation}
\label{eq:STE}
    \begin{split}
        \text { Forward : } &w_{b}=\operatorname{sign}\left(w_{r}\right) \\
        \text { Backward : } &\frac{\partial c}{\partial w_{r}}=\frac{\partial c}{\partial w_{b}} \mathbf{1}_{\left|w_{r}\right| \leq 1}
    \end{split}
\end{equation}$$

Unlike the forward phase where weights and activations are produced with deterministic quantization, in the gradient phase, the low bit gradients should be generated by stochastic quantization [@Gupta2015; @Zhou2018]. DoReFa [@Zhou2016a] first successfully trained a network with gradient bit-widths less than eight and achieved a comparable result with $k$-bit quantization arithmetic. This low bit-width gradient scheme could accelerate training in edge devices with little impact to network accuracy but minimal inference acceleration compared to BNNs. DoReFa quantizes the weights, features, and gradients into many levels obtaining a larger dynamic range than BNNs. They trained AlexNet on ImageNet from scratch with 1-bit weights, 2-bit activations, and 6-bit gradients. They obtained 46.1% top-1 accuracy (9.8% loss comparing with the full precision counterpart). [\[eq:quant-dorefa\]](#eq:quant-dorefa){reference-type="ref+label" reference="eq:quant-dorefa"} shows the weight quantizing approach. $w$ is the weights (the same as in [\[eq:STE\]](#eq:STE){reference-type="ref+label" reference="eq:STE"}), $\operatorname{limit}$ is a limit function applied to the weights keeping them in the range of \[0, 1\], and $\operatorname{quantize}_k$ quantizes the weights into $k$-levels. Feature quantization is performed using the $f_{\alpha}^{k}=\operatorname{quantize}_k$ function. $$\begin{equation}
\label{eq:quant-dorefa}
\begin{split}
    &f_{w}^{k} = 2 \operatorname{quantize}_k \left( \operatorname{limit}(w_r) \right) - 1 \\
    \text{where } &\operatorname{quantize}_k(w_r) = \frac{1}{2^k - 1} \operatorname{round}\left( \left( 2^k - 1\right) w_r \right), \\
    \text{and } &\operatorname{limit}(x)=\frac{\operatorname{tanh}(x)}{2\operatorname{max}(|\operatorname{tanh}(x)|)} + \frac{1}{2}
\end{split}
\end{equation}$$ In DoReFa, gradient quantization is shown in [\[eq:quant-dorefa-gradients\]](#eq:quant-dorefa-gradients){reference-type="ref+label" reference="eq:quant-dorefa-gradients"}, where $\mathrm{d}r={\partial c}/{\partial r}$ is the backprogagated gradient of the cost function $c$ to output $r$. $$\begin{equation}
 \label{eq:quant-dorefa-gradients}
\tilde{f}_{\gamma}^{k} = 2 \operatorname{max}_{0}(|\mathrm{d} r|)\left[\operatorname{quantize}_{k}\left(\frac{\mathrm{d} r}{2 \max_{0}(|\mathrm{d} r|)}+\frac{1}{2}\right)-\frac{1}{2}\right]
\end{equation}$$

As in deep feed forward networks, the exploding gradient problem can cause BNN's not to train. To address this issue, Hou [@Hou2017] formulated the binarization effect on the network loss as an optimization problem which was solved by a proximal Newton's algorithm with diagonal Hessian approximation that directly minimizes the loss with respect to the binary weights. This optimization found 0.09% improvement on MNIST dataset compared with BNN.

Alpha-Blending (AB) [@Liu2019c] was proposed as a replacement for STE. Since STE directly sets the quantization function gradients to 1, a hypothesis was made that STE tuned networks could suffer accuracy losses. [14](#fig:bnn-training){reference-type="ref+label" reference="fig:bnn-training"} shows that AB introduces an additional scale coefficient $\alpha$. Real-valued weights and quantized weights are both kept. During training $\alpha$ is gradually raised to 1 until a fully quantized network is realized.

<figure id="fig:bnn-training" data-latex-placement="h">
<figure>

</figure>
<figure>

</figure>
<figcaption>STE and AB: STE directly bypasses the quantizer while AB calculates gradients for real-valued weights by introducing additional coefficients <span class="math inline"><em>α</em></span> <span class="citation" data-cites="Liu2019c"></span></figcaption>
</figure>

#### Low Numerical Precision Training:

Training with low numerical precision involves taking the low precision values into both forward and backward propagation while maintaining the full precision accumulated results. Mixed Precision [@Micikevicius2017; @Das2018] training uses FP16 or 16-bit integer (INT16) for weight precision. This has been shown to be inaccurate for gradient values. As shown in [15](#fig:quantize-mixed-precision-training){reference-type="ref+label" reference="fig:quantize-mixed-precision-training"}, full precision weights are maintained for gradient updating, while other operands use half-float. A *loss scaling* technique is applied to keep very small magnitude gradients from affecting the computation since any value less than $2^{-24}$ becomes zero in half-precision [@Micikevicius2017]. Specifically, a scaler is introduced to the loss value before backpropagation. Typically, the scaler is a bit-shift optimal value $2^n$ obtained empirically or by statistical information.

<figure id="fig:quantize-mixed-precision-training" data-latex-placement="!tbh">
<embed src="8.pdf" style="width:45.0%" />
<figcaption>Mixed Precision Training <span class="citation" data-cites="Micikevicius2017"></span>: FP16 is applied in the forward and backward pass, while FP32 weights are maintained for the update.</figcaption>
</figure>

In TensorFlow-Lite [@Jacob2017], training proceeds with real values while quantization effects are simulated in the forward pass. Real-valued parameters are quantized to lower precision before convolutional layers. BN layers are folded into convolution layers. More details are described in [4.3.2](#sec:quant-deploy-framework){reference-type="ref+label" reference="sec:quant-deploy-framework"}.

As in binarized networks, STE can also be applied to reduced precision training such as 8-bit integers [@Krishnamoorthi2018].

## Quantization Deployment {#sec:quant-deploy}

In this section, we describe implementations of quantization deployed in popular frameworks and hardware. In [4.3.1](#sec:quant-deploy-intro){reference-type="ref+label" reference="sec:quant-deploy-intro"} we give an introduction to deployment issues. In [4.3.2](#sec:quant-deploy-framework){reference-type="ref+label" reference="sec:quant-deploy-framework"}, we discuss deep learning libraries and frameworks. We introduce their specification in [\[tab:quantize-libraries\]](#tab:quantize-libraries){reference-type="ref+label" reference="tab:quantize-libraries"} and then compare their performance in [\[tab:quant-framework-acc\]](#tab:quant-framework-acc){reference-type="ref+label" reference="tab:quant-framework-acc"}. We also discuss hardware implementations of DNNs in [4.3.3](#sec:quant-deploy-hw){reference-type="ref+label" reference="sec:quant-deploy-hw"}. Dedicated hardware is designed or programmed to support efficient processing of quantized networks. Specialized CPU and GPU operations are discussed. Finally, in [4.3.4](#sec:quant-deploy-compiler){reference-type="ref+label" reference="sec:quant-deploy-compiler"} we discuss DNN compilers.

### Deployment Introduction {#sec:quant-deploy-intro}

With significant resource capability, large organizations and institutions usually have their own proprietary solutions for applications and heterogeneous platforms. Their support to the quantization is either inference only or as well as training. The frameworks don't always follow the same idea of quantization. Therefore there are differences between them, so performs.

With DNNs being applied in many application areas, the issue of efficient use of hardware has received considerable attention. Multicore processors and accelerators have been developed to accelerate DNN processing. Many types of accelerators have been deployed, including CPUs with instruction enhancements, GPUs, FPGAs, and specialized AI accelerators. Often accelerators are incorporated as part of a heterogeneous system. A Heterogeneous System Architecture (HSA) allows the different processors to integrate into a system to simultaneously access shared memory. For example, CPUs and GPUs using cache coherent shared virtual memory on the same System of Chip (SoC) or connected by PCIe with platform atomics can share the same address space [@Glossner2016]. Floating-point arithmetic units consume more energy and take longer to compute compared to integer arithmetic units. Consequently, low-bitwidth architectures are designed to accelerate computation [@Moudgill2020]. Specialized algorithms and efficient hardware can accelerate neural network processing during both training and inference [@Reuther2019].

### Efficient Kernels {#sec:quant-deploy-framework}

Typically low precision inference in only executed on convolutional layers. Intermediate values passed between layers use 32-bit floating-point. This makes many of the frameworks amenable to modifications.

[\[tab:quantize-libraries\]](#tab:quantize-libraries){reference-type="ref+label" reference="tab:quantize-libraries"} gives a list of major low precision acceleration frameworks and libraries. Most of them use INT8 precision. We will next describe some popular and open-source libraries in more detail.

::: table*
[]{#tab:quantize-libraries label="tab:quantize-libraries"}
:::

#### Tensor RT

[@Vanholder2016; @Wu2020] is an nVidia developed C++ library that facilitates high-performance inference on NVIDIA GPUs. It is a low precision inference library that eliminates the bias term in convolutional layers. It requires a calibration set to adjust the quantization thresholds for each layer or channel. Afterwards the quantized parameters are represented by 32-bit floating-point scalar and INT8 weights.

Tensor RT takes a pre-trained floating-point model and generates a reusable optimized 8-bit integer or 16-bit half float model. The optimizer performs network profiling, layer fusion, memory management, and operation concurrency. [\[eq:tensorRT\]](#eq:tensorRT){reference-type="ref+label" reference="eq:tensorRT"} shows the convolution-dequantization dataflow in Tensor RT for 8-bit integers. The intermediate result of convolution by INT8 input feature $\mathbf{F}_{i8}$ and weights $\mathbf{W}_{i8}$ are accumulated into INT32 tensor $\mathbf{O}_{i32}$. They are dequantized by dividing by the feature and weight scales $s_f, s_w$.

$$\begin{equation}
\label{eq:tensorRT}
        \mathbf{O}_{i32} = \mathbf{F}_{i8} \ast \mathbf{W}_{i8}
        ,\quad
        \mathbf{O}_{f32} = \frac{\mathbf{O}_{i32}}{ s_f \times s_w}
\end{equation}$$

Tensor RT applies a variant of *max-abs* quantization to reduce storage requirements and calculation time of the zero point term $z$ in [\[eq:quantization-max-abs\]](#eq:quantization-max-abs){reference-type="ref+label" reference="eq:quantization-max-abs"} by finding the proper threshold instead of the absolute value in the floating-point tensor. KL-divergence is introduced to make a trade-off between numerical dynamic range and precision of the INT8 representation [@Migacz2017]. KL calibration can significantly help to avoid accuracy loss.

The method traverses a predefined possible range of scales and calculates the KL-divergences for all the points. It then selects the scale which minimizes the KL-divergence. KL-divergence is widely used in many post training acceleration frameworks. nVidia found a model calibrated with 125 images showed only 0.36% top-1 accuracy loss using GoogLeNet on the Imagenet dataset.

#### Intel MKL-DNN

[@Rodriguez2018] is an optimized computing library for Intel processors with Intel AVX-512, AVX-2, and SSE4.2 Instruction Set Architectures (ISA). The library uses FP32 for training and inference. Inference can also be performed using 8-bits in convolutional layers, ReLU activations, and pooling layers. It also uses Winograd convolutions. MKL-DNN uses *max-abs* quantization shown in [\[eq:quantization-max-abs\]](#eq:quantization-max-abs){reference-type="ref+label" reference="eq:quantization-max-abs"}, where the feature adopts unsigned 8-bit integer $n_f = 256$ and signed 8-bit integer weights $n_w = 128$. The rounding function $f(\cdot)$ in [\[eq:quantization\]](#eq:quantization){reference-type="ref+label" reference="eq:quantization"} uses nearest integer rounding. [\[eq:quantization-mkl-dnn-quantize\]](#eq:quantization-mkl-dnn-quantize){reference-type="ref+label" reference="eq:quantization-mkl-dnn-quantize"} shows the quantization applied on a given tensor or each channel in a tensor. The maximum of weights $R_w$ and features $R_f$ is calculated from the maximum of the absolute value (nearest integer rounding) of the tensor $\mathbb{T}_f$ and $\mathbb{T}_w$. The feature scale $s_f$ and weights scale $s_w$ are generated using $R_w$ and $R_f$. Then quantized 8-bit signed integer weights $\mathbf{W}_{s8}$, 8-bit unsigned integer feature $\mathbf{F}_{u8}$ and 32-bit unsigned integer biases $\mathbf{B}_{u32}$ are generated using the scales and a nearest rounding function $\|\cdot\|$.

$$\begin{equation}
\label{eq:quantization-mkl-dnn-quantize}
    \begin{split}
        R_{\{f,w\}} &= max((abs(\mathbb{T}_{\{f,w\}})) \\
        s_f &= \frac{255}{R_f},\ s_w = \frac{127}{R_w} \\
        \mathbf{W}_{s8}&=\|s_{w}\times\mathbf{W}_{f32}\| \in[-127,127] \\ 
        \mathbf{F}_{u8}&=\|s_{f}\times\mathbf{F}_{f32}\| \in[0,255] \\ 
        \mathbf{B}_{s32}&=\|s_{f}\times s_{w}\times \mathbf{B}_{f32}\| \in[-2^{31}, 2^{31}-1]
    \end{split}
\end{equation}$$

An affine transformation using 8-bit multipliers and 32-bit accumulates results in [\[eq:quantization-mkl-dnn-f2i\]](#eq:quantization-mkl-dnn-f2i){reference-type="ref+label" reference="eq:quantization-mkl-dnn-f2i"} with the same scale factors as defined in [\[eq:quantization-mkl-dnn-quantize\]](#eq:quantization-mkl-dnn-quantize){reference-type="ref+label" reference="eq:quantization-mkl-dnn-quantize"} and $\ast$ denoting convolution. It is an approximation since rounding is ignored.

$$\begin{equation}
\label{eq:quantization-mkl-dnn-f2i}
\begin{split}
    \mathbf{O}_{s 32}&=\mathbf{W}_{s 8} \ast \mathbf{F}_{u 8}+\mathbf{b}_{s 32}  \\
    &\approx s_{f} s_{w}\left(\mathbf{W}_{f 32} \ast \mathbf{F}_{f 32}+\mathbf{b}_{f 32}\right) \\
    &=s_{f}\times s_{w}\times \mathbf{O}_{f 32}
\end{split}
\end{equation}$$

[\[eq:quantization-mkl-dnn-appr\]](#eq:quantization-mkl-dnn-appr){reference-type="ref+label" reference="eq:quantization-mkl-dnn-appr"} is the affine transformation with FP32 format. $D$ is the dequantization factor.

$$\begin{equation}
\label{eq:quantization-mkl-dnn-appr}
\begin{split}
    \mathbf{O}_{f32}&=\mathbf{W}_{f32} \ast \mathbf{F}_{f32}+\mathbf{b}_{f32} \\
    &\approx \frac{1}{s_{f} s_{w}} \mathbf{O}_{s32}=D \times \mathbf{O}_{s32} \\
    &\text{where}\ D = \frac{1}{s_{f} s_{w}}
\end{split}
\end{equation}$$

Weight quantization is done prior to inference. Activation quantization factors are prepared by sampling the validation dataset to find a suitable range (similar to Tensor RT). The quantization factors can be either FP32 in the supported devices, or rounded to the nearest power-of-two format to enable bit-shifts. Rounding reduces accuracy by about 1%.

MKL-DNN assumes activations are non-negative (ReLU activated). Local Response Normalization (LRN), a function to pick the local maximum in a local distribution, is used to avoid over-fitting. BN, FCL, and soft-max using 8-bit inference are not currently supported.

#### TensorFlow-Lite (TF-Lite)

[@Abadi2016] is an open source framework by Google for performing inference on mobile or embedded devices. It consists of two sets of tools for converting and interpreting quantized networks. Both PTQ and QAT are available in TF-Lite.

*GEMM low-precision (Gemmlowp)* [@Google2018] is a Google open source gemm library for low precision calculations on mobile and embedded devices. It is used in TF-Lite. Gemmlowp uses asymmetric quantzation as shown in [\[eq:quantization-gemmlowp-quantize\]](#eq:quantization-gemmlowp-quantize){reference-type="ref+label" reference="eq:quantization-gemmlowp-quantize"} where $\mathbf{F}, \mathbf{W}, \mathbf{O}$ denotes feature, weights and output, respectively. $s_f, s_w$ are the scales for feature and weights, respectively. $\mathbf{F}_{f32}$ is Feature value in 32-bit floating. Similarly, $\mathbf{W}_{f32}$ is the Weight value in 32-bit floating point. $\mathbf{F}_q, \mathbf{W}_q$ are the quantized Features and Weights, respectively. Asymmetric quantization introduces the zero points ($\mathbf{z}_f$ and $\mathbf{z}_w$). This produces a more accurate numerical encoding.

$$\begin{equation}
\label{eq:quantization-gemmlowp-quantize}
\begin{split}
\mathbf{O}_{f32} &= \mathbf{F}_{f32} \ast \mathbf{W}_{f32} \\
&= s_f \times (\mathbf{F}_q + \mathbf{z}_f) \ast s_w \times (\mathbf{W}_q + \mathbf{z}_w) \\
&= s_f \times s_w \times \underline{(\mathbf{F}_q + \mathbf{z}_f) \ast (\mathbf{W}_q + \mathbf{z}_k)}
\end{split}
\end{equation}$$

The underlined part in [\[eq:quantization-gemmlowp-quantize\]](#eq:quantization-gemmlowp-quantize){reference-type="ref+label" reference="eq:quantization-gemmlowp-quantize"} is the most computationally intensive. In addition to the convolution, the zero point also requires calculation. Gemmlowp reduces many multi-add operations by multiplying an all-ones matrix as the *bias* matrix $P$ and $Q$ in [\[eq:quantization-gemmlowp-trick\]](#eq:quantization-gemmlowp-trick){reference-type="ref+label" reference="eq:quantization-gemmlowp-trick"}. This allows four multiplies to be dispatched in a three stage pipeline [@Krishnamoorthi2018], to produce the quantized output $\mathbf{O}_q$. $\mathbf{F}, \mathbf{W}, \mathbf{z}$ are the same as in [\[eq:quantization-gemmlowp-quantize\]](#eq:quantization-gemmlowp-quantize){reference-type="ref+label" reference="eq:quantization-gemmlowp-quantize"}.

$$\begin{equation}
\label{eq:quantization-gemmlowp-trick}
\begin{split}
\mathbf{O}_q &= (\mathbf{F}_q + \mathbf{z}_f \times P) \ast (\mathbf{W}_q + \mathbf{z}_w \times Q) \\
&= \mathbf{F}_q \ast \mathbf{W}_q \\
&+ \mathbf{z}_f \times \mathbf{P} \times \mathbf{W}_q \\
&+ \mathbf{z}_w \times \mathbf{Q} \times \mathbf{F}_q \\
&+ \mathbf{z}_f \times \mathbf{z}_w \times \mathbf{P} \times \mathbf{Q}
\end{split}
\end{equation}$$

#### Ristretto

[@Gysel2018] is a tool for Caffe quantization. It uses retraining to adjust the quantized parameters. Ristretto uses a three-part quantization strategy: 1) a modified fixed-point format Dynamic Fixed Point (DFP) which permits the limited bit-width precision to dynamically carry data, 2) bit-width reduced floating-point numbers called mini float which follows the IEEE-754 standard [@Society2008], and 3) integer power of 2 weights that force parameters into power of 2 values to replace multiplies with bit shift operations.

DPF is shown in [\[eq:quantization-ristretto-dynamic-fixed-point\]](#eq:quantization-ristretto-dynamic-fixed-point){reference-type="ref+label" reference="eq:quantization-ristretto-dynamic-fixed-point"} where $s$ takes one sign bit, $\text{FL}$ denotes the fractional length, and $x$ is the mantissa. The total bit-width is $B$. This quantization can encode data from various ranges to a proper format by adjusting the fractional length.

$$\begin{equation}
\label{eq:quantization-ristretto-dynamic-fixed-point}
    (-1)^{s} \cdot 2^\text{-FL} \sum_{i=0}^{B-2} 2^{i} \cdot x_{i}
\end{equation}$$

A bit shift convolution conversion is shown in [\[eq:quantization-ristretto-pow-of-2-weiths\]](#eq:quantization-ristretto-pow-of-2-weiths){reference-type="ref+label" reference="eq:quantization-ristretto-pow-of-2-weiths"}. The convolution by input $\mathbf{F}_j$ and weights $\mathbf{W}_j$ and bias $\mathbf{b}_i$ are transformed into shift arithmetic by rounding the weights to the nearest power of 2 values. Power of 2 weights provides inference acceleration while dynamic fixed point provides better accuracy.

$$\begin{equation}
\label{eq:quantization-ristretto-pow-of-2-weiths}
    \begin{split}
        \mathbf{O}_{i}&=\sum_{j}\left[\mathbf{F}_{j} \cdot \mathbf{W}_{j}\right]+\mathbf{b}_{i} \\
        &\approx \sum_{j}\left[\mathbf{F}_{j} \ll \operatorname{ round }\left(\log _{2}\left(\mathbf{W}_{j}\right)\right)\right]+\mathbf{b}_{i}
    \end{split}
\end{equation}$$

#### NCNN

[@Tencent2019] is a standalone framework from Tencent for efficient inference on mobile devices. Inspired by Ristretto and Tensor-RT, it works with multiple operating systems and supports low precision inference [@BUG19892019]. It performs channel-wise quantization with KL calibration. The quantization results in 0.04% top-1 accuracy loss on ILSVRC-2012. NCNN has implementations optimized for ARM NEON. NCNN also replaces $3\times3$ convolutions with simpler Winograd convolutions [@Lavin2016].

#### Mobile AI Compute Engine (MACE)

[@Xiaomi2019] from Xiaomi supports both post-training quantization and quantization-aware training. Quantization-aware training is recommended as it exhibits lower accuracy loss . Post-training quantization requires statistical information from activations collected while performing inference. This is typically performed with batch calibration of input data. MACE also supports processor implementations optimized for ARM NEON and Qualcomm's Hexagon digital signal processor. OpenCL acceleration is also supported. Winograd convolutions can be applied for further acceleration as discussed in [4.2.2](#sec:quant-method-log){reference-type="ref+label" reference="sec:quant-method-log"}.

#### Quantized Neural Network PACKage (QNNPACK)

[@Dukhan2018] is a Facebook produced open-source library optimized for edge computing especially for mobile low precision neural network inference. It has the same method of quantization as TF-Lite including using a zero-point. The library has been integrated into PyTorch [@Paszke2019] to provide users a high-level interface. In addition to Winograd and FFT convolution operations, the library has optimized gemm for cache indexing and feature packing. QNNPACK has a full compiled solution for many mobile devices and has been deployed on millions of devices with Facebook applications.

Panel Dot product (PDOT) is a key feature of QNNPACK's highly efficient gemm library. It assumes computing efficiency is limited with memory, cache, and bandwidth instead of Multiply and Accumulate (MAC) performance. PDOT computes multiple dot products in parallel as shown in [16](#fig:quantize-qnnpack-optimize){reference-type="ref+label" reference="fig:quantize-qnnpack-optimize"}. Rather than loading just two operands per MAC operation, PDOT loads multiple columns and rows. This improves convolution performance about $1.41\times$ $2.23\times$ speedup for MobileNet on mobile devices [@Dukhan2018].

<figure id="fig:quantize-qnnpack-optimize" data-latex-placement="!tbh">
<embed src="8.pdf" style="width:45.0%" />
<figcaption>PDOT: computing dot product for several points in parallel.</figcaption>
</figure>

#### Paddle

[@Baidu2019] applies both QAT and PTQ quantization with using zero-points. The dequantization operation can be performed prior to convolution as shown in [\[eq:quantize-dequantize-before-gemm\]](#eq:quantize-dequantize-before-gemm){reference-type="ref+label" reference="eq:quantize-dequantize-before-gemm"}. Paddle uses this feature to do floating-point gemm-based convolutions with quantize-dequantized weights and features within the framework data-path. It introduces quantization error while maintaining the data in format of floating-point. This quantize-dequantize-convolution pipeline is called simu-quantize and its results are approximately equal to a FP32-\>INT8-\>Convolutional-\>FP32 (quantize - convolutional - dequantize) three stage model.

Simu-quantize maintains the data at each phase in 32-bit floating-point facilitating backward propagation. In the Paddle framework, during backpropagation, gradients are added to the original 32-bit floating-point weights rather than the quantized or the quantize-dequantized weights. $$\begin{equation}
    \mathbf{O}_{f32} = (\frac{\mathbf{F}_q}{(n - 1)} \times \mathbf{F}_{max}) \ast (\frac{\mathbf{W}_q}{(n - 1)} \times \mathbf{W}_{max})
\label{eq:quantize-dequantize-before-gemm}
\end{equation}$$

Paddle uses *max-abs* in three ways to quantize parameters: 1) the average of the max absolute value in a calculation window, 2) the max absolute value during a calculation window, and 3) a sliding average of the max absolute value of the window. The third method is described in [\[eq:quantize-paddle-sliding-avg-max\]](#eq:quantize-paddle-sliding-avg-max){reference-type="ref+label" reference="eq:quantize-paddle-sliding-avg-max"}, where $V$ is the max absolute value in the current batch, $V_t$ is the average value of the sliding window, and $k$ is a coefficient chosen by default as 0.9.

The Paddle framework uses a specialized toolset, PaddleSlim, which supports Quantization, Pruning, Network Architecture Search, and Knowledge Distilling. They found 86.47% size reduction of ResNet-50, with 1.71% ILSVRC-2012 top-1 accuracy loss. $$\begin{equation}
V_t = (1 - k) \times V + k \times V_{t-1}
\label{eq:quantize-paddle-sliding-avg-max}
\end{equation}$$

::: table*
[]{#tab:quant-framework-acc label="tab:quant-framework-acc"}
:::

### Hardware Platforms {#sec:quant-deploy-hw}

[17](#fig:quant-hw-all){reference-type="ref+label" reference="fig:quant-hw-all"} shows AI chips, cards, and systems plotted by peak operations verses power in log scale originally published in [@Reuther2019]. Three normalizing lines are shown at 100 GOPS/Watt, 1 TOP/Watt, and 10 TOPs/Watt. Hardware platforms are classified along several dimensions including: 1) training or inference, 2) chip, card, or system form factors, 3) datacenter or mobile, and 4) numerical precision. We focus on low precision general and specialized hardware in this section.

![Hardware platforms for neural networks efficiency deploy, adopted from [@Reuther2019].](chips-perf-vs-power.pdf){#fig:quant-hw-all width="95%"}

#### Programmable Hardware:

Quantized networks with less than 8-bits of precision are typically implemented in FPGAs but may also be executed on general purpose processors.

BNN's have been implemented on a Xilinx Zynq heterogeneous FPGA platform [@Zhao2017]. They have also been implemented on Intel Xeon CPUs and Intel Arria 10 FPGA heterogeneous platforms by dispatching bit operation to FPGAs and other operations to CPUs [@Moss2017]. The heterogeneous system shares the same memory address space. Training is typically mapped to CPUs. FINN [@Umuroglu2016a] is a specialized framework for BNN inference on FPGAs. It contains binarized fully connected, convolutional, and pooling layers. When deployed on a Zynq-7000 SoC, FINN has achieved 12.36 million images per second on the MNIST dataset with 4.17% accuracy loss.

Binarized weights with 3-bit features have been implemented on Xilinx Zynq FPGAs and Arm NEON processors [@Preuser2018]. The first and last layer of the network use 8-bit quantities but all other layers use binary weights and 3-bit activation values. On an embedded platform, Zynq XCZU3EG, they performed 16 images per second for inference. To accelerate Tiny-YOLO inference, significant efforts were taken including: 1) replacing max-pool with stride 2 convolution, 2) replacing leaky ReLU with ReLU, and 3) revising the hidden layer output channel. The improved efficiency on the FPGA from 2.5 to 5 frames per second with 1.3% accuracy loss.

TNN [@Alemdar2017] is deployed on an FPGA with specialized computation units optimized for ternary value multiplication. A specific FPGA structure (dimensions) is determined during synthesis to improve hardware efficiency. On the Sakura-X FPGA board they achieved 255k MNIST image classifications per second with an accuracy of 98.14%. A scalable design implemented on a Xilinx Virtex-7 VC709 board dramatically reduced hardware resources and power consumption but at a significantly reduced throughput of 27k CIFAR-10 images per second [@Prost-Boucle2017]. Power consumption for CIFAR-10 was 6.8 Watts.

Reducing hardware costs is a key objective of logarithmic hardware. Xu [@Xu2019] adopted $\sqrt{2}$ based logarithmic quantization with 5-bits of resolution. This showed 50.8% top-1 accuracy and dissipated a quarter of the power while using half the chip area. Half precision inference has a top-1 accuracy of 53.8%.

#### General Hardware:

In addition to specialized hardware, INT8 quantization has been widely adopted in many general purpose processor architectures. In this section we provide a high-level overview. A detailed survey on hardware efficiency for processing DNNs can be found in [@Reuther2019].

CNN acceleration on ARM CPUs was originally implemented by ARM advanced SIMD extensions known as NEON. The ARM 8.2 ISA extension added NEON support for 8-bit integer matrix operations [@Arm2015]. These were implemented in the CPU IP cores Cortex-A75 and A55 [@Arm2020a] as well as the Mali-G76 GPU IP core [@Arm2020]. These cores have been integrated into the Kirin SoC by Huawei, Qualcomm Snapdragon SoC, MediaTek Helio SoC, and Samsung Exynos [@Ignatov2019]. For example on Exynos 9825 Octa, 8-bit integer quantized MobileNet v2 can process an image in 19ms (52 images per second) using the Mali-G76 [@Ignatov2019].

Intel improved the integer performance about 33% with Intel Advanced Vector Extension 512 (AVX-512) ISA [@Rodriguez2018]. This 512-bit SIMD ISA extension included a Fused Multiply-Add (FMA) instruction.

Low precision computation on nVidia GPUs was enabled since the Pascal series of GPUs [@NVIDIACorporation2015]. The Turing GPU architecture [@NVIDIACorporation2018] introduced specialized units to processes INT4 and INT8. This provides real-time integer performance on AI algorithms used in games. For embedded platforms, nVidia developed Jetson platforms [@NVIDIACorporation2018a]. They use CUDA Maxwell cores [@NVIDIACorporation2014] that can process half-precision types. For the data center, nVidia developed the extremely high performance DGX system [@NVIDIACorporation2017]. It contains multiple high-end GPUs interconnected using nVidia's proprietary bus nVLINK. A DGX system can perform 4-bit integer to 32-bit floating point operations.

### DNN Compilers {#sec:quant-deploy-compiler}

Heterogeneous neural networks hardware accelerators are accelerating deep learning algorithm deployment [@Reuther2019]. Often exchange formats can be used to import/export models. Further, compilers have been developed to optimize models and generate code for specific processors. However several challenges remain:

- Network Parsing: Developers design neural network models on different platforms using various frameworks and programming languages. However, they have common parts, such as convolution, activation, pooling, etc. Parsing tools analyze the model compositions and transfer them into the unified representation.

- Structure Optimization: The model may contain operations used in training that aren't required for inference. Tool-kits and compilers should optimize these structures (e.g. BN folding as discussed in [2.5](#sec:cnn-bn){reference-type="ref+label" reference="sec:cnn-bn"}).

- Intermediate Representation (IR): An optimized model should be properly stored for further deployment. Since the inference engine is uncertain, the stored IR should include the model architecture and the trained weights. A compiler can then read the model and optimize it for a specific inference engine.

- Compression: Compilers and optimizers should optionally be able to automatically compress arbitrary network structures using pruning and quantization.

- Deployment: The final optimized model should be mapped to the target engine(s) which may be heterogeneous.

Open Neural Network Exchange (ONNX) [@ONNX] is an open-source tool to parse AI models written for a variety diverse frameworks. It imports and exports models using an open-source format facilitating the translation of neural network models between frameworks. It is thus capable of network parsing provided low-level operations are defined in all target frameworks.

TVM [@Chen2018], Glow [@Rotem2018], OpenVINO [@Intel], and MLIR [@Lattner2020] are deep learning compilers. They differ from frameworks such as Caffe in that they store intermediate representations and optimize those to map models onto specific hardware engines. They typically integrate both quantization-aware training and calibration-based post-training quantization. We summarize key features below. They perform all the operations noted in our list. A detailed survey can be found in [@Li2020].

TVM [@Chen2018] leverages the efficiency of quantization by enabling deployment of quantized models from PyTorch and TF-Lite. As a compiler, TVM has the ability to map the model on general hardware such as Intel's AVX and nVidia's CUDA.

Glow [@Rotem2018] enables quantization with zero points and converts the data into 8-bit signed integers using a calibration-based method. Neither Glow or TVM currently support quantization-aware training although they both announced future support for it [@Rotem2018].

MLIR [@Lattner2020] and OpenVINO [@Intel] have sophisticated quantization support including quantization-aware training. OpenVINO integrates it in TensorFlow and PyTorch while MLIR natively supports quantization-aware training. This allows users to fine-tune an optimized model when it doesn't satisfy accuracy criteria.

## Quantization Reduces Over-fitting {#sec:quant-reduce-OF}

In addition to accelerating neural networks, quantization has also been found in some cases to result in higher accuracy. As examples: 1) 3-bit weights VGG-16 outperforms its full precision counterpart by 1.1% top-1 [@Leng2017], 2) AlexNet reduces 1.0% top-1 error of the reference with 2-bit weights and 8-bit activations [@Faraone2018], 3) ResNet-34 with 4-bit weights and activation obtained 74.52% top-1 accuracy while the 32-bit version is 73.59% [@Mishra2018], 4) Zhou showed a quantized model reduced the classification error by 0.15%, 2.28%, 0.13%, 0.71%, and 1.59% on AlexNet, VGG-16, GoogLeNet, ResNet-18 and ResNet-50, respectively [@Zhou2017a], and 5) Xu showed reduced bit quantized networks help to reduce over-fitting on Fully Connected Networks (FCNs). By taking advantage of strict constraints in biomedical image segmentation they improved segmentation accuracy by 1% combined with a $6.4\times$ memory usage reduction [@Xu2018a].

# Summary {#sec:summary}

In this section we summarize the results of Pruning and Quantization.

## Pruning {#sec:summary-pruning}

[3](#sec:pruning){reference-type="ref+label" reference="sec:pruning"} shows pruning is an important technique for compressing neural networks. In this paper, we discussed pruning techniques categorized as 1) static pruning and 2) dynamic pruning. Previously, static pruning was the dominant area of research. Recently, dynamic pruning has become a focus because it can further improve performance even if static pruning has first been performed.

Pruning can be performed in multiple ways. Element-wise pruning improves weight compression and storage. Channel-wise and shape-wise pruning can be accelerated with specialized hardware and computation libraries. Filter-wise and layer-wise pruning can dramatically reduce computational complexity.

Though pruning sometimes introduces incremental improvement in accuracy by escaping a local minima [@Augasta2013], accuracy improvements are better realized by switching to a better network architecture [@Blalock2020]. For example, a separable block may provide better accuracy with reduced computational complexity [@Howard2017]. Considering the evolution of network structures, performance may also be bottlenecked by the structure itself. From this point of view, Network Architecture Search and Knowledge Distillation can be options for further compression. Network pruning can be viewed as a subset of NAS but with a smaller searching space. This is especially true when the pruned architecture no longer needs to use weights from the unpruned network (see [3.3](#sec:pruning-comparison){reference-type="ref+label" reference="sec:pruning-comparison"}). In addition, some NAS techniques can also be applied to the pruning approach including borrowing trained coefficients and reinforcement learning search.

Typically, compression is evaluated on large data-sets such as the ILSVRC-2012 dataset with one thousand object categories. In practice, resource constraints in embedded devices don't allow a large capacity of optimized networks. Compressing a model to best fit a constrained environment should consider but not be limited to the deployment environment, target device, speed/compression trade-offs, and accuracy requirements [@Cai2019a].

Based on the reviewed pruning techniques, we recommend the following for effective pruning:

- Uniform pruning introduces accuracy loss therefore setting the pruning ratio to vary by layers is better [@Liu2019].

- Dynamic pruning may result in higher accuracy and maintain higher network capacity [@Wu2018a].

- Structurally pruning a network may benefit from maturing libraries especially when pruning at a high level [@Wen2016a].

- Training a pruned model from scratch sometimes, but not always (see [3.3](#sec:pruning-comparison){reference-type="ref+label" reference="sec:pruning-comparison"}), is more efficient than tunning from the unpruned weights [@Liu2019Rethingking].

- Penalty-based pruning typically reduces accuracy loss compared with magnitude-based pruning [@Ye2018]. However, recent efforts are narrowing the gap [@Gale2019].

## Quantization {#sec:summary-quantization}

[4](#sec:quantization){reference-type="ref+label" reference="sec:quantization"} discusses quantization techniques. It describes binarized quantized neural networks, and reduced precision networks, along with their training methods. We described low-bit dataset validation techniques and results. We also list the accuracy of popular quantization frameworks and described hardware implementations in [4.3](#sec:quant-deploy){reference-type="ref+label" reference="sec:quant-deploy"}.

Quantization usually results in a loss of accuracy due to information lost during the quantization process. This is particularly evident on compact networks. Most of the early low bit quantization approaches only compare performance on small datasets (e.g., MNIST, and CIFAR-10) [@Dettmers2015; @Han2015a; @Lin2015; @Rastegari2016; @Venkatesh2017b; @Zhou2017a]. However, observations showed that some quantized networks could outperform the original network (see: [4.4](#sec:quant-reduce-OF){reference-type="ref+label" reference="sec:quant-reduce-OF"}). Additionally, non-uniform distribution data may lead to further deterioration in quantization performance [@Zhu2019]. Sometimes this can be ameliorated by normalization in fine-tuning [@Micikevicius2017] or by non-linear quantization (e.g., log representation) [@Miyashita2016].

Advanced quantization techniques have improved accuracy. Asymmetric quantization [@Jacob2017] maintains higher dynamic range by using a zero point in addition to a regular scale parameter. Overheads introduced by the zero point were minimized by pipelining the processing unit. Calibration based quantization [@Migacz2017] removed zero points and replaced them with precise scales obtained from a calibrating dataset. Quantization-aware training was shown to further improve quantization accuracy.

8-bit quantization is widely applied in practice as a good trade-off between accuracy and compression. It can easily be deployed on current processors and custom hardware. Minimal accuracy loss is experienced especially when quantization-aware training is enabled. Binarized networks have also achieved reasonable accuracy with specialized hardware designs.

Though BN has advantages to help training and pruning, an issue with BN is that it may require a large dynamic range across a single layer kernel or between different channels. This may make layer-wise quantization more difficult. Because of this per channel quantization is recommended [@Krishnamoorthi2018].

To achieve better accuracy following quantization, we recommend:

- Use asymmetrical quantization. It preserves flexibility over the quantization range even though it has computational overheads [@Jacob2017].

- Quantize the weights rather than the activations. Activation is more sensitive to numerical precision [@Gong2019].

- Do not quantize biases. They do not require significant storage. High precision biases in all layers [@Hwang2014], and first/last layers [@Rastegari2016; @Zhou2016a], maintain higher network accuracy.

- Quantize kernels channel-wise instead of layer-wise to significantly improve accuracy [@Krishnamoorthi2018].

- Fine-tune the quantized model. It reduces the accuracy gap between the quantized model and the real-valued model [@Wu2018].

- Initially train using a 32-bit floating point model. Low-bit quantized model can be difficult to train from scratch - especially compact models on large-scaled data-sets [@Zhou2016a].

- The sensitivity of quantization is ordered as gradients, activations, and then weights [@Zhou2016a].

- Stochastic quantization of gradients is necessary when training quantized models [@Gupta2015; @Zhou2016a].

# Future Work {#sec:future-work}

Although punning and quantization algorithms help reduce the computation cost and bandwidth burden, there are still areas for improvement. In this section we highlight future work to further improvement quantization and prunning.

**Automatic Compression.** Low bit width quantization can cause significant accuracy loss, especially when the quantized bit-width is very narrow and the dataset is large [@Zhou2016a; @Lin2017a]. Automatic quantization is a technique to automatically search quantization encoding to evaluate accuracy loss verses compression ratio. Similarly, automatic prunning is a technique to automatically search different prunning approaches to evaluate the sparsity ratio versus accuracy. Similar to hyperparameter tuning [@Yogatama2014], this can be performed without human intervention using any number of search techniques (e.g. random search, genetic search, etc.).

**Compression on Other Types of Neural Networks.** Current compression research is primarily focused on CNNs. More specifically, research is primarily directed towards CNN classification tasks. Future work should also consider other types of applications such as object detection, speech recognition, language translation, etc. Network compression verses accuracy for different applications is an interesting area of research.

**Hardware Adaptation.** Hardware implementations may limit the effectiveness of pruning algorithms. For example, element-wise pruning only slightly reduces computations or bandwidth when using im2col-gemm on GPU [@Zhang2016]. Similarly, shape-wise pruning is not typically able to be implemented on dedicated CNN accelerators. Hardware-software co-design of compression techniques for hardware accelerators should be considered to achieve the best system efficiency.

**Global Methods.** Network optimizations are typically applied separately without information from one optimization informing any other optimization. Recently, approaches that consider optimization effectiveness at multiple layers have been proposed. [@Li2020b] discusses pruning combined with tensor factorization that results in better overall compression. Similar techniques can be considered using different types and levels of compression and factorization.

# Conclusions {#sec:discussion}

Deep neural networks have been applied in many applications exhibiting extraordinary abilities in the field of computer vision. However, complex network architectures challenge efficient real-time deployment and require significant computation resources and energy costs. These challenges can be overcome through optimizations such as network compression. Network compression can often be realized with little loss of accuracy. In some cases accuracy may even improve.

Pruning can be categorized as static ([3.1](#sec:pruning-static){reference-type="ref+label" reference="sec:pruning-static"}) if it is performed offline or dynamic ([3.2](#sec:pruning-dynamic){reference-type="ref+label" reference="sec:pruning-dynamic"}) if it is performed at run-time. The criteria applied to removing redundant computations if often just a simple magnitude of weights with values near zero being pruned. More complicated methods include checking the $l_p$-norm. Techniques such as LASSO and Ridge are built around $l_1$ and $l_2$ norms. Pruning can be performed element-wise, channel-wise, shape-wise, filter-wise, layer-wise and even network-wise. Each has trade-offs in compression, accuracy, and speedup.

Quantization reduces computations by reducing the precision of the datatype. Most networks are trained using 32-bit floating point. Weights, biases, and activations may then be quantized typically to 8-bit integers. Lower bit width quantizations have been performed with single bit being termed a binary neural network. It is difficult to (re)train very low bit width neural networks. A single bit is not differentiable thereby prohibiting back propagation. Lower bit widths cause difficulties for computing gradients. The advantage of quantization is significantly improved performance (usually 2-3x) and dramatically reduced storage requirements. In addition to describing how quantization is performed we also included an overview of popular libraries and frameworks that support quantization. We further provided a comparison of accuracy for a number of networks using different frameworks [\[tab:quantize-libraries\]](#tab:quantize-libraries){reference-type="ref+label" reference="tab:quantize-libraries"}.

In this paper, we summarized pruning and quantization techniques. Pruning removes redundant computations that have limited contribution to a result. Quantization reduces computations by reducing the precision of the datatype. Both can be used independently or in combination to reduce storage requirements and accelerate inference.

# Quantization Performance Results {#sec:quant-perf-clct}

:::: center
::: supertabular
p0.15p0.20p0.04p0.04p0.09p0.09p0.09

AlexNet & QuantNet & 1 & 32 & -1.70% & -1.50% & [@Yang2019]\
& BWNH & 1 & 32 & -1.40% & -0.70% & [@Hu2018]\
& SYQ & 2 & 8 & -1.00% & -0.60% & [@Faraone2018]\
& TSQ & 2 & 2 & -0.90% & -0.30% & [@Wang2018a]\
& INQ & 5 & 32 & -0.87% & -1.39% & [@Zhou2017a]\
& PACT & 4 & 3 & -0.60% & -1.00% & [@Choi2018]\
& QIL & 4 & 4 & -0.20% & - & [@Jung2018]\
& Mixed-Precision & 16 & 16 & -0.16% & - & [@Micikevicius2017]\
& PACT & 32 & 5 & -0.10% & -0.20% & [@Choi2018]\
& QIL & 5 & 5 & -0.10% & - & [@Jung2018]\
& QuantNet & 3(±4) & 32 & -0.10% & -0.10% & [@Yang2019]\
& ELNN & 3(±4) & 32 & 0.00% & 0.20% & [@Leng2017]\
& DoReFa-Net & 32 & 3 & 0.00% & -0.90% & [@Zhou2016a]\
& TensorRT & 8 & 8 & 0.03% & 0.00% & [@Migacz2017]\
& PACT & 2 & 2 & 0.10% & -0.70% & [@Choi2018]\
& PACT & 32 & 2 & 0.20% & -0.20% & [@Choi2018]\
& DoReFa-Net & 32 & 5 & 0.20% & -0.50% & [@Zhou2016a]\
& QuantNet & 3(±2) & 32 & 0.30% & 0.00% & [@Yang2019]\
& DoReFa-Net & 32 & 4 & 0.30% & -0.50% & [@Zhou2016a]\
& WRPN & 2 & 32 & 0.40% & - & [@Mishra2018]\
& DFP16 & 16 & 16 & 0.49% & 0.59% & [@Das2018]\
& PACT & 3 & 2 & 0.50% & -0.10% & [@Choi2018]\
& PACT & 4 & 2 & 0.50% & -0.10% & [@Choi2018]\
& SYQ & 1 & 8 & 0.50% & 0.80% & [@Faraone2018]\
& QIL & 3 & 3 & 0.50% & - & [@Jung2018]\
& FP8 & 8 & 8 & 0.50% & - & [@Wang2018b]\
& BalancedQ & 32 & 2 & 0.60% & -2.00% & [@Zhou2017c]\
& ELNN & 3(±2) & 32 & 0.80% & 0.60% & [@Leng2017]\
& SYQ & 1 & 4 & 0.90% & 0.80% & [@Faraone2018]\
& QuantNet & 2 & 32 & 0.90% & 0.30% & [@Yang2019]\
& FFN & 2 & 32 & 1.00% & 0.30% & [@Wang2017]\
& DoReFa-Net & 32 & 2 & 1.00% & 0.10% & [@Zhou2016a]\
& Unified INT8 & 8 & 8 & 1.00% & - & [@Zhu2019]\
& DeepShift-PS & 6 & 32 & 1.19% & 0.67% & [@Elhoushi2019]\
& WEQ & 4 & 4 & 1.20% & 1.00% & [@Park2017]\
& LQ-NETs & 2 & 32 & 1.30% & 0.80% & [@Zhang2018d]\
& SYQ & 2 & 2 & 1.30% & 1.00% & [@Faraone2018]\
& LQ-NETs & 1 & 2 & 1.40% & 1.40% & [@Zhang2018d]\
& BalancedQ & 2 & 2 & 1.40% & -1.00% & [@Zhou2017c]\
& WRPN-2x & 8 & 8 & 1.50% & - & [@Mishra2018]\
& DoReFa-Net & 1 & 4 & 1.50% & - & [@Zhou2016a]\
& DeepShift-Q & 6 & 32 & 1.55% & 0.81% & [@Elhoushi2019]\
& WRPN-2x & 32 & 8 & 1.60% & - & [@Mishra2018]\
& WEQ & 3 & 4 & 1.60% & 1.10% & [@Park2017]\
& WRPN-2x & 8 & 4 & 1.70% & - & [@Mishra2018]\
& WRPN-2x & 4 & 8 & 1.70% & - & [@Mishra2018]\
& SYQ & 1 & 2 & 1.70% & 1.60% & [@Faraone2018]\
& ELNN & 2 & 32 & 1.80% & 1.80% & [@Leng2017]\
& WRPN-2x & 4 & 4 & 1.90% & - & [@Mishra2018]\
& WRPN-2x & 32 & 4 & 1.90% & - & [@Mishra2018]\
GoogLeNet & Mixed-Precision & 16 & 16 & -0.10% & - & [@Micikevicius2017]\
& DeepShift-PS & 6 & 32 & -0.09% & -0.09% & [@Elhoushi2019]\
& DFP16 & 16 & 16 & -0.08% & 0.00% & [@Das2018]\
& AngleEye & 16 & 16 & 0.05% & 0.45% & [@Guo2018]\
& AngleEye & 16 & 16 & 0.05% & 0.45% & [@Guo2018]\
& ShiftCNN & 3 & 4 & 0.05% & 0.09% & [@Gudovskiy2017]\
& DeepShift-Q & 6 & 32 & 0.27% & 0.29% & [@Elhoushi2019]\
& LogQuant & 32 & 6 & 0.36% & 0.28% & [@Cai2018]\
& ShiftCNN & 2 & 4 & 0.39% & 0.29% & [@Gudovskiy2017]\
& TensorRT & 8 & 8 & 0.45% & 0.19% & [@Migacz2017]\
& LogQuant & 6 & 32 & 0.64% & 0.67% & [@Cai2018]\
& INQ & 5 & 32 & 0.76% & 0.25% & [@Zhou2017a]\
& ELNN & 3(±4) & 32 & 2.40% & 1.40% & [@Leng2017]\
& ELNN & 3(±2) & 32 & 2.80% & 1.60% & [@Leng2017]\
& LogQuant & 6 & 6 & 3.43% & 0.78% & [@Cai2018]\
& QNN & 4 & 4 & 5.10% & 7.80% & [@Hubara2016a]\
& QNN & 6 & 6 & 5.20% & 8.10% & [@Hubara2016a]\
& ELNN & 2 & 32 & 5.60% & 3.50% & [@Leng2017]\
& BWN & 1 & 32 & 5.80% & 4.80% & [@Rastegari2016]\
& AngleEye & 8 & 8 & 6.00% & 3.20% & [@Guo2018]\
& TWN & 2 & 32 & 7.50% & 4.80% & [@Li2016]\
& ELNN & 1 & 32 & 8.40% & 5.70% & [@Leng2017]\
& BWN & 2 & 32 & 9.70% & 6.50% & [@Rastegari2016]\
& ShiftCNN & 1 & 4 & 11.26% & 7.36% & [@Gudovskiy2017]\
& LogQuant & 32 & 3 & 13.50% & 8.93% & [@Cai2018]\
& LogQuant & 3 & 3 & 18.07% & 12.85% & [@Cai2018]\
& LogQuant & 4 & 4 & 18.57% & 13.21% & [@Cai2018]\
& LogQuant & 32 & 4 & 18.57% & 13.21% & [@Cai2018]\
& BNN & 1 & 1 & 24.20% & 20.90% & [@Courbariaux2016]\
& AngleEye & 6 & 6 & 52.10% & 57.35% & [@Guo2018]\
MobileNet & HAQ-Cloud & 6 & 6 & -0.38% & -0.23% & [@Wang2018]\
V1 & HAQ-Edge & 6 & 6 & -0.38% & -0.34% & [@Wang2018]\
& MelinusNet59 & 1 & 1 & -0.10% & - & [@Bethge2020]\
& HAQ-Edge & 5 & 5 & 0.24% & 0.08% & [@Wang2018]\
& PACT & 6 & 6 & 0.36% & 0.26% & [@Choi2018]\
& PACT & 6 & 6 & 0.36% & 0.26% & [@Choi2018]\
& HAQ-Cloud & 5 & 5 & 0.85% & 0.48% & [@Wang2018]\
& HAQ-Edge & 4 & 4 & 3.42% & 1.95% & [@Wang2018]\
& PACT & 5 & 5 & 3.82% & 2.20% & [@Choi2018]\
& PACT & 5 & 5 & 3.82% & 2.20% & [@Choi2018]\
& HAQ-Cloud & 4 & 4 & 5.49% & 3.25% & [@Wang2018]\
& PACT & 4 & 4 & 8.38% & 5.66% & [@Choi2018]\
MobileNet & HAQ-Edge & 6 & 6 & -0.08% & -0.11% & [@Wang2018]\
V2 & HAQ-Cloud & 6 & 6 & -0.04% & 0.01% & [@Wang2018]\
& Unified INT8 & 8 & 8 & 0.00% & - & [@Zhu2019]\
& PACT & 6 & 6 & 0.56% & 0.25% & [@Choi2018]\
& HAQ-Edge & 5 & 5 & 0.91% & 0.34% & [@Wang2018]\
& HAQ-Cloud & 5 & 5 & 2.36% & 1.31% & [@Wang2018]\
& PACT & 5 & 5 & 2.97% & 1.67% & [@Choi2018]\
& HAQ-Cloud & 4 & 4 & 4.80% & 2.79% & [@Wang2018]\
& HAQ-Edge & 4 & 4 & 4.82% & 2.92% & [@Wang2018]\
& PACT & 4 & 4 & 10.42% & 6.53% & [@Choi2018]\
ResNet-18 & RangeBN & 8 & 8 & -0.60% & - & [@Banner2018]\
& LBM & 8 & 8 & -0.60% & - & [@Zhong2020]\
& QuantNet & 5 & 32 & -0.30% & -0.10% & [@Yang2019]\
& QIL & 5 & 5 & -0.20% & - & [@Jung2018]\
& QuantNet & 3(±4) & 32 & -0.10% & -0.10% & [@Yang2019]\
& ShiftCNN & 3 & 4 & 0.03% & 0.12% & [@Gudovskiy2017]\
& LQ-NETs & 4 & 32 & 0.20% & 0.50% & [@Zhang2018d]\
& QIL & 3 & 32 & 0.30% & 0.30% & [@Jung2018]\
& LPBN & 32 & 5 & 0.30% & 0.40% & [@Cai2017]\
& QuantNet & 3(±2) & 32 & 0.40% & 0.20% & [@Yang2019]\
& PACT & 32 & 4 & 0.40% & 0.30% & [@Choi2018]\
& SeerNet & 4 & 1 & 0.42% & 0.18% & [@Cao2019]\
& ShiftCNN & 2 & 4 & 0.54% & 0.34% & [@Gudovskiy2017]\
& PACT & 5 & 5 & 0.60% & 0.30% & [@Choi2018]\
& INQ & 4 & 32 & 0.62% & 0.10% & [@Zhou2017a]\
& Unified INT8 & 8 & 8 & 0.63% & - & [@Zhu2019]\
& QIL & 5 & 5 & 0.80% & - & [@Jung2018]\
& LQ-NETs & 3(±4) & 32 & 0.90% & 0.80% & [@Zhang2018d]\
& QIL & 3 & 3 & 1.00% & - & [@Jung2018]\
& DeepShift-Q & 6 & 32 & 1.09% & 0.47% & [@Elhoushi2019]\
& ELNN & 3(±2) & 32 & 1.10% & 0.70% & [@Leng2017]\
& PACT & 32 & 3 & 1.20% & 0.70% & [@Choi2018]\
& PACT & 4 & 4 & 1.20% & 0.60% & [@Choi2018]\
& QuantNet & 2 & 32 & 1.20% & 0.60% & [@Yang2019]\
& ELNN & 3(±4) & 32 & 1.30% & 0.60% & [@Leng2017]\
& DeepShift-PS & 6 & 32 & 1.44% & 0.67% & [@Elhoushi2019]\
& ABC-Net & 5 & 32 & 1.46% & 1.18% & [@Lin2017a]\
& ELNN & 3(±2) & 32 & 1.60% & 1.10% & [@Leng2017]\
& DoReFa-Net & 32 & 5 & 1.70% & 1.00% & [@Zhou2016a]\
& SYQ & 2 & 8 & 1.90% & 1.40% & [@Faraone2018]\
& DoReFa-Net & 32 & 4 & 1.90% & 1.10% & [@Zhou2016a]\
& LQ-NETs & 3 & 3 & 2.00% & 1.60% & [@Zhang2018d]\
& DoReFa-Net & 5 & 5 & 2.00% & 1.30% & [@Zhou2016a]\
& ELNN & 2 & 32 & 2.10% & 1.50% & [@Leng2017]\
& QIL & 2 & 32 & 2.10% & 1.30% & [@Jung2018]\
& DoReFa-Net & 32 & 3 & 2.10% & 1.40% & [@Zhou2016a]\
& QIL & 4 & 4 & 2.20% & - & [@Jung2018]\
& LQ-NETs & 2 & 32 & 2.20% & 1.60% & [@Zhang2018d]\
& GroupNet-8 & 1 & 1 & 2.20% & 1.40% & [@Zhuang2019]\
& PACT & 3 & 3 & 2.30% & 1.40% & [@Choi2018]\
& DoReFa-Net & 4 & 4 & 2.30% & 1.50% & [@Zhou2016a]\
& TTN & 2 & 32 & 2.50% & 1.80% & [@Zhu2016]\
& TTQ & 2 & 32 & 2.70% & 2.00% & [@Zoph2017]\
& AddNN & 32 & 32 & 2.80% & 1.50% & [@Chen2019b]\
& ELNN & 2 & 32 & 2.80% & 1.50% & [@Leng2017]\
& LPBN & 32 & 4 & 2.90% & 1.70% & [@Cai2017]\
& PACT & 32 & 2 & 2.90% & 2.00% & [@Choi2018]\
& DoReFa-Net & 3 & 3 & 2.90% & 2.00% & [@Zhou2016a]\
& QuantNet & 1 & 32 & 3.10% & 1.90% & [@Yang2019]\
& INQ & 2 & 32 & 3.10% & 1.90% & [@Zhou2017a]\
ResNet-34 & WRPN-2x & 4 & 4 & -0.93% & - & [@Mishra2018]\
& WRPN-2x & 4 & 8 & -0.89% & - & [@Mishra2018]\
& QIL & 4 & 4 & 0.00% & - & [@Jung2018]\
& QIL & 5 & 5 & 0.00% & - & [@Jung2018]\
& WRPN-2x & 4 & 2 & 0.01% & - & [@Mishra2018]\
& WRPN-2x & 2 & 4 & 0.09% & - & [@Mishra2018]\
& WRPN-2x & 2 & 2 & 0.27% & - & [@Mishra2018]\
& SeerNet & 4 & 1 & 0.35% & 0.17% & [@Cao2019]\
& Unified INT8 & 8 & 8 & 0.39% & - & [@Zhu2019]\
& LCCL & & & 0.43% & 0.17% & [@Dong2017a]\
& QIL & 3 & 3 & 0.60% & - & [@Jung2018]\
& WRPN-3x & 1 & 1 & 0.90% & - & [@Mishra2018]\
& WRPN-3x & 1 & 1 & 1.21% & - & [@Mishra2018]\
& GroupNet-8 & 1 & 1 & 1.40% & 1.00% & [@Zhuang2019]\
& dLAC & 2 & 16 & 1.67% & 0.89% & [@Venkatesh2017b]\
& LQ-NETs & 3 & 3 & 1.90% & 1.20% & [@Zhang2018d]\
& GroupNet\*\*-5 & 1 & 1 & 2.70% & 2.10% & [@Zhuang2019]\
& IR-Net & 1 & 32 & 2.90% & 1.80% & [@Qin2020a]\
& QIL & 2 & 2 & 3.10% & - & [@Jung2018]\
& WRPN-2x & 1 & 1 & 3.40% & - & [@Mishra2018]\
& WRPN-2x & 1 & 1 & 3.74% & - & [@Mishra2018]\
& LQ-NETs & 2 & 2 & 4.00% & 2.30% & [@Zhang2018d]\
& GroupNet-5 & 1 & 1 & 4.70% & 3.40% & [@Zhuang2019]\
& ABC-Net & 5 & 5 & 4.90% & 3.10% & [@Lin2017a]\
& HWGQ & 1 & 32 & 5.10% & 3.40% & [@Cai2017]\
& WAGEUBN & 8 & 8 & 5.18% & - & [@Yang2020]\
& ABC-Net & 3 & 3 & 6.60% & 3.90% & [@Lin2017a]\
& LQ-NETs & 1 & 2 & 6.70% & 4.40% & [@Zhang2018d]\
& LQ-NETs & 4 & 4 & 6.70% & 4.40% & [@Zhang2018d]\
& BCGD & 1 & 4 & 7.60% & 4.70% & [@Yin2019]\
& HWGQ & 1 & 2 & 9.00% & 5.60% & [@Cai2017]\
& IR-Net & 1 & 1 & 9.50% & 6.20% & [@Qin2020a]\
& CI-BCNN (add) & 1 & 1 & 11.07% & 6.39% & [@Wang2019]\
& Bi-Real & 1 & 1 & 11.10% & 7.40% & [@Xu2018b]\
& WRPN-1x & 1 & 1 & 12.80% & - & [@Mishra2018]\
& WRPN & 1 & 1 & 13.05% & - & [@Mishra2018]\
& CI-BCNN & 1 & 1 & 13.59% & 8.65% & [@Wang2019]\
& DoReFa-Net & 1 & 4 & 14.60% & - & [@Zhou2016a]\
& DoReFa-Net & 1 & 2 & 20.40% & - & [@Zhou2016a]\
& ABC-Net & 1 & 1 & 20.90% & 14.80% & [@Lin2017a]\
& BNN & 1 & 1 & 29.10% & 24.20% & [@Zhou2016a]\
ResNet-50 & Mixed-Precision & 16 & 16 & -0.12% & - & [@Micikevicius2017]\
& DFP16 & 16 & 16 & -0.07% & -0.06% & [@Das2018]\
& QuantNet & 5 & 32 & 0.00% & 0.00% & [@Yang2019]\
& LQ-NETs & 4 & 32 & 0.00% & 0.10% & [@Zhang2018d]\
& FGQ & 32 & 32 & 0.00% & - & [@Mellempudi2017]\
& TensorRT & 8 & 8 & 0.13% & 0.12% & [@Migacz2017]\
& PACT & 5 & 5 & 0.20% & -0.20% & [@Choi2018]\
& QuantNet & 3(±4) & 32 & 0.20% & 0.00% & [@Yang2019]\
& Unified INT8 & 8 & 8 & 0.26% & - & [@Zhu2019]\
& ShiftCNN & 3 & 4 & 0.29% & 0.15% & [@Gudovskiy2017]\
& ShiftCNN & 3 & 4 & 0.31% & 0.16% & [@Gudovskiy2017]\
& PACT & 4 & 4 & 0.40% & -0.10% & [@Choi2018]\
& LPBN & 32 & 5 & 0.40% & 0.40% & [@Graham2017]\
& ShiftCNN & 2 & 4 & 0.67% & 0.41% & [@Gudovskiy2017]\
& DeepShift-Q & 6 & 32 & 0.81% & 0.21% & [@Elhoushi2019]\
& DeepShift-PS & 6 & 32 & 0.84% & 0.31% & [@Elhoushi2019]\
& PACT & 5 & 32 & 0.90% & 0.20% & [@Choi2018]\
& QuantNet & 3(±2) & 32 & 0.90% & 0.40% & [@Yang2019]\
& PACT & 4 & 32 & 1.00% & 0.20% & [@Choi2018]\
& dLAC & 2 & 16 & 1.20% & - & [@Venkatesh2017b]\
& QuantNet & 2 & 32 & 1.20% & 0.60% & [@Yang2019]\
& AddNN & 32 & 32 & 1.30% & 1.20% & [@Chen2019b]\
& LQ-NETs & 4 & 4 & 1.30% & 0.80% & [@Zhang2018d]\
& LQ-NETs & 2 & 32 & 1.30% & 0.90% & [@Zhang2018d]\
& INQ & 5 & 32 & 1.32% & 0.41% & [@Zhou2017a]\
& PACT & 3 & 32 & 1.40% & 0.50% & [@Choi2018]\
& IAO & 8 & 8 & 1.50% & - & [@Jacob2017]\
& PACT & 3 & 3 & 1.60% & 0.50% & [@Choi2018]\
& HAQ & 2MP & 4MP & 1.91% & - & [@Wang2018]\
& HAQ & MP & MP & 2.09% & - & [@Wang2018]\
& LQ-NETs & 3 & 3 & 2.20% & 1.60% & [@Zhang2018d]\
& LPBN & 32 & 4 & 2.20% & 1.20% & [@Graham2017]\
& Deep Comp. & 3 & MP & 2.29% & - & [@Han2015]\
& PACT & 4 & 2 & 2.40% & 1.20% & [@Choi2018]\
& ShiftCNN & 2 & 4 & 2.49% & 1.64% & [@Gudovskiy2017]\
& FFN & 2 & 32 & 2.50% & 1.30% & [@Wang2017]\
& UNIQ & 4 & 8 & 2.60% & - & [@Baskin2018]\
& QuantNet & 1 & 32 & 3.20% & 1.70% & [@Yang2019]\
& SYQ & 2 & 8 & 3.70% & 2.10% & [@Faraone2018]\
& FGQ-TWN & 2 & 8 & 4.29% & - & [@Mellempudi2017]\
& PACT & 2 & 2 & 4.70% & 2.60% & [@Choi2018]\
& LQ-NETs & 2 & 2 & 4.90% & 2.90% & [@Zhang2018d]\
& SYQ & 1 & 8 & 5.40% & 3.40% & [@Faraone2018]\
& DoReFa-Net & 4 & 4 & 5.50% & 3.30% & [@Zhou2016a]\
& DoReFa-Net & 5 & 5 & 5.50% & -0.20% & [@Zhou2016a]\
& FGQ & 2 & 8 & 5.60% & - & [@Mellempudi2017]\
& ABC-Net & 5 & 5 & 6.30% & 3.50% & [@Lin2017a]\
& FGQ-TWN & 2 & 4 & 6.67% & - & [@Mellempudi2017]\
& HWGQ & 1 & 2 & 6.90% & 4.60% & [@Cai2017]\
ResNet-100 & IAO & 8 & 8 & 1.40% & - & [@Jacob2017]\
ResNet-101 & TensorRT & 8 & 8 & -0.01% & 0.05% & [@Migacz2017]\
& FGQ-TWN & 2 & 8 & 3.65% & - & [@Mellempudi2017]\
& FGQ-TWN & 2 & 4 & 6.81% & - & [@Mellempudi2017]\
ResNet-150 & IAO & 8 & 8 & 2.10% & - & [@Jacob2017]\
ResNet-152 & TensorRT & 8 & 8 & 0.08% & 0.04% & [@Migacz2017]\
& dLAC & 2 & 16 & 1.20% & 0.64% & [@Venkatesh2017b]\
SqueezeNet & AngleEye & 16 & 16 & 0.00% & 0.01% & [@Guo2018]\
& ShiftCNN & 3 & 4 & 0.01% & 0.01% & [@Gudovskiy2017]\
& ShiftCNN & 2 & 4 & 1.01% & 0.71% & [@Gudovskiy2017]\
& AngleEye & 8 & 8 & 1.42% & 1.05% & [@Guo2018]\
& AngleEye & 6 & 6 & 28.13% & 27.43% & [@Guo2018]\
& ShiftCNN & 1 & 4 & 35.39% & 35.09% & [@Gudovskiy2017]\
VGG-16 & ELNN & 3(±4) & 32 & -1.10% & -1.00% & [@Leng2017]\
& ELNN & 3(±2) & 32 & -0.60% & -0.80% & [@Leng2017]\
& AngleEye & 16 & 16 & 0.09% & -0.05% & [@Guo2018]\
& DFP16 & 16 & 16 & 0.11% & 0.29% & [@Das2018]\
& AngleEye & 8 & 8 & 0.21% & 0.08% & [@Guo2018]\
& SeerNet & 4 & 1 & 0.28% & 0.10% & [@Cao2019]\
& DeepShift-Q & 6 & 32 & 0.29% & 0.11% & [@Elhoushi2019]\
& FFN & 2 & 32 & 0.30% & -0.20% & [@Wang2017]\
& DeepShift-PS & 6 & 32 & 0.47% & 0.30% & [@Elhoushi2019]\
& DeepShift-Q & 6 & 32 & 0.72% & 0.29% & [@Elhoushi2019]\
& INQ & 5 & 32 & 0.77% & 0.08% & [@Elhoushi2019]\
& TWN & 2 & 32 & 1.10% & 0.30% & [@Li2016]\
& ELNN & 2 & 32 & 2.00% & 0.90% & [@Leng2017]\
& TSQ & 2 & 2 & 2.00% & 0.70% & [@Wang2018a]\
& AngleEye & 16 & 16 & 2.15% & 1.49% & [@Guo2018]\
& BWN & 2 & 32 & 2.20% & 1.20% & [@Rastegari2016]\
& AngleEye & 8 & 8 & 2.35% & 1.76% & [@Guo2018]\
& ELNN & 1 & 32 & 3.30% & 1.80% & [@Leng2017]\
& AngleEye & 6 & 6 & 9.07% & 6.58% & [@Guo2018]\
& AngleEye & 6 & 6 & 22.38% & 17.75% & [@Guo2018]\
& LogQuant & 3 & 3 & - & 0.99% & [@Cai2018]\
& LogQuant & 4 & 4 & - & 0.51% & [@Cai2018]\
& LogQuant & 6 & 6 & - & 0.83% & [@Cai2018]\
& LogQuant & 32 & 3 & - & 0.82% & [@Cai2018]\
& LogQuant & 32 & 4 & - & 0.36% & [@Cai2018]\
& LogQuant & 32 & 6 & - & 0.31% & [@Cai2018]\
& LogQuant & 6 & 32 & - & 0.76% & [@Cai2018]\
& LDR & 5 & 4 & - & 0.90% & [@Miyashita2016]\
& LogNN & 5 & 4 & - & 1.38% & [@Miyashita2016]\
:::
::::

**Tailin Liang** received the B.E. degree in Computer Science and B.B.A from the University of Science and Technology Beijing in 2017. He is currently working toward a Ph.D. degree in Computer Science at the School of Computer and Communication Engineering, University of Science and Technology Beijing. His current research interests include deep learning domain-specific processors and co-designed optimization algorithms.

**John Glossner** received the Ph.D. degree in Electrical Engineering from TU Delft in 2001. He is the Director of the Computer Architecture, Heterogeneous Computing, and AI Lab at the University of Science and Technology Beijing. He is also the CEO of Optimum Semiconductor Technologies and President of both the Heterogeneous System Architecture Foundation and Wireless Innovation Forum. John's research interests include the design of heterogeneous computing systems, computer architecture, embedded systems, digital signal processors, software defined radios, artificial intelligence algorithms, and machine learning systems.

**Lei Wang** received the B.E. and Ph.D. degrees in 2006 and 2012 from the University of Science and Technology Beijing. He then served as an assistant researcher at the Institute of Automation of the Chinese Academy of Sciences during 2012-2015. He was a joint Ph.D. of Electronic Engineering at The University of Texas at Dallas during 2009-2011. Currently, he is an adjunct professor at the School of Computer and Communication Engineering, University of Science and Technology Beijing.

**Shaobo Shi** received the B.E. and Ph.D. degrees in 2008 and 2014 from the University of Science and Technology Beijing. He then served as an assistant researcher at the Institute of Automation of the Chinese Academy of Sciences during 2014-2017. Currently, he is a deep learning domain-specific processor engineer at Huaxia General Processor Technology. As well serve as an adjunct professor at the School of Computer and Communication Engineering, University of Science and Technology Beijing.

**Xiaotong Zhang** received the M.E. and Ph.D. degrees from the University of Science and Technology Beijing in 1997 and 2000, respectively, where he was a professor of Computer Science and Technology. His research interest includes the quality of wireless channels and networks, wireless sensor networks, networks management, cross-layer design and resource allocation of broadband and wireless networks, and the signal processing of communication and computer architecture.
