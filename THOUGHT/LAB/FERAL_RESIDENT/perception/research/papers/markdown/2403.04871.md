::: CCSXML
\<ccs2012\> \<concept\> \<concept_id\>10002951.10003317.10003325\</concept_id\> \<concept_desc\>Information systems Information retrieval query processing\</concept_desc\> \<concept_significance\>500\</concept_significance\> \</concept\> \<concept\> \<concept_id\>10002951.10002952.10002971\</concept_id\> \<concept_desc\>Information systems Data structures\</concept_desc\> \<concept_significance\>500\</concept_significance\> \</concept\> \</ccs2012\>
:::

# Introduction

Due to the representation strength of modern deep learning models, vector embeddings have become a powerful first-class datatype for wide-ranging applications that use retrieval-augmented generation [@llamaindex; @langchain] or similarity-based search [@borisyuk_visrel_2021; @liu_pre-trained_2021; @du_amazon_2022]. As a result, vector databases and indices are seeing increasing adoption in many production use cases. These systems provide an efficient approximate-nearest-neighbor (ANN) search interface over embedded unstructured data *e.g.,* images, text, video, or user profiles.

However, many applications must jointly query *both unstructured and structured data*, requiring ANN search in combination with predicate filtering. For example, customers on an e-commerce site can search for t-shirts similar to a reference image, while filtering on price [@weianalyticdbv2020]. Similarly, researchers performing a literature review may search with both natural language queries and filters on publication date, keywords or topics [@rekabsaz_tripclick_2021]. Likewise, a data scientist working on outlier detection can find misclassified images by retrieving those that look similar to a reference dog but have the label \"cat\" [@fastdup; @labelbox].

To leverage diverse data modalities, applications need data management systems that effectively support *hybrid search queries*, i.e., similarity search with structured predicates. Such systems require (1) **query performance**, *i.e.,* efficient and accurate search despite variance in workload characteristics, such as selectivity, attribute correlations, and scale, and (2) **expressive query semantics**: support for diverse query predicates that may not be known a priori (*e.g.,* user-entered keywords, range searches, or regex matching).

Unfortunately, existing systems fall short of these goals. Three commonly used methods are pre-filtering [@wangmilvus2021; @weianalyticdbv2020], post-filtering [@zhang_vbase_2023; @noauthor_filtered_nodate; @noauthorfaiss2023; @weianalyticdbv2020; @wangmilvus2021], and specialized data structures for low-cardinality predicate sets [@wang_navigable_2022; @gollapudi_filtered-diskann_2023; @wu_hqann_2022; @mohoney_high-throughput_2023]. *Pre-filtering* first finds all records in the dataset that pass the query predicate, then performs brute force similarity-search over the filtered vector set. This approach scales poorly, becoming inefficient for medium to high selectivity predicates on large datasets. Alternatively, *post-filtering* first searches an ANN index, then removes results that fail the query predicate. Since the database vectors closest to the query vector may not pass the predicate, post-filtering methods must typically expand the search scope. This is often expensive, particularly for search predicates with low selectivity or low correlation to the query vector, as we show in Figure [2](#fig4:correlation_and_clustering){reference-type="ref" reference="fig4:correlation_and_clustering"}. Milvus [@wangmilvus2021], Weaviate [@noauthor_filtered_nodate], AnalyticDB-V [@weianalyticdbv2020], and FAISS-IVF [@noauthorfaiss2023] build systems using these two core methods, and suffer from their performance limitations.

Recognizing these limitations, recent works construct *specialized indices* designed for hybrid search workloads with low-cardinality predicate sets consisting of equality predicate operators. For example, Filtered-DiskANN [@gollapudi_filtered-diskann_2023] outperforms prior baselines, but restricts the cardinality of the predicate set to about 1,000 and only supports equality predicates. HQANN [@wu_hqann_2022] and NHQ [@ashenon3_nhq_2023] similarly constrain the predicate set to a small number of equality filters and in addition allow only a single structured attributes per dataset entry. These methods are often impractical since many applications have large, or unbounded predicate sets that are unknown a priori. In general, the possible predicate set's cardinality grows exponentially with each attribute's cardinality, which itself may be large. Thus, we instead propose a *predicate-agnostic* index, which can support arbitrary and unbounded predicate sets.

In this paper, we propose **ACORN** (ANN Constraint-Optimized Retrieval Network), a novel approach for performant and predicate-agnostic hybrid search that can serve high-cardinality and unbounded predicate sets. We propose two indices: *ACORN-$\gamma$*, designed for high-efficiency search, and *ACORN-1*, designed for low construction overhead in resource-constrained settings. Both methods modify the hierarchical navigable small world (HNSW) index, a state-of-the-art graph-based index for ANN search, and are easy to implement in existing HNSW libraries.

ACORN tackles both the *performance limitations* of pre- and post- filtering, as well as the *semantic limitations* of specialized indices. ACORN proposes the idea of *predicate subgraph traversal* during search. As the name implies, the search strategy traverses the subgraph of the ACORN index induced by the set of nodes that pass the query predicate. ACORN designs the index such that these arbitrary predicate subgraphs approximate an HNSW index. Unlike pre- and post-filtering, this allows ACORN to provide sublinear retrieval times despite variance in *correlation* between query vectors and predicates, which we find to be a major challenge for existing hybrid search systems. ACORN also serves wide-ranging predicate sets by employing a predicate-agnostic construction that alters HNSW's algorithm to create a denser graph. Specifically, we introduce a predicate-agnostic neighbor expansion strategy in ACORN-$\gamma$ based on target predicate selectivity thresholds, which can be estimated empirically with or without knowing the predicate set. In conjunction, we propose a predicate-agnostic compression heuristic to efficiently manage the index space footprint while maintaining efficient search. We also explore the trade-off space between search performance and construction overhead, designing ACORN-1 to approximate ACORN-$\gamma$'s search performance while further reducing the time-to-index (TTI) for resource-constrained settings.

We systematically evaluate ACORN-$\gamma$ and ACORN-1 on four datasets: SIFT1M [@jegou_product_2011], Paper [@wang_navigable_2022], LAION [@schuhmann_laion-400m_2021], and TripClick [@rekabsaz_tripclick_2021]. Our evaluation includes both prior benchmark datasets, with simple, low-cardinality predicate sets, which prior specialized indices can serve, as well as more complex datasets with millions of possible predicates, which existing indices cannot to handle. On each, ACORN-$\gamma$ achieves state-of-the-art hybrid search performance with 2--1000$\times$ higher queries per second (QPS) at 0.9 recall compared to prior methods. We find that ACORN-1 empirically approximates ACORN-$\gamma$, attaining at most $5\times$ lower QPS at fixed recall but 9--53$\times$ lower TTI compared to ACORN-$\gamma$. Our detailed evaluation demonstrates the effectiveness of ACORN's predicate-subgraph traversal strategy and predicate-agnostic construction techniques.

# Background

Existing methods for Approximate Nearest Neighbor (ANN) search can be broadly categorized as tree-based [@bentley_multidimensional_1975; @bernhardsson_annoy_nodate; @beygelzimer_cover_2006; @dasgupta_random_2008; @houle_rank-based_2015; @lu_vhp_2020; @muja_scalable_2014; @silpa-anan_optimised_2008], hashing-based [@andoni_near-optimal_2008; @andoni_practical_2015; @andoni_optimal_2015; @gong_idec_2020; @indyk_approximate_1998; @jafari_mmlsh_2020; @li_io_2020; @liu_ei-lsh_2021; @lu_r2lsh_2020; @lv_intelligent_2017; @park_neighbor-sensitive_2015; @sundaram_streaming_2013; @zheng_pm-lsh_2020; @gionis_similarity_1999], quantization-based [@ge_optimized_2014; @guo_accelerating_2020; @jegou_product_2011; @johnson_billion-scale_2017; @lempitsky_inverted_2012], and graph-based [@fu_fast_2019; @malkov_approximate_2014; @gollapudi_filtered-diskann_2023; @jayaram_subramanya_diskann_2019; @malkov_efficient_2018; @singh_freshdiskann_2021; @zhao_song_2020]. In this work build on HNSW, a graph-based method that is empirically one of the best-performing on high-dimensional datasets, and we adapt it to support hybrid search.

Graph-based ANN methods have gained popularity due their state-of-the-art performance on varied ANN benchmarks [@aumuller_ann-benchmarks_2020; @simhadri_results_2022]. These methods typically perform search using a greedy routing strategy that traverses a graph index, starting from a pre-defined entry point. The index itself forms a proximity graph $G(V,E)$, such that each dataset point is represented by a vertex and edges connect nearby points. The index construction algorithm typically aims to approximate subgraphs of the Delaunay graph [@lee_two_1980]. While the Delaunay graph guarantees convergence of a greedy routing algorithm, it is impossible to efficiently construct for arbitrary metric spaces [@navarro_searching_2002]. Thus graph methods focus on more tractable approximations of Delaunay subgraphs, such as the *Relative Neighbor Graph (RNG)* [@toussaint_relative_1980; @lankford_regionalization_1969], and the *Nearest-Neighbor Graph (NNG)* [@dong_efficient_2011; @algorithms_kgraph_2023].

## Hierarchical Navigable Small Worlds {#sec:graph-based-methods}

As Figure [1](#fig:hnsw_diagram){reference-type="ref" reference="fig:hnsw_diagram"} illustrates, HNSW forms a hierarchical, multi-level graph index with bounded degree. Below we briefly summarize the HNSW search and construction algorithm.

<figure id="fig:hnsw_diagram" data-latex-placement="t">
<img src="diagrams/hnsw.png" style="width:30.0%" />
<figcaption>Schematic drawing of search over an HNSW index. The search path is shown by blue arrows, beginning on level 2 and ending on level 0 at the query point, shown in green.</figcaption>
</figure>

***The HNSW construction algorithm*** iteratively inserts each point into the graph index, to construct a navigable graph with bounded degree, specified by parameter $M$. For each inserted element $v$, first, a maximum layer index $l$ is stochastically chosen using an exponentially decaying probability distribution, normalized by the constant $m_L = 1/ln(M)$. The level assignment probability ensures that the expected characteristic path length increases with the level index. Intuitively, the upper-most level contains the longest-range links, which will be traversed first by the search algorithm, and the bottom-most level contains the shortest-range links, which are traversed last by the search algorithm. The insertion procedure then proceeds in two phases. In the first phase, a greedy search is performed iteratively from the top layer, beginning at a pre-defined entrypoint down to the $(l+1)$th layer. At each of these levels, the greedy subroutine chooses a single node that becomes the entry-point into the next layer. In the second phase, the greedy search iterates over level $l$ to level 0. The greedy search at each level now chooses $\textit{efc}$ nodes as candidate edges. Of these candidates, at most $M$ are selected to become neighbors of $v$ according to an RNG-based pruning algorithm [@jaromczyk_relative_1992]. At level 0, the degree bound is increased $2\times M$, which is shown to empirically improve performance.

***The HNSW search algorithm*** begins its traversal from a pre-defined entry point at the upper-most layer of the multilayer graph, illustrated in Figure [1](#fig:hnsw_diagram){reference-type="ref" reference="fig:hnsw_diagram"}. The traversal then follows an iterative search strategy from the top level downwards. At each level a greedy search is used to choose a single node, which becomes the entry-point into the next level. Once the bottom level is reached, rather than greedily choosing a single node, the search algorithm greedily chooses $K$ nearest elements to return. We outline this process in Algorithm [\[alg:hnsw-ann-search\]](#alg:hnsw-ann-search){reference-type="ref" reference="alg:hnsw-ann-search"}. The search parameter $\textit{efs}$ provides a tradeoff between search quality and efficiency by controlling the size of the dynamic candidate list stored during the bottom level's greedy search.

::: algorithm
$e \gets$ entry-point to hnsw graph\
$W \gets \emptyset$  $L \gets level(e)$   $W \gets$ SEARCH-LAYER($x_q, e, \textit{ef}=\textit{efs}, l=0$) 

 
:::

# Problem Definition and Challenges

In this section we formally define the hybrid search setting and then analyze the performance challenges that existing predicate-agnostic methods, i.e., pre- and post-filtering, face. Our analysis leads us to explore several important workload characteristics. Specifically, we will consider predicate selectivity, the dataset size, and *query correlation*, which we introduce, formally define and find to be a major challenge for post-filtering methods.

## Hybrid Search Definitions {#subsec:hybrid_search_def}

Let $D = \{e_1, e_2, ..., e_n\} = \{(x_1, a_1), (x_2, a_2), ..., (x_n, a_n)\}$ be a dataset consisting of $n$ entities, each with a vector component, $x_i \in \mathbb{R}^d$, and a structured attribute-tuple, $a_i$, associated with entity $e_i$. Let $X = \{x_1, x_2, ..., x_n\}$ denote the set of vectors in the dataset, and $dist(a,b)$ is the metric distance between any two points. Let $A = \{a_1, a_2, ..., a_n\}$ be the set of structured attributes in the dataset. We will denote $X_p \subseteq X$ as the subset of vectors corresponding to entities in the dataset that pass a given predicate $p$. We refer to the *selectivity* $(s)$ of predicate $p$ as the fraction of entities from $D$ that satisfy the predicate, where $0 \leq s \leq 1$.

We consider the ***hybrid search problem***, described as follows. Given a dataset $D$, target $K$, and query $q = (x_q, p_q)$, where $x_q \in \mathbb{R}^d$, and $p_q$ is a predicate, retrieve $x_q$'s $K$ nearest neighbors that pass the predicate $p_q$. We will specifically focus on the problem of *approximate* nearest neighbor search w.r.t $x_q$. Here, our goal is to maximize both search accuracy and search efficiency. We will measure accuracy by $\textit{recall}@K = \frac{G \cap R}{K}$, where $G$ is the ground truth set of $K$ nearest neighbors to $x_q$ that satisfy $p_q$, and $R$ is the retrieved set.

## Search Performance of Baseline Methods {#subsec:prelim_hybrid_search_perf}

We now analyze the search complexity of two predominant baseline methods, pre- and post-filtering. We will consider how varied workload characteristics impact the search behavior of these methods. Through our analysis, we will make the standard assumption that distance computations dominate search performance. We note that HNSW's unfiltered search complexity is $O(\log(n) + K)$.

***Pre-filtering*** linearly scans $X_p$, computing distances over each point that passes the search predicate. This yields a hybrid search complexity of $O(|X_p|) = O(sn + K)$. While pre-filtering always achieves perfect recall, its search complexity scales poorly for large dataset sizes or selectivities, growing linearly in either variable.

***Post-filtering***, by contrast, performs ANN-search over $X$ to find the closet query vector to $x_q$, then expands the search scope to find $K$ vectors that pass the query predicate, $p$. Intuitively, search performance varies depending on *correlation* between the query vector and the vectors in $X_p$. When the vectors of $X_p$ are close to the query vector, post-filtering over HNSW has a search complexity of $O(\log(n) + K)$. If the vectors in $X_p$ are uniformly distributed within $X$, then post-filtering's expected search complexity is $O(\log(n) + K/s)$. However, vectors of $X_p$ may be far away from the query vector, leading to a worst case of $O(n)$ search performance.

We see that the search performance of either baseline *is not robust* to variations in selectivity, dataset size, and query correlation. We empirically verify these limitations in section [7](#sec:eval){reference-type="ref" reference="sec:eval"} (figures [20](#fig:tripclick){reference-type="ref" reference="fig:tripclick"}, [24](#fig2:laion1m_correlation){reference-type="ref" reference="fig2:laion1m_correlation"}).

### Formalizing Query Correlation {#subsec:prelim_workload_characteristics}

We will now formalize the notion of *query correlation*, which we find is key challenge for post-filtering-based systems. As Figure [2](#fig4:correlation_and_clustering){reference-type="ref" reference="fig4:correlation_and_clustering"} shows, query correlation occurs when the vectors of $X_p$ are non-uniformly distributed in $X$ and instead cluster together relative to the vectors in X. We refer to this phenomenon as *predicate clustering*. When predicate clustering occurs, a query vector may be either close or far away from the predicate cluster containing its search targets, inducing query correlation.

<figure id="fig4:correlation_and_clustering" data-latex-placement="t">
<img src="diagrams/clustering_and_correlation.png" style="width:70.0%" />
<figcaption>Schematic drawing of a dataset with no predicate clustering (top), a dataset with predicate clustering and positive query correlation (middle), and a dataset with predicate clustering and negative query correlation (bottom). Dark blue circles show points that pass the predicate, and light gray circles show points that fail the predicate. The query vectors are shown in green.</figcaption>
</figure>

***Definition: Query Correlation.***

# Theoretical Ideal Hybrid Search Performance with HNSW {#sec:prelim_theoretical_ideal}

For a given hybrid search query, we define the theoretically ideal search performance using HNSW data structures as the performance attainable if we knew the search predicate $p_q$ during construction. In this case, we could construct an HNSW index over $X_p$. We call this the ***oracle partition index*** for that query. The complexity of searching this index is $Os(\log sn + K)$. Notably, the search performance of the oracle partition index outperforms both pre- and post-filtering across variations in predicate selectivity, data size, and query correlation. While pre-filtering's search scales in $|X_p|$, search over the oracle partition scales *sublinearly* in $|X_p|$. The oracle partition is also robust under variations in query correlation: it does not require the search scope expansion used in post-filtering.

Despite its ideal search performance, the oracle partition index requires us to know all search predicates in advance and to create a full HNSW index per predicate. In practice, the oracle partition index is not possible to construct because query predicate sets are often unknown during construction and have high or unbounded cardinality. Building an HNSW per predicate would require prohibitive amounts of space and time. Thus, in this work, we will instead approximate search over the oracle partition index for a particular query, without ever explicitly constructing this index.

# ACORN Overview

<figure id="fig:predicate subgraph" data-latex-placement="t!">
<img src="diagrams/predicate_subgraph.png" style="width:30.0%" />
<figcaption>An illustration of the predicate subgraph, shown by the green nodes. ACORN searches over the predicate subraph to emulate search over an oracle partition index.</figcaption>
</figure>

<figure id="fig:search_diagram" data-latex-placement="hbt!">
<img src="diagrams/search.png" style="width:70.0%" />
<figcaption aria-hidden="true"></figcaption>
</figure>

We now describe ACORN, a predicate-agnostic approach for state-of-the-art hybrid search. We propose two variants, which we refer to as **ACORN-$\gamma$** ([5.1](#subsec:acorn-gamma-search){reference-type="ref" reference="subsec:acorn-gamma-search"}, [5.2](#subsec:acorn-gamma-construction){reference-type="ref" reference="subsec:acorn-gamma-construction"}) and **ACORN-1** ([5.3](#subsec:acorn-1){reference-type="ref" reference="subsec:acorn-1"}). We design ACORN-$\gamma$ to achieve efficient search performance, and we design ACORN-1 to approximate ACORN-$\gamma$'s search performance while further reducing the algorithm's time to index (TTI) and space footprint for resource-constrained settings.

ACORN's core idea is to search over the index's *predicate subgraph*, i.e., the subgraph induced by $X_p$ for a given search predicate $p$, as shown in Figure [3](#fig:predicate subgraph){reference-type="ref" reference="fig:predicate subgraph"}. We modify the HNSW construction algorithms so that arbitrary predicate subgraphs emulate an HNSW oracle partition index without the need to explicitly construct one. ACORN-$\gamma$ achieves this by constructing a denser version of HNSW, which we parameterize by a neighbor list expansion factor, $\gamma$, a compression factor, $M_\beta$, and the HNSW parameters, $\textit{efc}$ and $M$. Then by adding a filter step during search to ignore neighbors that fail the predicate, we find ACORN-$\gamma$'s search can efficiently navigate to and traverse over the predicate subgraph, even under variations in query correlation. Meanwhile, ACORN-$1$ expands neighbor lists *during search* rather than during construction to *approximate* ACORN-$\gamma$'s dense graph structure without building it.[^1]

Overall, ACORN prescribes a simple and general framework for performant hybrid search based on the idea of *predicate-subgraph traversal*. The core techniques we propose are *predicate-agnostic* neighbor-list expansions and pruning during construction in combination with predicate-based filtering during search. While this framework can be applied to a variety of graph-based ANN indices, in this work we focus on HNSW due their state-of-the-art performance and widespread use.

::: small
:::

## ACORN-$\gamma$ Search Algorithm {#subsec:acorn-gamma-search}

Algorithm [\[alg:acorn-search-layer\]](#alg:acorn-search-layer){reference-type="ref" reference="alg:acorn-search-layer"} outlines the greedy search algorithm ACORN uses at each level, beginning from the top level at a pre-defined entry-point. The main difference between ACORN's search algorithm and that of HNSW is how neighbor look-ups (line 9) are performed at each visited node, $c$. While HNSW simply checks the neighbor list, $N^l(c)$, ACORN performs additional steps to recover an appropriate neighborhood for the given search predicates.

::: algorithm
$T \gets e$ $C \gets e$ $W \gets e$

 
:::

Specifically, ACORN-$\gamma$ uses two neighbor look-up strategies, a simple filter method, shown in Figure [4](#fig:search_diagram){reference-type="ref" reference="fig:search_diagram"}(a), and a compression-based heuristic, shown in Figure [4](#fig:search_diagram){reference-type="ref" reference="fig:search_diagram"}(b), which is compatible with the compression strategy we optionally apply during construction, detailed in Section [5.2](#subsec:acorn-gamma-construction){reference-type="ref" reference="subsec:acorn-gamma-construction"}. For each visited node, $v$, the filter-based neighbor look-ups simply scan the neighbor list $N^l(v)$ to find the sub-list of neighbors that pass the predicate, $N_p^l(v)$. If $N_p^l(v)$ contains more than $M$ nodes, we take the first $M$ and return this as $v$'s neighborhood. The compression-based neighbor look-ups instead partially expand the neighbor set $N^l(v)$ to include a subset of $v$'s two-hop neighbors, before performing filtering and truncation. This procedure entails two phases. The first phase iterates through the first $M_\beta$ nodes of $N^l(v)$, simply filtering as in the previous strategy. The second phase iterates over the remainder of the neighbor list, expanding the search neighborhood to include neighbors of neighbors, before again filtering according to the query predicate. $M_\beta$ is a construction parameter which we will discuss in the next section.

## ACORN-$\gamma$ Construction Algorithm {#subsec:acorn-gamma-construction}

We construct the ACORN-$\gamma$ index by applying two core modifications to the HNSW indexing algorithm: first, we expand each node's neighbor list, and Both of these steps are summarized in Figure [5](#fig:construction_diagram){reference-type="ref" reference="fig:construction_diagram"}.

***Neighbor List Expansion.*** While HNSW collects $M$ approximate nearest neighbors as candidate edges for each node in the index, ACORN collects $M\cdot \gamma$ approximate nearest neighbors as candidate edges per node.

One simple choice for $\gamma$ is $\frac{1}{s_{min}}$, where $s_{min}$ is the minimum predicate selectivity we plan to serve before resorting to pre-filtering. This leads to a simple *cost-based model* during search: if the estimated predicate selectivity of a given query is greater than $1/\gamma$, search the ACORN-$\gamma$ index, otherwise pre-filter.

***Compression.*** A key challenge with ACORN-$\gamma$'s neighbor expansion step is that it increases index size and TTI. The increased index size poses a significant issue particularly for memory-resident graph indices, like HNSW. To address this, we introduce a *predicate-agnostic pruning* technique. While we could apply compression to the full index, as discussed in Section [6.1](#subsec:discussion_indexsize){reference-type="ref" reference="subsec:discussion_indexsize"}, we specifically target the bottom level's neighbor lists since they contribute most significantly to the indexing overhead. This follows from the exponentially decaying level assignment probability ACORN uses.

The core idea of the pruning procedure is to precisely retain each node's nearby neighbors in the index, while approximating farther away neighbor during search. During search we can recover the first $M_\beta$ neighbors of each node $v$ directly from the neighbor list $N^l(v)$, and approximate remaining neighbors by looking at 2-hop neighbors during search, as we described in Section [5.1](#subsec:acorn-gamma-search){reference-type="ref" reference="subsec:acorn-gamma-search"}.

Figure [5](#fig:construction_diagram){reference-type="ref" reference="fig:construction_diagram"} outlines this pruning procedure applied to node $v$'s candidate neighbor list. The algorithm iterates over the ordered candidate edge list and keeps the first $M_\beta$ candidates.

<figure id="fig:construction_diagram" data-latex-placement="!t">
<img src="diagrams/construction.png" style="width:100.0%" />
<figcaption>A comparison of HNSW and ACORN-<span class="math inline"><em>γ</em></span>’s strategies for (a) selecting candidate edges, shown for <span class="math inline"><em>M</em></span>=3, and (b) pruning candidate edges for each inserted node <span class="math inline"><em>v</em></span>, shown for <span class="math inline"><em>M</em></span>=3, <span class="math inline"><em>M</em><sub><em>β</em></sub></span>=2, <span class="math inline"><em>γ</em></span>=2.</figcaption>
</figure>

We now briefly describe why HNSW's pruning, a *metadata-blind* mechanism, is insufficient for hybrid search. Consider the simple scenario shown in Figure $\ref{fig:construction_diagram}$. For a node, $v$, inserted into the HNSW index at an arbitrary level $l$, the algorithm generates candidates neighbors $a, b$ and $c$. HNSW's pruning rule iterates over $v$'s candidate neighbor list in order of nearest to farthest neighbors. Node $b$ is pruned since there exists a neighbor $a$ such that $b$ is closer to $a$ than to $v$. This RNG-approximation strategy corresponds to pruning the longest edge of the triangle formed by a triplet $v, a, b$. In this case, we can prune the edge $v - b$ and expect a search path to traverse from $v$ to $b$ via $a$. The problem with this technique arises when we consider the hybrid search setting for an arbitrary predicate. Say $v$ and $b$ pass a given query predicate $p_q$, but $a$ does not. Then $v, b, a$ do not form a triangle in the predicate subgraph, and we cannot expect to find the path from $v$ to $b$ through $a$. As a result, HNSW's pruning mechanism will falsely prune edge $v-b$. If we had complete knowledge of all possible query predicates, we could ensure that we only prune edges of triangles such that all three vertices always exist in the same subset of possible predicate subgraphs. FilteredDiskANN [@gollapudi_filtered-diskann_2023] takes this approach by restricting the set of possible query predicates. However, for arbitrary query predicates, ensuring this property holds becomes intractable.

## ACORN-1 {#subsec:acorn-1}

We now describe ACORN-1, an alternative approach which aims to approximate ACORN-$\gamma$'s search performance, while further minimizing index size and TTI. ACORN-1 achieves this by performing the neighbor expansion step solely during search, rather than during construction, as ACORN-$\gamma$ does. ACORN-1's construction corresponds to the original HNSW index without pruning. This construction corresponds to ACORN-$\gamma$'s construction algorithm, with fixed parameters $\gamma = 1$ and $M_\beta = M$.

ACORN-1's main difference from ACORN-$\gamma$ during search, is its neighbor lookup strategy. Specifically, at each visited node, $v$, during greedy search, ACORN-1 uses a full neighbor list expansion to consider all one-hop and two-hop neighbors of $v$, before applying the predicate filter and truncating the resulting neighbor list to size $M$. Figure [4](#fig:search_diagram){reference-type="ref" reference="fig:search_diagram"}(c) outlines this procedure.

# Discussion {#sec:discussion}

In this section we analyze the ACORN index's space complexity, construction complexity and search performance. We focus our attention on ACORN-$\gamma$, since ACORN-1's index construction represents a special case of ACORN-$\gamma$ for fixed parameters ($\gamma=1$, $M_{\beta}=M$), and we empirically show that ACORN-1 search approximates ACORN-$\gamma$ in Section [7](#sec:eval){reference-type="ref" reference="sec:eval"}.

## Index Size {#subsec:discussion_indexsize}

The average memory consumption per node of the ACORN-$\gamma$ index is $O(M_\beta + M + m_L\cdot M\cdot \gamma)$, assuming the number of bytes per edge is constant. For comparison, average memory consumption per node for the HNSW index scales $O(M + m_L\cdot M)$. Overall, ACORN-$\gamma$ increases the bottom-level's memory consumption by $O(M_\beta)$ per node, and increases the higher levels memory consumption by a factor of $\gamma$ per node.

To understand ACORN's memory consumption we evaluate the average number of neighbors stored per node. At level 0, compression is applied to the candidate edge lists of size $M*\gamma$ resulting in neighbor sets of length $M_\beta$ plus a compressed set which scales $O(M)$. We show this empirically in figure [30](#fig:pruning_comparison){reference-type="ref" reference="fig:pruning_comparison"}. On higher levels, nodes have at most $M*\gamma$ edges. We multiply this by the average number of levels that an element is added to, given by $\mathbb{E}[l + 1] = E[-\ln(\text{unif}(0,1)) * m_L] = m_L + 1$.

While we specifically target compression to level 0 in this work, because it uses the most space, compression could be applied to more levels in bottom-up order to further reduce the index size for large datasets. Denoting $nc$ as the chosen number of compressed levels, the average memory consumption per node in this generalized case is $O(nc(M_{\beta} + M) + (m_L - nc)(M\cdot \gamma)$.

## Construction Complexity {#subsec:discussion_construction}

For fixed parameters $M, M_\beta$ and $\textit{efc}$, ACORN-$\gamma$'s overall expected construction complexity scales $O(n\cdot \gamma\cdot \log(n)\cdot \log(\gamma))$. Compared to HNSW, which has $O(n\cdot \log(n))$ expected construction complexity, ACORN-$\gamma$ increases TTI by a factor of $\gamma\cdot \log(\gamma))$ due to the expanded edge lists it generates.

We now describe ACORN's construction complexity in detail by decomposing it into the following three factors **(i)** the number of nodes in the dataset, given by $n$ **(ii)** the expected number of levels searched to insert each node into the index, and **(iii)** the expected complexity of searching on each level. By design, ACORN's expected maximum level index scales $O(\log n)$ according to its level-assignment probability, which is the same as HNSW. This provides our bound on (ii).

Turning our attention to (iii), we will first consider the length of the search path and then consider the computation cost incurred at each visited node. For the HNSW level probability assignment, it is known that the expected greedy search path length is bounded by a constant $S=\frac{1}{1- \exp(-m_L)}$ [@malkov_efficient_2018]. We can bound ACORN's expected search path length by $O(\gamma)$ since the path reaches a greedy minima in a constant number of steps and proceeds to expand the search scope by at most $M\cdot \gamma$ nodes to collect up to $M\cdot \gamma$ candidate neighbors during construction.

The computation complexity at each visited node along the search path is $O(\log(\gamma))$, seen as follows. For each node visited, we first check its neighbor list to find at most $M$ un-visited nodes, on which we perform distance computations in $O(M\cdot d)$ time. Then, we update the sorted lists of candidate nodes and results in $O(M\cdot d\cdot log(\gamma\cdot M))$ time. Treating $M$ and $\gamma$ as constants, we see that at each visited node the computation complexity is $O(\log\gamma)$ and for greedy search at each level, the complexity is $O(\gamma\cdot \log(\gamma))$. Multiplying by $n\cdot \log(n)$ yields ACORN's final expected construction complexity, $O(n\cdot \gamma\cdot \log(n)\cdot \log(\gamma)$.

## Search Analysis {#subsec:discussion_search}

Turning our attention to ACORN-$\gamma$'s search algorithm, we will first point out several properties of HNSW that ACORN's predicate subgraphs aim to emulate. In Figure [10](#fig:sift_and_paper){reference-type="ref" reference="fig:sift_and_paper"} we empirically show that ACORN's search performance approximates that of the HNSW oracle partition index. We will then describe ACORN's expected search complexity. We define $l: X \to \mathbb{N}$ to be the mapping of nodes to there maximum level index in ACORN-$\gamma$.

### Index and Search Properties

Intuitively, for a given query, ACORN's predicate subgraph will emulate the HNSW oracle partition index when the predicate subgraph forms a hierarchical structure, each node in the subgraph has degree close to $M$, the subgraph has a fixed entrypoint at its maximum level index that we can efficiently find during search, and the subgraph is connected. We will examine each of these properties separately and consider when they hold. We also note one main difference between ACORN's predicate subgraphs and HNSW that arises due to ACORN's predicate-agnostic pruning: each level of ACORN approximates a KNN graph, while each level of HNSW approximates a RNG graph. While this difference does not affect ACORN's expected search complexity in Section [6.3.2](#subsubsec:search_complexity){reference-type="ref" reference="subsubsec:search_complexity"}, Malkov et al. [@malkov_efficient_2018] demonstrated that the RNG-based pruning empirically improves performance.

***Hierarchy.*** First, we observe that the arbitrary predicate subgraph $G(X_p)$ forms a controllable hierarchy similar to the HNSW *oracle partition index* built over $X_p$ with parameter $M$. This is by design. ACORN-$\gamma$'s construction fixes $M$, and consequently $m_L$, the level normalization constant. As a result, nodes of $X_p$ in the ACORN-$\gamma$ index are sampled at rates equal to the level probabilities of the HNSW partition. Ensuring this level sampling holds allows us to bound the expected greedy search path length at each level by a constant, $S$, as Malkov et al. [@malkov_efficient_2018] previously show.

***Bounded Degree.*** Next, we will describe degree bounds, an important factor that impacts greedy search efficiency and convergence. While HNSW upper bounds the degree of each node by M *during construction*, ACORN-$\gamma$ enforces this upper bound *during search*. This ensures ACORN's search performs a constant number of distance computations per visited node. We now focus our attention on lower bounding the degree of nodes visited during ACORN-$\gamma$'s search over the predicate subgraph.

If a node in the predicate subgraph has degree much lower than $M$, this could adversely impact the search convergence and thus recall. For a dataset and query predicate that exhibit no predicate clustering, for any node $v$ in $G(X_p)$, $$\mathbb{E} \bigr[|N_p^l(v)| \bigl] = |N^l(v)| \cdot s =  \gamma \cdot M \cdot s  > M, \forall s > s_{min}$$ This also holds as a lower bounds for datasets with predicate clustering, in which case $Pr(x \in N^l_p(v)) > s, \forall x \in N^l(v)$ where $v$ is a node in the predicate cluster. Thus we will continue our lower bound analysis of node degrees under the worst case assumption of no predicate clustering. Using the binomial concentration inequality with parameter $s$, and union-bounding over the expected search path length, we show that for the search path $\mathcal{P} = v_1 - ... - v_y$ over an arbitrary predicate subgraph:

We also analyze the probability that the subgraph traversal gets disconnected, which we bound by:

***Fixed Entry-point.*** Similar to HNSW, ACORN's search begins from a fixed entry-point, chosen during construction. This pre-defined entry-point provides a simple and effective strategy that is also predicate-independent and robust to variations in *query correlation*, as we empirically show in Figure [24](#fig2:laion1m_correlation){reference-type="ref" reference="fig2:laion1m_correlation"}.

Intuitively, we expect the search to successfully navigate from ACORN's fixed entry-point, $e$, to the predicate-subgraph entry-point, $e_p$, when we find a node that passes the predicate on an upper level of the index that is fully connected. In this case, there will exist a one-hop path from $e$ to $e_p$. We consider $e_p$ to be an arbitrary node that passes a given predicate $p$ and is on the maximum level of the predicate subgraph. The index's neighbor expansion parameter, $\gamma$, causes the index's upper levels to be denser and, specifically those with less than $M\cdot \gamma$ nodes, to be fully connected. When these fully connected levels contain at least one node that passes the predicate, the search is guaranteed to route from $e$ to $e_p$. Since ACORN samples all nodes with equal probability at each level, the probability that nodes passing a given predicate, $p$, exist on some level is proportional to the predicate's selectivity, which takes a lower bound of $s_{min} = 1/\gamma$.

***Connectivity.***

### Search Complexity {#subsubsec:search_complexity}

ACORN-$\gamma$'s expected search complexity scales: $$O((d + \gamma) \cdot \log(s\cdot n) + \log(1/s))$$ This approximates the HNSW oracle partition's expected search complexity, $O(d \cdot \log(s\cdot n))$. Intuitively, ACORN-$\gamma$'s search path performs some filtering at the upper levels before likely entering and traversing the predicate sub-graph, during which ACORN incurs a small overhead compared to HNSW search in order to perform the predicate filtering step over each neighbor list.

We derive ACORN-$\gamma$'s search complexity by considering two stages of its search traversal. In the first stage, search begins from a pre-defined entry-point $e$, which need not pass the query predicate. In this stage, the search performs filtering only, dropping down each level on which the filtered neighbor list, $N_p(e)$, is found to be empty. Once the traversal reaches the first node, $e_p$ that passes the predicate, it enters the second stage, beginning its traversal over the predicate subgraph $G(X_p)$.

In stage 1 the greedy search path on each layer has length 1, and occurs over $O(\log n - \log(s\cdot n))$ expected levels, yielding the complexity $O(\log(1/s)$. We see this because the expected maximum level index of the full ACORN index graph scales $O(\log n)$ based on its level-assignment probability [@malkov_efficient_2018]. Meanwhile, the predicate subgraph $G(X_p)$ of size $s\cdot n$ has an expected maximum level index of $O(\log(s\cdot n))$, once again according to its level sampling procedure.

The second stage of the search traverses the predicate subgraph in expected $O((d + \gamma)\cdot \log(s\cdot n))$ complexity. As we previously describe, the expected maximum level index of the predicate subgraph scales $O(\log(s\cdot n))$. At each level, the expected greedy path length can be bounded by a constant $S$ due to the index level sampling procedure employed during construction. For each node visited along the greedy path, we perform distance computations in $O(d)$ time on at most $M$ neighbors, and perform a constant-time predicate evaluations over at most $M\cdot \gamma$ neighbors.

# Evaluation {#sec:eval}

::: small
:::

We evaluate ACORN through a series of experiments on real and synthetic datasets. Overall, our results show the following:

- ACORN-$\gamma$ achieves state-of-the-art hybrid search performance, outperforming existing methods by 2-1,000$\times$ higher QPS at 0.9 recall on both prior benchmark datasets with simple, low-cardinality predicate sets, and more complex datasets with high-cardinality predicate sets.

- ACORN-$\gamma$ and ACORN-1 are predicate-agnostic methods, providing robust search performance under variations in predicate operators, predicate selectivity, query correlation, and dataset size.

- ACORN-1 and ACORN-$\gamma$ exhibit trade-offs between search performance and construction overhead. While ACORN-$\gamma$ achieves up to 5$\times$ higher QPS than ACORN-1 at fixed recalls, ACORN-1 can be constructed with 9-53$\times$ lower time-to-index (TTI).

We now discuss our results in detail. We first describe the datasets ([7.1](#subsec:Datasets){reference-type="ref" reference="subsec:Datasets"}) and baselines ([7.2](#subsec:benchmarked-methods){reference-type="ref" reference="subsec:benchmarked-methods"}) we use. Then, we present a systematic evaluation of ACORN's search peformance ([7.3](#subsec:search_perf){reference-type="ref" reference="subsec:search_perf"}). Finally, we assess ACORN's construction efficiency ([7.4](#subsec:construction_oh){reference-type="ref" reference="subsec:construction_oh"}). We run all experiments on an AWS m5d.24xlarge instance with 370 GB of RAM, 96 vCPUs, .

## Datasets {#subsec:Datasets}

We conduct our experiments on two datasets with low-cardinality predicate sets (LCPS) and two datasets with high-cardinality predicate sets (HCPS). The LCPS datasets allow us to benchmark prior works that only support a constrained set of query predicates. The HCPS datasets consist of more complex and realistic query workloads, allowing us to more rigorously evaluate ACORN's search performance. Table [\[tab:dataset\]](#tab:dataset){reference-type="ref" reference="tab:dataset"} provides a concise summary of all datasets.

### Datasets with Low Cardinality Predicate Sets {#subsubsec:lcps_dataset}

We use SIFT1M [@jegou_product_2011] and Paper [@wang_navigable_2022], the two largest publically-available datasets used to evaluate recent specialized indices [@wang_navigable_2022; @gollapudi_filtered-diskann_2023]. For both datasets, we follow related works [@gollapudi_filtered-diskann_2023; @wang_navigable_2022; @wangmilvus2021] to generate structured attributes and query predicates: for each base vector, we assign a random integer in the range $1-12$ to represent structured attributes; and for each query vector, the associated query predicate performs an exact match with a randomly chosen integer in the attribute value domain. The resulting query predicate set has a cardinality of 12.

[SIFT1M:]{.underline} The SIFT1M dataset was introduced by Jegou et al. in 2011 for ANN search. It consists of a collection of 1M base vectors, and 10K query vectors. All of the vectors are 128-dimensional local SIFT descriptors [@lowe_distinctive_2004] from INRIA Holidays images [@jegou_hamming_2008].

[Paper:]{.underline} Introduced by Wang et al. in 2022, the Paper dataset consists of about 2M base vectors and 10K query vectors. The dataset is generated by extracting and embedding the textual content from an in-house corpus of academic papers.

### Datasets with High Cardinality Predicate Sets {#subsubsec:hcps_datasets}

We use the TripClick and LAION datasets in our experiments with HCPS datasets.

[TripClick:]{.underline} The TripClick dataset, introduced by Rekabsaz et al. in 2021 for text retrieval, contains a *real* hybrid search query workload and base dataset from the click logs of a health web search engine.

[LAION:]{.underline} The LAION dataset [@schuhmann_laion-400m_2021] consists of 400M image embeddings and captions describing each image. The vector embeddings are generated from web-scraped images using CLIP [@radford_learning_2021], a multi-modal language-vision model. In our evaluation, we construct two base datasets using 1M and 25M LAION subsets, both consisting of image vectors and text captions as a structured attribute. We also generate an additional structured attribute consisting of a keyword list. We assign each image embedding its keyword list by taking the 3 words with highest text-to-image CLIP scores from a candidate list of 30 common adjectives and nouns (e.g., \"animal\", \"scary\").

To evaluate a series of micro-benchmarks, we generate four query workloads. For each query workload, we sample 1K vectors from the dataset as query vectors. We construct the regex query workload with predicates that perform regex-matching over the image captions. For each query predicate, we randomly choose strings of 2-10 regex tokens (e.g., \"`^[0-9]`\"). In addition, we construct three query workloads with predicates, similar to TripClick, that take a keyword list and filter out entities that do not have at least one matching keyword. Using this setup, we are able to easily control for correlation in the workload, and we generate a no correlation (no-cor), positive correlation (pos-cor), and negative correlation (neg-cor) workload. Figure [6](#fig:Sample laion query and results){reference-type="ref" reference="fig:Sample laion query and results"} demonstrates some example queries and multi-modal retrieval results taken from each.

<figure id="fig:Sample laion query and results" data-latex-placement="!t">
<img src="diagrams/correlation_samples.png" style="width:70.0%" />
<figcaption>The figure contrasts retrieval results using vector-only similarity search (bottom left) versus hybrid search (right) on the LAION dataset. Both use the same query image (top left), and the hybrid search queries also include a structured query filter consisting of a keyword list, here containing a single keyword. The table on the right shows examples from three hybrid search query workloads: positive query correlation (top), no query correlation (middle), and negative query correlation (bottom). </figcaption>
</figure>

## Benchmarked Methods {#subsec:benchmarked-methods}

We briefly overview the methods we benchmark along with tested parameters. We implement ACORN-$\gamma$, ACORN-1, pre-filtering, and HNSW post-filtering in C++ in the FAISS codebase [@noauthorfaiss2023].

[HNSW Post-filtering:]{.underline} To implement HNSW post-filtering, for each hybrid query with predicate selectivity $s$, we over-search the HNSW index, gathering $K / s$ candidate results before applying the query filter. We note that this differes to some prior work [@gollapudi_filtered-diskann_2023], where HNSW post-filtering is implemented by collecting only $K$ candidate results, leading to significantly worse baseline query performance than ours. For the SIFT1M, Paper and LAION datasets, we use the FAISS's default HNSW construction parameters: $M=32, \textit{efc}=40$. For the TripClick dataset, we find that the HNSW index for these parameters is unable to obtain high recalls for the standard ANN search task, thus we perform parameter tuning, as is standard. We perform a grid search for $M \in \{32, 64, 128\}$ and $\textit{efc} \in \{40, 80, 120, 160, 200\}$ and choose the pair the obtains the highest QPS at 0.9 Recall for ANN search. For TripClick, we choose $M=128, \textit{efc}=200$. We generate each recall-QPS curve by varying the search parameter $\textit{efs}$ from 10 to 800 in step sizes of 50.

[Pre-filtering:]{.underline}

[Filtered-DiskANN:]{.underline} We evaluate both algorithms implemented in FilteredDiskANN [@noauthor_diskann_2023], namely FilteredVamana and StitchedVamana. For both, we follow the recommended construction and search parameters according to the hyper-parameter tuning procedure described by Gollapudi et al. [@gollapudi_filtered-diskann_2023]. For FilteredVamana, we use construction parameters $L=90, R=96$, which generated the Pareto-Optimal recall-QPS curve from a parameter sweep over $R\in \{32, 64, 96\}$ and L between 50 and 100. For StitchedVamana, we use construction parameters $R_{small}=32, L_{small}=100, R_{stitched}=64$ and $\alpha=1.2$, which generated the Pareto-Optimal recall-QPS curve from a parameter sweep over $R_{small}, R_{stitched} \in \{32, 64, 96\}$ and $L_{small}$ between 50 and 100. To generate the recall-QPS curves we vary $L$ from 10 to 650 in increments of 20 for FilteredVamana, and $L_{small}$ from 10 to 330 in increments of 20 for StitchedVamana.

[NHQ]{.underline}: We evaluate the two algorithms, NHQ-NPG_NSW and NHQ-NPG_KGraph, proposed in [@wang_navigable_2022]. For both we use the recommended parameters in the released codebase [@ashenon3_nhq_2023]. These parameters were selected using a hyperparameter grid search in order to generate the Pareto-optimal recall-QPS curve for either algorithm on the SIFT1M and Paper datasets. We generate the recall-QPS curve by varying $L$ between 10 and 310 in steps of 20. In Figures [12](#fig1a:query_perf_sift){reference-type="ref" reference="fig1a:query_perf_sift"} and [9](#fig1b:query_perf_Paper){reference-type="ref" reference="fig1b:query_perf_Paper"}, we show the query performance of KGraph, the more performant of the two algorithms.

<figure id="fig:legend_large">
<p><img src="figures/legend_large.png" style="width:60.0%" alt="image" /> <span id="fig:legend_large" data-label="fig:legend_large"></span></p>
</figure>

<figure id="fig:sift_and_paper" data-latex-placement="!th">
<figure id="fig1a:query_perf_sift">
<img src="figures/sift1m.png" />
<figcaption>SIFT1M Dataset</figcaption>
</figure>
<figure id="fig1b:query_perf_Paper">
<img src="figures/paper.png" />
<figcaption>Paper Dataset</figcaption>
</figure>
<figcaption>Recall@10 vs QPS on SIFT1M and Paper</figcaption>
</figure>

[Milvus:]{.underline} We test four Milvus algorithms: IVF-Flat, IVF-SQ8, HNSW, and IVF-PQ [@noauthor_milvus_2023]. For each we test the same parameters as Gollapudi et al. [@gollapudi_filtered-diskann_2023]. Since we find that the four Milvus algorithms achieve similar search performance, for simplicity, Figures [12](#fig1a:query_perf_sift){reference-type="ref" reference="fig1a:query_perf_sift"} and [9](#fig1b:query_perf_Paper){reference-type="ref" reference="fig1b:query_perf_Paper"} show only the method with Pareto-Optimal recall-QPS performance.

[Oracle Partition Index]{.underline}: We implement this method by constructing an HNSW index for each possible query predicate in the LCPS datasets. For a given hybrid query, we search the HNSW partition corresponding to the query's predicate. To construct each HNSW partition and generate the recall-QPS curve, we use the same parameters as the HNSW post-filtering method, described above.

[ACORN-$\gamma$]{.underline}: We choose the construction parameters $M$ and $\textit{efc}$ to be the same as the HNSW post-filtering baseline, described above. We find that ACORN-$\gamma$'s search performance is relatively in-sensitive to the choice of the construction parameter $M_{\beta}$, as Figure [30](#fig:pruning_comparison){reference-type="ref" reference="fig:pruning_comparison"}c shows. Thus, to maintain modest construction overhead, we choose $M_\beta$ to be a small multiple of $M$, *i.e.,* $M_{\beta}=M$ or $M_{\beta}=2M$, picking the parameter for each dataset that obtains higher QPS at 0.9 Recall. We choose the construction parameter $\gamma$ according to the expected minimum selectivity query predicates of each dataset *i.e.,* $\gamma=12$ for SIFT1M and Paper, $\gamma=30$ for LAION, and $\gamma=80$ for TripClick. To generate the recall-QPS curve, we follow the same procedure described above for HNSW post-filtering.

[ACORN-1:]{.underline} We construct ACORN-$1$ and generate the recall-QPS curve following the same procedure we use for ACORN-$\gamma$, except that we fix $\gamma=1$ and $M_{\beta} = M$.

## Search Performance Results {#subsec:search_perf}

We will begin our evaluation with benchmarks on the LCPS datasets, on which we are able to run all baseline methods as well as the oracle partition method. We will then present an evaluation on the HCPS datasets. On these datasets, the FilteredDiskANN and NHQ algorithms fail because they assume are unable to handle the high cardinality query predicate sets and non-equality predicate operators. As of this writing, we also find that Milvus cannot support `regex-match` predicates and `contains` predicates over variable length lists. As a result, we instead focus on comparing ACORN to the pre- and post-filtering baselines for the HCPS datasets. We report QPS averaged over 50 trials.

### Benchmarks on LCPS Datasets {#subsubsec:search_perf_sift_and_paper}

Figure [10](#fig:sift_and_paper){reference-type="ref" reference="fig:sift_and_paper"} shows that ACORN-$\gamma$ achieves state-of-the-art hybrid search performance and best approximates the theoretically ideal *oracle partition strategy* on the SIFT1M and Paper datasets. Notably, even compared to NHQ and FilteredDiskANN, which specialize for LCPS datasets, ACORN-$\gamma$ consistently achieves 2-10$\times$ higher QPS at fixed recall values, while maintaining generality. Additionally, we see ACORN-1 approximates ACORN-$\gamma$'s search performance, attaining about 1.5-5$\times$ lower QPS than ACORN-$\gamma$ across a range of recall values.

:::::: small
::::: threeparttable
::: {#tab:dist_comps}
                          SIFT 1M             Paper
  ------------------ ------------------ ------------------
   Oracle Partition        398.0              281.1
    ACORN-$\gamma$     611.0 (+53.5%)     383.7 (+36.6%)
       ACORN-1        999.6 (+151.0%)    567.8 (+101.2%)
   HNSW Post-filter   1837.8 (+362.6%)   1425.5 (+406.2%)

  : \# Distance Computations to Achieve $0.8$ Recall
:::

::: tablenotes
Percentage difference is shown in parenthesis and is relative to oracle partition method
:::

[]{#tab:dist_comps label="tab:dist_comps"}
:::::
::::::

To further investigate the relative search efficiency of ACORN-$\gamma$ and ACORN-$1$, we turn our attention to Table [1](#tab:dist_comps){reference-type="ref" reference="tab:dist_comps"}, which shows the number of distance computations required of either method to obtain Recall@10 equal to 0.8. We see that the oracle partition method is the most efficient, requiring the fewest number of distance computations on both datasets. ACORN-$\gamma$ is the next most efficient according to number of distance computations. While ACORN-$\gamma$ approximates the oracle partition method, it's predicate-agnostic design precludes the same RNG-based pruning used to construct the oracle partitions. Rather than approximating RNG-graphs, ACORN-$\gamma$'s levels approximate KNN-graphs, which are less efficient to search over explaining the performance gap. The table additionally shows that ACORN-1 is less efficient than ACORN-$\gamma$, which is explained by the candidate edge generation used in ACORN-1. While the ACORN-$\gamma$ index stores up to $M\times \gamma$ edges per node during construction, ACORN-1 stores only up to $M$ edges per node during construction, and *approximates* an edge list of size $M*\gamma$ for each node during search using its neighbor expansion strategy. This approximation results in slight degradation to neighbor list quality and thus search performance. Finally, we see from the table, that HNSW post-filtering is the least efficient of the listed methods. This is because while ACORN-1 and ACORN-$\gamma$ almost exclusively traverse over nodes that pass the query predicates, the post-filtering algorithm is less discriminating and wastes distance computations on nodes failing the query predicate.

Returning to Figure [10](#fig:sift_and_paper){reference-type="ref" reference="fig:sift_and_paper"}, we see that the relative search efficiency, measured by QPS versus recall, of the oracle partition method, ACORN-$\gamma$, and ACORN-1 is not only affected by distance computations, but is also affected by vector dimensionality. We see that both ACORN-1 and ACORN-$\gamma$ perform closer to the oracle partition method on the Paper dataset, while the performance gap grows slightly on SIFT1M. This is due to the cost of performing the filtering step over neighbor lists during search, which, relative to the cost of distance computations, is higher on SIFT1M than Paper since SIFT1M uses slightly lower-dimensional vectors.

### Benchmarks on HCPS Datasets

Figure [14](#fig:tripclick_and_laion){reference-type="ref" reference="fig:tripclick_and_laion"} shows that ACORN outperforms the baselines by $30 - 50\times$ higher QPS at 0.9 recall on TripClick and LAION-1M, and as before, ACORN-1 approximates ACORN-$\gamma$'s search performance. On both datasets, pre-filtering is prohibitively expensive, obtaining perfect recall at the price of efficiency. Meanwhile, post-filtering fails to obtain high recall, likely due to the presence of varied query correlation and predicate selectivity, which we further explore further next.

<figure id="fig:tripclick_and_laion" data-latex-placement="!t">
<figure id="fig1a:query_perf_sift">
<img src="figures/tripclick_sel-1.png" />
<figcaption>TripClick (areas)</figcaption>
</figure>
<figure id="fig1a:query_perf_sift">
<img src="figures/tripclick_dates.png" />
<figcaption aria-hidden="true"></figcaption>
</figure>
<figure id="fig1b:query_perf_tripclick">
<img src="figures/laion1m_regex.png" />
<figcaption>LAION1M (regex)</figcaption>
</figure>
<figcaption>Recall@10 vs QPS on TripClick and LAION-1M</figcaption>
</figure>

[Varied Predicate Selectivity:]{.underline}[]{#subsubsec:search_sel label="subsubsec:search_sel"} We use the Tripclick dataset to evaluate ACORN's search performance across a range of realistic predicate selectivities. Figure [20](#fig:tripclick){reference-type="ref" reference="fig:tripclick"} demonstrates that for each predicate selectivity percentile, ACORN-$\gamma$ achieves 5-50x higher QPS at 0.9 recall compared to the next best-performing baseline. Once again ACORN-1 trails behind ACORN-$\gamma$. We see that for low selectivity predicates, the pre-filtering method is most competitive, while the post-filtering baselines suffers from over 10$\times$ lower QPS than ACORN at fixed recall. However, for high selectivity predicates, pre-filtering becomes less competitive while the post-filtering baseline obtains higher throughput, although its recall remains low.

[Varied Query Correlation:]{.underline}[]{#subsubsec:search_corr label="subsubsec:search_corr"} Next we control for query correlation and evaluate ACORN on three different query workloads using the LAION-1M dataset. Figure [24](#fig2:laion1m_correlation){reference-type="ref" reference="fig2:laion1m_correlation"} demonstrates that ACORN-$\gamma$ is robust to variations in query correlation and attains 28-100$\times$ higher QPS at 0.9 recall than the next best baseline in each case. In the negative correlation case, the performance gap between post-filtering and the ACORN methods is the largest since post-filtering cannot successfully route towards nodes that pass the predicate. In the positive correlation case, ACORN-$\gamma$ once again outperforms the baselines, but post-filtering become more competitive, although it is still unable to attain recall above 0.9. The pre-filtering method's QPS remains relatively unchanged, and is only affected by small variations in predicate selectivity for each query workload. As before, ACORN-1 approaches ACORN-$\gamma$'s search performance.

<figure id="fig:tripclick" data-latex-placement="!ht">
<figure id="tripclick_1p">
<img src="figures/tripclick_sel1.png" />
<figcaption>1p Sel (s=0.0127)</figcaption>
</figure>
<figure id="tripclick_25p">
<img src="figures/tripclick_sel25.png" />
<figcaption>25p Sel (s=0.0485)</figcaption>
</figure>
<figure id="tripclick_50p">
<img src="figures/tripclick_sel50.png" />
<figcaption>50p Sel (s=0.1215)</figcaption>
</figure>
<figure id="tripclick_75p">
<img src="figures/tripclick_sel75.png" />
<figcaption>75p Sel (s=0.2529)</figcaption>
</figure>
<figure id="tripclick_99p">
<img src="figures/tripclick_sel99.png" />
<figcaption>99p Sel (s=0.6164)</figcaption>
</figure>
<figcaption>Recall@10 vs QPS for Varied Selectivity Query Filters on TripClick</figcaption>
</figure>

<figure id="fig2:laion1m_correlation" data-latex-placement="!tb">
<figure id="laion_negcorr">
<img src="figures/laion1m_negcorr.png" />
<figcaption>Neg. Correlation</figcaption>
</figure>
<figure id="laion_noscorr">
<img src="figures/laion1m_nocorr.png" />
<figcaption>No Correlation</figcaption>
</figure>
<figure id="laion_poscorr">
<img src="figures/laion1m_poscorr.png" />
<figcaption>Pos. Correlation</figcaption>
</figure>
<figcaption>Recall@10 vs QPS on LAION1M</figcaption>
</figure>

<figure id="fig4:laion_scaling" data-latex-placement="tb">
<img src="figures/laion25m.png" style="width:35.0%" />
<figcaption>Recall@10 vs QPS on LAION-25M</figcaption>
</figure>

[Scaling Dataset Size:]{.underline}[]{#subsubsec:search_scale label="subsubsec:search_scale"} Figure [25](#fig4:laion_scaling){reference-type="ref" reference="fig4:laion_scaling"} shows ACORN's search performance on LAION-25M with the no-correlation query workload, demonstrating that the performance gap between ACORN and existing baselines only grows as the dataset size scales. At 0.9 recall, ACORN-$\gamma$ achieves over three orders of magnitude higher QPS than the next best-performing baseline. As before, ACORN-1's search performance approximates that of ACORN-$\gamma$.

## Index Construction {#subsec:construction_oh}

### TTI and Space Footprint

First, we analyze ACORN's space footprint and indexing time. Table [\[tab:TTI\]](#tab:TTI){reference-type="ref" reference="tab:TTI"} and [\[tab:indexsize\]](#tab:indexsize){reference-type="ref" reference="tab:indexsize"} show the time-to-index and index size of ACORN-$\gamma$ and ACORN-$1$ compared to the best-performing baselines.

We first consider ACORN-$\gamma$'s construction overhead. Table [\[tab:TTI\]](#tab:TTI){reference-type="ref" reference="tab:TTI"} shows that across all datasets, ACORN-$\gamma$'s TTI is at most 11$\times$ higher than HNSW's, and at most 2.15$\times$ higher than that of StitchedVamana, the best performing specialized index. We see that while ACORN-$\gamma$ achieves superior search performance by leveraging a neighbor-list expansion during *construction*, ACORN-1 provides a close approximation at lower TTI and space footprint by instead performing the neighbor-list expansion during *search*. The two algorithms exhibit a trade-off between search performance and construction overhead.

### ACORN-$\gamma$ Pruning

Given ACORN-$\gamma$'s higher construction overhead, we investigate the efficiency of its predicate-agnostic compression strategy in reducing index construction costs while maintaining search performance.

::: small
[]{#tab:TTI label="tab:TTI"}
:::

::: small
[]{#tab:indexsize label="tab:indexsize"}
:::

::: small
[]{#tab:avg_out_deg label="tab:avg_out_deg"}
:::

Turning our attention to Figure [30](#fig:pruning_comparison){reference-type="ref" reference="fig:pruning_comparison"}, we evaluate three different pruning strategies applied to ACORN-$\gamma$'s neighbor lists during construction: **i)** ACORN's predicate-agnostic pruning strategy at varied levels of compression indicated by different $M_{\beta}$ (Mb) values, where $Mb=768$ represents no pruning, and lower values represent more aggressive pruning, **ii)** a metadata-aware RNG-based pruning approach, which is employed by FilteredDiskANN's algorithms, and **iii)** HNSW's metadata-blind pruning. In addition, the figure shows search performance measured by recall at 20,000 QPS. We note that the recall ranges of the recall-QPS curve generated by different pruning methods varied significantly, leading us to choose a QPS threshold rather than a recall threshold. Meanwhile the metadata-aware RNG-base pruning results in similar search performance to ACORN-$\gamma$'s pruning, but it is less efficient by TTI and space footprint than ACORN's pruning for small values of $M_\beta$ (e.g., $M_\beta = 32, 64)$.

::: small
<figure id="fig:pruning_comparison" data-latex-placement="t">
<p><img src="figures/pruning_legend.png" alt="image" /> <span id="fig:pruning_legend" data-label="fig:pruning_legend"></span></p>
<figure id="fig:pruning_tti">
<img src="figures/pruning_tti.png" />
<figcaption>TTI</figcaption>
</figure>
<figure id="fig:pruning_index_size">
<img src="figures/pruning_index_size.png" />
<figcaption aria-hidden="true"></figcaption>
</figure>
<figure id="fig:pruning_num_pruned">
<img src="figures/pruning_num_pruned.png" />
<figcaption aria-hidden="true"></figcaption>
</figure>
<figure id="fig:acorn_pruning_search">
<img src="figures/acorn_pruning_search.png" />
<figcaption>Search Perf.</figcaption>
</figure>
<figcaption>Comparison of pruning methods on SIFT1M and their impact on TTI (a), space footprint of the index (b), the number of candidate edges pruned (c) and search performance (d). <span class="math inline"><em>M</em><sub><em>β</em></sub></span> values used for ACORN-<span class="math inline"><em>γ</em></span> are shown along the x-axis.</figcaption>
</figure>
:::

<figure id="fig:graph_quality" data-latex-placement="t">
<p><img src="figures/tripclick_connectivity_legend.png" alt="image" /> <span id="fig:graph_quality_legend" data-label="fig:graph_quality_legend"></span></p>
<figure id="fig:num_scc">
<img src="figures/tripclick_connectivity_num_scc.png" />
<figcaption># SCC</figcaption>
</figure>
<figure id="fig:graph_height">
<img src="figures/tripclick_connectivity_graph-height.png" />
<figcaption>Graph Height</figcaption>
</figure>
<figure id="fig:out_degree">
<img src="figures/tripclick_connectivity_out-degree.png" />
<figcaption>Avg Out Degree</figcaption>
</figure>
<figcaption></figcaption>
</figure>

# Related Work

***Pre- & Post-filtering-based Systems.*** Many hybrid search systems rely on pre- and post-filtering. While several systems have developed pre-processing methods to perform *faster filtering* during search, these systems fail to reduce the *excessive and expensive distance computations* which bottleneck performance. Weaviate [@noauthor_filtered_nodate] creates an inverted index for structured data ahead of time, then uses it at query time to create a bitmap of eligible candidates during post-filtering. Milvus [@wangmilvus2021] likewise creates an approved list of points by maintaining a distribution of attributes over the dataset in order to map commonly used query filters to a list of approved points before performing pre- or post-filtering. Several space-partitioning indices like FAISS-IVF [@baranchuk_revisiting_2018; @johnson_billion-scale_2017] and LSH [@andoni_practical_2015] store metadata information in the index, allowing them to rapidly filter entities during post-filtering. Despite the optimized filtering steps in each of these approaches, the core problems of pre- and post-filtering remain, particularly for low correlation or selectivity predicates.

***Specialized Indices.*** Alternatively, several recent works develop novel graph-based algorithms for hybrid search, often improving performance for a constrained set of predicates. NHQ [@wang_navigable_2022] encodes attributes alongside vectors, and then uses a \"fusion distance\" during search that accounts for vector distances as well as attribute matches. This approach supports only equality query predicates and assumes each dataset entity has only one structured attribute. Filtered-DiskANN [@gollapudi_filtered-diskann_2023] proposes two algorithms: FilteredVamana and StitchedVamana. Both methods constrain the query filter cardinality to about $1,000$ with only equality predicates so that the index construction steps can use this knowledge to appropriately generate and prune candidate edge lists. Similarly HQI [@mohoney_high-throughput_2023] optimizes batch query-processing by assuming a limited cardinality of $20$ query predicates to design an efficient partitioning scheme.

# Conclusion

We proposed ACORN, the first approach for efficient hybrid search across vectors and structured data that supports large and diverse sets of query predicates. ACORN uses a simple, yet effective, search strategy based on the core idea of *predicate subgraph traversal*. We presented two indices, ACORN-$\gamma$ and ACORN-1, that implement this search strategy by modifying the HNSW indexing algorithm. Our results show that ACORN achieves state-of-the-art hybrid search performance on both prior benchmarks, involving simple, low-cardinality query predicate sets, as well as more complex benchmarks involving new predicate operators and high cardinality predicate sets. Across both types of benchmarks, ACORN-$\gamma$ achieves 2--1,000$\times$ higher QPS at 0.9 recall than prior methods, and ACORN-1 approximates ACORN-$\gamma$'s search performance with 9--53$\times$ lower TTI for resource-constrained settings.

::: acks
The authors would like to thank Peter Bailis for his valuable feedback on this work.

This research was supported in part by affiliate members and other supporters of the Stanford DAWN project, including Meta, Google, and VMware, as well as Cisco, SAP, and a Sloan Fellowship. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors.
:::

[^1]: For highly selective queries where even ACORN's predicate subgraph would be disconnected within the larger ACORN graph, ACORN falls back to pre-filtering, which is effective for such queries. ACORN is configured with a minimum selectivity, $s_{min}$, under which it should use pre-filtering when a query is estimated to be more selective than $s_{min}$. We describe how to configure $\gamma$ based on $s_{min}$ in Section [5.2](#subsec:acorn-gamma-construction){reference-type="ref" reference="subsec:acorn-gamma-construction"}.
