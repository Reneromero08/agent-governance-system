::: CCSXML
\<ccs2012\> \<concept\> \<concept_id\>10002951.10003317.10003338\</concept_id\> \<concept_desc\>Information systems Retrieval models and ranking\</concept_desc\> \<concept_significance\>500\</concept_significance\> \</concept\> \</ccs2012\>
:::

# Introduction {#sec:introduction}

Neural Information Retrieval (NIR) has gained increasing popularity since the introduction of pre-trained Large Language Models (LLMs) [@DBLP:series/synthesis/2021LinNY]. NIR models learn a vector representation of short pieces of text, known as an *embedding*, that captures the contextual semantics of the input, thereby enabling more effective matching of queries to documents and, thus, first-stage retrieval [@INR-071].

One major focus in NIR is what we call *learned sparse retrieval* (LSR) [@epic; @splade-sigir2021; @formal2021splade; @formal2022splade; @lassance2022efficient-splade]. LSR repurposes an LLM to encode an input into *sparse* embeddings, a vector in an inner product space where each dimension corresponds with a term in the model's vocabulary. When a coordinate is nonzero in an embedding, that indicates that the corresponding term is semantically relevant to the input. Similarity between embeddings is typically determined by inner product, so that retrieval given a query becomes the problem known as Maximum Inner Product Search (MIPS): Finding the top-$k$ vectors that maximize inner product with a query vector.

LSR is attractive for three reasons. First, LSR models are competitive with *dense retrieval* models that encode text into dense vectors [@DBLP:series/synthesis/2021LinNY; @karpukhin-etal-2020-dense; @xiong2021approximate; @reimers-2019-sentence-bert; @santhanam-etal-2022-colbertv2; @colbert2020khattab; @10.1007/978-3-031-56060-6_1]. Importantly, evidence suggests that some LSR models generalize better to out-of-domain datasets [@bruch2023fusion; @lassance2022efficient-splade].

Second, because of the one-to-one mapping between dimensions and vocabulary terms, sparse embeddings are *interpretable* by design. A user can easily understand the embedding space, explain retrieval results, and debug relevance issues. Such properties may be of interest in medical and security applications, for example.

The final reason for their popularity is that sparse embeddings retain many of the benefits of classical lexical models such as BM25 [@bm25original] while addressing one of their major weaknesses. That is because, sparse embeddings can, at least in theory, be indexed and retrieved using the all-too-familiar inverted index-based machinery [@tonellotto2018survey], while at the same time, remedying the *vocabulary mismatch* problem due to the incorporation of contextual signals.

Their performance, interpretability, and similarity to lexical models make LSR an important area of research. Efforts in this space include improving the effectiveness of sparse embeddings [@formal2022splade; @formal2021splade] and the efficiency of sparse retrieval algorithms [@bruch2023sinnamon; @bruch2023bridging; @formal2023tois-splade; @10.1145/3576922; @mallia2022guided-traversal].

The latter category is justified because, despite the apparent compatibility of sparse embeddings with inverted indexes, efficient retrieval remains a challenge. That is so because the weights learned by LSR models exhibit statistical properties that do not conform to the assumptions under which popular inverted index-based retrieval algorithms operate [@bruch2023sinnamon; @mackenzie2021wacky; @crane2017wsdm]. For example, algorithms such as WAND [@broder2003wand] and MaxScore [@maxscore], that are designed for term frequency-based lexical models, function far better than their worst-case complexity would suggest, *if* queries are short and term frequencies follow a Zipfian distribution. In LSR, queries are often longer and, crucially, frequencies are no longer Zipfian [@bruch2023sinnamon]. That deviation from assumptions often translates to increased per-query latency.

Overcoming these limitations requires either forcing LSR models to produce the "right" distribution, or designing retrieval algorithms that have fewer restrictive assumptions. As an example of the first direction, Efficient [Splade]{.smallcaps} [@lassance2022efficient-splade] applies $L_1$ regularization and uses dedicated query and document encoders to make queries shorter. As another, [@lassance2023static-pruning] statically prunes documents (or inverted lists) to produce embeddings that approximately maintain semantics but with statistics that are more friendly to dynamic pruning algorithms.

Works in the second direction [@bruch2023sinnamon; @bruch2023bridging] take a leaf out of the Approximate Nearest Neighbor (ANN) literature [@bruch2024foundations]: Algorithms that produce *approximate*, as opposed to *exact*, top-$k$ sets. This relaxation makes it easier to trade off accuracy for large gains in efficiency.

Approximate retrieval offers great potential and serves as a bridge between dense and sparse retrieval [@bruch2023bridging]. So appealing is this paradigm that the 2023 BigANN Challenge[^1] at NeurIPS dedicated a track to learned sparse embeddings. Submissions were evaluated on the [Splade]{.smallcaps} [@formal2023tois-splade] embeddings of the [Ms Marco]{.smallcaps} [@nguyen2016msmarco] Passage dataset, and were ranked by the highest throughput past $90\%$ accuracy (i.e., recall with respect to exact search). The results were intriguing: the top two submissions were graph-based ANN methods designed for dense vectors, while other approaches, including an optimized approximate inverted index-based design struggled.

Inspired by BigANN, we present a novel ANN algorithm that we call [Seismic]{.smallcaps} (**S**pilled Clust**e**ring of **I**nverted Lists with **S**ummaries for **M**aximum **I**nner Produ**c**t Search) and that admits effective and efficient retrieval over learned sparse embeddings. Pleasantly, our design uses in a new way two familiar data structures: the inverted and the forward index. In particular, we extend the inverted index by introducing a novel organization of inverted lists into geometrically-cohesive blocks. Each block is equipped with a "sketch," serving as a *summary* of the vectors contained in it. The summaries allow us to skip over a large number of blocks during retrieval and save substantial compute. When a summary indicates that a block must be examined, we use the forward index to retrieve exact embeddings of its documents and compute inner products.

We evaluate [Seismic]{.smallcaps} against strong baselines, including the top (open-source) submissions to the BigANN Challenge. We additionally include classic inverted index-based retrieval and impact-sorted indexes as reference points for completeness. Experimental results show average per-query latency in **microsecond territory** on various sparse embeddings of [Ms Marco]{.smallcaps} [@nguyen2016msmarco]. Impressively, [Seismic]{.smallcaps}**outperforms the graph-based winning solutions of the BigANN Challenge by a factor of at least 3.4 at 95% accuracy on [Splade]{.smallcaps} and 12 on Efficient [Splade]{.smallcaps}**, with the margin widening substantially as accuracy increases. Other baselines, including state-of-the-art inverted index-based algorithms, are **consistently one to two orders of magnitude slower than [Seismic]{.smallcaps}**.

In summary, we make the following contributions in this work:

- We study an empirical property of learned sparse embeddings that we call the "concentration of importance";

- We present [Seismic]{.smallcaps}, a novel ANN algorithm for retrieval over learned sparse vectors that is based on a geometrical organization of the inverted index, and leverages the concentration of importance;

- We report, through extensive experiments, remarkable gains in query latency in exchange for a negligible loss in *retrieval* accuracy, outperforming several state-of-the-art baselines, including the winning submissions to the 2023 BigANN Challenge; and,

- We given an in-depth analysis of [Seismic]{.smallcaps} in an ablation study.

# Related Work {#sec:related}

This section reviews notable related research. We summarize the thread of work on learned sparse embeddings, then discuss methods that approach the problem of retrieval over such vector collections.

## Learned Sparse Representations

Learned sparse representations were investigated [@zamani2018sparse] even before the emergence of pre-trained LLMs. But the rise of LLMs supercharged this research and led to a flurry of activity on the topic [@dai2019contextaware; @10.1145/3366423.3380258; @10.1145/3397271.3401204; @epic; @zhao-etal-2021-sparta; @sparterm; @formal2021splade; @formal2023tois-splade; @unicoil]. First attempts at this include DeepCT and HDCT by Dai and Callan [@dai2019contextaware; @10.1145/3366423.3380258; @10.1145/3397271.3401204].

DeepCT used the Transformer [@vaswani2017attention] encoder of BERT [@devlin2019bert] to extract contextual features of a word into an embedding, which can be viewed as a feature vector that characterizes the term's syntactic and semantic role in a given context. DeepCT linearly combines a term's contextualized embedding and summarizes it as a term *weight* for terms that are present in a document. Because the vocabulary associated with a document remains the same, it does not address the vocabulary mismatch problem.

One way to address vocabulary mismatch is to use a generative model, such as doc2query [@nogueira2019document] or docT5query [@nogueira2019doc2query], to expand documents with relevant terms *and* boost existing terms by repeating them in the document, implicitly performing term re-weighting. In fact, [uniCoil-T5]{.smallcaps} [@unicoil; @ma2022document] expands its input with DocT5Query [@nogueira2019doc2query] before learning and producing a sparse representation.

Formal *et al.* build on SparTerm [@sparterm] and propose [Splade]{.smallcaps} [@splade-sigir2021]. Their construction introduces sparsity-inducing regularization and a log-saturation effect on term weights, so that the sparse representations learned by [Splade]{.smallcaps} are typically relatively sparser. Interestingly, [Splade]{.smallcaps} showed competitive results with respect to state-of-the-art dense and sparse methods [@splade-sigir2021].

In a later work, Formal *et al.* make adjustments to [Splade]{.smallcaps}'s pooling and expansion mechanisms, and introduce distillation into its training. This second version, called [Splade]{.smallcaps} v2, reached state-of-the-art results on the [Ms Marco]{.smallcaps} [@nguyen2016msmarco] passage ranking task as well as the [Beir]{.smallcaps} [@thakur2021beir] zero-shot evaluation benchmark [@formal2021splade]. The [Splade]{.smallcaps} model has undergone many other rounds of improvements which have been documented in the latest work by the same authors [@formal2023tois-splade]. Among these, one notable extension is the Efficient [Splade]{.smallcaps} which, as we already noted, attempts to make the learned embeddings more friendly to inverted index-based algorithms.

## Retrieval Algorithms

The Information Retrieval literature offers a wide array of algorithms tailored to retrieval on text collections [@tonellotto2018survey]. They are often *exact* and scale easily to massive datasets. MaxScore [@maxscore] and WAND [@broder2003wand], and subsequent improvements [@ding2011bmwand; @topk_bmindexes; @mallia2019faster-blockmaxwand; @mallia2017blockmaxwand_variableBlocks], are examples that, essentially, solve the MIPS problem over "bag-of-words" representations of text, such as BM25 [@bm25original] or TF-IDF [@salton1988term].

These algorithms operate on an inverted index, augmented with additional data to speed up query processing. One that features prominently is the maximum attainable partial inner product---an upper-bound. This enables the possibility of navigating the inverted lists, one document at a time, and deciding quickly if a document may belong to the result set. Effectively, such algorithms (safely) *prune* the parts of the index that cannot be in the top-$k$ set. That is why they are often referred to as *dynamic pruning* techniques.

Although efficient in practice, dynamic pruning methods are designed specifically for text collections. Importantly, they ground their performance on several pivotal assumptions: non-negativity, higher sparsity rate for queries, and a Zipfian shape of the length of inverted lists. These assumptions are valid for TF-IDF or BM25, which is the reason why dynamic pruning works well and the worst-case time complexity of MIPS is seldom encountered in practice.

These assumptions do not necessarily hold for collections of learned sparse representations, however. Learned vectors may be real-valued, with a sparsity rate that is closer to uniform across dimensions [@bruch2023sinnamon; @mackenzie2021wacky]. Mackenzie *et al.* [@10.1145/3576922] find that learned sparse embeddings reduce the odds of pruning or early-termination in the document-at-a-time (DaaT) and Score-at-a-Time (SaaT) paradigms.

The most similar work to ours is [@bruch2023bridging]. The authors investigate if *approximate* MIPS algorithms for *dense* vectors port over to *sparse* vectors. They focus on *inverted file* (IVF) where vectors are partitioned into clusters during indexing, with only a fraction of clusters scanned during retrieval. They show that IVF serves as an efficient solution for sparse MIPS. Interestingly, the authors cast IVF as dynamic pruning and turn that insight into a novel organization of the inverted index for approximate MIPS for general sparse vectors. Our index structure can be viewed as an extension of theirs.

Finally, we briefly describe another ANN algorithm over dense vectors: HNSW [@hnsw2020], a graph-based algorithm that constructs a graph where each document is a node and two nodes are connected if they are deemed "similar." Similarity is based on Euclidean distance, but [@ip-nsw18] shows inner product results in a structure that is capable of solving MIPS rather quickly and accurately. As we learn in the presentation of our empirical analysis, algorithms that adapt IP-HNSW [@ip-nsw18] to sparse vectors work remarkably well.

# Definitions and Notation {#sec:definition}

Suppose we have a collection $\mathcal{X} \subset \mathbb{R}_{+}^{d}$ of nonnegative *sparse* vectors. If $x \in \mathcal{X}$, then $x$ is a $d$-dimensional vector where the vast majority of its coordinates are $0$ and the rest are real positive values. We use superscript to enumerate a collection: $x^{(j)}$ is the $j$-th vector in $\mathcal{X}$.

We use lower-case letters (e.g., $x$) to denote a vector, call $1 \leq i \leq d$ its *coordinate*, and write $x_i$ for its $i$-th *value*. Together, we refer to a coordinate and value pair as an *entry*, and say an entry is non-zero if it has a non-zero value. A sparse vector can be identified as a set of non-zero entries: $\{ (i, x_i) \;|\; x_i \neq 0 \}$.

Sparse MIPS aims to solve the following problem to find, from $\mathcal{X}$, the set $\mathcal{S}$ of top $k$ vectors whose inner product with the query vector $q \in \mathbb{R}^{d}$ is maximal: $$\begin{equation}
    \mathcal{S} = \mathop{\mathrm{arg\,max}}^{(k)}_{x \in \mathcal{X}} \; \langle q, x \rangle.
    \label{equation:mips}
\end{equation}$$

Let us define a few concepts that we frequently refer to. The $L_p$ norm of a vector denoted by $\lVert \cdot \rVert_p$ is defined as $\lVert x\rVert_p = (\sum_i \lvert x_i\rvert^p)^{1/p}$. We call the $L_p$ norm of a vector its $L_p$ *mass*. Additionally:

::: definition
Consider a vector $x$ and a permutation $\pi$ that sorts the entries of $x$ by their absolute value: $\lvert x_{\pi_i} \rvert \geq \lvert x_{\pi_{i+1}} \rvert$. For a constant $\alpha \in [0, 1]$, denote by $1 \leq j \leq d$ the smallest integer such that: $$\begin{equation*}
        \sum_{i=1}^j \lvert x_{\pi_i} \rvert \leq \alpha \lVert x \rVert_1.
\end{equation*}$$ We call $\tilde{x}$ made up of $\{ (\pi_i, x_{\pi_i}) \}_{i=1}^j$, the *$\alpha$-mass subvector* of $x$. Clearly, $\lVert \tilde{x} \rVert_1 \leq \alpha \lVert x \rVert_1$.
:::

# Concentration of Importance {#subsec:property}

Recently, Daliri *et al.* [@daliri2023sampling] presented a sketching algorithm for sparse vectors that rest on the following simple principle: Coordinates that contribute more heavily to the $L_2$ norm of a vector, weigh more significantly on the inner product between vectors. Using that intuition, they report that if we were to drop the non-zero coordinates of a sparse vector with a probability proportional to its contribution to the $L_2$ mass, we can reduce the size of a collection while approximately maintaining inner products between vectors.

Inspired by [@daliri2023sampling], we examined two state-of-the-art LSR techniques: [Splade]{.smallcaps} [@formal2022splade] and Efficient [Splade]{.smallcaps} [@lassance2022efficient-splade]. Our analysis reveals a parallel property, which we call the "concentration of importance." In particular, we observe that the LSR techniques place a disproportionate amount of the total $L_1$ mass of a vector on just a small subset of the coordinates.

Let us demonstrate this phenomenon on the [Ms Marco]{.smallcaps} Passage dataset [@nguyen2016msmarco] with the [Splade]{.smallcaps} embeddings.[^2] We take every vector, sort its entries by value, and measure the fraction of the $L_1$ mass preserved by considering a given number of top entries. For queries, the top $10$ entries yield $0.75$-mass subvectors. For documents, the top $50$ (about $30$% of non-zero entries), give $0.75$-mass subvectors. We illustrated our measurements in Figure [1](#fig:energy){reference-type="ref" reference="fig:energy"}.

<figure id="fig:energy" data-latex-placement="t">
<embed src="imgs/L1_distribution2_new.pdf" />
<figcaption>Fraction of <span class="math inline"><em>L</em><sub>1</sub></span> mass preserved by keeping only the top non-zero entries with the largest absolute value. <span id="fig:energy" data-label="fig:energy"></span></figcaption>
</figure>

These results bring us naturally to our next question: What are the ramifications of the concentration of importance for inner product between queries and documents? One way to study that is as follows: We take the top-$10$ document vectors for each query, prune each document vector by keeping a fraction of its non-zero entries with the largest value. We do the same for query vectors. We then compute the inner product between the trimmed-down queries and documents and report the results in Figure [2](#fig:energy2){reference-type="ref" reference="fig:energy2"}.

The figure shows that, if we took the top $10\%$ of the most "important" coordinates from queries ($9$) and documents ($20$), we preserve, on average, $85\%$ of the full inner product. Keeping $12$ query and $25$ document coordinates bumps that up to $90\%$.

Our results confirm that LSR tends to concentrate importance on a few coordinates. Furthermore, a partial inner product between the largest entries (by absolute value) approximates the full inner product with arbitrary accuracy. As we will see shortly, this property, which is in agreement with [@daliri2023sampling], can help speed up query processing and reduce space consumption rather substantially.

# Proposed Algorithm {#sec:methodology}

We now introduce [Seismic]{.smallcaps}, a novel ANN algorithm that allows effective and efficient approximate retrieval over learned sparse representations. The design of [Seismic]{.smallcaps} uses two important and familiar data structures: the inverted index and the forward index. In an nutshell, we use a forward index for inner product computation, and an inverted index to pinpoint the subset of documents that must be evaluated. Figure [3](#fig:inverted){reference-type="ref" reference="fig:inverted"} gives an overview of the overall design.

<figure id="fig:energy2" data-latex-placement="t">
<embed src="imgs/importance_good_new.pdf" />
<figcaption>Fraction of inner product (with <span class="math inline">95%</span> confidence intervals) preserved by inner product between the top query and document coordinates with the largest absolute value. <span id="fig:energy2" data-label="fig:energy2"></span></figcaption>
</figure>

<figure id="fig:inverted" data-latex-placement="t">
<embed src="imgs/index.pdf" />
<figcaption>The design of <span class="smallcaps">Seismic</span>. Inverted lists are independently partitioned into geometrically-cohesive blocks. Each block is a set of document identifiers with a summary vector. The inner product of a query with the summary approximates the inner product attainable with the documents in that block. The forward index stores the complete vectors (including values).</figcaption>
</figure>

[Seismic]{.smallcaps} is novel in the following ways. First, it uses an organization of the inverted index that blends together *static* and *dynamic* pruning to significantly reduce the number of documents that must be evaluated during retrieval. Second, it partitions inverted lists into geometrically-cohesive blocks to facilitate efficient skipping of blocks. Finally, we attach a *summary* to each block, whose inner product with a query approximates---albeit not necessarily in an unbiased manner---the inner product of the query with documents contained in the block.

## Static Pruning

[Seismic]{.smallcaps} heavily relies on the concentration of importance property discussed in Section [4](#subsec:property){reference-type="ref" reference="subsec:property"}. The property shows that a small subset of the most important coordinates of the sparse embedding of a query and document vector can be used to effectively approximate their inner product. We incorporate this result in [Seismic]{.smallcaps} during the construction of the inverted index through *static pruning*.

Concretely, for coordinate $i$, we build its inverted list by gathering all $x \in \mathcal{X}$ whose $x_i \neq 0$. We then sort the inverted list by $x_i$'s value in decreasing order (breaking ties arbitrarily), so that the document whose $i$-th coordinate has the largest value appears at the beginning of the list. We then prune the inverted list by keeping at most the first $\lambda$ entries for a fixed $\lambda$---our first hyper-parameter. We denote the resulting inverted list for coordinate $i$ by $\mathcal{I}_i$.

## Blocking of Inverted Lists {#sec:methodology:blocking}

[Seismic]{.smallcaps} also introduces a novel blocking strategy on inverted lists. It partitions each inverted list into $\beta$ small blocks---our second hyper-parameter. The rationale behind a blocked organization of an inverted list is to group together documents that are *similar* in terms of their local representations, so as to facilitate a *dynamic pruning* strategy, to be described shortly.

We defer the determination of similarity to a clustering algorithm. In other words, the documents whose ids are present in an inverted list are given as input to a clustering algorithm, which subsequently partitions them into $\beta$ clusters. Each cluster is then turned into one block, consisting of the id of documents whose vectors belong to the same cluster. Conceptually, each block is "atomic" in the following sense: if the dynamic pruning algorithm decides we must visit a block, *all* the documents in that block are fully evaluated.

We note that any geometrical (supervised or unsupervised) clustering algorithm may be readily used. We use a shallow variant [@chierichetti2007clusterPruning] of K-Means as follows. Given a set of vectors $\mathcal{S}$, we uniformly-randomly sample $\beta$ vectors, $\{ \mu^{(j)} \}_{j=1}^\beta$, from $\mathcal{S}$, and use them as cluster representatives. For each $x \in \mathcal{S}$, we find $j^\ast = \mathop{\mathrm{arg\,max}}_j \langle x, \mu^{(j)} \rangle$, and assign $x$ to the $j^\ast$-th cluster.

## Per-block Summary Vectors

So far we have described how we statically prune inverted lists to the top $\lambda$ entries and then partition them into $\beta$ blocks using a clustering algorithm. We now describe how this structure can be used as a basis for a novel dynamic pruning method.

We need an efficient way to determine if a block should be evaluated. To that end, [Seismic]{.smallcaps} leverages the concept of a *summary* vector: a $d$-dimensional vector that "represents" the documents in a block. The summary vectors are stored in the inverted index, one per block, and are meant to serve as an efficient way to compute a good-enough approximation of the inner product between a query and the documents within the block.

One realization of this idea is to upper-bound the full inner product attainable by documents in a block. In other words, the $i$-th coordinate of the summary vector of a block would contain the maximum $x_i$ among the documents in that block. This construction can be best described as a vectorization of the upper-bound *scalars* in blocked variants of WAND [@ding2011bmwand].

More precisely, our summary function $\phi: 2^\mathcal{X} \rightarrow \mathbb{R}^d$ takes a block $B$ from the universe of all blocks $2^\mathcal{X}$, and produces a vector whose $i$-th coordinate is simply: $$\begin{equation}
    \label{equation:summary}
    \phi(B)_i = \max_{x \in B} x_i.
\end{equation}$$ This summary is *conservative*: its inner product with the query is no less than the inner product between the query and any of its document: $\langle q, \phi(B) \rangle \geq \langle q, x \rangle$ for all $x \in B$ and an arbitrary query $q$.

The caveat, however, is that the number of non-zero entries in summary vectors grows quickly with the block size. That is the source of two potential issues: 1) the space required to store summaries increases; and 2) as inner product computation takes time proportional to the number of non-zero entries, the time required to evaluate a block could become unacceptably high.

We may address that caveat by applying pruning and quantization, with the understanding that any such method may take away the conservatism of the summary. As we will empirically show, there are many pruning or quantization candidates to choose from.

In particular, we use the following technique that builds on the concentration of importance property: We prune $\phi(B)$, obtained from Equation ([\[equation:summary\]](#equation:summary){reference-type="ref" reference="equation:summary"}), by keeping only its $\alpha$-mass subvector. That, $\alpha$, is our third and last indexing hyper-parameter.

We further reduce the size of summaries by applying scalar quantization. With the goal of reserving a single byte for each value, we subtract the minimum value $m$ from each summary entry, and divide the resulting range into $256$ sub-intervals of equal size. A value in the summary is replaced with the index of the sub-interval it maps to. To reconstruct a value approximately, we multiply the id of its sub-interval by the size of the sub-intervals, then add $m$.

## Forward Index

[Seismic]{.smallcaps} blends together two data structures. The first is an inverted index that tells us which documents to examine. To make it practical, we apply approximations that allow us to gain efficiency with a possible loss in accuracy. A forward index, which is simply a look-up table that stores the exact document vectors, helps correct those errors and offers a way to compute the exact inner products between a query and the documents within a block, whenever that block is deemed a good candidate for evaluation.

We must note that, documents belonging to the same block are not necessarily stored consecutively in the forward index. This is simply infeasible because the same document may belong to different inverted lists and, thus, to different blocks. Because of this layout, computing the inner products may incur many cache misses, which are detrimental to query latency. In our implementation, we extensively use prefetching instructions to mitigate this effect.

## Recap

We summarize the discussion above in Algorithm [\[algorithm:indexing\]](#algorithm:indexing){reference-type="ref" reference="algorithm:indexing"}. When indexing a collection $\mathcal{X} \subset \mathbb{R}^d$, for every coordinate $i \in \{ 1, \dots, d\}$, we form its inverted list, recording only the document identifiers (Line [\[algorithm:indexing:inverted-list\]](#algorithm:indexing:inverted-list){reference-type="ref" reference="algorithm:indexing:inverted-list"}). We then sort the list in decreasing order of values (Line [\[algorithm:indexing:sort\]](#algorithm:indexing:sort){reference-type="ref" reference="algorithm:indexing:sort"}), and apply static pruning by keeping, for each inverted list, the $\lambda$ elements with the largest value (Line [\[algorithm:indexing:static-pruning\]](#algorithm:indexing:static-pruning){reference-type="ref" reference="algorithm:indexing:static-pruning"}). We then apply clustering to the inverted list to derive at most $\beta$ blocks (Line [\[algorithm:indexing:clustering\]](#algorithm:indexing:clustering){reference-type="ref" reference="algorithm:indexing:clustering"}). Once documents are assigned to the blocks, we then build the block summary using the procedure described earlier (Line [\[algorithm:indexing:summary\]](#algorithm:indexing:summary){reference-type="ref" reference="algorithm:indexing:summary"}).

Algorithm [\[algorithm:retrieval\]](#algorithm:retrieval){reference-type="ref" reference="algorithm:retrieval"} shows the query processing logic in [Seismic]{.smallcaps}. We use the concentration of importance property to (a) select a subset of the query coordinates $q_\textsf{cut}$ (Line [\[algorithm:retrieval:q-cut\]](#algorithm:retrieval:q-cut){reference-type="ref" reference="algorithm:retrieval:q-cut"}), and (b) define a novel dynamic pruning strategy (Lines [\[algorithm:retrieval:summary-ip\]](#algorithm:retrieval:summary-ip){reference-type="ref" reference="algorithm:retrieval:summary-ip"}--[\[algorithm:retrieval:skip\]](#algorithm:retrieval:skip){reference-type="ref" reference="algorithm:retrieval:skip"}) that allows to skip blocks in the inverted lists of the coordinates in $q_\textsf{cut}$.

:::: algorithm
**Input:** Collection $\mathcal{X}$ of sparse vectors in $\mathbb{R}^{d}$; $\lambda$: Maximum length of each inverted list; $\beta$: Maximum number of blocks per inverted list; $\alpha$: Fraction of the overall importance preserved by each summary.\

::: algorithmic
$\mathcal{S} \leftarrow \{ j \;|\; x^{(j)}_i \neq 0,\; x^{(j)} \in \mathcal{X} \}$ []{#algorithm:indexing:inverted-list label="algorithm:indexing:inverted-list"} [Sort]{.smallcaps} $\mathcal{S}$ in decreasing order by $x_i$ for all $x \in \mathcal{S}$ []{#algorithm:indexing:sort label="algorithm:indexing:sort"} $\mathcal{I}_i \leftarrow \{ \mathcal{S}_{i,1}, \mathcal{S}_{i,2}, \ldots, \mathcal{S}_{i,\lambda} \}$ []{#algorithm:indexing:static-pruning label="algorithm:indexing:static-pruning"} [Cluster]{.smallcaps} $\mathcal{I}_i$ into $\beta$ partitions, $\{ B_{i, j} \}_{j=1}^\beta$ []{#algorithm:indexing:clustering label="algorithm:indexing:clustering"} $S_{i, j} \leftarrow \alpha$-mass subvector of $\phi(B_{i, j})$ []{#algorithm:indexing:summary label="algorithm:indexing:summary"} $\mathcal{I}_i$, $\{ S_{i, j} \} \; \forall i, j$
:::
::::

[Seismic]{.smallcaps} adopts a coordinate-at-a-time traversal (Line [\[algorithm:retrieval:traversal\]](#algorithm:retrieval:traversal){reference-type="ref" reference="algorithm:retrieval:traversal"}) of the inverted index. For each coordinate $i \in q_\textsf{cut}$, it evaluates the blocks using their summary. The documents within a block are evaluated further if the approximation with the summary is greater than a fraction of the minimum inner product in the Min-[Heap]{.smallcaps}. That means that, the forward index retrieves the complete document vector in the selected block and computes inner products. A document whose inner product is greater than the minimum score in the Min-[Heap]{.smallcaps} is inserted into the heap. Note that, Algorithm [\[algorithm:retrieval\]](#algorithm:retrieval){reference-type="ref" reference="algorithm:retrieval"} takes two hyper-parameters: an integer $\textsf{cut}$, and $\textsf{heap\_factor}\in (0, 1)$.

:::: algorithm
**Input:** $q$: query; $k$: number of results; [cut]{.sans-serif}: number of largest query entries considered; [heap_factor]{.sans-serif}: a correction factor to rescale the summary inner product; $\mathcal{I}_i$'s and $S_{i, j}$'s: inverted lists and summaries obtained from Algorithm [\[algorithm:indexing\]](#algorithm:indexing){reference-type="ref" reference="algorithm:indexing"}.\

::: algorithmic
$q_\textsf{cut}\leftarrow$ the top [cut]{.sans-serif} entries of $q$ with the largest value []{#algorithm:retrieval:q-cut label="algorithm:retrieval:q-cut"} [Heap]{.smallcaps}$\leftarrow \emptyset$ []{#algorithm:retrieval:traversal label="algorithm:retrieval:traversal"} $r \leftarrow \langle q, S_{i, j} \rangle$ []{#algorithm:retrieval:summary-ip label="algorithm:retrieval:summary-ip"} []{#algorithm:retrieval:skip label="algorithm:retrieval:skip"} $p = \langle q, {\sf ForwardIndex}[d] \rangle$ [Heap]{.smallcaps}.insert$(p, d)$ [Heap]{.smallcaps}.pop_min() [Heap]{.smallcaps}
:::
::::

# Generalized Architecture {#sec:architecture}

What we presented in Section [5](#sec:methodology){reference-type="ref" reference="sec:methodology"} is an instance of a more general algorithm. Conceptually, [Seismic]{.smallcaps} can be viewed as the application of the following logical functions to a collection of sparse vectors.

**Clustering with Spillage**. We group together documents that share a non-zero coordinate (as inverted lists), then partition them into blocks. This is an instance of *clustering with spillage*, where an item may belong to multiple clusters. The inverted index as *coarse* clustering is efficient for sparse vectors, though other algorithms that allow spillage may very well suit other distributions.

**Sketching**. We summarize clusters by taking the maximum of each coordinate. While we use the upper-bound vector to obtain a conservative estimate, a more general design admits other types of summaries such as centroids, medoids or any other sketch [@woodruff2014sketching].

**Compression**. We used pruning and quantization to reduce the total size of summaries by paying particular attention to the $L_1$ mass. In theory, however, any number of other compression schemes may be utilized, such as [@bruch2023sinnamon; @daliri2023sampling].

**Routing**. We identify the subset of clusters that must be fully evaluated by sequentially scanning summaries and comparing their inner product with the minimum score so far. Routing a query to the right cluster, however, need not follow that paradigm strictly. We may consider all summaries at once and decide which clusters to probe in one go---a process akin to the "IVF" approach to ANN [@pq].

# Experiments {#sec:experiments}

We now evaluate [Seismic]{.smallcaps} experimentally. Specifically, we are interested in investigating the performance of [Seismic]{.smallcaps} in the following ways: (a) its accuracy, latency, space usage, and indexing time against existing solutions, and (b) an ablation study of the impact of the different components of [Seismic]{.smallcaps} on performance.

In what follows, we unpack these questions through an empirical evaluation on two public datasets. We note that, due to space constraints, we excluded many combinations of datasets and LSR models (e.g., [uniCoil-T5]{.smallcaps} embeddings of [NQ]{.smallcaps}) from the presentation of our results. However, the reported trends hold consistently.

## Setup

**Datasets**. We experiment on two publicly-available datasets: [Ms Marco]{.smallcaps} v1 Passage [@nguyen2016msmarco] and Natural Questions ([NQ]{.smallcaps}) from [Beir]{.smallcaps} [@thakur2021beir]. [Ms Marco]{.smallcaps} is a collection of $8.8$M passages in English. In our evaluation, we use the smaller "dev" set of queries for retrieval, which includes $6{,}980$ questions. [NQ]{.smallcaps} is a collection of $2.68$M questions in English. We use it in combination with its "test" set of $7{,}842$ queries.

**Learned Sparse Representations**. We evaluate [Seismic]{.smallcaps} with embeddings generated by three LSR models:

- [Splade]{.smallcaps} [@formal2022splade]. Each non-zero entry is the importance weight of a term in the BERT [@devlin2019bert] WordPiece [@wordpiece] vocabulary consisting of $30$,$000$ terms. We use the `cocondenser-ensembledistil`[^3] version of [Splade]{.smallcaps} that yields MRR@10 of $38$.$3$ on the [Ms Marco]{.smallcaps} dev set. The number of non-zero entries in documents (queries) is, on average, $119$ ($43$) for [Ms Marco]{.smallcaps} and $153$ ($51$) for [NQ]{.smallcaps}.

- Efficient [Splade]{.smallcaps} [@lassance2022efficient-splade]. Similar to [Splade]{.smallcaps}, but there are $181$ ($5.9$) non-zero entries in [Ms Marco]{.smallcaps} documents (queries). We use the `efficient-splade-V-large`[^4] version, yielding MRR@10 of $38$.$8$ on the [Ms Marco]{.smallcaps} dev set. We refer to this model as [E-Splade]{.smallcaps}.

- [uniCoil-T5]{.smallcaps} [@unicoil; @ma2022document]. Expands passages with relevant terms generated by DocT5Query [@nogueira2019doc2query]. [uniCoil-T5]{.smallcaps} achieves MRR@10 of $35$.$2$ on the [Ms Marco]{.smallcaps} dev set. There are, on average, $68$ ($6$) non-zero entries in [Ms Marco]{.smallcaps} documents (queries).

It is worth highlighting that these embedding models belong to different families. [Splade]{.smallcaps} and [E-Splade]{.smallcaps} perform expansion for both queries and documents. On the other hand, [uniCoil-T5]{.smallcaps} only performs document expansion and does so using a generative model.

We generate the [Splade]{.smallcaps} and [E-Splade]{.smallcaps} embeddings using the original code published on GitHub.[^5] [uniCoil-T5]{.smallcaps} embeddings are based on the original implementation on GitHub.[^6] After generating the embeddings, we replicate the performance in terms of MRR@10 on the [Ms Marco]{.smallcaps} dev set to confirm that our replication achieves the same performance presented in the original papers.

::: table*
:::

**Baselines**. We compare [Seismic]{.smallcaps} with five state-of-the-art retrieval solutions. Two of these are the winning solutions of the "Sparse Track" at the 2023 BigANN Challenge[^7] at NeurIPS. These include:

- [GrassRMA]{.smallcaps}: A graph-based method for dense vectors adapted to sparse vectors that appears in the BigANN challenge as "[sHnsw]{.smallcaps}."[^8]

- [PyAnn]{.smallcaps}: Another graph-based ANN solution.[^9]

The other three baselines are inverted index-based solutions:

- [Ioqp]{.smallcaps} [@mpg22-desires]: Impact-sorted query processor written in Rust. We choose [Ioqp]{.smallcaps} because it is known to outperform JASS [@jass2015], a widely-adopted open-source impact-sorted query processor.

- [SparseIvf]{.smallcaps} [@bruch2023bridging]: An inverted index where lists are partitioned into blocks through clustering. At query time, after finding the $N$ closest clusters to the query, a coordinate-at-a-time algorithm traverses the inverted lists. The solution is approximate because only documents that belong to top $N$ clusters are considered.

- [Pisa]{.smallcaps} [@MSMS2019]: An inverted index-based C++ library based on `ds2i` [@pefi-SIGIR14] that uses highly-optimized blocked variants of WAND. [Pisa]{.smallcaps} is *exact* as it traverses inverted lists in a rank-safe manner.

We also considered the method by Lassance *et al.* [@lassance2023static-pruning]. Their approach statically prunes either inverted lists (by keeping $p$-quantile of elements), documents (by keeping a fixed number of top entries), or all coordinates whose value is below a threshold. While simple, [@lassance2023static-pruning] is only able to speed up query processing by 2--4$\times$ over [Pisa]{.smallcaps} on [E-Splade]{.smallcaps} embeddings of [Ms Marco]{.smallcaps}. We found it to be ineffective on [Splade]{.smallcaps} and generally far slower than [GrassRMA]{.smallcaps} and [PyAnn]{.smallcaps}. As such we do not include it in our discussions.

We build [Ioqp]{.smallcaps} and [Pisa]{.smallcaps} indexes using Anserini[^10] and apply recursive graph bisection [@mpm21-sigir]. For [Ioqp]{.smallcaps}, we vary the *fraction* (of the total collection) hyper-parameter in $[0.1, 1]$ with step size of $0.05$. For [SparseIvf]{.smallcaps}, we sketch documents using [Sinnamon]{.smallcaps}$_\textsc{Weak}$ and a sketch size of $1{,}024$, and build $4 \sqrt{N}$ clusters, where $N$ is the number of documents in the collection. For [GrassRMA]{.smallcaps} and [PyAnn]{.smallcaps}, we build different indexes by running all possible combinations of $ef_c \in  \{1000, 2000\}$ and $M \in \{16, 32, 64, 128, 256 \}$. During search we test $ef_s \in [5, 100]$ with step size $5$, then $[100, 400]$ with step $10$, $[100, 1000]$ with step $100$, and finally $[1000, 5000]$ with step $500$. We apply early stopping when accuracy saturates.

Our grid search for [Seismic]{.smallcaps} on [Ms Marco]{.smallcaps} is over: $\lambda \in [1500, 7500]$ with step size of $500$, $\beta \in [150, 750]$ with step $50$, and $\alpha \in [0.1, 0.5]$ with $0.1$. Best results are achieved with $\lambda=6{,}000$, $\beta=400$, and $\alpha=0.4$. The grid search for [Seismic]{.smallcaps} on [NQ]{.smallcaps} is over: $\lambda \in \{4500, 5250, 6000\}$, $\beta \in \{300, 350, 400, 450, 525, 600, 700, 800\}$, and $\alpha \in \{0.3, 0.4, 0.5\}$. Best results are achieved with $\lambda=5{,}250$, $\beta=525$, and $\alpha=0.5$. [Seismic]{.smallcaps} employs 8-bit scalar quantization for summaries. Moreover, [Seismic]{.smallcaps} uses matrix multiplication to efficiently multiply the query vector with all quantized summaries of an inverted list.

**Evaluation Metrics**. We evaluate all methods using three metrics:

- Latency ($\mu$sec.). The time elapsed, in *microseconds*, from the moment a query vector is presented to the index to the moment it returns the requested top $k$ document vectors running in single thread mode. Latency does not include embedding time.

- Accuracy. The percentage of true nearest neighbors recalled in the returned set. By measuring the recall of an approximate set given the exact top-$k$ set, we study the impact of the different levers in an algorithm on its overall accuracy as a retrieval engine.

- Index size (MiB). The space the index occupies in memory.

**Reproducibility and Hardware Details**. We implemented [Seismic]{.smallcaps} in Rust.[^11] We compile [Seismic]{.smallcaps} by using the version $1{.}77$ of Rust and use the highest level of optimization made available by the compiler. We conduct experiments on a server equipped with one Intel i9-9900K CPU with a clock rate of $3{.}60$ GHz and $64$ GiB of RAM. The CPU has $8$ physical cores and $8$ hyper-threaded ones. We query the index using a single thread.

## Results

We now present our experimental results. We begin by comparing the performance of [Seismic]{.smallcaps} with baselines. We then ablate [Seismic]{.smallcaps} to understand the impact of our design choices on performance.

### Accuracy-Latency Trade-off

Table [\[table:results1\]](#table:results1){reference-type="ref" reference="table:results1"} details retrieval performance in terms of average per-query latency for [Splade]{.smallcaps}, [E-Splade]{.smallcaps}, and [uniCoil-T5]{.smallcaps} on [Ms Marco]{.smallcaps}, and [Splade]{.smallcaps} on [NQ]{.smallcaps}. We frame the results as the trade-off between effectiveness and efficiency. In other words, we report mean per-query latency at a given accuracy level.

The results on these datasets show [Seismic]{.smallcaps}'s remarkable relative efficiency, reaching a latency that is often one to two orders of magnitude smaller. Overall, [Seismic]{.smallcaps} consistently outperforms all baselines at all accuracy levels, including [GrassRMA]{.smallcaps} and [PyAnn]{.smallcaps}, which in turn perform better than other inverted index-based baselines---confirming the findings of the BigANN Challenge.

We make a few additional observations. [Ioqp]{.smallcaps} appears to be the slowest method across datasets. This is not surprising considering the distributional abnormalities of learned sparse vectors, as discussed earlier. [SparseIvf]{.smallcaps} generally improves over [Ioqp]{.smallcaps}, but [Seismic]{.smallcaps} speeds up query processing further. In fact, the minimum speedup over [Ioqp]{.smallcaps}([SparseIvf]{.smallcaps}) on [Ms Marco]{.smallcaps} is $84.6\times$ ($22.3\times$) on [Splade]{.smallcaps}, $24.9\times$ ($20.9\times$) on [E-Splade]{.smallcaps}, and $143.3\times$ ($53.6\times$) on [uniCoil-T5]{.smallcaps}.

[Seismic]{.smallcaps} consistently outperforms [GrassRMA]{.smallcaps} and [PyAnn]{.smallcaps} by a substantial margin, ranging from $2.6\times$ ([Splade]{.smallcaps} on [Ms Marco]{.smallcaps}) to $21.6\times$ ([E-Splade]{.smallcaps} on [Ms Marco]{.smallcaps}) depending on the level of accuracy. In fact, as accuracy increases, the latency gap between [Seismic]{.smallcaps} and the two graph-based methods widens. This gap is much larger when query vectors are sparser, such as with [E-Splade]{.smallcaps} embeddings. That is because, when queries are highly sparse, inner products between queries and documents become smaller, reducing the efficacy of a greedy graph traversal. As one data point, [PyAnn]{.smallcaps} over [E-Splade]{.smallcaps} embeddings of [Ms Marco]{.smallcaps} visits roughly $40{,}000$ documents to reach $97\%$ accuracy, whereas [Seismic]{.smallcaps} evaluates just $2{,}198$ documents.

Finally, we highlight that [Pisa]{.smallcaps} is the slowest (albeit, *exact*) solution. On [Ms Marco]{.smallcaps}, [Pisa]{.smallcaps} processes queries in about $100{,}325$ microseconds on [Splade]{.smallcaps} embeddings. On [E-Splade]{.smallcaps} and [uniCoil-T5]{.smallcaps}, its average latency is $7{,}947$ and $9{,}214$ microseconds, respectively. We note that its high latency on [Splade]{.smallcaps} is largely due to the much larger number of non-zero entries in queries.

<figure id="fig:msmarco-mrr-plot" data-latex-placement="t">
<embed src="imgs/MSMARCO_Plot_new.pdf" />
<figcaption>MRR@10 on <span class="smallcaps">Ms Marco</span>.<span id="fig:msmarco-mrr-plot" data-label="fig:msmarco-mrr-plot"></span></figcaption>
</figure>

We conclude with a remark on the relationship between retrieval accuracy (as measured by recall with respect to exact search) and ranking quality (such as MRR and NDCG [@Jarvelin2002] given relevance judgments). Even though ranking quality is not our primary focus, we measured MRR@10 on [Ms Marco]{.smallcaps} for the approximate top-$k$ sets obtained from [Seismic]{.smallcaps}, and plot that as a function of per-query latency in Figure [4](#fig:msmarco-mrr-plot){reference-type="ref" reference="fig:msmarco-mrr-plot"}. While MRR@10 is relatively stable, we do notice a drop in the low-latency (and thus low-accuracy) regime. Perhaps more interesting is the fact that [Seismic]{.smallcaps} can speed up retrieval over [Splade]{.smallcaps} so much that if the time budget is tight, using [Splade]{.smallcaps} embeddings gets us to a higher MRR@10 faster.

### Space and Build Time

Table [\[table:results-rq2\]](#table:results-rq2){reference-type="ref" reference="table:results-rq2"} records the time it takes to index the entire [Ms Marco]{.smallcaps} collection embedded with [Splade]{.smallcaps} with different methods, and the size of the resulting index. We perform this experiment on a machine with two Intel Xeon Silver 4314 CPUs clocked at 2.40GHz, with 32 physical cores plus 32 hyper-threaded ones and 512 GiB of RAM. We build the indexes by using multi-threading parallelism with $64$ cores.

We left out the build time for [Ioqp]{.smallcaps} because its index construction has many external dependencies (such as Anserini and graph bisection) that makes giving an accurate estimate difficult.

Trends for other datasets are similar to those reported in Table [\[table:results-rq2\]](#table:results-rq2){reference-type="ref" reference="table:results-rq2"}. Notably, indexes produced by approximate methods are larger. That makes sense: using more auxiliary statistics helps narrow the search space dynamically and quickly. Among the highly efficient methods, the size of [Seismic]{.smallcaps}'s index is mild, especially compared with [GrassRMA]{.smallcaps}. Importantly, [Seismic]{.smallcaps} builds its index in a fraction of the time it takes [PyAnn]{.smallcaps} or [GrassRMA]{.smallcaps} to index the collection.

## Ablation Study

We now take [Seismic]{.smallcaps} apart to study the impact of its components. We take the [Splade]{.smallcaps} embeddings of [Ms Marco]{.smallcaps} and analyze the impact of (a) quantization on summaries; (b) two strategies to partition inverted lists; and (c) two methods for building the summary vectors.

**Quantization of Summaries**. We empirically observe that the scalar quantization applied to summaries does not hinder the effectiveness or the efficiency of [Seismic]{.smallcaps}. Indeed, it reduces the memory footprint of the summaries by a factor of $4$.

**Fixed vs. Geometric Blocking**. We delegate inverted list blocking to a clustering algorithm. In this section, we wish to understand the impact of *geometric* clustering on the performance of [Seismic]{.smallcaps}. To that end, we compare two realizations of the index. In one, called "geometric" blocking, we use a variant of K-Means as described in Section [5.2](#sec:methodology:blocking){reference-type="ref" reference="sec:methodology:blocking"}. Separately, in what we call "fixed" blocking, we take the impact-sorted inverted lists and chunk them into fixed-size groups. We then compare the performance of these two configurations on the accuracy-latency trade-off space. Figure [5](#fig:variable_vs_fixed){reference-type="ref" reference="fig:variable_vs_fixed"} reports our results, showing that geometric blocking significantly outperforms fixed blocking for all ranges of hyper-parameters considered.

<figure id="fig:variable_vs_fixed" data-latex-placement="t">
<embed src="imgs/Fixed_vs_KMeans_blocking_new.pdf" />
<figcaption>Fixed vs. geometric blocking. Data sampled from parameters: <span class="math inline">cut ∈ {1, …, 10}</span> and <span class="math inline">heap_factor ∈ {0.7, 0.8, 0.9, 1.0}</span>.<span id="fig:variable_vs_fixed" data-label="fig:variable_vs_fixed"></span></figcaption>
</figure>

**Fixed vs. Importance-based Summaries**. Recall that, our summary vectors are $\alpha$-mass subvectors of the vector produced by Equation ([\[equation:summary\]](#equation:summary){reference-type="ref" reference="equation:summary"}). In a sense, the summary reflects the distribution of documents within a block. Here, we contrast that "importance-based" summary generation with a simple alternative: Keeping a *fixed* number of top entries of the vector from Equation ([\[equation:summary\]](#equation:summary){reference-type="ref" reference="equation:summary"}). The drawback of this alternative is that we store the same number of entries for each block regardless of the number of documents in the block or the distribution of their importance, thus weakening the performance of [Seismic]{.smallcaps}.

Figure [6](#fig:summary_energy){reference-type="ref" reference="fig:summary_energy"} visualizes the latency-accuracy trade-off of these different settings. It is clear that, for a fixed time budget, importance-based summaries lead to better accuracy than fixed-length summaries. Moreover, summaries with $128$ top entries take $2{,}687$ MiB of space, while importance-based summaries with $\alpha=0.5$ consume $2{,}885$ MiB (without quantization). Reducing $\alpha$ to $0.4$ and $0.3$ lowers the size to $2{,}303$ and $1{,}801$ MiB, respectively.

<figure id="fig:summary_energy" data-latex-placement="t">
<embed src="imgs/Energy_vs_fixed_summary_new.pdf" />
<figcaption>Fixed (<span class="math inline">128</span> top entries per summary) vs. importance-based (<span class="math inline"><em>α</em></span>-mass subvectors) summaries.<span id="fig:summary_energy" data-label="fig:summary_energy"></span></figcaption>
</figure>

**Forward Index**. The forward index could use $32$- or $16$-bit floating points to store vector values. We use half-precision, leading to $4{,}113$ MiB of space usage at negligible cost to accuracy and no impact on latency. We confirm that [PyAnn]{.smallcaps} too uses this representation.

<figure id="fig:beirplot" data-latex-placement="t">
<embed src="imgs/BEIR_Plot_new.pdf" />
<figcaption>NDCG@10 on the <span class="smallcaps">Quora</span> dataset.<span id="fig:beirplot" data-label="fig:beirplot"></span></figcaption>
</figure>

# Concluding Remarks {#sec:conclusions}

We presented [Seismic]{.smallcaps}, a novel approximate algorithm that facilitates effective and efficient retrieval over learned sparse embeddings. We showed empirically its remarkable efficiency on a number of embeddings of publicly-available datasets. [Seismic]{.smallcaps} outperforms existing methods, including the winning, graph-based algorithms at the BigANN Challenge in NeurIPS 2023 that use similar-sized (or larger) indexes.

One of the exciting opportunities that our research creates is that it offers a new way of thinking about sparse embedding models. Let us explain how. When [Splade]{.smallcaps} proved difficult to scale because state-of-the-art inverted index-based solutions failed to process queries fast enough, the community moved towards [E-Splade]{.smallcaps} and other variants that reduce query processing time, but that exhibit degraded performance in zero-shot settings. Evidence suggests, for example, that [E-Splade]{.smallcaps} embeddings of [Quora]{.smallcaps}---a [Beir]{.smallcaps} dataset---yield NDCG@10 of $0.76$ while [Splade]{.smallcaps} embeddings yield $0.83$.

[Seismic]{.smallcaps} changes that equation. As we visualize in Figure [7](#fig:beirplot){reference-type="ref" reference="fig:beirplot"}, for any given time budget, [Seismic]{.smallcaps} retrieves a better-quality top-$k$ set from the [Splade]{.smallcaps} embeddings of [Quora]{.smallcaps}. The key take-away message is clear: [Seismic]{.smallcaps} speeds up retrieval over [Splade]{.smallcaps} so dramatically that switching to [E-Splade]{.smallcaps} becomes unnecessary and, in fact, detrimental to both efficiency and effectiveness.

As future work, we intend to explore the application of compression techniques for inverted lists [@CSUR21] to further reduce the size of inverted and forward indexes.

**Acknowledgements**. This work was partially supported by the Horizon Europe RIA "Extreme Food Risk Analytics" (EFRA), grant agreement n. 101093026, by the PNRR - M4C2 - Investimento 1.3, Partenariato Esteso PE00000013 - "FAIR - Future Artificial Intelligence Research" - Spoke 1 "Human-centered AI" funded by the European Commission under the NextGeneration EU program, by the PNRR ECS00000017 Tuscany Health Ecosystem Spoke 6 "Precision medicine & personalized healthcare" funded by the European Commission under the NextGeneration EU programme, by the MUR-PRIN 2017 "Algorithms, Data Structures and Combinatorics for Machine Learning", grant agreement n. 2017K7XPAN_003, and by the MUR-PRIN 2022 "Algorithmic Problems and Machine Learning", grant agreement n. 20229BCXNW.

[^1]: <https://big-ann-benchmarks.com/neurips23.html>

[^2]: The `cocondenser-ensembledistill` checkpoint was obtained from <https://huggingface.co/naver/splade-cocondenser-ensembledistil>.

[^3]: Checkpoint at <https://huggingface.co/naver/splade-cocondenser-ensembledistil>

[^4]: Checkpoints at <https://huggingface.co/naver/efficient-splade-V-large-doc> and <https://huggingface.co/naver/efficient-splade-V-large-query>.

[^5]: <https://github.com/naver/splade>

[^6]: <https://github.com/castorini/pyserini/blob/master/docs/experiments-unicoil.md>

[^7]: <https://big-ann-benchmarks.com/neurips23.html>

[^8]: C++ code is publicly available at <https://github.com/Leslie-Chung/GrassRMA>.

[^9]: C++ code is publicly available at <https://github.com/veaaaab/pyanns>.

[^10]: <https://github.com/castorini/anserini>

[^11]: Our code is publicly available at <https://github.com/TusKANNy/seismic>.
