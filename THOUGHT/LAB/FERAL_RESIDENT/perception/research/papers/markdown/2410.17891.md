# Model {#sec:model}

![The overview of our approach to adapt autoregressive (AR) models to diffusion models. **Left**: The shift operation in AR models enables the output layer $h_i$ to approximate the distribution of next tokens $x_{i+1}$ in hidden representations through the cross entropy (CE) loss. **Middle**: We remove the causal mask gradually during training eventually making our model bi-directional. **Right**: inside the diffusion models we shift the logits to compute the loss with the next token (i.e., the loss on $h_i$ would be with respect to $x_{i+1}$), while perceptually, the diffusion models are still functioning as recovering the original signals (since $h_i$ corresponds to $x_{i+1}$ in AR loss).](fig/pipeline_v3.pdf){#fig:pipeline width="97%"}

We begin by formulating the continuous-time discrete diffusion process (§[1.1](#sec:continuous){reference-type="ref" reference="sec:continuous"}) and establishing a connection between the discrete diffusion and autoregressive objectives (§[1.2](#sec:uni){reference-type="ref" reference="sec:uni"}). Based on this equivalence, we propose an adaptation approach (§[\[sec:ada\]](#sec:ada){reference-type="ref" reference="sec:ada"}) and a sampling algorithm (§[1.4](#sec:sampling){reference-type="ref" reference="sec:sampling"}) for diffusion models adapted from AR models. The whole process is illustrated in Figure [1](#fig:pipeline){reference-type="ref" reference="fig:pipeline"}.

## Continuous-time Discrete Diffusion Processes {#sec:continuous}

Following Eq.[\[eq:qxt\]](#eq:qxt){reference-type="ref" reference="eq:qxt"} and $q(\bm{x}_t|\bm{x}_0)=\sum_{\bm{x}_s}q(\bm{x}_t|\bm{x}_s)q(\bm{x}_s|\bm{x}_0)$, the forward transition distribution between arbitrary points $s < t$ can be derived as $$\begin{equation}
    q(\bm{x}_t|\bm{x}_s)=\text{Cat}(\bm{x}_t;\bm{\overline{Q}}_{s|t}^\top \bm{x}_s) =\frac{\alpha_t}{\alpha_s}\bm{x}_s+(1-\frac{\alpha_t}{\alpha_s})\bm{m},
\end{equation}$$ with $\bm{\overline{Q}}_{s|t} \coloneqq \bm{\overline{Q}}_s^{-1}\bm{\overline{Q}}_t=\frac{\alpha_t}{\alpha_s}I+(1-\frac{\alpha_t}{\alpha_s})\mathbf{1}\bm{m}^\top$. The corresponding backward transition distribution conditional on $\bm{x}_0$ is also available in closed form, $$\begin{equation}
\label{eq:qxs}
    q(\bm{x}_s|\bm{x}_t, \bm{x}_0) = \frac{q(\bm{x}_t|\bm{x}_s)q(\bm{x}_s|\bm{x}_0)}{q(\bm{x}_t|\bm{x}_0)}=
    \begin{cases}
        \frac{\alpha_s-\alpha_t}{1-\alpha_t}\bm{x}_0+\frac{1-\alpha_s}{1-\alpha_t}\bm{m} & \text{if } \bm{x}_t=\bm{m}, \\
        \bm{x}_0 & \text{if }\bm{x}_t\neq\bm{m}. \\
    \end{cases}
\end{equation}$$ In discrete diffusion processes, we aim to approximate the backward transition distribution $q(\bm{x}_s|\bm{x}_t,\bm{x}_0)$ using a denoising model $p_{\theta}(\bm{x_s}|\bm{x}_t, f_{\theta}(\bm{x}_t))$, where $f_{\theta}(\bm{x}_t)$, an approximation of $\bm{x}_0$, is usually the output of neural networks such as a transformer [@NIPS2017_3f5ee243]. We can define the denoising model to have a similar form of backward transitions as $p_{\theta}(\bm{x_s}|\bm{x}_t)=\frac{\alpha_s-\alpha_t}{1-\alpha_t}f_{\theta}(\bm{x}_t)+\frac{1-\alpha_s}{1-\alpha_t}\bm{m}$. According to the training objective in Eq.[\[eq:ori-loss\]](#eq:ori-loss){reference-type="ref" reference="eq:ori-loss"}, the KL-divergence of $\mathcal{L}_T$ at each step $t$ can be simplified to a reweighted cross-entropy function, $$\begin{equation}
    \KL(q(\bm{x}_s|\bm{x}_t,\bm{x}_0)||p_{\theta}(\bm{x}_s||\bm{x}_t))=-\frac{\alpha_s-\alpha_t}{1-\alpha_t}\delta_{\bm{x}_t,\bm{m}}\bm{x}_0^\top\log f_{\theta}(\bm{x}_t),
\end{equation}$$ where $\delta_{a,b}$ is the indicator function for $a=b$. If we take the limit and let $T\rightarrow \infty$, the first two terms of Eq.[\[eq:ori-loss\]](#eq:ori-loss){reference-type="ref" reference="eq:ori-loss"} will approach 0 and some constant, respectively. Thus the evidence lower bound (ELBO) effectively becomes $\mathcal{L}_T$ and $$\begin{equation}
\label{eq:L_T}
    \lim_{T\rightarrow \infty}\mathcal{L}_T = \int_{0}^{1}\frac{\alpha_t^\prime}{1-\alpha_t}\mathbb{E}_{q(\mathbf{x}_t|\mathbf{x}_0)}[\delta_{\bm{x}_t,\bm{m}}\bm{x}_0^\top\log f_{\theta}(\bm{x}_t)]\,dt.
\end{equation}$$ The full derivation is listed in Appendix [\[appendix:loss\]](#appendix:loss){reference-type="ref" reference="appendix:loss"}. The same form of ELBO which is invariant to noise schedule but related to the signal-to-noise ratio (SNR) is also introduced in @kingma2021variational [@shi2024simplified]. Following @austin2021structured, we choose the noise schedule $\alpha_t=1-t$, then $\frac{-\alpha_t^\prime}{1-\alpha_t}=\frac{1}{t}$. The previous discussion focused on the single token $\bm{x}_t$, and can be applied independently to a text sequence of $N$ tokens $\mathbf{x}_t=[\bm{x}_t^{1}, \bm{x}_t^{2}\dots, \bm{x}_t^{N}]$. During training, we do not compute integral loss in Eq.[\[eq:L_T\]](#eq:L_T){reference-type="ref" reference="eq:L_T"} for efficiency consideration; instead, we sample $t$ for each data point. The final loss at $t$ is $$\begin{equation}
% \textstyle
\label{eq:dm-loss}
    \mathcal{L}_{t}^{1:N} = \frac{1}{t}\mathbb{E}_{q(\mathbf{x}_t|\mathbf{x}_0)}\left[-\sum_{n=1}^N\delta_{\mathbf{x}_t^n,\bm{m}}(\mathbf{x}_0^{n})^\top\log f_{\theta}(\mathbf{x}_t^{1:N})_n\right],
\end{equation}$$ where $f_{\theta}(\mathbf{x}_t^{1:N})_n$ denotes the whole input sequence is fed into the transformer model and the $n$-th output token is indexed.

## Unifying Language Modeling Objectives {#sec:uni}

The training objective of autoregressive (AR) language models is the negative log-likelihood of each ground-truth token provided the preceding tokens, $$\begin{equation}
\label{eq:ce}
    \mathcal{L}_{AR}^{1:N} = -\sum_{n=1}^N (\mathbf{x}_0^{n})^\top \log f_{\theta}(\mathbf{x}_0^{1:n-1})_{n-1}.
\end{equation}$$ Comparing Eq.[\[eq:ce\]](#eq:ce){reference-type="ref" reference="eq:ce"} against Eq.[\[eq:dm-loss\]](#eq:dm-loss){reference-type="ref" reference="eq:dm-loss"}, we note that while both take the form of cross-entropy functions, Eq.[\[eq:dm-loss\]](#eq:dm-loss){reference-type="ref" reference="eq:dm-loss"} includes an additional reweighting term $\frac{1}{t}$ and an indicator function $\delta_{\mathbf{x}_t^n,\bm{m}}$. They result from the definition of discrete diffusion processes (§[1.1](#sec:continuous){reference-type="ref" reference="sec:continuous"}). The reweighting emphasizes smaller $t$ where $\mathbf{x}_t$ contains fewer masked tokens, and this can be regarded as the importance sampling [@nichol2021improved]. The indicator specifies which tokens are masked for prediction. The AR training objective Eq.[\[eq:ce\]](#eq:ce){reference-type="ref" reference="eq:ce"}, on the other hand, constrains the context to be unidirectional via attention masking and shifts the targets so that each token predicts the next token instead of itself. These discrepancies form the basis of our adaptation framework, which is detailed in §[\[sec:ada\]](#sec:ada){reference-type="ref" reference="sec:ada"}.

In fact, an alternative way to understand AR modeling, through the lens of diffusion models, is to consider a diffusion process where the forward pass deterministically masks right-to-left and token-by-token [@austin2021structured; @hoogeboom2022autoregressive]. This yields a backward process generating one token at a time from left to right, running with $T=N$ denoising steps in total. As discussed in @austin2021structured, the loss objective of this diffusion process is equivalent to standard cross-entropy (Eq.[\[eq:ce\]](#eq:ce){reference-type="ref" reference="eq:ce"}) commonly used to train AR language models. This crafted diffusion process for AR models represents a special case of discrete diffusion (§[1.1](#sec:continuous){reference-type="ref" reference="sec:continuous"}), yet it is limited to unidirectional context and sequential token generation. In contrast, general discrete diffusion processes can leverage bidirectional context and support parallel generation in arbitrary orders.

## Adaptation

Building on the connection between AR modeling and discrete diffusion processes, we construct an adaptation recipe next. Figure [1](#fig:pipeline){reference-type="ref" reference="fig:pipeline"} shows an overview of our adaptation approach. We use attention mask annealing, shift operations, and a time-embedding free architecture to narrow the differences between AR and DLMs.

[]{#sec:ada label="sec:ada"}

<figure id="algo:line-sample-shift" data-latex-placement="ht">
<div class="minipage">
<div class="algorithm">
<div class="algorithmic">
<p><strong>Input:</strong> network <span class="math inline"><em>f</em><sub><em>θ</em></sub></span> initialized by existing models, training corpus <span class="math inline"><em>p</em><sub><em>d</em><em>a</em><em>t</em><em>a</em></sub>(<strong>x</strong><sub>0</sub><sup>1 : <em>N</em></sup>)</span>, mask token <span class="math inline"><strong>m</strong></span>. <strong>Output:</strong> model parameters <span class="math inline"><em>θ</em></span>. Draw <span class="math inline"><strong>x</strong><sub>0</sub><sup>1 : <em>N</em></sup> ∼ <em>p</em><sub><em>d</em><em>a</em><em>t</em><em>a</em></sub></span> and set <span class="math inline"><em>labels</em> ← <strong>x</strong><sub>0</sub><sup>1 : <em>N</em></sup></span> Sample <span class="math inline"><em>t</em> ∈ <em>Uniform</em>(0, 1)</span> Sample <span class="math inline"><strong>x</strong><sub><em>t</em></sub><sup>1 : <em>N</em></sup> ∼ <em>q</em>(<strong>x</strong><sub><em>t</em></sub>|<strong>x</strong><sub>0</sub>)</span> Anneal the attention mask <span class="math inline"><em>a</em><em>t</em><em>t</em><em>n</em>_<em>m</em><em>a</em><em>s</em><em>k</em></span> Forward <span class="math inline"><em>logits</em> ← <em>f</em><sub><em>θ</em></sub>(<strong>x</strong><sub><em>t</em></sub><sup>1 : <em>N</em></sup>)</span> with <span class="math inline"><em>a</em><em>t</em><em>t</em><em>n</em>_<em>m</em><em>a</em><em>s</em><em>k</em></span> Right shift <span class="math inline"><em>logits</em></span> by one position <span id="algo:line-loss" data-label="algo:line-loss"></span> <span class="math inline">$\mathcal{L}_t=\frac{1}{t}\delta_{x_t, m}$</span>CE<span class="math inline">(<em>logits</em>, <em>labels</em>)</span> <span class="math inline">⊳</span> Eq.<a href="#eq:dm-loss" data-reference-type="ref" data-reference="eq:dm-loss">[eq:dm-loss]</a> Backprop with <span class="math inline">ℒ<sub><em>t</em></sub></span> and update <span class="math inline"><em>θ</em></span></p>
</div>
</div>
</div>
<div class="minipage">
<div class="algorithm">
<div class="algorithmic">
<p><strong>Input:</strong> Trained diffusion model <span class="math inline"><em>f</em><sub><em>θ</em></sub></span>, sampling algorithm <span class="math inline"><em>τ</em></span>, mask token <span class="math inline"><strong>m</strong></span>, start token <span class="math inline"><strong>s</strong></span>. <strong>Output:</strong> generated sample <span class="math inline"><strong>x</strong><sub>0</sub></span>. <strong>Initialize</strong> <span class="math inline"><strong>x</strong><sub><em>T</em></sub><sup>1 : <em>N</em></sup> = <strong>m</strong></span>. Forward <span class="math inline"><em>logits</em> ← <em>f</em><sub><em>θ</em></sub>(<strong>x</strong><sub><em>t</em></sub><sup>1 : <em>N</em></sup>)</span> Sample <span class="math inline">$\tilde{\bm{x}}_0^{1:N}\sim \textit{Categorical}(\tau(\textit{logits}))$</span> <span class="math inline">$\bm{x}_{t-1}^n=q(\bm{x}_{t-1}^n|\bm{x}_{t}^n,\tilde{\bm{x}}_0^n)$</span> <span class="math inline">⊳</span> Eq.<a href="#eq:qxs" data-reference-type="ref" data-reference="eq:qxs">[eq:qxs]</a> Right shift <span class="math inline"><strong>x</strong><sub><em>t</em> − 1</sub><sup>1 : <em>N</em></sup> = [<strong>s</strong>, <strong>x</strong><sub><em>t</em> − 1</sub><sup>1 : <em>N</em> − 1</sup>]</span> <span id="algo:line-sample-shift" data-label="algo:line-sample-shift"></span> <strong>Return</strong> <span class="math inline"><strong>x</strong><sub>0</sub><sup>2 : <em>N</em></sup></span></p>
</div>
</div>
</div>
<figcaption>Sampling</figcaption>
</figure>

#### Attention Mask Annealing {#sec:mask-anneal}

The prediction of the $n$-th token, given all preceding tokens, $f_{\theta}(\mathbf{x}_0^{1:n-1})$, is usually implemented by causal attention masking in transformer-based AR language models. As shown in Figure [1](#fig:pipeline){reference-type="ref" reference="fig:pipeline"}, causal attention masks set all entries in the upper triangle of the self-attention matrices to zero, so each token cannot attend to its respective future tokens. Such causal masking prevents the model from learning right-to-left dependencies for more general diffusion processes. To address this limitation while preserving left-to-right conditionals during adaptation, we introduce an incremental annealing process from causal masks to full attention matrices. During annealing, the causal mask is not immediately removed; instead, it is retained at a controlled ratio, as shown in the middle part of Figure [1](#fig:pipeline){reference-type="ref" reference="fig:pipeline"}. At each training step, we sample the amount of context from the right side and progressively increase this amount till we obtain the full attention mask.

#### Shift Operation {#sec:shift-ope}

AR models also apply a shifting operation, where the target output is the input sequence shifted left by one position. In other words, the prediction target of the $(n\!-\!1)$-th token is the $n$-th token, contrasting with typical diffusion models that try to predict masked tokens at their original positions. When initializing text diffusion models with AR model parameters, the model would tend to output the hidden representations of the shifted input sequence. If we continue to optimize the cross-entropy objective based on the original token positions, the model struggles to adapt due to misalignment between input and output. Instead, we maintain the shift operation (Algo.[\[alg:training\]](#alg:training){reference-type="ref" reference="alg:training"}, line [\[algo:line-loss\]](#algo:line-loss){reference-type="ref" reference="algo:line-loss"}), treating the output logits at each position as corresponding to the next token. When calculating the objective, we align prediction targets so that the diffusion model learns to recover the original signals. This process is illustrated in the right panel of Figure [1](#fig:pipeline){reference-type="ref" reference="fig:pipeline"}.

#### Time-Embedding-Free Architecture

Many diffusion models for text generation [@li2022diffusion; @dieleman2022continuous; @gulrajani2023likelihoodbased; @lou2023discrete; @shi2024simplified] incorporate time embedding layers to represent the information of current timesteps $t$, which can explicitly indicate the noise scale of the input noisy data. While inferring these timesteps can be challenging for image diffusion models [@ho2020denoising; @li2024autoregressive], some discrete text diffusion models [@he-etal-2023-diffusionbert] assert that timesteps $t$ can be easily learned implicitly based on the number of mask tokens. Since AR models are not equipped with time embedding layers, we also choose not to use the time embedding, resulting in no additional parameters compared to previous diffusion models.

## Sampling {#sec:sampling}

Following @shi2024simplified, we initialize $\bm{x}_T$ with all tokens and then sample tokens according to the time reversal $q(\bm{x}_s|\bm{x}_t, \bm{x}_0)$ in Eq.[\[eq:qxs\]](#eq:qxs){reference-type="ref" reference="eq:qxs"}. At each timestep, if $\bm{x}_t$ is a mask, it will jump to the predicted $\bm{x}_0$ at time $s$ with probability $\frac{\alpha_s-\alpha_t}{1-\alpha_t}$. After $T$ iterations, the model generates the full sequence. Since our adapted models are trained with the shift operation, at each sampling iteration, we shift back the generated sentence and prepend a start token before the next forward pass (Algo.[\[alg:sampling\]](#alg:sampling){reference-type="ref" reference="alg:sampling"}, line [2](#algo:line-sample-shift){reference-type="ref" reference="algo:line-sample-shift"}). Usually larger $T$ requires more interactions of computation, and can yield texts in higher quality, and this trade-off can be controlled easily through $T$. Through experiments, we find that the output generated by diffusion models is diverse and scattered. Therefore, for conditional generation tasks, we improve the sampling procedure to ensure that only tokens with high probabilities from neural networks are denoised [@ghazvininejad-etal-2019-mask; @maskgit; @Zheng2023ARD], so that the model could predict tokens mostly relevant to the input. In addition, existing sampling techniques for AR language models, including top-$k$ and nucleus sampling [@holtzmancurious], can be seamlessly applied to diffusion models as well.
