# Introduction

Recent advances in deep learning led to exceeding human-level performance in many tasks, including computer vision, machine translation, voice recognition, and language understanding. Real-world applications of DNNs rely heavily on their efficiency. Both mobile and cloud platforms greatly benefit from reduced latency and energy efficiency achieved by some form of model compression. In this work, we consider two mainstream techniques used in practice; pruning and quantization.

Pruning methods remove individual weights [@zhu2017prune; @han2015learning], or sometimes groups of weights [@he2017channel; @mishra2022semipruningnvidia]. This procedure can reduce the memory footprint. Furthermore, not having to perform the computations with weights that are zeroed out can make network inference more efficient. On the other hand, quantization reduces the bit-width used for both the weights and the computation used in networks, leading to both predictable memory savings and reductions in the necessary compute. In both scenarios, the hardware used for making use of these optimization schemes needs to take them into account.

Depending on the availability of training data and computing budget, most methods for pruning and quantization fall into one of two families. The first family includes fine-tuning approaches, namely quantization-aware training (QAT) and fine-tuning with pruning in the loop. The second family includes post-training approaches such as post-training quantization (PTQ). Previously, pruning techniques primarily relied on fine-tuning; however, some post-training pruning methods appeared recently as fine-tuning is not desirable for large language models [@frantar2023sparsegpt].

Despite the importance of model efficiency and the plethora of approaches for pruning and quantization, the two fields are mostly disjoint. The literature presents little insight into which of the two techniques is more accurate. In practice, there is only limited time to compress a network and limited energy to spend on making deep learning inference hardware. For this reason, we ask the question: Should one focus on quantization or pruning for compression?

We present an extensive study comparing pruning and quantization in equal settings. First, we consider different data distributions and analyze the conditions under which each method is preferable. We match our findings with real weight tensors from pre-trained models. Second, we consider a post-training scenario and evaluate single-layer output errors for both methods. Because the comparison might depend on the specific choice of optimization method, we compare the two with theoretical bounds that apply regardless of the optimization method. Finally, we provide a full-model comparison for the most common scenario of fine-tuning networks after either pruning or quantization.

In our comparison, we intentionally avoid considering the hardware aspects of pruning and quantization. Instead, we focus solely on the accuracy of both methods, given similar theoretical compression ratios. A coarse discussion on the hardware necessary for both methods can be found in section [6](#sec:discussion){reference-type="ref" reference="sec:discussion"}.

# Assumptions

In our work, we assume FP16 as the basic data type and measure any gains in compression with respect to it. Using FP16 for inference generally does not lead to a loss in accuracy. Neural networks are also very commonly trained with FP16, making it a common baseline. Thus, we compare 50% pruning sparsity to INT8 quantization, 75% sparsity to INT4 quantization and so forth. We also assume no overhead on storing the sparsity mask for pruning and relegate such hardware-specific implementations to section [6](#sec:discussion){reference-type="ref" reference="sec:discussion"}.

For the pruning experiments, we consider magnitude pruning. It is common to do fine-tuning after or during pruning [@zhu2017prune]. Several works have independently shown that despite its simplicity, it is tough to improve upon magnitude pruning and fine-tuning [@gale2019state; @blalock2020state]. To our knowledge, no pruning algorithm exists that consistently outperforms this method.

For the quantization experiments, we use symmetric uniform quantization, which is defined by just the quantization scale factor and the bit-width. The scale is represented as a floating-point number and is used to map floating-point values to the integer grid. Further details on symmetric uniform quantization can be found in [@Nagel2021AWP]. Uniform quantization is the standard in the quantization literature, and symmetric quantization is mostly employed for the weights. In all our experiments, we use a quantization range estimator minimizing the mean-squared error on weights by grid search [@Nagel2021AWP].

# Comparison on statistical distributions {#sec:comparison_distr}

<figure id="fig:quant_pruning_gaussian" data-latex-placement="t">
<div class="small">
<table>
<tbody>
<tr>
<td style="text-align: center;"><img src="fig/quant_pruning_error_backup.png" style="width:90.0%" alt="image" /></td>
</tr>
</tbody>
</table>
</div>
<figcaption>Comparison for a standard normal distribution. (left) Distributions after pruning and quantization for INT4 and 75% pruning. (middle) The squared error weighted by probability. (right) SNR for different compression ratios.</figcaption>
</figure>

Before diving into comparison results, we first describe theoretically what the quantization error and pruning error are. Looking at this with a theoretical lens helps with understanding the later experimental difference between the two methods. We start off by describing and analyzing both methods on simple data distributions.

In order to compare the error of pruning and quantization, we will frequently use the signal-to-noise ratio measure defined in the log scale: $\text{SNR}_{dB}=10\log_{10}\left(\mathbb E\left[ W^2\right]/  \mathbb E \left[ (W-F(W))^2 \right]\right)$, where $F(W)$ is the quantization or pruning function. This measure is the same as a scaled logarithm of an MSE measure. Both are often employed to analyze the sensitivity of neural network layers to quantization, and they are theoretically well-founded to correlate with network performance [@lin2016fixed; @adaround].

## Quantization error

For quantization, we consider symmetric uniform quantization, which is also called integer quantization. Given a bit-width $b$ and the scale $\delta$, the grid nodes are defined as $q_i = \delta i, i \in \{-2^b, \dots, 0, 2^b-1\}$. The quantization operation rounding-to-nearest $Q(w)$ and the corresponding quantization error $R(w)$ are defined as: $$\begin{align}
\label{eq:fp8_nearest}
Q(w) = q_i, \text{ } i=\mathop{\mathrm{arg\,min}}_{i}|w-q_i|, & & R(w) = Q(w) - w.
\end{align}$$

<figure id="fig:kurt" data-latex-placement="t">
<div class="small">
<table>
<tbody>
<tr>
<td style="text-align: center;"><img src="fig/kurtosis_st_mse.png" style="width:13cm" alt="image" /></td>
</tr>
</tbody>
</table>
</div>
<figcaption>Comparing the error of pruning and quantization for a student-t distribution, simulating the presence of significant outliers. We plot the results for different magnitudes of the outliers, as per the kurtosis on the x-axis. (left) the pruning error, which does not change under the presence of more severe outliers. (middle) the quantization SNR, which is reduced greatly when outliers increase (right) the trade-off regions where quantization and pruning are better.</figcaption>
</figure>

Following [@kuzmin2022fp8] we model neural network weights as a random variable $W \sim p(w)$. The expected value of the quantization MSE can be expressed as follows: $$\begin{equation}
\label{eq:quant_error}
\begin{split}
{\mathbb E}\left[\left(Q(W)-W)^2\right)\right]=
\int\limits_{q_{min}}^{q_{max}}R^2(w)p(w)dw + 
\int\limits_{-\infty}^{q_{min}}(w-q_{min})^2p(w)dw + \int\limits_{q_{max}}^{\infty}(q_{max}-w)^2p(w)dw,
\end{split}
\end{equation}$$ where $q_{min}=\min_{i}{q_i}$ and $q_{max}=\max_{i}{q_i}$ are the quantization range limits. The left term corresponds to the rounding error, and the right two terms correspond to the clipping error. We use this analytic formulation for our distribution results below, the details are given in appendix [10](#section:appendix_analytical_error_mse){reference-type="ref" reference="section:appendix_analytical_error_mse"}.

## Pruning error

We consider magnitude pruning $T(x)= x \cdot \mathds{1}_{-t \leq x \leq t}$. This simply sets the values closest to zero to actual zero. Given this, the expected error of pruning is expressed as follows: $$\begin{equation}
\label{eq:pruning_error}
\begin{split}
{\mathbb E}\left[T(W)^2\right]=
\int\limits_{-t}^{t}w^2p(w)dw,
\end{split}
\end{equation}$$ where $t$ is the threshold value that controls how much is pruned. Given the compression ratio $c\in(0,1)$, we find the threshold value which satisfies $P(-t \leq W \leq t)=c$. In case of a symmetric zero-mean distribution, the threshold can be expressed as $t=F_{W}^{-1}\left(\frac{1}{2}+\frac{c}{2}\right)$, where $F(w)=P(W\leq w)$ is the CDF function and $F^{-1}(p)$ is its inverse. The expected pruning error in equation [\[eq:pruning_error\]](#eq:pruning_error){reference-type="ref" reference="eq:pruning_error"} is similar to the clipping error for quantization (see the second and the third term in equation [\[eq:quant_error\]](#eq:quant_error){reference-type="ref" reference="eq:quant_error"}), and can also be computed analytically. We also use this formulation for our results below.

## Analytical comparison

**Standard normal distribution.** Let us first look at a standard normal distribution. As many weights in neural networks are roughly Gaussian-shaped, this distribution is useful for our understanding of the comparison. As we can see from figure [1](#fig:quant_pruning_gaussian){reference-type="ref" reference="fig:quant_pruning_gaussian"} (middle), the errors for both methods have very different behavior. The quantization error oscillates between the quantization nodes and has a moderate range. The pruning error effectively corresponds to rounding many weights to zero and thus has a higher error. As we can see in figure [1](#fig:quant_pruning_gaussian){reference-type="ref" reference="fig:quant_pruning_gaussian"} (right), this results in a higher SNR for quantization, e.g. 19.1 dB for INT4 quantization versus only 5.6 dB for 75% pruning. We see similar results for different compression ratios. For this distribution, quantization achieves a much higher signal-to-noise ratio.

<figure id="fig:quant_pruning_tensors">
<table>
<tbody>
<tr>
<td style="text-align: center;"><img src="fig/combined_level1_2.png" style="width:13cm" alt="image" /></td>
</tr>
</tbody>
</table>
<figcaption>(left) Comparison on all the weights from PyTorch model zoo (46 models) combined with 3 large language models (Bloom-3b, Llama-3b, OPT-2.7b). (left) Pruning SNR versus quantization SNR for every tensor. (right) Pruning is preferable at high compression ratios for tensors with high sample kurtosis values.</figcaption>
</figure>

**Distributions with heavy tails.** The trade-off is expected to change when more significant outliers are introduced. The quantization grid is expected to be effected strongly by outliers as it increases the quantization grid in size, whereas the pruning method is expected to be hardly effected with outliers as it only affects weights around zero. We thus analyze both quantization and pruning errors in the presence of many outliers. To simulate a distribution with outliers, we use a truncated Student's-t distribution with $\nu=2$, and a symmetric range $(-r,r)$ (the PDF is defined in appendix [11](#section:appendix_truncated_st){reference-type="ref" reference="section:appendix_truncated_st"}). This distribution is nice as it gives a non-trivial weight to the tail ends of the distribution close to $r$. The wider the range $r$ is, the heavier are the tails of the distribution.

In order to introduce a quantitative measure of the number of outliers, we will use the distribution's kurtosis given by $\text{Kurt}[X]={\mathbb E}\left[\left(X-\mu\right)^4\right]/\left({\mathbb E}\left[\left(X-\mu\right)^2\right]\right)^2$, where $\mu$ is the mean. We will see later that this kurtosis measure is predictive of quantiation and pruning performance for real layers. To increase the number of outliers, we will increase the range $r$. The results are given in figure [2](#fig:kurt){reference-type="ref" reference="fig:kurt"}. The kurtosis range is chosen so that it includes most of the weights from the model zoo. We see that despite the significant outliers and high kurtosis, quantization still has higher SNR in most of the cases for moderate compression. Pruning is better however in the region of high clipping range and very high compression rate, e.g. 2-3 bits per value (see figure [2](#fig:kurt){reference-type="ref" reference="fig:kurt"} on the right).

## Experiments on real weight tensors

The previous discussion was mostly theoretical. We set out to see happens when we do a similar analysis on real neural network weights. In order to investigate this, we compare the pruning and quantization SNR on the weight tensors for all the pre-trained models from the PyTorch model zoo[^3] (46 models in total, the details are give in appendix [14](#section:appendix_pytorch_zoo_tensors_details){reference-type="ref" reference="section:appendix_pytorch_zoo_tensors_details"}) combined with weight tensors from 3 large language models, namely Bloom-3b [@bigscience_workshop_2022], Llama-3b [@openlm2023openllama], OPT-2.7b [@zhang2022opt]. Each tensor is quantized using an integer grid of bit widths from 2 to 8. The results are shown in the figure [3](#fig:quant_pruning_tensors){reference-type="ref" reference="fig:quant_pruning_tensors"}(left). We see a similar trend to our previous discussion that pruning becomes more beneficial for lower bitwidth/higher sparsity ratios.

In order to match the analytical results from figure [2](#fig:kurt){reference-type="ref" reference="fig:kurt"}, we consider the sample kurtosis of every weight tensor given by $k=\frac{1}{n}\sum_{i=1}^n(x_i-\overline{x})^4 / \left[\frac{1}{n}\sum_{i=1}^n(x_i-\overline{x})^2\right]^2$. See figure [3](#fig:quant_pruning_tensors){reference-type="ref" reference="fig:quant_pruning_tensors"} (right). We consider a range of kurtosis values for every quantization bit-width. Using a kernel density estimator, we compute the probability density of encountering a tensor for which pruning has higher SNR than quantization SNR. We compare the PDF to that for quantization and thus determine the region where each method is preferable. The results are given in figure [3](#fig:quant_pruning_tensors){reference-type="ref" reference="fig:quant_pruning_tensors"} on the right. We see that the results from the previous theoretical section (figure [2](#fig:kurt){reference-type="ref" reference="fig:kurt"} on the right) hold very nicely. We can also see that as predicted, the kurtosis is indeed a good metric for predicting if a tensor should be quantized or pruned for optimal accuracy.

# Per-layer comparison {#sec:per_layer_comparison}

Most PTQ methods compress the model layer by layer. Given one layer, we use the mean-squared error of the output activations as an objective for optimization. As [@adaround] shows, minimizing per layer MSE on the output activations of each layer is a computationally affordable second-order approximation of the loss function. The local MSE objective correlates well with the task loss and is often used in practice in DNN compression and quantization literature [@hubara2021accurate; @brecq; @zhang2015accelerating]. Our experiments in appendix [13](#section:appendix_sqnr_correlation){reference-type="ref" reference="section:appendix_sqnr_correlation"} confirm this. For the experiments in this section, we will use SNR as it represents a normalized version of MSE. As opposed to section [3](#sec:comparison_distr){reference-type="ref" reference="sec:comparison_distr"} where we used SNR on weights, in this section, we will use SNR on the output activations instead.

The goal of a PTQ method is to minimize the error in the output activations of the compressed layer by optimizing over the quantized weights subject to integer range constraints. Similarly, for pruning, the weights are optimized subject to a sparsity constraint. As the underlying combinatorial optimization problem for both methods is NP-hard [@pia2017mixed; @foucart_mathematical_2013], in practice, each method relies on some form of heuristic providing a reasonably good solution given a realistic compute budget. This means that any practical comparison between pruning and quantization would depend on the choice of the method for both and would be open to debate of the optimality of the algorithm. In order to eliminate this dependence, we provide a tight lower bound on the output errors for quantization. For pruning we provide a way to solve the problem exactly for moderate dimensionalities. This way, we can provide a comparison that holds regardless of the algorithm used for each method.

## Post-training quantization {#sec:sdp_lower_bound}

We set out to formulate a way by which we can get relatively tight bounds for comparison when quantizing a single layer with the MSE as the objective. The higher bound is simple to obtain by using a solution with a heuristic quantization algorithm, but for the lower bound, we have to reformulate the problem. The mean-squared error of the output activations of a quantized layer can be expressed as: $$\begin{align}
\min_{\mathbold{w}}E(\mathbold{w}) = & \left\lVert\mathbold{X} \delta \mathbold{w}-\mathbold{X}\mathbold{w}_{orig}\right\rVert_2^2 \\
\text{s.t. } &\mathbold{w} \in \mathbb{Z}^{n},  \nonumber \\
&w_{min} \leq w_i \leq w_{max},  \nonumber
\end{align}$$ where $X$ is the input data in an unfolded form, and $w_{orig}$ are the floating point weights. The quantized weights are computed as the product of the quantization scale $\delta$, and the integer weights $\mathbold{w}$. $w_{min}$ and $w_{max}$ are the integer limits. We ignore the averaging operation to simplify the notation, as it is not important for optimization. We also note that this problem can be solved independently for each output channel of a convolution or every row of a fully-connected layer weight.

This problem is an instance of a mixed-integer quadratic program: $$\begin{align}
\tilde{E}(\mathbold{w})= & \frac{1}{2} \mathbold{w}^T \mathbold{P} \mathbold{w}-\mathbold{q}^T \mathbold{w}, \\
 \text{s.t. } &\mathbold{w} \in \mathbb{Z}^{n},  \nonumber\\
 &w_{min} \leq w_i \leq w_{max}, \nonumber
\end{align}$$ where $\mathbold{P}=2\delta^2 \mathbold{X}^T\mathbold{X}$, $\mathbold{q} = 2(\mathbold{w}_{orig}^T\mathbold{X}^T)\mathbold{X} \delta$. In order to simplify the objective, we can omit the constant term that is irrelevant for the optimization $c=\left\lVert\mathbold{X}\mathbold{w}_{orig}\right\rVert_2^2$, i.e. $\tilde {E}(\mathbold{W})=E(\mathbold{W})-c$.

In order to find the lower bound of the objective, we follow [@park2018semidefinite] and relax the integer constraint to $w_i (w_i - 1) \geq 0$, which allows the weight to take values within the interval from 0 to 1. In order to obtain the lower bound, we will consider the dual version of the relaxed problem: $$\begin{align}
L(\mathbold{\lambda})= & \max -\gamma, \\
\text{s.t. } & \begin{bmatrix}
\mathbold{P}-\mathbf{diag}(\mathbold{\lambda}) & q + \frac{1}{2} \lambda \\
\left(q + \frac{1}{2}\lambda\right)^T  & \gamma 
\end{bmatrix} \succeq 0,   \nonumber \\ 
&\mathbold{\lambda} \geq 0,   \nonumber
\label{eq:sdp_dual}
\end{align}$$ where $\mathbold{\lambda} \in \mathbb{R}^n$, $\gamma \in \mathbb{R}$. The dual problem is convex, and its solution can be used as a lower bound on the solution of the original problem, i.e., $\tilde{E}(\mathbold{w}) \geq L(\mathbold{\lambda})$. The dual has a semi-definite constraint which can be solved with a semi-definite programming (SDP) solver with $\mathcal{O}(n^3)$ complexity. In our work, we used CVX solver  [@cvx]. As discussed in [@park2018semidefinite], this bound is a computationally efficient alternative to branch-and-bound approaches, while tightness is better than that for the alternative methods introduced in [@buchheim2015ellipsoid]. We use this approach for estimating the lower bound for MSE on the output activations for PTQ below.

## Post-training pruning

We also need a similar lower bound for pruning for comparison. To the best of our knowledge we are not aware of the ways to provide a tight lower bound for this problem, therefore we formulate a way to solve a problem for moderate dimensionalities exactly. Similar to quantization, post-training pruning of one layer of the network can mathematically be expressed as solving the following optimization problem: $$\begin{align}
E &= \min_{\mathbold{\hat{w}}} \left\lVert\mathbold{X}\hat{\mathbold{w}}-\mathbold{X}\mathbold{w}_{orig}\right\rVert_2^2 \\
\text{s.t.} &\left\lVert\hat{\mathbold{w}}\right\rVert_0 \leq s,  \nonumber
\label{eq:pruning_l0}
\end{align}$$ where the number of non-zero elements $s$ in the solution is theoretically constrained by using the $L_0$ norm, which is non-convex and not smooth. In order to solve the problem, we introduce the sparsity mask $m \in \mathbb{R}^n$: $$\begin{align}
E(\mathbold{w}) =& \min_{\mathbold{w},\mathbold{m}} \left\lVert\mathbold{X}(\mathbold{m}\odot\mathbold{w}) -\mathbold{X}\mathbold{w}_{orig}\right\rVert_2^2, \\
 \text{s.t.} &\left\lVert\mathbold{m}\right\rVert_1 = s, \nonumber \\
 & -\mathbold{m} \odot l \leq \mathbold{\hat{w}}  \leq \mathbold{m} \odot u \nonumber\\ 
 & l,u > 0, m_i \in \{0,1\}, \nonumber
\end{align}$$ where $\odot$ is an element-wise product operation, and $l,u \in \mathbb{R}$ are constants chosen such that any solution satisfies the constraint $-\mathbold{m} \odot l \leq \mathbold{\hat{w}}  \leq \mathbold{m} \odot u$. We solve this problem using the branch-and-bound method implemented in the Gurobi solver [@gurobi] that gives the global solution.

## Experiments

With our algorithms in the bag, we can now compare quantization versus pruning in the post-training settings with theoretical bounds. In each case, we analyze individual layers of several networks. Given a batch of input data, we optimize the pruned or quantized weights to minimize the error between the output activations and the output of the uncompressed layer. We provide a range between two SNR values for each method in each case. The performance of the heuristic method gives the first value, and the second value is given by the error lower bound or the global solution, which translates into SNR upper bound.

As a heuristic method for pruning, we use magnitude pruning with a fixed sparsity mask $m$ and data-optimized weights $\mathbold{w}$ given by $\mathbold{w} = \underset{\rm \mathbold{w}}{\rm \text{argmin}} \left\lVert\mathbold{X}(\mathbold{m}\odot\mathbold{w}) -\mathbold{X}\mathbold{w}_{orig}\right\rVert_2^2$. This is a convex problem and has a unique solution. As a heuristic method for quantization, we use the mixed-integer solver introduced in [@park2018semidefinite]. We clip every sample in order to satisfy the integer quantization range constraint.

We chose a representative set of 10 layers, including 9 convolutional layers (one 3x3 convolutional layer and 8 point-wise convolutions) from MobileNet-V2, EfficientNet-lite, and Resnet-18, and one fully-connected layer from ViT. The full details for reproducing the experiments are given in appendix [15](#section:appendix_per_layer_experiments_details){reference-type="ref" reference="section:appendix_per_layer_experiments_details"}. Due to the high computational complexity of the global solution for pruning, the layers had to be split into chunks. The slice of 4 input channels over all output channels was used for 3x3 convolutions. In the case of linear layers and point-wise convolutions, slices 36 input features over all the output features were used.

The results are shown in figure [4](#fig:quant_pruning_level2){reference-type="ref" reference="fig:quant_pruning_level2"} grouped by bit-width. The rectangles indicate the full range of the pruning and quantization methods between the heuristic solution and the error lower bound or the global solution. Whenever a rectangle for each chunk intersects the diagonal line, the ranking of the two methods could depend on the optimization method, while in cases below or above the diagonal, the ranking is guaranteed regardless of the optimizer. We see that quantization mostly outperforms pruning for moderate compression, while methods become more comparable for higher compression ratios.

# Full-model comparison

<figure id="fig:quant_pruning_level2">
<table>
<tbody>
<tr>
<td style="text-align: center;"><img src="fig/level2_all.png" style="width:7cm" alt="image" /></td>
</tr>
</tbody>
</table>
<figcaption>Comparison in the post-training scenario. Each box corresponds to a subset of one of 10 layers from the 4 different models that were used, with 7 different bit-width comparison points. The ranges of the box indicate the lower and higher-bounds found by the algorithms.</figcaption>
</figure>

Now that we have seen the comparison between the methods in the PTQ setting, we turn to fine-tuning quantized and pruned models. This is the setting where pruning is applied in most, and it is possible that fine-tuning can change the models significantly enough that the performance between the two methods changes.

In order to provide a fair comparison of pruning and quantization, we chose the two most commonly used methods with performance competitive to state-of-the-art. For quantization-aware training, we used the widely adapted LSQ method suggested in [@lsq; @{lsq+}]. Following this approach, we jointly learn the weights and quantization scales, keep the batch norm layers unfolded, and re-estimated the batch norm statistics after training to avoid wrong running estimates due to oscillations [@nagel2022overcoming]. We use the method suggested in [@zhu2017prune] for pruning, which gradually increases the sparsity during fine-tuning and re-estimates batch norm statistics after training.

In our experiments we used a set of 4 models trained for 4 tasks including Resnet18, Resnet50 [@resnet], MobileNet-V2 [@mobilenetv2], MobileNet-V3-small [@MobileNetV3], EfficientNet-lite [@EfficientNet], and ViT [@vit] trained on ImageNet classification [@imagenet]; DeepLab-V3 [@deeplabv3] with MobileNet-V2 backbone trained for semantic segmentation on Pascal VOC [@pascal]; EfficientDet [@tan2020efficientdet] trained for object detection on MS COCO [@mscoco]; OPT-350 fine-tuned on WikiText-103.

For a fair comparison, we used the same amount of epochs of fine-tuning for each method (full details on hyperparameters are given in appendix [16](#section:appendix_full_model_experiments){reference-type="ref" reference="section:appendix_full_model_experiments"}). The results given in table[\[tbl:01_full_model_results\]](#tbl:01_full_model_results){reference-type="ref" reference="tbl:01_full_model_results"} suggest that pruning almost never leads to higher accuracy than quantization if an equal compression rate is considered. The differences are sufficiently large enough that the small purported improvements by some methods [@srinivas2022cyclical] will likely not close the gap.

To study the effect of training time, we also performed an ablation with 2 times longer fine-tuning on a subset of 3 models (Resnet50, EfficientNet, and ViT). The results are given in appendix [17](#section:appendix_longer_finetuning){reference-type="ref" reference="section:appendix_longer_finetuning"}. We observe that pruned models generally benefit from fine-tuning more, and in particular pruning becomes more beneficial for most compression ratios on Resnet50. However, for the other models, quantization is still more beneficial due to a larger gap in performance.

::: table*
+--------------+-------+--------+---------+----------+----------+----------+----------+----------+----------+----------+
| Model        | Orig. | Metric | Method  | 8b       | 7b       | 6b       | 5b       | 4b       | 3b       | 2b       |
+:=============+======:+:=======+:========+=========:+=========:+=========:+=========:+=========:+=========:+=========:+
| Resnet-18    | 69.7  | acc.   | quant.  | **70.5** | **70.5** | **70.6** | **70.3** | **70.0** | **68.9** | **67.3** |
|              |       |        +---------+----------+----------+----------+----------+----------+----------+----------+
|              |       |        | pruning | 70.3     | 70.1     | 69.9     | 69.5     | 69.3     | 68.3     | 66.8     |
+--------------+-------+--------+---------+----------+----------+----------+----------+----------+----------+----------+
| Resnet-50    | 76.1  | acc.   | quant.  | 76.4     | **76.4** | **76.4** | **76.3** | **76.2** | **75.5** | 72.3     |
|              |       |        +---------+----------+----------+----------+----------+----------+----------+----------+
|              |       |        | pruning | **76.6** | **76.4** | 76.2     | 76.1     | 75.9     | 75.4     | **74.3** |
+--------------+-------+--------+---------+----------+----------+----------+----------+----------+----------+----------+
| MobileNet-V2 | 71.7  | acc.   | quant.  | **71.9** | **72.0** | **71.7** | **71.6** | **70.9** | **68.6** | **59.1** |
|              |       |        +---------+----------+----------+----------+----------+----------+----------+----------+
|              |       |        | pruning | 68.1     | 65.6     | 61.9     | 56.3     | 48.0     | 34.0     | 21.2     |
+--------------+-------+--------+---------+----------+----------+----------+----------+----------+----------+----------+
| EfficientNet | 75.4  | acc.   | quant.  | **75.2** | **75.3** | **75.0** | **74.6** | **74.0** | **71.5** | **60.9** |
|              |       |        +---------+----------+----------+----------+----------+----------+----------+----------+
|              |       |        | pruning | 72.5     | 70.9     | 68.1     | 63.6     | 56.4     | 44.5     | 27.1     |
+--------------+-------+--------+---------+----------+----------+----------+----------+----------+----------+----------+
| MobileNet-V3 | 67.4  | acc.   | quant.  | **67.7** | **67.6** | **67.1** | **66.3** | **64.7** | **60.8** | **50.5** |
|              |       |        +---------+----------+----------+----------+----------+----------+----------+----------+
|              |       |        | pruning | 65.6     | 64.4     | 62.4     | 60.2     | 56.1     | 31.7     | 0.0      |
+--------------+-------+--------+---------+----------+----------+----------+----------+----------+----------+----------+
| ViT          | 81.3  | acc.   | quant.  | **81.5** | **81.4** | **81.4** | **81.0** | **80.4** | **78.4** | **72.2** |
|              |       |        +---------+----------+----------+----------+----------+----------+----------+----------+
|              |       |        | pruning | 76.6     | 76.6     | 76.2     | 73.1     | 72.4     | 71.5     | 69.4     |
+--------------+-------+--------+---------+----------+----------+----------+----------+----------+----------+----------+
| DeepLab-V3   | 72.9  | mIoU   | quant.  | **72.3** | **72.3** | **72.4** | **71.9** | **70.8** | **63.2** | **17.6** |
|              |       |        +---------+----------+----------+----------+----------+----------+----------+----------+
|              |       |        | pruning | 65.2     | 62.8     | 56.8     | 47.7     | 32.9     | 18.6     | 10.0     |
+--------------+-------+--------+---------+----------+----------+----------+----------+----------+----------+----------+
| EfficientDet | 40.2  | mAP    | quant.  | **39.6** | **39.6** | **39.6** | **39.2** | **37.8** | **33.5** | **15.5** |
|              |       |        +---------+----------+----------+----------+----------+----------+----------+----------+
|              |       |        | pruning | 34.5     | 33.0     | 30.9     | 27.9     | 24.2     | 17.9     | 8.0      |
+--------------+-------+--------+---------+----------+----------+----------+----------+----------+----------+----------+
| OPT-350m     | 14.8  | perpl. | quant.  | **14.8** | **14.8** | **14.9** | **15.0** | **15.3** | **15.9** | **19.9** |
|              |       |        +---------+----------+----------+----------+----------+----------+----------+----------+
|              |       |        | pruning | 18.0     | 19.7     | 22.6     | 27.2     | 35.4     | 53.5     | 101.4    |
+--------------+-------+--------+---------+----------+----------+----------+----------+----------+----------+----------+
:::

#### Combining pruning and quantization

Another interesting questions is whether pruning is beneficial in combination with quantization. To answer it, we perfromed an experiment on pruning quantized Resnet-18, MobileNet-V2 and ViT with different pruning ratios. The results are given on figure [5](#fig:combinations){reference-type="ref" reference="fig:combinations"}. On x-axis we plot the expected bit-widths which is a product of the base bit-width and the sparsity in the pruned model including the natural sparsity. The points marked by crosses are quantized models with only natural sparsity and no extra pruning applied. As we can see, mild degrees of pruning are beneficial in the combinations. However, we note that no extra overhead was assumed for storing the pruning mask.

<figure id="fig:combinations" data-latex-placement="htb">
<div class="small">
<table>
<tbody>
<tr>
<td style="text-align: center;"><img src="fig/combinations_resnet18.png" style="width:4.2cm" alt="image" /></td>
<td style="text-align: center;"><img src="fig/combinations_mobilenet_v2.png" style="width:4.2cm" alt="image" /> <img src="fig/combinations_vit.png" style="width:4.2cm" alt="image" /></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
</div>
<figcaption>Combining pruning and quantization on ImageNet models. The average bit-widths shown on x axis is computed as a product of the base bit-width and the density of non-zero weight elements. Different pruning ratios are applied to each base bitwidth model. Quantized models with only natural sparsity and no extra pruning are marked with crosses.</figcaption>
</figure>

# Discussion {#sec:discussion}

#### Other types of pruning

While we solely focused in our comparison on unstructured pruning in which individual weights are removed, our results translate to semi-structured and structured pruning. Unstructured pruning has more degrees of freedom and is a strict superset of what can be represented by (semi-)structured pruning. Therefore, unstructured pruning gives an upper bound of the accuracy for all pruning methods. This means that for the cases in which quantization is better than unstructured pruning, quantization will also be better than (semi-)structured pruning. However, we can not make any claims for (semi-)structured pruning for the few scenarios in which pruning is better than quantization.

#### Natural sparsity in quantized tensors

In our comparison, we used a theoretical compression ratio for quantization, which depends on the bitwidth. However, we also observe that quantized tensors naturally contain many zeros; for example, 8-bit tensors from PyTorch model zoo have an average sparsity of 13% while 4-bit tensors are 35% sparse. We give more details on this in appendix [12](#section:appendix_quant_sparsity){reference-type="ref" reference="section:appendix_quant_sparsity"}.

#### Representations learned in the compressed models

To provide insights into representations learned during pruning or QAT, we studied the evolution of models during fine-tuning. We found that fine-tuning after pruning tends to recover the original representation, while quantization-aware training leads to learning completely new representations. We provide further details on these experiments in appendix [18](#section:appendix_representations){reference-type="ref" reference="section:appendix_representations"}.

#### Hardware implications

So far, we have deliberately avoided discussing the hardware implementations of pruning and quantization and focused solely on the accuracy of both methods at the same ideal compression rates. However, in practice, the hardware considerations do matter for the usability of the methods.

The analysis above assumed an idealistic case for pruning in terms of memory size and data transfer. Since the pruning is unstructured, in order to achieve memory savings in practice, one would need at least 1 bit of information for each weight indicating whether a weight is pruned or not. On top of 16-bit weights, this gives a 6.25% storage overhead at a minimum. Quantization does not have this overhead, as INT8 is just 8 bits smaller than 16 bits, and the only storage overhead is a single scaling factor per tensor (or channel).

Also, in terms of the cost of computations done by the hardware, there is a difference between the two methods. For pruning, any hardware would have to take the densely stored weights and mask and either decompress them to the dense format with all weights and many 0s or take the pruning into account in the compute itself. No compute benefits are gained in the former, as the dense calculations are done in the uncompressed number format. In the latter, dedicated hardware to take into account the 0s is necessary. The overhead for this is generally non-trivial, leading vendors to implement more semi-structured pruning schemes [@mishra2022semipruningnvidia]. Similarly, it is rare to see unstructured activation compression for the same reason that this needs to happen algorithmically on-the-fly. In contrast, quantization gives quadratic improvements in the compute. Going from INT8 to INT4 theoretically improves the compute performance by a factor 4, although practical gains depend on the memory overhead (which improves by only a factor 2x) and the existence of other formats in the same hardware compute unit.

**Impact** Using pruning or quantization leads to power reduction on many architectures and enables new applications on mobile platforms. We see only a positive impact from this on the whole. In some cases both pruning and quantization might lead to biased predictions, a further discussion can be found in [@hooker2020characterising].

**Limitations** First, our work has not extensively considered the hardware implications of pruning or quantization. Second, we do not study combinations of pruning and quantization apart from analyzing the inherent sparsity due to pruning. We leave this for future work. Finally, we consider only uniform quantization and ignore the other formats, such as low-precision floating or logarithmic quantization, although these are not likely to change the results presented in this paper.

# Related work

#### Quantization

Integer quantization, or fixed-point quantization, is one of the most widely used techniques for inference, allowing to reduce the latency and improved energy efficiency. There are two main families of methods for model quantization. The first family includes post-training quantization (PTQ) methods [@lin2016icml; @dfq; @dong2019hawq; @bannerposttraining; @huaweiquant; @zeroshotquant; @adaround; @brecq], which improve the model accuracy based on per-layer optimization of the quantized weights in a data-optimized fashion. The second family includes quantization-aware training methods [@Gupta2015; @jacob2018cvpr; @dorefa; @pact2018; @louizos2018relaxed; @lsq; @tqt; @{lsq+}; @differentiablequantization; @nagel2022overcoming] which usually fine-tune the model with quantization in the loop using straight-through estimator (STE) for computing the gradient of rounding operations. A more comprehensive overview of quantization methods can be found in [@whitepaper].

#### Pruning

Neural network pruning is one of the oldest methods to compress neural networks [@lecun1989optimal; @hassibi1993optimal]. A central problem in pruning is how to choose which weights to prune. Approaches published in the literature include: binary gating, in which a binary gate is learned on each individual weight [@louizos2017learning; @louizos2018learning; @van2020bayesian]; sensitivity-based methods [@lee2018snip; @lee2019signal; @yu2018nisp; @frantar2022optimal; @frantar2023sparsegpt] in which sensitivity, based on a weights' gradient or hessian diagonal value, is used, and magnitude pruning [@han2015deep; @narang2017exploring; @zhu2017prune; @mishra2022semipruningnvidia; @srinivas2022cyclical]. While conceptually simple, magnitude-based methods have been shown to consistently outperform more intricate methods at scale [@gale2019state; @blalock2020state]. Weight re-initialization schemes [@frankle2018lottery; @frankle2019stabilizing] or mask-reinitialization [@srinivas2022cyclical] yield additional minor improvements. While most pruning approaches require fine-tuning and yield unsatisfactory results in post-training scenarios, recent adaptations of Hessian-based sensitivity approaches [@lecun1989optimal; @hassibi1993optimal], in which the Hessian of a layerwise reconstruction loss is used instead of the task loss Hessian, show good pruning results in post-training pruning of large language models [@frantar2022optimal; @frantar2023sparsegpt].

#### Combining pruning and quantization

A number of works study combinations of pruning and quantization with different levels of granularity [@han2015deep; @van2020bayesian; @hu2021opq; @yang2020automatic; @tung2018deep; @yang2020automatic].

#### Comparing pruning and quantization

Despite the large amount of work on pruning, quantization, and combining them, there is little literature comparing the two methods. To the best of our knowledge, the closest work that performs a comparison of pruning versus non-uniform quantization  [@idelbayev2021empirical]. The work considers only small-scale models and provides only an empirical comparison with no further analysis. Another related study is [@namburi2023cost].

# Conclusion

We have seen in this paper that in several settings, unstructured pruning only performs better than quantization in rare cases. In our theoretical analysis of distributions and on the real-layer-data, pruning is only better than quantization, compressing the network to an equivalent of 2 or 3 bits. This amount of compression comes with such a degree of a drop in performance it is rarely used in practice. The post-training quantization results are also informative. In the setting without fine-tuning, we have shown with theoretical bounds on many layers in neural networks that quantization is almost always provably better than pruning. Our hypothesis is that quantized layers are more accurate than pruned ones, as shown in the theoretical and PTQ setting, and fine-tuning a network is still highly dependent on that. This is in line with fine-tuning results, in which for many networks trained under the same conditions, quantization always has higher performance than pruning.

The conclusion is clear: Quantization generally outperforms pruning for neural networks. Taking into account the unfavorable hardware implications for pruning described, it could be argued that the conclusion holds even stronger. Based on this research, we recommend quantizing neural networks when efficiency is required before pruning is explored.

# Acknowledgement

We would like to thank Marios Fournarakis and Yelisei Bondarenko for their help with performing QAT experiments.

# Expected quantization error computation {#section:appendix_analytical_error_mse}

The expected quantization error is a sum of two terms, the rounding error $E_{r}$ and the clipping error $E_{c}$:

$$\begin{equation}
{\mathbb E}(W-Q(W))^2=E_{r}+E_{c},
\end{equation}$$

$$\begin{equation}
E_{r}=\int \limits_{q_{min}}^{q_{max}}R_{q}^2(w)p(w)dw,
\end{equation}$$

$$\begin{equation}
\label{eq:clipping_error_split}
E_{c}=\int \limits_{-\infty}^{q_{min}}(w-q_{min})^2p(w)dw + \int \limits_{q_{max}}^{\infty}(q_{max}-w)^2p(w)dw.
\end{equation}$$

The rounding error $E_{r}$ can be split into two sub-intervals for each interval $(q_i,q_{i+1})$ where the first sub-interval corresponds to rounding up and the second sub-interval corresponds to rounding down:

$$\begin{equation}
\begin{aligned}
\label{eq:rounding_error_split}
 E_{r}= \sum_{i=1}^{|q|} \int \limits_{q_i}^{q_{i+1}}R^2(w)dw= 
 \sum_{i=1}^{|q|}   \int\limits_{q_i}^{(q_{i}+q_{i+1})/2} (w-q_i)^2 p(w)dw + \\ 
\sum_{i=1}^{|q|}  \int\limits_{(q_{i}+q_{i+1})/2}^{q_{i+1}} (q_{i+1}-w)^2 p(w)dw.
\end{aligned}
\end{equation}$$

In order to simplify the computation, we introduce the following function: $$\begin{equation}
I(a,b,w_{0}) \coloneqq \int \limits_{a}^{b} (w-w_0)^2 p(w)dw.
\end{equation}$$ Thus we can redefine the rounding error as: $$\begin{equation}
E_{r} = \sum_{i=1}^{|q|} \left[ I(q_i,(q_{i}+q_{i+1})/2,q_i) + I((q_{i}+q_{i+1})/2,q_{i+1},q_{i+1})  \right].
\end{equation}$$

We note that the clipping error $E_{cw}$ can also be expressed using $I_w(a,b,w_{0})$: $$\begin{equation}
\begin{split}
 E_{c}= I(w_{min},q_{min},q_{min}) + I(q_{max},w_{max},q_{max}).
 \end{split}
\end{equation}$$

where $w_{min}$ and $w_{max}$ are the limits of a truncated distribution. The analytical expressions for $I(w_{min},q_{min},q_{min})$ for different distributions are given in the Appendix of [@kuzmin2022fp8]. Thus, given the explicit definition of the quantization grid and the probability density function, we can analytically compute the rounding error for different distributions, for example, the Gaussian, Uniform, or Student's t-distribution.

# Truncated Student's-t distribution {#section:appendix_truncated_st}

The PDF of a truncated t-distribution with zero mean and unit variance is given by:

$$\begin{equation}
f(x,\nu,l)=\frac{p(x,\nu)}{\Phi(l)-\Phi(-l)} \mathds{1}_{-l \leq x \leq l},
\end{equation}$$

where $-l$ and $l$ are the truncation limits. $p(x,\nu)$ are the PDF and the CDF of the non-truncated t-distribution given by:

$$\begin{equation}
p(x,\nu)=\frac{1}{\sqrt{\nu}\text{B}\left( \frac{1}{2}, \frac{\nu}{2} \right)}\left(1+\frac{x^2}{\nu}\right)^{\frac{-\nu+1}{2}},
\end{equation}$$

$$\begin{equation}
\Phi(x)= \frac{1}{2}+x\Gamma\left(\frac{\nu+1}{2}\right)\times \frac{2\text{F}_1\left(\frac{1}{2}, \frac{\nu+1}{2}; \frac{3}{2}, \frac{-x^2}{\nu}\right)}{\sqrt{\pi \nu} \Gamma \left(\frac{\nu}{2} \right)},
\end{equation}$$

where $F_1$ is the hypergeometric function.

Kurtosis value of this distribution depending on the symmetric truncation range $l$ is plotted in figure [6](#fig:st_kurtosis_appendix){reference-type="ref" reference="fig:st_kurtosis_appendix"}.

<figure id="fig:st_kurtosis_appendix">
<table>
<tbody>
<tr>
<td style="text-align: center;"><img src="fig/st_kurtosis_appendix.png" style="width:6cm" alt="image" /></td>
</tr>
</tbody>
</table>
<figcaption>Kurtosis of a symmetric truncated t-distribution as a function of the truncation range.</figcaption>
</figure>

# Sparsity in quantized tensors {#section:appendix_quant_sparsity}

As we noted in section [6](#sec:discussion){reference-type="ref" reference="sec:discussion"}, quantized tensors naturally have some sparsity. The sparsity ratio tends to become higher if lower quantization bit-widths are used. Below we give a table with the average sparsity for all PyTorch model zoo tensors depending on the bit-width:

  num. bits                8     7     6     5     4     3     2
  ---------------------- ----- ----- ----- ----- ----- ----- -----
  avg. quant. sparsity    13%   16%   20%   27%   35%   46%   59%

  : Natural sparsity in quantized tensors.

As we can see, the sparsity values become very significant, especially for low bit-width values.

<figure id="fig:sqnr_accuracy_correlation" data-latex-placement="t">
<div class="small">
<table>
<tbody>
<tr>
<td style="text-align: center;"><img src="fig/quant_pruning_noise.png" style="width:7cm" alt="image" /></td>
</tr>
</tbody>
</table>
</div>
<figcaption>Correlation between per-layer SNR and full-model accuracy for pruning and quantization noise.</figcaption>
</figure>

# Correlation between model accuracy and per layer SNR {#section:appendix_sqnr_correlation}

In this section, we measure the correlation between SNR at individual layers of a network and the final model accuracy. It is important to study to which degree the two measures are correlated as we used SNR for our experiments in section [4](#sec:per_layer_comparison){reference-type="ref" reference="sec:per_layer_comparison"}. We note that in this section we use linear scale SNR which is a normalized MSE in contrast to log-scale SNR used in section  [4](#sec:per_layer_comparison){reference-type="ref" reference="sec:per_layer_comparison"}.

The results are given in figure [7](#fig:sqnr_accuracy_correlation){reference-type="ref" reference="fig:sqnr_accuracy_correlation"}. We pruned and quantized single layers of Resnet-18 and plotted activations SNR versus the full model accuracy drop. We observe a strong correlation between SNR and accuracy which confirms our assumption made in section [4](#sec:per_layer_comparison){reference-type="ref" reference="sec:per_layer_comparison"}.

# Details of PyTorch model zoo tensors experiments {#section:appendix_pytorch_zoo_tensors_details}

We quantized and pruned all the PyTorch model zoo weights tensors. All the convolutional and fully-connected layers were considered, the list is given below (45 models in total).

Classification models:

alexnet, resnet18, resnet34, resnet50, resnet101, resnet152, resnext50-32x4d, resnext101-32x8d, wide-resnet50-2, wide-resnet101-2, vgg11, vgg11-bn, vgg13, vgg13-bn, vgg16, vgg16-bn, vgg19-bn, vgg19, squeezenet1-0, squeezenet1-1, inception-v3, densenet121, densenet169, densenet201, densenet161, googlenet, mobilenet-v2, mobilenet-v3-large, mobilenet-v3-small, mnasnet0-5, mnasnet1-0, shufflenet-v2-x0-5, shufflenet-v2-x1-0.

Object detection models:

fasterrcnn-resnet50-fpn, fasterrcnn-mobilenet-v3-large-320-fpn, fasterrcnn-mobilenet-v3-large-fpn, maskrcnn-resnet50-fpn, keypointrcnn-resnet50-fpn, retinanet-resnet50-fpn, ssd300-vgg16, ssdlite320-mobilenet-v3-large.

Semantic segmentation models:

lraspp-mobilenet-v3-large.

Video classification models:

r3d-18, mc3-18, r2plus1d-18.

# Details of per-layer experiments {#section:appendix_per_layer_experiments_details}

To reduce the computational complexity of finding the global solution for pruning, the layers had to be split into chunks. The slice of 4 input channels over all output channels was used for 3x3 convolutions. In the case of linear layers and point-wise convolutions, slices 36 input features over all the output features were used.

In section, we provide details on per-layer experiments we performed in section [4](#sec:per_layer_comparison){reference-type="ref" reference="sec:per_layer_comparison"}. In table [1](#table:per_layer_details){reference-type="ref" reference="table:per_layer_details"} we give the names of the models and the layers we used along with the sub-problem dimensionality we considered for each chunk. Depending on the layer, the experiment took from approximately an hour up to six CPU weeks.

::: {#table:per_layer_details}
+-------------------+-----------------------------------+------------------+
| Model             | Layer                             | sub-problem dim. |
+:==================+:==================================+:================:+
| MobileNetV2       | features.8.conv.0.0               | 32               |
|                   +-----------------------------------+------------------+
|                   | features.8.conv.1.0               | 32               |
|                   +-----------------------------------+------------------+
|                   | features.8.conv.0.0               | 32               |
|                   +-----------------------------------+------------------+
|                   | features.11.conv.1.0              | 32               |
+-------------------+-----------------------------------+------------------+
| EfficientNet-lite | blocks.1.1.conv_pw                | 32               |
|                   +-----------------------------------+------------------+
|                   | blocks.6.0.se.conv_expand_pruning | 32               |
+-------------------+-----------------------------------+------------------+
| Resnet-18         | layer1.0.conv2                    | 36               |
|                   +-----------------------------------+------------------+
|                   | layer1.1.conv1                    | 36               |
|                   +-----------------------------------+------------------+
|                   | layer3.0.downsample.0             | 32               |
|                   +-----------------------------------+------------------+
|                   | layer2.0.downsample.0             | 32               |
+-------------------+-----------------------------------+------------------+
| ViT               | blocks.2.attn.proj                | 36               |
+-------------------+-----------------------------------+------------------+

: Details of per-layer experiments.
:::

# Details of the full-model experiments {#section:appendix_full_model_experiments}

In our experiments we used a set of 9 models trained for 4 tasks including Resnet18, Resnet50 [@resnet], MobileNet-V2 [@mobilenetv2], MobileNet-V3-small [@MobileNetV3], EfficientNet-lite [@EfficientNet], and ViT [@vit] trained on ImageNet classification [@imagenet]; DeepLab-V3 [@deeplabv3] with MobileNet-V2 backbone trained for semantic segmentation on Pascal VOC [@pascal]; EfficientDet [@tan2020efficientdet] trained for object detection on MS COCO [@mscoco], and OPT-350m fine-tuned on WikiText-103.

In table [2](#table:full_model_details){reference-type="ref" reference="table:full_model_details"} we provide details of the full-model experiments.

:::: center
::: {#table:full_model_details}
  Model                Batch size   Weight decay   Optimizer   FT num. epochs   Learning rate
  ------------------- ------------ -------------- ----------- ---------------- ---------------
  Resnet-18               256          1.0e-4         SGD            20            1.0e-3
  Resnet-50               128          1.0e-4         SGD            20            1.0e-5
  MobileNet-V2            128          5.0e-5         SGD            20            1.0e-5
  EfficientNet-lite       128          5.0e-5         SGD            20            1.0e-5
  MobileNet-V3            128          1.0e-4         SGD            20            1.0e-3
  ViT                     128          1.0e-4        Adam            20            1.0e-4
  DeepLab-V3               16           0.0           SGD           200            1.0e-6
  EfficientDet             16          5.0-5         Adam            20            1.0e-5
  OPT-350m                512           0.1          Adam            3             5.0e-5

  : Details of full-model experiments.
:::
::::

As optimal learning for quantization and pruning depends on the compression ratio, we performed a grid search with the step size corresponding to multiplying the basic learning rate above by negative and positive powers of $0.33$. For pruning of all the models except for DeepLab-V3 we gradually increase sparsity during the first 15 epochs of fine-tuning and we use the remaining 5 epochs to recover the accuracy with fixed sparsity. A similar scheme is used for DeepLab-V3 with 150 epochs of gradual sparsity increase and 50 remaining epochs of fine-tuning.

For quantization experiments, we use per-tensor quantization and Adam optimizer with a learning rate of 1.0e-5 for quantization scales optimization. We compress weights only and do not prune or quantize activations.

# Full-model experiments with longer fine-tuning {#section:appendix_longer_finetuning}

In this section we report the results for QAT and magnitude pruning with twice as longer fine-tuning compared to table [2](#table:full_model_details){reference-type="ref" reference="table:full_model_details"}. The results are given in table [\[tbl:01_full_model_results_longer\]](#tbl:01_full_model_results_longer){reference-type="ref" reference="tbl:01_full_model_results_longer"}

::: table*
+--------------+-------+--------+---------+----------+----------+----------+----------+----------+----------+----------+
| Model        | Orig. | Metric | Method  | 8b       | 7b       | 6b       | 5b       | 4b       | 3b       | 2b       |
+:=============+======:+:=======+:========+=========:+=========:+=========:+=========:+=========:+=========:+=========:+
| Resnet-50    | 76.1  | acc.   | quant.  | **76.6** | 76.6     | 76.5     | **76.3** | 76.2     | 75.5     | 72.3     |
|              |       |        +---------+----------+----------+----------+----------+----------+----------+----------+
|              |       |        | pruning | **76.6** | **76.7** | **76.6** | **76.3** | **76.3** | **75.8** | **74.8** |
+--------------+-------+--------+---------+----------+----------+----------+----------+----------+----------+----------+
| EfficientNet | 75.4  | acc.   | quant.  | **75.2** | **75.3** | **75.0** | **74.6** | **74.0** | **71.8** | **61.5** |
|              |       |        +---------+----------+----------+----------+----------+----------+----------+----------+
|              |       |        | pruning | 73.0     | 71.9     | 69.5     | 65.8     | 60.0     | 50.7     | 34.6     |
+--------------+-------+--------+---------+----------+----------+----------+----------+----------+----------+----------+
| ViT          | 81.3  | acc.   | quant.  | **81.9** | **81.8** | **81.7** | **81.4** | **80.8** | **78.9** | **73.7** |
|              |       |        +---------+----------+----------+----------+----------+----------+----------+----------+
|              |       |        | pruning | 80.3     | 79.7     | 79.0     | 77.7     | 76.3     | 74.3     | 71.6     |
+--------------+-------+--------+---------+----------+----------+----------+----------+----------+----------+----------+
:::

# Analysis of representations learned during fine-tuning in QAT and pruning {#section:appendix_representations}

<figure id="fig:resnet18_representations_2d">
<table>
<tbody>
<tr>
<td style="text-align: center;"><img src="fig/resnet18_pruning_sqnr_2d.png" style="width:14cm" alt="image" /></td>
</tr>
<tr>
<td style="text-align: center;">(a)</td>
</tr>
<tr>
<td style="text-align: center;"><img src="fig/resnet18_pruning_cka_2d.png" style="width:14cm" alt="image" /></td>
</tr>
<tr>
<td style="text-align: center;">(b)</td>
</tr>
<tr>
<td style="text-align: center;"><img src="fig/resnet18_quant_sqnr_2d.png" style="width:14cm" alt="image" /></td>
</tr>
<tr>
<td style="text-align: center;">(c)</td>
</tr>
<tr>
<td style="text-align: center;"><img src="fig/resnet18_quant_cka_2d.png" style="width:14cm" alt="image" /></td>
</tr>
<tr>
<td style="text-align: center;">(d)</td>
</tr>
</tbody>
</table>
<figcaption>The evolutions of representations at different convolutional layers of Resnet-18 during QAT and fine-tuning after pruning. We plot SNR and CKA distances between activations of each layer of quantized/pruned model and the original activations.</figcaption>
</figure>

As fine-tuning significantly improves the accuracy after pruning or quantization, it is interesting to investigate whether fine-tuning recovers the original model. To answer this question, we study how representations at each layer change during the course of fine-tuning by comparing them to the original model representations.

For this purpose, we sample activations from two models, Resnet18 and ViT, after each epoch of fine-tuning, and also directly after quantization/pruning. We measure distances between the original activations and the corresponding activations of the quantized and pruned models. To measure the distance between two feature maps we consider two distance metrics, log-scale SNR and the central kernel alignment (CKA) distance (Kornblith, et al. PMLR, 2019).

We show the results in figure [8](#fig:resnet18_representations_2d){reference-type="ref" reference="fig:resnet18_representations_2d"}. We observe qualitative agreement between both metrics. Curiously, the results show different trends for pruning and quantization. For pruning, the representations tend to become closer to the original representation during fine-tuning. However, for quantization the fine-tuning rather learns a representation that is different from the original one. In order to provide more convenient visualizations, we show one-dimensional plots of both distance metrics at the last non-classifier layer in figure [9](#fig:resnet18_representations_1d){reference-type="ref" reference="fig:resnet18_representations_1d"} (a-b). We can see that even in the cases of larger distances fine-tuning after pruning tends to recover the representations while it is not the case for QAT.

For ViT we observe qualitatively the same behavior, see figure [9](#fig:resnet18_representations_1d){reference-type="ref" reference="fig:resnet18_representations_1d"} (c). We omit SNR plots as this measure rapidly becomes negative both for pruning and quantization in ViT. However, CKA evolution follows the same pattern as in the case of Resnet-18 and confirms similar observations.

As we see, fine-tuning during QAT effectively tends to training different representations, while fine-tuning after pruning has a tendency towards recovering the original model.

<figure id="fig:resnet18_representations_1d">
<table>
<tbody>
<tr>
<td style="text-align: center;"><img src="fig/resnet18_cka_1d.png" style="width:14cm" alt="image" /></td>
</tr>
<tr>
<td style="text-align: center;">(a)</td>
</tr>
<tr>
<td style="text-align: center;"><img src="fig/resnet18_sqnr_1d.png" style="width:14cm" alt="image" /></td>
</tr>
<tr>
<td style="text-align: center;">(b)</td>
</tr>
<tr>
<td style="text-align: center;"><img src="fig/vit_cka_1d.png" style="width:14cm" alt="image" /></td>
</tr>
<tr>
<td style="text-align: center;">(c)</td>
</tr>
</tbody>
</table>
<figcaption>Distances between activations of pruned/quantized models to original activations at the last non-classification layer.</figcaption>
</figure>

[^1]: Code is available at <https://github.com/Qualcomm-AI-research/pruning-vs-quantization>

[^2]:     Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc.

[^3]: <https://pytorch.org/serve/model_zoo.html>.
