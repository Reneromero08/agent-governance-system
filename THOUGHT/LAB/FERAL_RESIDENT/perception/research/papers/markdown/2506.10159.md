# Introduction

Deep representation learning seeks to map each input ${\boldsymbol x}$ into a compact embedding ${\boldsymbol z}$ that preserves semantic similarity and facilitates downstream tasks such as classification or retrieval [@bengio2013representation]. Contrastive learning methods, including SimCLR [@chen2020simple] and SupCon [@khosla2020supervised], have advanced the state of the art by pulling together positive pairs and pushing apart negatives in the embedding space. However, these approaches rely on deterministic point estimates for each sample, which do not express uncertainty or capture multiple plausible representations.

To address this limitation, we introduce a probabilistic *Variational Contrastive Learning* (VCL) approach, which extends deterministic embeddings to *probabilistic embeddings* by maximizing the evidence lower bound (ELBO) within the contrastive learning framework. Unlike variational autoencoders (VAEs) [@kingma2019introduction], which employ a decoder to reconstruct inputs from latent variables, VCL omits explicit decoders. Instead, we show that the InfoNCE loss can serve as a surrogate for the ELBO reconstruction term, yielding a principled probabilistic formulation of contrastive learning. Our VCL framework offers several new perspectives on learned embeddings:

#### Understanding embeddings through distributions.

VCL maps each input ${\boldsymbol x}$ to an approximate posterior distribution $q_{\phi}({\boldsymbol z}| {\boldsymbol x})$, yielding a mean vector that serves as the embedding and a variance that quantifies uncertainty. This probabilistic representation not only captures richer information about each sample but also enables uncertainty-aware downstream decisions.

#### A probabilistic ELBO viewpoint beyond mutual information.

Minimizing the InfoNCE loss maximizes a lower bound on mutual information, consistent with the InfoMax principle [@linsker1988self]. However, mutual information alone may not directly correlate with downstream task performance [@Tschannen2020On], motivating geometric analyses based on alignment and uniformity [@wang2020understanding]. In contrast, our probabilistic formulation interprets contrastive learning through the ELBO: maximizing the ELBO without a decoder encourages the learned posterior $q_{\phi}({\boldsymbol z}| {\boldsymbol x})$ to match the true posterior $p({\boldsymbol z}| {\boldsymbol x})$, providing a principled objective for representation learning.

#### Controllable embedding distributions.

Standard contrastive learning imposes no explicit prior on embeddings, so their distribution emerges implicitly from the data and model architecture. VCL incorporates a prior $p({\boldsymbol z})$ into the objective, allowing one to specify and control the marginal embedding distribution $p({\boldsymbol z})$. For example, choosing a uniform prior on the hypersphere often improves empirical performance [@wang2020understanding] that makes negative samples more uniformly dispersed around each anchor, and domain-specific priors can encode known structure in the data.

#### Mitigating collapse phenomena.

Self-supervised contrastive learning (e.g., SimCLR) can suffer from *dimensional collapse*, where embeddings occupy only a low-dimensional subspace [@jing2021understanding]. A spherical uniform prior mitigates this by encouraging isotropic use of all embedding dimensions.

This Variational Contrastive Learning framework thus provides uncertainty‐aware embeddings, a new basis of CL with theoretical insights via the ELBO, and practical solutions to common collapse problems in contrastive learning. Our contributions are summarized as follows:

- We introduce *Variational Contrastive Learning* (VCL), a decoder‐free ELBO maximization framework that reinterprets the InfoNCE loss as a surrogate reconstruction term and incorporates a KL divergence regularizer to a uniform prior on the unit hypersphere.

- We propose a distributional embedding model using a projected normal posterior $q_\theta({\boldsymbol z}|{\boldsymbol x})$ that enables sampling, uncertainty quantification, and efficient KL computation on the hypersphere.

- We derive a theoretical connection between the optimal InfoNCE critic and the ELBO, showing that minimizing InfoNCE asymptotically maximizes the ELBO reconstruction term (Proposition [2](#prop:opt_inf){reference-type="ref" reference="prop:opt_inf"}) and providing a generalization bound (Theorem [3](#thm:genbnd_informal){reference-type="ref" reference="thm:genbnd_informal"}) of KL regularization.

- We demonstrate that VCL mitigates both dimensional collapse in self‐supervised contrastive learning via the KL regularizer, while preserving embedding structure. We show that VCL methods preserve or improve mutual information with labels, match or exceed classification accuracy of deterministic baselines, and provide meaningful implication of distributional embeddings.

# Preliminaries {#sec:prelim}

Let $\mathcal{D} = \{({\boldsymbol x}_i, {\boldsymbol y}_i)\}_{i=1}^N$ be a dataset of input ${\boldsymbol x}\in\mathcal{X}$ and label pairs drawn i.i.d. from the joint distribution $p({\boldsymbol x},{\boldsymbol y})$. An *encoder* $f_{\theta}\colon \mathcal{X}\to\mathbb{R}^d$, parameterized by $\theta$, maps each input ${\boldsymbol x}$ to a $d$-dimensional vector, which we then normalize to unit length: ${\boldsymbol z}= \frac{f_{\theta}({\boldsymbol x})}{\| f_{\theta}({\boldsymbol x})\|_{2}}$. Throughout this section, we define the temperature--scaled cosine similarity between embeddings ${\boldsymbol z}_i$ and ${\boldsymbol z}_j$ as $$\begin{align}
\label{eq:cos_sim}
    s({\boldsymbol z}_i, {\boldsymbol z}_j)
    = \frac{{\boldsymbol z}_i^\top {\boldsymbol z}_j}{\tau},
\end{align}$$ where $\tau > 0$ is the temperature hyperparameter. For any two probability distributions $q$ and $p$, we denote the Kullback--Leibler (KL) divergence by $D(q \,\|\, p) \;=\; \mathbb{E}_{{\boldsymbol z}\sim q}\!\left[\log\frac{q({\boldsymbol z})}{p({\boldsymbol z})}\right]$.

## Self-Supervised Contrastive Learning {#subsec:selfsup}

Self-supervised contrastive learning (SSCL) learns representations from *unlabeled* data by pulling together embeddings of semantically related views (positives) and pushing apart those of unrelated views (negatives). For an anchor ${\boldsymbol x}$, let ${\boldsymbol x}'_i$ denote a positive view sampled from $p({\boldsymbol x}'_i \mid {\boldsymbol x})$, and let $\{{\boldsymbol x}'_j\}_{j\neq i}$ be $N-1$ negative views drawn i.i.d. from the marginal $p({\boldsymbol x}')$. The InfoNCE loss [@oord2018representation] for anchor ${\boldsymbol x}$ is then $$\begin{align}
I_{\mathrm{NCE}}({\boldsymbol x};{\boldsymbol x}') 
&= - \mathbb{E}_{\substack{{\boldsymbol x}\sim p({\boldsymbol x})\\{\boldsymbol x}'_i\sim p({\boldsymbol x}'_i\mid{\boldsymbol x})\\\{{\boldsymbol x}'_j\}_{j\neq i}\sim p({\boldsymbol x}')}} 
\left[\log \frac{\exp\bigl(s({\boldsymbol z},{\boldsymbol z}'_i)\bigr)}{\sum_{j=1}^N \exp\bigl(s({\boldsymbol z},{\boldsymbol z}'_j)\bigr)}\right],
\end{align}$$ where ${\boldsymbol z}= f_\theta({\boldsymbol x})/\|f_\theta({\boldsymbol x})\|_2$ and $s(\cdot,\cdot)$ is the temperature‐scaled cosine similarity.

In practice, following SimCLR [@chen2020simple], we generate positives by applying two random augmentations $t',t''\sim\mathcal{T}$ to each sample ${\boldsymbol x}_i$, yielding $({\boldsymbol x}'_i,{\boldsymbol x}''_i) = (t'({\boldsymbol x}_i),\,t''({\boldsymbol x}_i))$.[^1] All other $2N-2$ augmented samples in the mini‐batch serve as negatives. Let $\mathcal{B}$ be the set of all $2N$ embeddings in the batch; then InfoNCE can be computed as $$\begin{align}
I_{\mathrm{NCE}}
&= -\frac{1}{2N} \sum_{{\boldsymbol z}\in\mathcal{B}}
\log \frac{\exp\bigl(s({\boldsymbol z},{\boldsymbol z}_p)\bigr)}
{\sum_{{\boldsymbol z}_n\in\mathcal{B}\setminus\{{\boldsymbol z}\}} \exp\bigl(s({\boldsymbol z},{\boldsymbol z}_n)\bigr)},
\end{align}$$ where ${\boldsymbol z}_p$ denotes the positive embedding corresponding to ${\boldsymbol z}$. Since InfoNCE lower‐bounds the mutual information $I({\boldsymbol x};{\boldsymbol x}')$ via $I({\boldsymbol x};{\boldsymbol x}')  \geq \log N - I_{\mathrm{NCE}}({\boldsymbol x};{\boldsymbol x}'),$ we can see that minimizing $I_{\mathrm{NCE}}$ encourages encoders to preserve the semantic information of ${\boldsymbol x}$ [@poole2019variational].

## Supervised Contrastive Learning {#subsec:supcon}

[@khosla2020supervised] extend the InfoNCE loss from the self‐supervised setting to a supervised context, calling the resulting method *Supervised Contrastive Learning* (SupCon). When class labels $y_i \in \{1,\dots,C\}$ are available, all samples sharing the same label can serve as positives.

Given a mini‐batch $\{({\boldsymbol x}_i,y_i)\}_{i=1}^B$, define for each anchor index $a$ $$\mathcal{A}(a) = \{1,2,\dots,B\}\setminus\{a\}, \text{ and }~
\mathcal{P}(a) = \{\,p\in\mathcal{A}(a): y_p = y_a\},$$ so that $\mathcal{P}(a)$ contains the indices of all positives for anchor $a$. The SupCon loss for anchor ${\boldsymbol x}_a$ is then $$\begin{align}
\label{eq:supcon_loss}
I_{\mathrm{SUP}}({\boldsymbol x}_a)
&= -\frac{1}{|\mathcal{P}(a)|} 
\sum_{p\in\mathcal{P}(a)}
\log 
\frac{\exp\bigl(s({\boldsymbol z}_a,{\boldsymbol z}_p)\bigr)}
{\displaystyle\sum_{j\in\mathcal{A}(a)}\exp\bigl(s({\boldsymbol z}_a,{\boldsymbol z}_j)\bigr)}.
\end{align}$$ Averaging over all anchors in the batch yields the full objective: $$\begin{align}
\mathcal{L}^{\mathrm{sup}}
&= \frac{1}{B}\sum_{a=1}^B I_{\mathrm{SUP}}({\boldsymbol x}_a).
\end{align}$$

## Variational Inference and the Evidence Lower Bound (ELBO) {#subsec:elbo}

In variational inference [@blei2017variational; @kingma2019introduction], we treat the data distribution $p({\boldsymbol x})$ as the marginal of a joint distribution over observed data ${\boldsymbol x}$ and latent variables ${\boldsymbol z}$, i.e., $p({\boldsymbol x}) = \int p({\boldsymbol x}| {\boldsymbol z})\,p({\boldsymbol z})\mathrm{d}{\boldsymbol z}.$ The latent variable ${\boldsymbol z}$ captures meaningful structure in ${\boldsymbol x}$, serving both as a hidden cause and as a compressed representation for downstream tasks. In representation learning, we interpret ${\boldsymbol z}$ as the embedding of ${\boldsymbol x}$.

The log‐evidence can be written with respect to any approximate posterior $q_\phi({\boldsymbol z}|{\boldsymbol x})$ as $$\begin{align}
\label{eq:evidence}
\log p({\boldsymbol x})
&= \log \mathbb{E}_{q_\phi({\boldsymbol z}\mid{\boldsymbol x})}\!\Bigl[\tfrac{p({\boldsymbol x},{\boldsymbol z})}{q_\phi({\boldsymbol z}\mid{\boldsymbol x})}\Bigr].
\end{align}$$ Rather than optimizing [\[eq:evidence\]](#eq:evidence){reference-type="eqref" reference="eq:evidence"} directly, variational methods maximize the *evidence lower bound* (ELBO) obtained as a result of applying Jensen's inequality: $$\begin{align}
\label{eq:ELBO}
\log p({\boldsymbol x})
&\ge \mathbb{E}_{q_\phi({\boldsymbol z}\mid{\boldsymbol x})}\!\bigl[\log p({\boldsymbol x}|{\boldsymbol z})\bigr]
    - D\bigl(q_\phi({\boldsymbol z}|{\boldsymbol x})\,\|\,p({\boldsymbol z})\bigr)
    \;=\; \mathcal{L}^{\rm ELBO}(\phi),
\end{align}$$ where $p({\boldsymbol z})$ is a fixed prior (commonly $\mathcal{N}(\mathbf{0},I_d)$). The ELBO decomposes into a *reconstruction* term $\mathbb{E}_{q}[\log p({\boldsymbol x}|{\boldsymbol z})]$ and a *regularizer* $D(q_\phi({\boldsymbol z}|{\boldsymbol x})\,\|\,p({\boldsymbol z}))$. Maximizing $\mathcal{L}^{\rm ELBO}$ thus balances (i) accurate reconstruction, (ii) posterior‐to‐prior regularization, and (iii) posterior accuracy. By $$\begin{align}
\label{eq:posterior_ap}
\log p({\boldsymbol x})
= \mathcal{L}^{\rm ELBO}(\phi)
+ D\bigl(q_\phi({\boldsymbol z}|{\boldsymbol x})\,\|\,p({\boldsymbol z}|{\boldsymbol x})\bigr),
\end{align}$$ for fixed $\log p({\boldsymbol x})$, maximizing the ELBO minimizes the KL divergence between the approximate and true posteriors [@blei2017variational].

The ELBO provides a tractable surrogate for marginal likelihood that can be optimized by standard gradient methods. It will serve as the theoretical backbone of our Variational Contrastive Learning framework, offering both a probabilistic interpretation and explicit control over latent uncertainty.

#### Relation to contrastive objectives.

Although the ELBO stems from latent‐variable modeling, its two components align naturally with contrastive objectives: the KL divergence term enforces *uniformity* in the embedding space, while the reconstruction term promotes *alignment* between embeddings and observations. In Section [3](#sec:VCL){reference-type="ref" reference="sec:VCL"}, we leverage this connection by adopting distributional embeddings in the contrastive framework and incorporating a KL‐based regularizer on the posterior.

# Variational Contrastive Learning (VCL) {#sec:VCL}

Unlike existing variational contrastive learning methods---which primarily focus on generative models with explicit decoders [@chen2025multi; @wang2024vcl]---our approach performs *decoder-free* ELBO maximization, making VCL a truly contrastive learning framework.

## Decoder-Free ELBO Maximization {#subsec:elbo}

Here we describe how to optimize two terms in ELBO [\[eq:ELBO\]](#eq:ELBO){reference-type="eqref" reference="eq:ELBO"} within a purely contrastive learning setup.

#### Reconstruction term.

The reconstruction term $\mathbb{E}_{q_\theta({\boldsymbol z}|{\boldsymbol x})}\bigl[\log p({\boldsymbol x}|{\boldsymbol z})\bigr]$ requires the true conditional $p({\boldsymbol x}|{\boldsymbol z})$, which is generally intractable. Instead, we approximate it via the embedding conditional $$\begin{align}
p({\boldsymbol z}'| {\boldsymbol z})
= \frac{p({\boldsymbol z},{\boldsymbol z}')}{\int p({\boldsymbol z},{\boldsymbol z}')\,\mathrm{d}{\boldsymbol z}'},
\end{align}$$ where ${\boldsymbol z}'\sim q_\theta(\cdot\mid{\boldsymbol x})$ captures semantics of ${\boldsymbol x}$. Thus, $$\begin{align}
\label{eq:dist_x_z}
\mathbb{E}_{q_\theta({\boldsymbol z}|{\boldsymbol x})}\bigl[\log p({\boldsymbol x}|{\boldsymbol z})\bigr]
&\approx
\mathbb{E}_{q_\theta({\boldsymbol z}|{\boldsymbol x})q_\theta({\boldsymbol z}'|{\boldsymbol x})}\bigl[\log p({\boldsymbol z}'| {\boldsymbol z})\bigr] \nonumber\\
&= \mathbb{E}\Bigl[\log\tfrac{p({\boldsymbol z},{\boldsymbol z}')}{\int p({\boldsymbol z},{\boldsymbol z}')\,\mathrm{d}{\boldsymbol z}'}\Bigr]
\approx \mathbb{E}\Bigl[\log\tfrac{e^{\psi({\boldsymbol z},{\boldsymbol z}')}}
{\sum_j e^{\psi({\boldsymbol z},{\boldsymbol z}'_j)}}\Bigr],
\end{align}$$ where we approximate $p({\boldsymbol z},{\boldsymbol z}')\!\approx\!e^{\psi({\boldsymbol z},{\boldsymbol z}')}$ via a critic $\psi$. Details on parameterizing $p({\boldsymbol z}'\mid{\boldsymbol z})$ appear in Section [3.2](#subsec:VSimCLR){reference-type="ref" reference="subsec:VSimCLR"}. The following lemma supports the approximation $\mathbb{E}_{q_\theta({\boldsymbol z}|{\boldsymbol x})}\bigl[\log p({\boldsymbol x}|{\boldsymbol z})\bigr] \approx \mathbb{E}_{q_\theta({\boldsymbol z}|{\boldsymbol x})q_\theta({\boldsymbol z}'|{\boldsymbol x})}\bigl[\log p({\boldsymbol z}'| {\boldsymbol z})\bigr]$. A further discussion on the approximation in [\[eq:dist_x_z\]](#eq:dist_x_z){reference-type="eqref" reference="eq:dist_x_z"} and a tightness condition is in Appendix [8.1](#app:recon_approx){reference-type="ref" reference="app:recon_approx"}.

::: {#lem:recon_app_jensen .lemma}
**Lemma 1**. *Let ${\boldsymbol x}$ and ${\boldsymbol z}$ be conditionally independent given ${\boldsymbol z}’$. Then, the reconstruction term in Section [3.1](#subsec:elbo){reference-type="ref" reference="subsec:elbo"} is bounded as $$\begin{align}
    \mathbb{E}_{q({\boldsymbol z}|{\boldsymbol x})} [ \log p({\boldsymbol x}|{\boldsymbol z}) ] \geq \mathbb{E}_{q({\boldsymbol z}|{\boldsymbol x})q({\boldsymbol z}’|{\boldsymbol x})}[\log p({\boldsymbol z}’|{\boldsymbol z})] + {\rm const}.,
\end{align}$$ where const. is independent of ${\boldsymbol z}$.*
:::

::: proof
*Proof.* The proof of Proposition [1](#lem:recon_app_jensen){reference-type="ref" reference="lem:recon_app_jensen"} is in Appendix [7.1](#app:recon_app_jensen){reference-type="ref" reference="app:recon_app_jensen"}. ◻
:::

Noting that the right‐hand side of [\[eq:dist_x_z\]](#eq:dist_x_z){reference-type="eqref" reference="eq:dist_x_z"} is (up to sign) the InfoNCE surrogate, setting $\psi(\cdot,\cdot)=s(\cdot,\cdot)$ in [\[eq:dist_x_z\]](#eq:dist_x_z){reference-type="eqref" reference="eq:dist_x_z"} where $s(\cdot,\cdot)$ is defined in [\[eq:cos_sim\]](#eq:cos_sim){reference-type="eqref" reference="eq:cos_sim"} yields $$\begin{align}
\label{eq:recon_infoNCE}
\mathbb{E}_{q_\theta({\boldsymbol z}|{\boldsymbol x})}\bigl[\log p({\boldsymbol x}|{\boldsymbol z})\bigr]
\approx
-\,I_{\mathrm{NCE}}({\boldsymbol x};{\boldsymbol x}').
\end{align}$$ Hence, minimizing the InfoNCE loss maximizes the reconstruction term without explicit decoders.

In contrast to VAE embeddings---which often rely on pixel‐level reconstruction through expressive decoder [@song2024scale]---VCL preserves semantics via contrastive objectives. The next proposition (proved in Appendix [7.2](#app:opt_inf){reference-type="ref" reference="app:opt_inf"}) provides a theoretical connection between InfoNCE and the reconstruction term.

::: {#prop:opt_inf .proposition}
**Proposition 2**. *Assume that: 1) the critic $\psi$ in InfoNCE is optimal; 2) $p({\boldsymbol z})<\infty,~\forall {\boldsymbol z}$; and 3) $0<\epsilon \leq p({\boldsymbol z}|{\boldsymbol z}') \leq g_+({\boldsymbol z}),~\forall {\boldsymbol z},{\boldsymbol z}'$ with a absolutely integrable $g:\mathcal{Z}\to(0,\infty)$. Then, as the number of negatives $N\to\infty$, $$\begin{align}
-\,I_{\mathrm{NCE}}({\boldsymbol x};{\boldsymbol x}') + \log N
\;\longrightarrow\;
\mathbb{E}\bigl[\log p({\boldsymbol z}'| {\boldsymbol z})\bigr]
- D\bigl(q_\theta({\boldsymbol z}'|{\boldsymbol x})\,\|\,p({\boldsymbol z}')\bigr)
- H\bigl(q_\theta({\boldsymbol z}'|{\boldsymbol x})\bigr),
\end{align}$$ where the expectation is over $q_\theta({\boldsymbol z}|{\boldsymbol x})q_\theta({\boldsymbol z}'|{\boldsymbol x})$, and $H(\cdot)$ denotes entropy.*
:::

#### Regularization.

Maximizing the ELBO requires choosing a prior $p({\boldsymbol z})$ and an approximate posterior $q_\theta({\boldsymbol z}\mid{\boldsymbol x})$. Although both are often taken as Gaussian distributions [@kingma2019introduction], this choice conflicts with the geometry of contrastive embeddings, which often lie on the unit hypersphere due to the normalization [@wang2020understanding]. Instead, we adopt non‐Gaussian priors and posteriors---one key distinction from standard VAE approaches.

Motivated by the uniformity property [@wang2020understanding] on the unit sphere $\mathcal{S}^{d-1} = \{{\boldsymbol z}\in\mathbb{R}^d : \|{\boldsymbol z}\|_2 = 1\}$, we set the prior $p({\boldsymbol z})$ to be the uniform distribution over $\mathcal{S}^{d-1}$. For the approximate posterior, we use the *projected normal* distribution [@hernandez2017general], which admits efficient KL‐divergence computation while enforcing ${\boldsymbol z}\in\mathcal{S}^{d-1}$. A random variable ${\boldsymbol z}\sim\mathcal{P}\mathcal{N}(\mu,K)$ is obtained by sampling $$\begin{align}
{\boldsymbol z}= \frac{{\bf u}}{\|{\bf u}\|_2}
\quad
\text{with}
\quad 
{\bf u}\sim \mathcal{N}(\mu,K).
\end{align}$$ In particular, $\mathcal{P}\mathcal{N}(0,I_d)$ reduces to the uniform distribution on $\mathcal{S}^{d-1}$, i.e., $\mathcal{P}\mathcal{N}(0,I_d) \overset{d}{=} {\rm Unif}(\mathcal{S}^{d-1})$.

With $q_\theta({\boldsymbol z}|{\boldsymbol x})=\mathcal{P}\mathcal{N}(\mu,K)$, the regularization term becomes $$\begin{align}
D\bigl(q_\theta({\boldsymbol z}|{\boldsymbol x})\,\|\,p({\boldsymbol z})\bigr)
= D\bigl(\mathcal{P}\mathcal{N}(\mu,K)\,\|\,\mathrm{Unif}(\mathcal{S}^{d-1})\bigr).
\end{align}$$ Since a closed‐form KL divergence between projected normals and the uniform sphere is intractable, we instead minimize the Gaussian KL as an upper bound---by the data processing inequality [@polyanskiy2025information]: $$\begin{align}
\label{eq:kl_bound}
D\bigl(\mathcal{N}(\mu,K)\,\|\,\mathcal{N}(0,I_d)\bigr)
\;\ge\;
D\bigl(\mathcal{P}\mathcal{N}(\mu,K)\,\|\,\mathrm{Unif}(\mathcal{S}^{d-1})\bigr).
\end{align}$$ In Appendix [8.2](#app:kl_surrogate){reference-type="ref" reference="app:kl_surrogate"}, we analyze the tightness of the gap in [\[eq:kl_bound\]](#eq:kl_bound){reference-type="eqref" reference="eq:kl_bound"} and show that the Gaussian KL divergence closely approximates the projected-normal KL divergence; the two exhibit nearly identical behavior throughout VCL training.

For $K=\mathrm{diag}(\sigma_1^2,\dots,\sigma_d^2)$, the Gaussian KL admits the closed form $$\begin{align}
\label{eq:upper_kl}
D(\mu,K)
= \frac12\sum_{i=1}^d\bigl(\sigma_i^2 + \mu_i^2 - 1 - \log\sigma_i^2\bigr).
\end{align}$$

The KL divergence term $D(\mu, K)$ grows linearly with the embedding dimension $d$, which can destabilize training when $d$ is large. To address this, we normalize the KL term by $d$, i.e., $\widetilde{D}(\mu,K) \;=\;\frac{1}{d}\,D(\mu,K),$ so that its magnitude remains comparable to the InfoNCE loss.

#### Final objective for maximizing ELBO.

By combining [\[eq:recon_infoNCE\]](#eq:recon_infoNCE){reference-type="eqref" reference="eq:recon_infoNCE"} and [\[eq:upper_kl\]](#eq:upper_kl){reference-type="eqref" reference="eq:upper_kl"}, we obtain the following (approximate) lower bound on the ELBO: $$\begin{align}
\label{eq:asym_bound}
\mathcal{L}^{\rm ELBO}(\theta)
\;\ge\;
-\,I_{\rm NCE}({\boldsymbol x};{\boldsymbol x}')
\;-\,D\bigl(\mu_{{\boldsymbol x}},\,K_{{\boldsymbol x}}\bigr),
\end{align}$$ where $\mu_{{\boldsymbol x}}$ and $K_{{\boldsymbol x}} = \mathrm{diag}(\sigma_{{\boldsymbol x},1},\ldots,\sigma_{{\boldsymbol x},d})$ are the parameters of $q_{\theta}(\mathbf{z}\mid{\boldsymbol x})$. Because this bound is asymmetric in $({\boldsymbol x},{\boldsymbol x}')$, we symmetrize it to define our final VCL objective: $$\begin{align}
\label{eq:VCL_obj}
\mathcal{L}^{\rm VCL}
= \frac{1}{2}\Bigl(
I_{\rm NCE}({\boldsymbol x};{\boldsymbol x}')
+ I_{\rm NCE}({\boldsymbol x}';{\boldsymbol x})
+ D(\mu_{{\boldsymbol x}},K_{{\boldsymbol x}})
+ D(\mu_{{\boldsymbol x}'},K_{{\boldsymbol x}'})
\Bigr).
\end{align}$$ Minimizing $\mathcal{L}^{\rm VCL}$ therefore maximizes the ELBO. Next, we introduce Variational SimCLR (VSimCLR), which is specifically designed to optimize this objective efficiently.

<figure id="fig:VsimCLR">

<figcaption>Graphical illustration of SimCLR and Variational SimCLR (VSimCLR).</figcaption>
</figure>

## Variational SimCLR (VSimCLR) {#subsec:VSimCLR}

We propose *Variational SimCLR* (VSimCLR), whose architecture is illustrated in Figure [\[subfig:VSimCLR\]](#subfig:VSimCLR){reference-type="ref" reference="subfig:VSimCLR"}. VSimCLR minimizes $\mathcal{L}^{\rm VCL}$ in [\[eq:VCL_obj\]](#eq:VCL_obj){reference-type="eqref" reference="eq:VCL_obj"}, thereby implicitly maximizing the ELBO and bringing the approximate posterior closer to the true posterior by [\[eq:posterior_ap\]](#eq:posterior_ap){reference-type="eqref" reference="eq:posterior_ap"}. Compared to SimCLR, VSimCLR differs in three key aspects: (i) the encoder outputs the parameters of a variational posterior rather than deterministic embeddings; (ii) embeddings are sampled from this posterior; and (iii) a KL divergence term between the approximate posterior and the prior is included in the loss.

Specifically, during training, each input ${\boldsymbol x}$ is first augmented twice to obtain ${\boldsymbol x}'$ and ${\boldsymbol x}''$, as in SimCLR. The encoder then maps ${\boldsymbol x}'$ and ${\boldsymbol x}''$ to posterior parameters $(\boldsymbol{\mu}',\boldsymbol{\sigma}')$ and $(\boldsymbol{\mu}'',\boldsymbol{\sigma}'')$, respectively. We then sample $$\begin{align}
\label{eq:bnbn}
{\boldsymbol z}' = \boldsymbol{\mu}' + \operatorname{diag}(\boldsymbol{\sigma}')\;\boldsymbol{\epsilon}_1,
\quad
\text{and}
\quad
{\boldsymbol z}'' = \boldsymbol{\mu}'' + \operatorname{diag}(\boldsymbol{\sigma}'')\;\boldsymbol{\epsilon}_2,
\end{align}$$ where $\boldsymbol{\epsilon}_1,\boldsymbol{\epsilon}_2 \overset{\rm i.i.d}{\sim} \mathcal{N}(\bm 0,I_d)$. After normalizing $\mathbf{z}'$ and $\mathbf{z}''$ to unit length, we compute the InfoNCE loss over the normalized embeddings in the batch and add the KL divergence $$\begin{align}
\frac{1}{d} D\bigl(\mathcal{N}(\boldsymbol{\mu},\operatorname{diag}(\boldsymbol{\sigma}^2)) \,\|\, \mathcal{N}(\bm 0,I_d)\bigr)
\end{align}$$ for each sample. Minimizing this combined objective effectively minimizes $\mathcal{L}^{\rm VCL}$ in [\[eq:VCL_obj\]](#eq:VCL_obj){reference-type="eqref" reference="eq:VCL_obj"} and thus maximizes the ELBO. Figure [1](#fig:VsimCLR){reference-type="ref" reference="fig:VsimCLR"} highlights these differences: VSimCLR replaces deterministic embeddings with the projected‐normal posterior $\mathcal{P}\mathcal{N}(\boldsymbol{\mu},\operatorname{diag}(\boldsymbol{\sigma}^2))$ and regularizes it via KL divergence to the standard normal.

## Variational SupCon (VSupCon)

Building on the variational embedding pipeline of VSimCLR, VSupCon simply swaps the unsupervised InfoNCE term for the supervised contrastive loss while retaining the KL regularizer. Concretely, for each input ${\boldsymbol x}$ with two augmentations ${\boldsymbol x}',{\boldsymbol x}''$, let the encoder output posterior parameters $(\hbox{\boldmath$\mu$}',K')$ and $(\hbox{\boldmath$\mu$}'',K'')$, and sample normalized embeddings $$\begin{align}
{\boldsymbol z}' \sim \mathcal{P}\mathcal{N}(\hbox{\boldmath$\mu$}',K'), 
\qquad 
{\boldsymbol z}'' \sim \mathcal{P}\mathcal{N}(\hbox{\boldmath$\mu$}'',K'').
\end{align}$$ Then the VSupCon objective is the symmetrized supervised loss plus the averaged, normalized KL penalties: $$\begin{align}
\mathcal{L}^{\mathrm{VSup}}
= \frac12\Bigl(\mathcal{L}^{\rm sup}({\boldsymbol z}',{\boldsymbol z}'') + \mathcal{L}^{\rm sup}({\boldsymbol z}'',{\boldsymbol z}')\Bigr)
\;+\;
\frac{1}{2d}\Bigl(D(\hbox{\boldmath$\mu$}',K') + D(\hbox{\boldmath$\mu$}'',K'')\Bigr).
\end{align}$$ Minimizing $\mathcal{L}^{\mathrm{VSup}}$ therefore aligns same‐class embeddings and regularizes their posterior distributions toward the uniform prior on the sphere.

## Generalization analysis for KL divergence

Generalization bounds quantify how a loss function performs on unseen data. While recent work has extensively studied the InfoNCE loss [@saunshi2019theoretical; @lei2023generalization; @hieu2025generalization], the KL regularizer in VCL has not yet received comparable theoretical treatment. To address this gap, we derive a generalization bound for the KL term under the deep neural network encoder function class $\mathcal{F}_{\bm{\Xi}}$, parameterized by a bounded weight set $\bm{\Xi}$ and equipped with Lipschitz activation functions, as considered in the recent work [@hieu2025generalization]. We introduce an informal version of the generalization bound below; the formal version is provided in Theorem [4](#thm:genbnd){reference-type="ref" reference="thm:genbnd"} in Appendix [7.3](#proof:thm){reference-type="ref" reference="proof:thm"}.

::: {#thm:genbnd_informal .theorem}
**Theorem 3** (Informal). *Let $\{\bm{x}_i\}_{i=1}^N \overset{\mathrm{i.i.d}}{\sim} p({\boldsymbol x})$, and let $D_{\mathrm{KL}}(f_{\bm\Theta};\bm x)$ denote the KL regularizer applied to the output of the encoder $f_{\bm{\Theta}} \in \mathcal{F}_{\bm{\Xi}}$ given input $\bm{x}$. Then, with high probability, it holds: $$\begin{align}
\sup_{f_{\bm\Theta} \in \mathcal{F}_{\bm\Xi}}
\left[
\mathbb{E}_{\bm{x} \sim p({\boldsymbol x})}\,D_{\rm KL}(f_{\bm\Theta};\bm{x})
-\frac{1}{N}\sum_{i=1}^N D_{\rm KL}(f_{\bm\Theta};\bm{x}_i)
\right]
\leq \tilde{\mathcal{O}}(1/\sqrt{N}).
\end{align}$$*
:::

Theorem [3](#thm:genbnd_informal){reference-type="ref" reference="thm:genbnd_informal"} shows that the generalization gap of the KL term decays as $\tilde{\mathcal{O}}(1/\sqrt{N})$. In contrast, as shown in [@hieu2025generalization Theorem 1], the gap of InfoNCE scales as $\tilde{\mathcal{O}}(1)$ and does not improve with the number of negative samples $N$. Moreover, their analysis assumes that $\{\bm{x}_i\}_{i=1}^N$ are drawn from class-conditional distributions, which is a strictly stronger assumption than our i.i.d. assumption from the marginal distribution. Therefore, the KL regularizer does not degrade the generalization guarantees of InfoNCE, while also providing principled uncertainty quantification.

<figure id="fig:emb_visualization">

<figcaption>Embedding visualization for SimCLR and VSimCLR on CIFAR-10 test set. (a) t-SNE of SimCLR; (b) t-SNE of VSimCLR; (c) UMAP of SimCLR; (d) UMAP of VSimCLR. VSimCLR preserves the characteristic cluster structure of contrastive learning while introducing probabilistic embeddings regularized by <a href="#eq:upper_kl" data-reference-type="eqref" data-reference="eq:upper_kl">[eq:upper_kl]</a>.</figcaption>
</figure>

# Experiments

We evaluate VCL with SimCLR and SupCon across five aspects: (i) embedding visualization, (ii) dimensional collapse, (iii) mutual information between embeddings and labels, (iv) classification accuracy, and (v) implications of distributional embeddings. Implementation and training details are provided in Appendix [9.1](#app:exp_detail){reference-type="ref" reference="app:exp_detail"}.

## Embedding Visualization

Figure [2](#fig:emb_visualization){reference-type="ref" reference="fig:emb_visualization"} presents t-SNE [@van2008visualizing] and UMAP [@mcinnes2018umap] projections of the embeddings learned by SimCLR and VSimCLR on the CIFAR-10 test set. Although VSimCLR incorporates an additional KL‐regularizer, which often disturb the contrastive learning objective, it preserves the characteristic cluster structure induced by contrastive learning. This confirms that our probabilisitic embeddings retain the semantic information learned by contrastive learning methods.

## Dimensional Collapse

::: wrapfigure
r0.35 ![image](figures/cifar10/All_eigenvalues.pdf){width="35%"}
:::

Contrastive learning methods such as SimCLR often suffer from *dimensional collapse*, where embeddings concentrate in a low‐dimensional subspace, underutilizing the full capacity of the representation space [@jing2021understanding]. To quantify this effect, let $\{{\boldsymbol z}_i\}_{i=1}^N$ be the test‐set embeddings and their covariance matrix $$\begin{align}
    C = \frac{1}{N}\sum_{i=1}^N ({\boldsymbol z}_i - \bar{{\boldsymbol z}})({\boldsymbol z}_i - \bar{{\boldsymbol z}})^\top,
\end{align}$$ with $\bar{{\boldsymbol z}} = \frac{1}{N}\sum_{i=1}^N {\boldsymbol z}_i$.

Figure [\[fig:emb_dim\]](#fig:emb_dim){reference-type="ref" reference="fig:emb_dim"} shows the singular values of $C$ for SimCLR and VSimCLR. VSimCLR produces a substantially flatter spectrum, indicating a higher effective rank and thus mitigating dimensional collapse. Remarkably, on CIFAR‐100, VSimCLR nearly doubles the number of dominant components compared to SimCLR. These results demonstrate that VSimCLR not only preserves semantic clustering but also leverages the embedding space more fully, and can be combined with other collapse‐mitigation strategies for further gains. Additional experiments on Caltech-256 and Tiny-ImageNet (Figure [6](#fig:additional_dim_collapse){reference-type="ref" reference="fig:additional_dim_collapse"}, Appendix [9.2](#app:dim_collapse){reference-type="ref" reference="app:dim_collapse"}) exhibit similar behavior.

## Mutual Information Comparison

::: wrapfigure
r0.3 ![image](figures/MI_comp.pdf){width="30%"}
:::

Figure [\[fig:MI_comp\]](#fig:MI_comp){reference-type="ref" reference="fig:MI_comp"} reports the estimated mutual information $I({\boldsymbol z};{\boldsymbol c})$ between the learned embeddings ${\boldsymbol z}$ and their true class labels ${\boldsymbol c}$ of CIFAR-10. We compute this using the Mixed KSG estimator [@gao2017estimating], which is well‐suited for mixed or multimodal distributions.

Both VSimCLR and VSupCon achieve mutual information on par with---or slightly exceeding---their non‐variational counterparts. These results indicate that VSimCLR ultimately preserves---or even improves---information between embeddings and labels, while also producing rich distributional representations.

:::::: table*
::::: center
:::: scriptsize
::: sc
+-----------------------------------+---------------+---------------+-------------------+---------------+---------------+
| **Method**                        | CIFAR-10      | CIFAR-100     | Tiny-imagenet     | STL10         | Caltech256    |
+:==================================+:=====:+:=====:+:=====:+:=====:+:=======:+:=======:+:=====:+:=====:+:=====:+:=====:+
| 2-3 (r)4-5 (r)6-7 (r)8-9 (r)10-11 | Top1  | Top5  | Top1  | Top5  | Top1    | Top5    | Top1  | Top5  | Top1  | Top5  |
+-----------------------------------+-------+-------+-------+-------+---------+---------+-------+-------+-------+-------+
| SimCLR                            | 78.42 | 98.52 | 49.56 | 78.84 | 38.95   | 66.89   | 60.44 | 95.80 | 43.14 | 66.15 |
+-----------------------------------+-------+-------+-------+-------+---------+---------+-------+-------+-------+-------+
| VSimCLR                           | 81.48 | 98.95 | 54.58 | 82.87 | 37.70   | 66.06   | 60.11 | 92.00 | 48.50 | 69.99 |
+-----------------------------------+-------+-------+-------+-------+---------+---------+-------+-------+-------+-------+
| SupCon                            | 93.60 | 99.71 | 70.79 | 89.11 | 57.60   | 77.16   | 75.88 | 98.51 | 87.06 | 91.64 |
+-----------------------------------+-------+-------+-------+-------+---------+---------+-------+-------+-------+-------+
| VSupCon                           | 93.85 | 99.68 | 71.66 | 89.42 | 48.30   | 72.84   | 75.76 | 96.99 | 83.06 | 91.29 |
+-----------------------------------+-------+-------+-------+-------+---------+---------+-------+-------+-------+-------+
:::
::::
:::::
::::::

## Classification

For classification, we use the posterior mean $\boldsymbol{\mu}_{\mathbf{x}}$ as the embedding and train a linear classifier.[^2] Table [\[tab:classification\]](#tab:classification){reference-type="ref" reference="tab:classification"} reports Top-1 and Top-5 accuracies on CIFAR-10, CIFAR-100, Tiny-ImageNet, STL-10, and Caltech-256. VSimCLR outperforms SimCLR on CIFAR-10 and CIFAR-100 in Top-1 accuracy, with similar gains in Top-5. On Caltech-256, VSimCLR also improves Top-1 accuracy substantially. Performance on Tiny-ImageNet and STL-10 remains comparable, with slight decreases (within experimental variance) likely due to the KL regularizer.

SupCon provides supervised baselines, and VSupCon further improves Top-1 accuracy on CIFAR-10 and CIFAR-100. Modest declines on Tiny-ImageNet, STL-10, and Caltech-256 reflect the trade-off of adding the KL term on datasets with higher complexity or fewer samples. We hypothesize that the drop in VSupCon arises from two factors: (i) VSimCLR's objective coincides with the VCL objective in [\[eq:VCL_obj\]](#eq:VCL_obj){reference-type="eqref" reference="eq:VCL_obj"}, whereas VSupCon's does not, creating a mismatch that may hinder proper ELBO maximization; and (ii) SupCon directly optimizes embeddings for classification, so an added KL term can conflict with that objective. We further study the effect of the KL regularizer in Appendix [9.4](#app:KL_classification){reference-type="ref" reference="app:KL_classification"}.

Although VCL is not designed to boost classification accuracy, VSimCLR consistently match or exceed their deterministic counterparts. This demonstrates that probabilistic embeddings preserve the alignment and uniformity [@wang2020understanding], while yielding meaningful uncertainty proxy.

<figure id="fig:examples" data-latex-placement="t">
<embed src="figures/VSimCLR_images_and_logdetvar.pdf" style="width:90.0%" />
<figcaption>Sample images from the CIFAR-10, organized by class (columns) and sorted by their corresponding <span class="math inline">$\log {\hbox{det}}(K)$</span> (rows). In each column, the top image has the highest <span class="math inline">$\log {\hbox{det}}(K)$</span>, the bottom image the lowest; the overlaid numbers indicate each image’s <span class="math inline">$\log {\hbox{det}}(K)$</span>.</figcaption>
</figure>

## Implications of Distributional Embeddings

#### Examples of CIFAR-10 with posterior.

We illustrate the interpretability of posterior using examples from CIFAR-10. Figure [3](#fig:examples){reference-type="ref" reference="fig:examples"} displays sample images alongside the log‐determinant $\log{\hbox{det}}(K)$ of their posterior covariance $K$ learned by VSimCLR. Top‐row images are common class members and exhibit larger $\log{\hbox{det}}(K)$---indicating broader posterior dispersion---whereas bottom‐row images are atypical or uncommon with smaller $\log{\hbox{det}}(K)$, reflecting more concentrated posteriors.[^3]

::: wrapfigure
r0.3 ![image](figures/Entropy_LogDet_LinRegression.pdf){width="30%"}
:::

#### Relationship between label-entropy and $\log{\hbox{det}}(K)$.

We quantify the relationship between posterior covariance and uncertainty using CIFAR-10H [@peterson2019human] and CIFAR-10C [@hendrycks2019robustness]. Figure [\[fig:lin_reg\]](#fig:lin_reg){reference-type="ref" reference="fig:lin_reg"} and [7](#fig:lin_reg_trace){reference-type="ref" reference="fig:lin_reg_trace"} plot posterior dispersion against the entropy of the CIFAR-10H soft labels [@ishida2023is; @jeong2023demystifying]; the negative slope of the linear fit (red line) indicates that images with lower $\log{\hbox{det}}(K)$---i.e., more concentrated posteriors---tend to have higher label entropy and thus greater ambiguity. Next, using CIFAR-10C, we examine how posterior covariance varies with corruption severity, which correlates with label uncertainty. Figures [4](#fig:cifar10c_vsim){reference-type="ref" reference="fig:cifar10c_vsim"} and [8](#fig:cifar10c_vsup){reference-type="ref" reference="fig:cifar10c_vsup"} show that $\log{\hbox{det}}(K)$ decreases as corruption strength increases, implying that lower posterior dispersion corresponds to higher uncertainty, consistent with Figure [\[fig:lin_reg\]](#fig:lin_reg){reference-type="ref" reference="fig:lin_reg"}. These results demonstrate that the dispersion of the learned posterior correlates with semantic uncertainty, highlighting the practical interpretability of VCL's distributional embeddings.

#### Use case of posterior.

As an example application of posterior, we consider CIFAR-100 under a label-scarce setting in which only a small number of labels per class are available to train a linear classifier. Table [1](#tab:label_scarcity){reference-type="ref" reference="tab:label_scarcity"} reports accuracies for SimCLR, VSimCLR, and VSimCLR+wt, with classifiers trained using cross-entropy (CE). Here, "+wt" denotes a weighted CE in which sample weights are proportional to posterior covariance to downweight ambiguous examples. Specifically, we use $\mathcal{L}_{wCE} = \sum_{i=1}^N w_i \log \phi_{c_i}({\boldsymbol z}_i), \text{ with } w_i \propto \log{\hbox{det}}(K) \text{ (after normalization)}$, where $\phi_{c_i}({\boldsymbol z}_i)$ is the estimated probability of the true class. Table [1](#tab:label_scarcity){reference-type="ref" reference="tab:label_scarcity"} shows that VCL variants improve over SimCLR and SupCon, with smaller gains for SupCon since it already leverages labels during pretraining. Moreover, weighting by posterior covariance further improves performance, supporting probabilistic embeddings as a confidence proxy. Additional experiments and discussion are provided in Appendix [9.5](#app:implication){reference-type="ref" reference="app:implication"}.

This counterintuitive finding---that typical (i.e., common) samples exhibit larger posterior dispersion---parallels the observation in concurrent work in [@guth2025learning], albeit under different settings: (i) Quantity: we analyze latent-space posterior via $\log{\hbox{det}}K$, whereas they study input-space marginal density $p(x)$; (ii) Observation: typical samples have larger $\log{\hbox{det}}K$, while they have lower marginal density. Although the quantities are measured in different spaces, both results indicate that typical samples are not the highest-density points. In our case, typical images yield larger posterior dispersion and atypical images smaller dispersion; since dispersion is inversely related to peak density, our result aligns with [@guth2025learning]. Hence, in both settings, "typical" $\neq$ "highest-density."

<figure id="fig:cifar10c_vsim" data-latex-placement="t">

<figcaption><span class="math inline">$\log {\hbox{det}}(K)$</span> of <strong>VSimCLR</strong> embeddings on CIFAR-10C under different corruption types and severities. “Severity” denotes the corruption level. Exact <span class="math inline">$\log {\hbox{det}}(K)$</span> values are in Table <a href="#tab:cifar10c_vsim_logdet" data-reference-type="ref" data-reference="tab:cifar10c_vsim_logdet">4</a>.</figcaption>
</figure>

:::::: center
::::: scriptsize
:::: sc
::: {#tab:label_scarcity}
  **Methods**    1 **Labels / class**   3 **Labels / class**   5 **Labels / class**   10 **Labels / class**   20 **Labels / class**           
  ------------- ---------------------- ---------------------- ---------------------- ----------------------- ----------------------- -- -- -- --
  SimCLR             12.22 (0.12)           21.37 (0.15)           26.37 (0.01)           33.09 (0.11)            38.00 (0.06)                
  VSimCLR            15.57 (0.09)           25.70 (0.19)           30.89 (0.11)           37.40 (0.08)            42.13 (0.10)                
  VSimCLR+wt       **15.97** (0.08)       **26.07** (0.20)       **31.12** (0.06)       **37.48** (0.08)        **42.36** (0.03)              
  SupCon             71.55 (0.04)           71.56 (0.05)           71.64 (0.02)           71.65 (0.03)            72.07 (0.05)                
  VSupCon            71.77 (0.12)         **71.79** (0.10)       **71.96** (0.09)       **72.07** (0.05)        **72.16** (0.04)              
  VSupCon+wt       **71.87** (0.02)       **71.78** (0.07)       **71.94** (0.07)       **72.07** (0.07)        **72.16** (0.06)              

  : Classification accuracy on CIFAR-100 with label scarcity. We use ResNet-18 back-bone and same augmentations for all experiments. We sample the labelled subset once and report the mean accuracy of five runs with (standard error).
:::
::::
:::::
::::::

# Conclusion

We have introduced *Variational Contrastive Learning* (VCL), a decoder-free ELBO-maximization framework that endows contrastive learning with principled probabilistic embeddings. By interpreting InfoNCE as a surrogate reconstruction term and regularizing with a KL divergence to a uniform prior on the unit sphere, VCL learns posterior without explicit decoders. We instantiated VCL in two variants---VSimCLR and VSupCon---by replacing deterministic embeddings with samples from $q_\theta({\boldsymbol z}\mid{\boldsymbol x})$ and adding a normalized KL term.

Theoretical and empirical results show that VCL preserves the properties of contrastive embeddings, mitigates dimensional collapse, maintains or improves mutual information with labels, and matches or exceeds deterministic baselines in classification accuracy, while also providing meaningful posterior uncertainty estimates.

We further analyzed the implications of probabilistic embeddings---spanning label uncertainty, typicality, and OOD behavior---through posterior-covariance dispersion. We also observed a counterintuitive but consistent pattern, echoed in concurrent diffusion-model work [@guth2025learning]: lower posterior-covariance dispersion is associated with higher sample uniqueness (i.e., more atypical or outlier examples), whereas typical samples exhibit larger posterior covariance dispersion.

# Related work

## Contrastive learning

Self‐supervised contrastive learning methods [@chen2020simple; @tian2020makes] train an encoder $f\colon \mathcal{X}\to \mathcal{S}^{d_z-1}$ by drawing semantically related views (positives) together in the embedding space while pushing unrelated views (negatives) apart. In the standard setup, each example is treated as its own category, and only its augmented copies count as positives. A variety of contrastive objectives---such as InfoNCE [@oord2018representation], Debiased Contrastive Loss [@chuang2020debiased], Unbiased Contrastive Loss [@barbano2022unbiased], triplet‐based losses [@chopra2005learning; @hermans2017defense], and others---have been used to learn robust representations for tasks ranging from dense prediction in computer vision [@wang2021dense] to multimodal alignment [@radford2021learning; @girdhar2023imagebind; @jeong2024anchors]. InfoNCE [@oord2018representation] in particular has been shown to lower‐bound mutual information [@poole2019variational], and subsequent work has revealed that its empirical success hinges on a balance of *alignment* and *uniformity* in the learned embeddings [@tschannen2019mutual; @wang2020understanding]. In the supervised setting, SupCon [@khosla2020supervised] extends this idea by using class labels to define positive pairs among same‐class samples, often surpassing cross‐entropy training in downstream performance. ProjNCE, a generalization of SupCon [@jeong2025generalizing], modifies SupCon loss so that it becomes a proper mutual information lower bound.

## Probabilistic contrastive learning

A growing body of work has begun to integrate probabilistic latent‐variable modeling with contrastive objectives. In the video domain, Park et al. represent each video clip as a Gaussian and combine them into a mixture model, learning these distributions via a stochastic contrastive loss that captures clip‐level uncertainty and obviates complex augmentation schemes [@park2022probabilistic]. For 3D point clouds, Wang et al. propose a Generative Variational‐Contrastive framework that models latent features as Gaussians, enforces distributional consistency across positive pairs by combining the variational autoencoder and contrastive learning [@wang2024generative]. In graph representation learning, Xie and Giraldo introduce Subgraph Gaussian Embedding Contrast, which maps subgraphs into a structured Gaussian space and employs optimal‐transport distances for robust contrastive objectives, yielding improved classification and link‐prediction performance [@xie2024variational].

On the theoretical front, Zimmermann et al. prove that contrastive objectives invert the data‐generating process under mild conditions, uncovering a deep connection to nonlinear independent component analysis [@zimmermann2021contrastive]. With a more generalized setting, Kirchhof et al. extend the InfoNCE loss so that the encoder predicts a full posterior distribution rather than a point, and prove that these distributions asymptotically recover the true aleatoric uncertainty of the data‐generating process [@kirchhof2023probabilistic].

## Variational Inference and Contrastive Learning

The most closely related line of work frames contrastive learning within a latent‐variable inference paradigm via Recognition‐Parametrised Models (RPMs) [@aitchison2021infonce; @walker2023unsupervised]. Aitchison and Ganev introduce RPMs as a class of Bayesian models whose (unnormalized) likelihood is defined implicitly through a recognition network [@aitchison2021infonce]. They show that, under RPMs, the ELBO decomposes into mutual information minus a KL term (up to a constant), and that for a suitable choice of prior the infinite‐sample InfoNCE objective coincides with this ELBO. Walker et al. consider RPMs by assuming conditional independence of observations given latent variables, and develop an EM algorithm that achieves exact maximum‐likelihood learning for discrete latents along with principled posterior inference [@walker2023unsupervised].

Other works recast variational inference itself as a contrastive estimation task. Rhodes and Gutmann's Variational Noise‐Contrastive Estimation (VNCE) derives a variational lower bound to the standard NCE objective, enabling joint learning of model parameters and latent posteriors in unnormalized models [@rhodes2019variational]. More recently, Ward et al. propose SoftCVI, which treats VI as a classification problem: they generate "soft" pseudo‐labels from the unnormalized posterior and optimize a contrastive‐style objective that yields zero‐variance gradients at the optimum [@ward2025softcvi].

## Dimensional collapse

In contrastive self-supervised learning, several approaches have been proposed to prevent dimensional collapse by regularizing either the embedding projector or the second-order statistics of the representations. Jing *et al.* [@jing2021understanding] first demonstrated that, despite the repulsive effect of negative samples, embeddings can still collapse to a low-dimensional subspace due to a combination of strong augmentations and implicit low-rank bias in weight updates. They introduced DirectCLR, which fixes a low-rank diagonal projector during training; this projector enforces the embeddings to occupy a predetermined subspace and was shown empirically to outperform SimCLR's learned linear projector.

Following this, several works have designed novel loss functions that explicitly regularize the covariance or cross-correlation of the embedding vectors. Ermolov *et al.* [@ermolov2021whitening] apply a whitening MSE loss so that positive pairs match under mean-square error while enforcing identity covariance. Barlow Twins [@zbontar2021barlow] minimize the deviation of the normalized cross-correlation matrix from the identity, effectively performing "soft whitening" to reduce redundancy. VICReg [@bardes2021vicreg] further augments this idea by combining variance, invariance, and covariance regularizers to avoid collapse without using negative samples; notably, VICReg allows its two branches to use different architectures or even modalities, enabling joint embedding across data types. More recently, He *et al.* [@he2024preventing] showed that orthogonal regularization of encoder weight matrices preserves representation diversity and prevents collapse.

# Proofs

## Proof of Lemma [1](#lem:recon_app_jensen){reference-type="ref" reference="lem:recon_app_jensen"} {#app:recon_app_jensen}

::: proof
*Proof.* With any auxiliary probability function $r({\boldsymbol z}’|{\boldsymbol x})$ and Jensen's inequality, we have $$\begin{align}
    \mathbb{E}_{q({\boldsymbol z}|{\boldsymbol x})} [ \log p({\boldsymbol x}|{\boldsymbol z}) ] 
    & \geq \mathbb{E}_{q({\boldsymbol z}|{\boldsymbol x}) r({\boldsymbol z}’|{\boldsymbol x})} \left[ \log \frac{p({\boldsymbol z}’|{\boldsymbol x}) p({\boldsymbol x}|{\boldsymbol z}’)}{r({\boldsymbol z}’|{\boldsymbol x})} \right] \nonumber \\
    & \overset{\rm (a)}{=} \mathbb{E}_{q({\boldsymbol z}|{\boldsymbol x}) r({\boldsymbol z}’|{\boldsymbol x})}[\log p({\boldsymbol z}’|{\boldsymbol z})] + \mathbb{E}_{r({\boldsymbol z}’|{\boldsymbol x})}[\log p({\boldsymbol x}|{\boldsymbol z}’)] + H(r({\boldsymbol z}’|{\boldsymbol x})) \nonumber\\
    & = \mathbb{E}_{q({\boldsymbol z}|{\boldsymbol x})q({\boldsymbol z}’|{\boldsymbol x})}[\log p({\boldsymbol z}’|{\boldsymbol z})] + {\rm const.},
\end{align}$$ where $\rm (a)$ follows by choosing $r(z’|x) = q(z’|x)$. This proves Lemma [1](#lem:recon_app_jensen){reference-type="ref" reference="lem:recon_app_jensen"}. ◻
:::

## Proof of Proposition [2](#prop:opt_inf){reference-type="ref" reference="prop:opt_inf"} {#app:opt_inf}

::: proof
*Proof.* Optimal critic [@ma-collins-2018-noise] for InfoNCE satisfies that $$\begin{align}
\label{eq:nce_inf_opt_proof}
    \psi^\star({\boldsymbol x},{\boldsymbol z})
    & \propto \log \frac{p({\boldsymbol x}|{\boldsymbol z})}{p({\boldsymbol x})} + \alpha({\boldsymbol z}),
\end{align}$$ where $\alpha({\boldsymbol z})$ only depends on ${\boldsymbol z}$. With the optimal critic, we then have $$\begin{align}
    I_{\rm NCE}({\boldsymbol x};{\boldsymbol x}')
    & = - \mathbb{E}\left[  \log \frac{e^{\psi({\boldsymbol z},{\boldsymbol z}'_i) }}{ \sum_{j=1}^N e^{\psi({\boldsymbol z},{\boldsymbol z}'_j)}} \right]  \nonumber \\
%   & = - \EE\left[  \log \frac{\frac{p(\zs|\zs'_i)}{p(\zs)} }{ \sum_{j} \frac{p(\zs|\zs'_j)}{p(\zs)} } \right]  \nonumber \\
    & = - \mathbb{E}\left[  \log \frac{ p({\boldsymbol z}|{\boldsymbol z}'_i) }{ \sum_{j=1}^N p({\boldsymbol z}|{\boldsymbol z}'_j) } \right]  \nonumber \\
    & = - \mathbb{E}\left[  \log \frac{ p({\boldsymbol z}|{\boldsymbol z}'_i) }{ \frac{1}{N}\sum_{j=1}^N p({\boldsymbol z}|{\boldsymbol z}'_j) } \right]  + \log N.
\end{align}$$ Given ${\boldsymbol z}$, since $p({\boldsymbol z}|{\boldsymbol z}'_j),~j\in\{1,2,\cdots,N\}$ are i.i.d. with $\mathbb{E}[p({\boldsymbol z}|{\boldsymbol z}_j')] = p({\boldsymbol z}) < \infty$, the strong law of large numbers yields $$\begin{align}
    \lim_{N\to\infty} \frac{1}{N} \sum_{j=1}^N p({\boldsymbol z}|{\boldsymbol z}'_j)
    & = p({\boldsymbol z}).
\end{align}$$ The continuous mapping theorem then gives $$\begin{align}
     \lim_{N\to\infty}  \log \frac{ p({\boldsymbol z}|{\boldsymbol z}'_i) }{ \frac{1}{N}\sum_{j=1}^N p({\boldsymbol z}|{\boldsymbol z}'_j) }
     & = \log \frac{ p({\boldsymbol z}|{\boldsymbol z}'_i) }{ p({\boldsymbol z})}.
\end{align}$$

Rearranging [\[eq:nce_inf_opt_proof\]](#eq:nce_inf_opt_proof){reference-type="eqref" reference="eq:nce_inf_opt_proof"} and taking $N\to\infty$, we obtain $$\begin{align}
\label{eq:dominated_conv}
    \lim_{N\to\infty} \left\{ I_{\rm NCE}({\boldsymbol x};{\boldsymbol x}') + \log N \right\} 
    & =  \lim_{N\to\infty} \mathbb{E}\left[  \log \frac{ p({\boldsymbol z}|{\boldsymbol z}'_i) }{ \frac{1}{N}\sum_{j=1}^N p({\boldsymbol z}|{\boldsymbol z}'_j) } \right] \nonumber \\
    & \overset{\rm (a)}{=} \mathbb{E}\left[  \lim_{N\to\infty} \log \frac{ p({\boldsymbol z}|{\boldsymbol z}'_i) }{ \frac{1}{N}\sum_{j=1}^N p({\boldsymbol z}|{\boldsymbol z}'_j) } \right] \nonumber \\
    & = \mathbb{E}\left[  \log \frac{ p({\boldsymbol z}|{\boldsymbol z}'_i) }{ p({\boldsymbol z})} \right],
\end{align}$$ where the equality $\rm (a)$ follows by dominated convergence theorem that is verifiable using the fact that $$\begin{align}
    \mathbb{E}\left[ \log \frac{ p({\boldsymbol z}|{\boldsymbol z}'_i) }{ \frac{1}{N}\sum_{j=1}^N p({\boldsymbol z}|{\boldsymbol z}'_j) } \right]
    & = \mathbb{E}\left[ \log p({\boldsymbol z}|{\boldsymbol z}'_i) - \log \frac{1}{N}\sum_{j=1}^N p({\boldsymbol z}|{\boldsymbol z}'_j) \right]   \nonumber \\
    & \leq \mathbb{E}\left[ \log g({\boldsymbol z}) - \log \epsilon \right]   \nonumber \\
    & \leq \log \mathbb{E}\left[ g({\boldsymbol z})\right] - \log\epsilon \nonumber \\
    & < \infty.
\end{align}$$ Rewriting [\[eq:dominated_conv\]](#eq:dominated_conv){reference-type="eqref" reference="eq:dominated_conv"} gives $$\begin{align}
    & \lim_{N\to\infty} \left\{ I_{\rm NCE}({\boldsymbol x};{\boldsymbol x}') + \log N \right\} \nonumber \\
    & =  \mathbb{E}\left[  \log \frac{ p({\boldsymbol z}|{\boldsymbol z}'_i) }{ p({\boldsymbol z}) } \right]  \nonumber \\
    & = \mathbb{E}\left[  \log \frac{ p({\boldsymbol z}'_i|{\boldsymbol z}) }{ p({\boldsymbol z}'_i) } \right]   \nonumber \\
    & = \mathbb{E}_{q_\theta({\boldsymbol z}_i'|{\boldsymbol x})q_\theta({\boldsymbol z}|{\boldsymbol x}) } \left[  \log p({\boldsymbol z}'_i|{\boldsymbol z}) \right]  + \mathbb{E}_{q_\theta({\boldsymbol z}_i'|{\boldsymbol x})}\left[ \log p({\boldsymbol z}'_i)  \right] \nonumber \\
    & = \mathbb{E}_{q_\theta({\boldsymbol z}_i'|{\boldsymbol x})q_\theta({\boldsymbol z}|{\boldsymbol x}) } \left[  \log p({\boldsymbol z}'_i|{\boldsymbol z}) \right]  + \mathbb{E}_{q_\theta({\boldsymbol z}_i'|{\boldsymbol x})}\left[ \log \frac{p({\boldsymbol z}'_i)}{q_\theta({\boldsymbol z}_i'|{\boldsymbol x})}  \right] + \mathbb{E}_{q_\theta({\boldsymbol z}_i'|{\boldsymbol x})}\left[ \log q_\theta({\boldsymbol z}'_i|{\boldsymbol x})  \right] \nonumber \\
    & = \mathbb{E}_{q_\theta({\boldsymbol z}_i'|{\boldsymbol x})q_\theta({\boldsymbol z}|{\boldsymbol x}) } \left[  \log p({\boldsymbol z}'_i|{\boldsymbol z}) \right]  - D( q_\theta({\boldsymbol z}_i'|{\boldsymbol x}) \| p({\boldsymbol z}'_i) ) -H( q_\theta({\boldsymbol z}'_i|{\boldsymbol x}) ) .
\end{align}$$ Substituting ${\boldsymbol z}_i'$ into ${\boldsymbol z}'$, this concludes the proof of Proposition [2](#prop:opt_inf){reference-type="ref" reference="prop:opt_inf"} ◻
:::

## Formal statement of Theorem [3](#thm:genbnd_informal){reference-type="ref" reference="thm:genbnd_informal"} and its proof {#proof:thm}

In this section, we provide detailed problem setup, formal statement of Theorem [3](#thm:genbnd_informal){reference-type="ref" reference="thm:genbnd_informal"}, its extension to random augmentation, and the proof of [3](#thm:genbnd_informal){reference-type="ref+Label" reference="thm:genbnd_informal"}.

### Problem setup and formal statement of Theorem [3](#thm:genbnd_informal){reference-type="ref" reference="thm:genbnd_informal"} {#problem-setup-and-formal-statement-of-theorem-thmgenbnd_informal}

First, we introduce the class of the deep forward neural network with bounded weights and Lipschitz activations.

Let $\{\bm x_i\}_{i=1}^N \subset \mathcal X$ be independent and identically distributed samples from a distribution $\mathcal D$ on $\mathcal X$. We consider depth-$L$ feed-forward networks $f_{\bm\Theta}:\mathcal X\to\mathbb R^{2d}$ of the form $$f_{\bm\Theta}(\bm x)
=\alpha_L\Bigl(\bm\Theta^L\,\alpha_{L-1}\bigl(\bm\Theta^{L-1}\,\cdots\alpha_1(\bm\Theta^1\bm x)\bigr)\Bigr),$$ where each activation $\alpha_\ell:\mathbb R^{d_\ell}\to\mathbb R^{d_\ell}$ is $\sigma_\ell$-Lipschitz. For each layer $\ell=1,\dots,L$, the weight matrices lie in $$\begin{equation}
\label{eq:weightdf}
\bm\Xi_{\ell}
:=\bigl\{\bm\Theta^\ell\in\mathbb R^{d_\ell\times d_{\ell-1}} :
\|\bm\Theta^\ell\|_2\le\rho_\ell,\;\|(\bm\Theta^\ell-\bm \Theta_0^\ell)^\top\|_{2,1}\le a_\ell\bigr\},
\end{equation}$$ where $\|\cdot\|_2$ is the spectral norm and $\|\cdot\|_{2,1}$ is the sum of column-wise $\ell_2$ norms, and $\{\bm \Theta_0^\ell\}_{\ell=1}^L$ are fixed reference matrices. Set $\bm\Xi=\prod_{\ell=1}^L\bm\Xi_\ell$ and define $$\begin{equation}
\label{eq:functionclass}
\mathcal F_{\bm\Xi}
=\bigl\{f_{\bm\Theta}:\bm\Theta\in\bm\Xi\bigr\}.
\end{equation}$$

In VCL the encoder output splits as $$\begin{equation}
\label{eq:twoparts}
f_{\bm\Theta}(\bm x)
=\begin{pmatrix}\mu_{\bm\Theta}(\bm x)\\\varepsilon_{\bm\Theta}(\bm x)\end{pmatrix},
\quad
\mu_{\bm\Theta},\,\varepsilon_{\bm\Theta}:\mathcal X\to\mathbb R^d.
\end{equation}$$ Given $\bm{x}$, the KL regularizer by output of $f_{\bm{\Theta}}(\bm{x})$ in [\[eq:twoparts\]](#eq:twoparts){reference-type="eqref" reference="eq:twoparts"} is $$\begin{equation}
\label{eq:KLloss}
D\bigl(f_{\bm\Theta};\bm x\bigr)
=\frac{1}{2d}\Bigl(
\|\mu_{\bm\Theta}(\bm x)\|_2^2
+\bigl\|\exp(\varepsilon_{\bm\Theta}(\bm x))\bigr\|_1
-d
-\bigl\langle\bm1_d,\varepsilon_{\bm\Theta}(\bm x)\bigr\rangle
\Bigr).
\end{equation}$$

::: {#thm:genbnd .theorem}
**Theorem 4**. *Fix $\delta\in(0,1)$. Under the assumptions on $\{\bm{x}_i\}_{i=1}^N$ and definitions on $\mathcal{F}_{\bm{\Xi}}$ [\[eq:functionclass\]](#eq:functionclass){reference-type="eqref" reference="eq:functionclass"} and $D\bigl(f_{\bm\Theta};\bm x\bigr)$ [\[eq:KLloss\]](#eq:KLloss){reference-type="eqref" reference="eq:KLloss"}, it holds that $$\begin{aligned}
\sup_{f_{\bm\Theta}\in\mathcal F_{\bm\Xi}}
\Bigl[
\mathbb{E}_{\bm x\sim\mathcal D}\,D(f_{\bm\Theta};\bm x)
&-\frac{1}{N}\sum_{i=1}^N D(f_{\bm\Theta};\bm x_i) 
\Bigr]\\
&\leq 
\widetilde{\mathcal O}\!\Bigl(
\frac{\eta\,B_x\prod_{\ell=1}^L(\rho_\ell\,\sigma_\ell)}{\sqrt{N}}
\Bigl[
\sqrt{\log W}\,\Bigl(\sum_{\ell=1}^L\bigl(a_\ell/\rho_\ell\bigr)^{2/3}\Bigr)^{3/2}
\;\vee\;
\sqrt{\log(1/\delta)}
\Bigr]
\Bigr),
\end{aligned}$$ with probability at least $1-\delta$, where $$W=\max_{\ell\in[L]}d_\ell,
\quad
B_x=\sup_{\bm x\in\mathcal X}\|\bm x\|_2,
\quad
\eta=\mathcal{O}\!\Bigl(1\vee e^{B_x\prod_{\ell=1}^L\rho_\ell\,\sigma_\ell}\Bigr).$$*
:::

#### Random augmentations.

One might be concerned that the encoder in [\[eq:twoparts\]](#eq:twoparts){reference-type="eqref" reference="eq:twoparts"} uses raw inputs, whereas in practice each sample $\bm x_i$ is fed through two random augmentations $t'_i,t''_i\sim\mathcal T$ (see [2.1](#subsec:selfsup){reference-type="ref+Label" reference="subsec:selfsup"}). To deal with this concern, we show that exactly the same bound of Theorem [4](#thm:genbnd){reference-type="ref" reference="thm:genbnd"} holds under this more practical setting.

Let $\mathcal T$ be a distribution over augmentation maps $t:\mathcal X\to\mathcal X$. Define the *augmented data law* $$\mathcal D_{\rm aug}
\;:=\;
\mathop{\mathrm{Law}}_{\,\bm x\sim\mathcal D,\;t\sim\mathcal T}\bigl(t(\bm x)\bigr)
\;=\;
\mathcal T_\#\mathcal D,$$ where $\mathcal{T}_\#\mathcal{D}$ denotes the push-forward of the measure $\mathcal{D}$ through the (random) map distribution $\mathcal{T}$.

If $$t_i\;\overset{\mathrm{i.i.d.}}{\sim}\;\mathcal T,
\quad
\bm x_i' = t_i(\bm x_i),$$ then by construction $$\bm x_1',\dots,\bm x_N'
\;\overset{\mathrm{i.i.d.}}{\sim}\;
\mathcal D_{\rm aug},$$ so the conditions of Theorem [4](#thm:genbnd){reference-type="ref" reference="thm:genbnd"} remain satisfied with $\mathcal D_{\rm aug}$ in place of $\mathcal D$.

::: {#cor:genbnd .corollary}
**Corollary 5**. *Let $\{t'_i\}_{i=1}^N$ and $\{t''_i\}_{i=1}^N$ be independent samples from $\mathcal T$, and set $\bm x_i' = t'_i(\bm x_i)$, $\bm x_i'' = t''_i(\bm x_i)$. Under the assumptions of Theorem [4](#thm:genbnd){reference-type="ref" reference="thm:genbnd"}, with probability at least $1-\delta$ we have $$\begin{aligned}
\sup_{f_{\bm\Theta}\in\mathcal F_{\bm\Xi}}
\Bigl[
\mathbb{E}_{\bm x\sim\mathcal D,\,t\sim\mathcal T}\,&D\bigl(f_{\bm\Theta};t(\bm x)\bigr)
-\frac{1}{2N}\sum_{i=1}^N\bigl(D(f_{\bm\Theta};\bm x_i') + D(f_{\bm\Theta};\bm x_i'')\bigr)
\Bigr]
\\
&\leq 
\widetilde{\mathcal O}\!\Bigl(
\frac{\eta\,B_{x,t}\prod_{\ell=1}^L(\rho_\ell\sigma_\ell)}{\sqrt{N}}
\Bigl[
\sqrt{\log W}\,\Bigl(\sum_{\ell=1}^L(a_\ell/\rho_\ell)^{2/3}\Bigr)^{3/2}
\;\vee\;
\sqrt{\log(1/\delta)}
\Bigr]
\Bigr),
\end{aligned}$$ where $\displaystyle B_{x,t} = \sup_{\bm x\in\mathcal X,\,t\in\mathcal T}\|t(\bm x)\|_2$.*
:::

One might wonder whether $B_{x,t}$ is finite in [5](#cor:genbnd){reference-type="ref+Label" reference="cor:genbnd"}. In practice, common vision augmentations---such as random resized cropping and horizontal flips [@krizhevsky2012imagenet; @perez2017effectiveness], color jittering (brightness, contrast, saturation perturbations) [@shorten2019survey; @chen2020simple], and additive Gaussian blur or noise [@hendrycks2020augmix]---always produce outputs that remain within a bounded neighborhood of the original inputs. For example, cropping a $256 \times 256$ image to $224\times 224$ keeps all pixel values in their original range, and color jitter is applied with limited intensity so that augmented images lie close in Euclidean norm to the source. Even when combining multiple operations (as in AugMix), the convex mixtures ensure augmented samples do not "drift" outside the natural image manifold. Hence, $\sup_{\bm{x}\in\mathop{\mathrm{\mathcal{X}}},\,t\in\mathcal{T}}\|t(\bm{x})\|_2$ remains finite under these widely adopted schemes.

Before proving Theorem [4](#thm:genbnd){reference-type="ref" reference="thm:genbnd"}, we need to introduce some background on Rademacher complexity, Dudley's entropy‐integral bound, and Lipschitz of KL divergence, which are necessary in the proof.

### Backround: Rademacher complexity and Dudley's entropy‐integral bound

We introduce the background on Rademacher complexity, Dudley's entropy‐integral bound, and covering number for the function class $\mathcal{F}_{\bm{\Xi}}$ in [\[eq:functionclass\]](#eq:functionclass){reference-type="eqref" reference="eq:functionclass"}, which are used in the proof of Theorem [4](#thm:genbnd){reference-type="ref" reference="thm:genbnd"}.

::: {#lem:rademacher .lemma}
**Lemma 6** (Rademacher generalization bound). *[@mohri2012foundations] Let $\mathcal{X}$ be a vector space and $\mathcal{D}$ a distribution over $\mathcal{X}$. Let $\mathcal{F}$ be a class of functions $f:\mathcal{X}\to[a,b]$. Draw samples $\mathcal{S} = \{\bm{x}_i\}_{i=1}^N \overset{\mathrm{i.i.d} }{\sim} \mathcal{D}^n$, and define the empirical Rademacher complexity $$\begin{equation}
  \label{eq:empirical_rademacher}
  \hat{\mathfrak{R}}_{S}(\mathcal{F})
  \;=\;
  \mathbb{E}_{\sigma}\Biggl[\sup_{f\in\mathcal{F}}\frac{1}{N}\sum_{i=1}^N  t_i\,f(\bm{x}_i)\Biggr],
\end{equation}$$ where $t_i\in\{-1,1\}$ are independent Rademacher variables. Then for any $\delta\in(0,1)$, with probability at least $1-\delta$, $$\sup_{f\in \mathcal{F}}\left|\mathbb{E}_{\bm{x}\sim\mathcal{D}}[\,f(\bm{x})\,]
  \;-\;
  \frac{1}{N}\sum_{i=1}^N f(\bm{x}_i) \right|
  \;\le\;
  2\,\hat{\mathfrak{R}}_{S}(\mathcal{F})
  \;+\;
  3\,(b - a)\,\sqrt{\frac{\ln(2/\delta)}{2N}}.$$*
:::

::: {#lem:dudley .lemma}
**Lemma 7** (Dudley's entropy‐integral bound). *[@bartlett2017spectrally Lemma 8.5] Let $\mathcal{X}$ be a vector space and $\mathcal{F}$ a class of functions $f:\mathcal{X}\to\mathbb{R}$. For a sample $\mathcal{S} = \{\bm{x}_i\}_{i=1}^N$, define $$\|f\|_{L_2(\mathcal{S} )}
  \;=\;
  \left(\tfrac{1}{N}\sum_{i=1}^N f(\bm{x}_i)^2\right)^{1/2},
  \quad
  B_{\mathcal{F}}
  \;=\;
  \sup_{f\in\mathcal{F}}\|f\|_{L_2(\mathcal{S} )}.$$ Then the empirical Rademacher complexity of $\mathcal{F}$ [\[eq:empirical_rademacher\]](#eq:empirical_rademacher){reference-type="eqref" reference="eq:empirical_rademacher"} satisfies $$\hat{\mathfrak{R}}_{\mathcal{S} }(\mathcal{F})
  \;\le\;
  \inf_{\alpha>0}
  \Biggl[
  4\,\alpha
  \;+\;
  \frac{12}{\sqrt{N}}
  \int_{\alpha}^{B_{\mathcal{F}}}
  \sqrt{\ln\mathcal{N}\bigl(\mathcal{F},\varepsilon,L_2(\mathcal{S} )\bigr)}\;d\varepsilon
  \Biggr],$$ where $\mathcal{N}(\mathcal{F},\varepsilon,L_2(\mathcal{S} ))$ is the covering number of $\mathcal{F}$ under the $\|\cdot\|_{L_2(\mathcal{S} )}$ metric.*
:::

::: {#lem:covering-FXi .lemma}
**Lemma 8** (Covering‐number bound for $\mathcal{F}_{\bm \Xi}$). *[@hieu2025generalization Proposition 4] Let $\mathcal{F}_{\bm{\Xi}}$ be the class of $L$‐layer neural networks with bounded weights and Lipschitz activation functions, in [\[eq:functionclass\]](#eq:functionclass){reference-type="eqref" reference="eq:functionclass"}. Let $$\mathcal{S} = \{\bm{x}_i\}_{i=1}^N \subset \mathcal{X}$$ be any fixed dataset. Define $$\begin{aligned}
    &\overline{ \mathcal{R} }_{\Xi} ^{2/3}
        = \sum_{l=1}^L \bigl(a_l\,B_{l-1}\,\rho_{l+}\bigr)^{2/3},\quad \\
         &\rho_{l+} = \rho_l \;\prod_{m=l+1}^L \bigl(\rho_m\,s_m\bigr),\\
        &B_l
        = \sup_{\bm{x}\in \mathcal{X}}\,\sup_{\bm{\Theta}\in \bm{\Xi} }\bigl\|f_{\bm{\Theta}}^{1\to l}(\bm{x})\bigr\|_2,
    \end{aligned}$$ where $f_{\bm{\Theta}}^{1\to l}(\bm{x})$ is the output of the first $l$ layers of the network. Then for every $\varepsilon>0$, $$\log\mathcal{N}\!\bigl(\mathcal{F}_{\bm{\Xi}},\varepsilon,L_{\infty,2}(S)\bigr)
  \;\le\;
  \frac{64\,\overline{ \mathcal{R} }_{\Xi}^2}{\varepsilon^2}
  \;\log\!\Bigl(\bigl(\tfrac{11\,\overline{ \mathcal{R} }_{\Xi}}{\varepsilon}+7\bigr)\,N\,W\Bigr).$$*
:::

### $\ell_2$-Lipschitzness of KL divergence

We introduce the Lipschitzness of the KL divergence term $D(f_{\bm\Theta};\bm x)$ in [\[eq:KLloss\]](#eq:KLloss){reference-type="eqref" reference="eq:KLloss"} with respect to the $\ell_2$ norm when we assume the output of the encoder is bounded by $B_L$. The Lipschitzness of the KL divergence term is crucial for the proof of Theorem [4](#thm:genbnd){reference-type="ref" reference="thm:genbnd"}. We will specify the bound $B_L$ later in the proof of Theorem [4](#thm:genbnd){reference-type="ref" reference="thm:genbnd"}.

::: {#prop:lip_KL .lemma}
**Lemma 9** ($\ell_2$--Lipschitzness of the KL term). *For some constant $B_L > 0$, define $$V_{B_L}  \coloneqq \left\{[\bm{\mu} ;\bm{\varepsilon} ]\in\mathbb{R}^{2d}: \bm{\mu} \in\mathbb{R}^d,\bm{\varepsilon}\in\mathbb{R}^d, \|[\bm{\mu} ;\bm{\varepsilon} ]\|_2 \leq  B_L.\right\},$$ and let $$\begin{equation}
  \label{eq:KLform}
  D([\bm{\mu};\bm{\varepsilon}])
  :=\frac{1}{2d}\sum_{j=1}^d
  \Bigl(\bm{\mu}_j^2 + \exp(\bm{\varepsilon}_j) - 1 - \bm{\varepsilon}_j\Bigr).
\end{equation}$$ Then, for any two pairs $[\bm{\mu};\bm{\varepsilon}],[\bm{\mu}';\bm{\varepsilon}'] \in V_{B_L}$, it holds that $$\bigl|D([\bm{\mu} ;\bm{\varepsilon} ])-D([\bm{\mu}' ;\bm{\varepsilon}'])\bigr|
  \;\le\;
  \;\frac{\eta}{\sqrt{d}}\cdot\bigl\|([\bm{\mu};\bm{\varepsilon} ])-([\bm{\mu}' ;\bm{\varepsilon}' ])\bigr\|_2,$$ where $\eta = B_L + (1+\exp(B_L))/2.$*
:::

::: proof
*Proof.* Compute the gradient of $D([\bm{\mu};\bm{\varepsilon}])$ in [\[eq:KLform\]](#eq:KLform){reference-type="eqref" reference="eq:KLform"} as $$\begin{equation}
  \label{eq:partialder}
  \nabla D\bigl([\bm{\mu}; \bm{\varepsilon}]\bigr)
  =
  \frac{1}{d}
  \renewcommand{\arraystretch}{1.5}
  \begin{bmatrix}
  \bm{\mu} \\ 
  \frac{\exp(\bm{\varepsilon}) - \bm{1}_d}{2}
  \end{bmatrix}
  \renewcommand{\arraystretch}{1}
\end{equation}$$ where $\bm{1}_d\coloneqq [1,\dots,1]\in\mathbb{R}^d$.

Fix $[\bm{\mu};\bm{\varepsilon}],[\bm{\mu}';\bm{\varepsilon}'] \in V_{B_L}$ arbitrarily. Then, applying Cauchy--Schwarz inequality with the convexity of $D([\bm{\mu};\bm{\varepsilon}])$ yields $$\bigl|D([\bm{\mu};\bm{\varepsilon} ])-D([\bm{\mu} ';\bm{\varepsilon} '])\bigr|
  \;\le\;
  \nabla D([\bm{\mu};\bm{\varepsilon}])^\top([\bm{\mu} ;\bm{\varepsilon} ]-[\bm{\mu} ';\bm{\varepsilon} '])
  \;\le\;
  \bigl\|\nabla D([\bm{\mu};\bm{\varepsilon}])\bigr\|_2\;\|[\bm{\mu};\bm{\varepsilon} ]-[\bm{\mu} ';\bm{\varepsilon} ']\|_2,$$

Hence, it suffices to show $\bigl\|\nabla D([\bm{\mu}; \bm{\varepsilon}])\bigr\|_2 \leq  \eta /\sqrt{d}$ for all $[\bm{\mu}; \bm{\varepsilon}] \in V_{B_L}$. By [\[eq:partialder\]](#eq:partialder){reference-type="eqref" reference="eq:partialder"}, triangle inequality gives $$\begin{aligned}
    \sup_{[\bm{\mu}; \bm{\varepsilon}] \in V_{B_L}}\|\nabla D([\bm{\mu};\bm{\varepsilon}])\|_2 &\leq \frac{1}{d} \sup_{[\bm{\mu}; \bm{\varepsilon}] \in V_{B_L}}\|\bm{\mu}\|_2 + \frac{1}{2d} \sup_{[\bm{\mu}; \bm{\varepsilon}] \in V_{B_L}}\|\exp(\bm{\varepsilon})\|_2 + \frac{1}{2\sqrt{d}}\\ 
    &{\leq} \frac{1}{d} B_L + \frac{1}{2\sqrt{d}}\exp({B_L}) + \frac{1}{2\sqrt{d}}
    % &\overset{\mathrm{(b)}}{\leq} \frac{1}{d}B_L + \frac{1}{2\sqrt{d}}\exp({B_L})  + \frac{1}{2\sqrt{d}},
  \end{aligned}$$ where in the last inequality, we use $\|\bm{\mu}\|_2 \leq \|[\bm{\mu} ;\bm{\varepsilon}  ]\|_2 \leq  B_L$ and $$\|\exp(\bm{\varepsilon})\|_2 = \sqrt{ \sum_{j=1}^d\exp(2\bm{\varepsilon}_j)} \leq \sqrt{d} \exp(B_L).$$ As $\frac{1}{\sqrt{d}}\leq 1$ for any $d \geq 1$, we have shown $\bigl\|\nabla D([\bm{\mu}; \bm{\varepsilon}])\bigr\|_2 \leq  \eta /\sqrt{d}$. ◻
:::

### Proof of [4](#thm:genbnd){reference-type="ref+Label" reference="thm:genbnd"} {#proof-of-thmgenbnd}

Note that $\rho_{\ell}$ bounded spectral norm of weight matrices and $\sigma_{\ell}$-Lipschitzness of activation function for each layer $\ell\in[L]$ yield $$\begin{equation}
\label{eq:bndBL}
  \sup_{f_{\bm{\Theta}}\in \mathcal{F}_{\bm{\Xi}}, \bm{x}\in \mathcal{X}} \|f_{\bm{\Theta}}(\bm{x})\|_2 \leq  \underbrace{\sup_{\bm{x}\in \mathcal{X}}\left\|\bm{x}\right\|_2 \cdot \prod_{\ell=1}^L \rho_{\ell} \sigma_{\ell}}_{B_L}.
\end{equation}$$ Therefore, for all $\bm{x}\in \mathcal{X}$, $\left\|[{\bm \mu}_{\bm{\Theta}}(\bm{x}); {\bm{\varepsilon}}_{\bm{\Theta}}(\bm{x})]\right\|_2 \leq  B_L$ holds for all $\bm{\Theta}\in \mathcal{F}_{\bm{\Xi}}$. Then, we can apply the $\ell_2$ Lipschitzness of KL divergence in [9](#prop:lip_KL){reference-type="ref+Label" reference="prop:lip_KL"} to bound the KL divergence term $D(f_{\bm{\Theta}};\bm{x})$ in [\[eq:KLloss\]](#eq:KLloss){reference-type="eqref" reference="eq:KLloss"} as $$\begin{equation}
\label{eq:LipschitzKL1}
\begin{aligned}
\left|D(f_{\bm{\Theta}}; \bm{x}_i)-D(f_{\bm{\Theta}'}; \bm{x}_i)\right| &\leq \eta\cdot\left\|f_{\bm{\Theta}}(\bm{x}_i)-f_{\bm{\Theta}'}(\bm{x}_i)\right\|_2 \\ 
&\leq \eta \cdot \max_{i\in[N]} \left\|f_{\bm{\Theta}}(\bm{x}_i)-f_{\bm{\Theta}'}(\bm{x}_i)\right\|_2 \\
& = \frac{\eta}{\sqrt{d}}\cdot \|f_{\bm{\Theta}}-f_{\bm{\Theta}'}\|_{L_{\infty,2 (\mathcal{S})}}, \quad \quad\forall f_{\bm{\Theta}}, f_{\bm{\Theta}'}\in \mathcal{F}_{\bm{\Xi}}.
\end{aligned}
\end{equation}$$ Since the bound in [\[eq:LipschitzKL1\]](#eq:LipschitzKL1){reference-type="eqref" reference="eq:LipschitzKL1"} yields $$\begin{aligned}
  \left\|D(f_{\bm{\Theta}})-D(f_{\bm{\Theta}'})\right\|_{L_2(\mathcal{S})}&=\sqrt{\frac{1}{N}\sum_{i=1}^N \left(D(f_{\bm{\Theta}; \bm{x}_i})-D(f_{\bm{\Theta}'; \bm{x}_i})\right)^2}\\ 
  &\leq \frac{\eta}{\sqrt{d}}\cdot \|f_{\bm{\Theta}}-f_{\bm{\Theta}'}\|_{L_{\infty,2 (\mathcal{S})}},\quad\forall f_{\bm{\Theta}}, f_{\bm{\Theta}'}\in \mathcal{F}_{\bm{\Xi}},
\end{aligned}$$ we have $$\log\mathcal{N}\left(\Delta, \epsilon, {L_2}(\mathcal{S})\right) \leq  \log \mathcal{N}\left(\mathcal{F}_{\bm{\Xi}},\frac{\epsilon \sqrt{d}}{\eta}, L_{\infty,2}(\mathcal{S})\right),$$ where we define $\Delta$ as the class of KL divergence terms $D(f_{\bm{\Theta}})$ in [\[eq:KLloss\]](#eq:KLloss){reference-type="eqref" reference="eq:KLloss"}, i.e., $$\Delta := \left\{D(f_{\bm{\Theta}}): f_{\bm{\Theta}}\in \mathcal{F}_{\bm{\Xi}}\right\}.$$ Now, we can apply the covering number bound in [8](#lem:covering-FXi){reference-type="ref+Label" reference="lem:covering-FXi"} to bound the covering number $\mathcal{N}\left(\Delta, \epsilon, {L_2}(\mathcal{S})\right)$ as $$\begin{equation}
  \label{eq:coveringbnd3}
  \begin{aligned}
    \mathcal{N}\!\bigl(\Delta,\epsilon,L_2(\mathcal{S})\bigr)\;\le\;
\frac{64\,\eta^2\,\overline{\mathcal{R}}_{\bm{\Xi}}^2}{\epsilon^{2}d}
\;\log\!\Bigl(\bigl(\tfrac{11\eta\overline{\mathcal{R}}_{\bm{\Xi}}}{\epsilon \sqrt{d}}+7\bigr)\,N\,W\Bigr).
    % &\leq \frac{64\eta^2 {{\overline{\mathcal{R}}}_{\mathcal{A}}}^2}{\epsilon^2} \log\left(\left({\frac{11\overline{\mathcal{R}}}_{\mathcal{A}}}{\epsilon}+7\right)2nNW\right), 
  \end{aligned}
\end{equation}$$

Define $B_{\Delta}:=\sup_{D(f_{\bm{\Theta}})\in \Delta} \|D(f_{\bm{\Theta}})\|_{L_2(\mathcal{S})}$. We further bound the empirical Rademacher complexity $\hat{R}_{\mathcal{S}}(\Delta)$ by applying the Dudley's entropy integral [7](#lem:dudley){reference-type="ref+Label" reference="lem:dudley"} with the bound on covering number [\[eq:coveringbnd3\]](#eq:coveringbnd3){reference-type="eqref" reference="eq:coveringbnd3"} by the choosing $\alpha =\frac{1}{\sqrt{N}}$: $$\begin{equation}
\label{eq:bndRad1}
\begin{aligned}
  \widehat{\mathfrak{R}}_{\mathcal{S}}(\Delta)&\leq  \frac{4}{\sqrt{N}} + \frac{12}{\sqrt{N}} \int_{\frac{1}{\sqrt{N}}}^{B_{\Delta} } \sqrt{\log\left( \mathcal{N}\!\bigl(\mathcal{D},\epsilon,L_2(\mathcal{S})\bigr)\right)}d \epsilon \\ 
  & \leq \frac{4}{\sqrt{N}} + \frac{96 \eta \overline{\mathcal{R}}_{\bm{\Xi}}}{\sqrt{Nd}} \sqrt{\log\left(\left(\frac{11 \eta \overline{\mathcal{R}}_{\bm{\Xi}}\sqrt{N}}{\sqrt{d}}+7\right)NW\right)} \int_{\frac{1}{\sqrt{N}}}^{B_{\Delta}} \frac{1}{\epsilon} d \epsilon \\ 
  &\overset{\eqref{eq:coveringbnd3}}{\leq} \frac{4}{\sqrt{N}} + \frac{96 \eta \overline{\mathcal{R}}_{\bm{\Xi}}}{\sqrt{Nd}} \sqrt{\log\left(\left(\frac{11 \eta \overline{\mathcal{R}}_{\bm{\Xi}}\sqrt{N}}{\sqrt{d}}+7\right)NW\right)} \log (\sqrt{N}B_{\Delta}).
\end{aligned}
\end{equation}$$

Now, we apply the Rademacher generalization bound [6](#lem:rademacher){reference-type="ref+Label" reference="lem:rademacher"} with the bound on the empirical Rademacher complexity in [\[eq:bndRad1\]](#eq:bndRad1){reference-type="eqref" reference="eq:bndRad1"} by letting $M=\sup_{\bm{\Theta}\in \bm{\Xi}, \bm{x}\in \mathcal{X}} \left|D(f_{\bm{\Theta}}; \bm{x})\right|$. Hence, it holds that $$\begin{equation}
\label{eq:Radebnd1}
\begin{aligned}
&\sup_{f_{\bm{\Theta}}\in\mathcal{F}_{\bm{\Xi}}}\left|\mathop{\mathrm{\mathbb{E}}}_{\bm{x}\sim\mathcal{D}} \left[D(f_{\bm{\Theta}}; \bm{x})\right] - \frac{1}{N} \sum_{i=1}^N  D(f_{\bm{\Theta}}(\bm{x}_i))\right|\\ 
&\quad\leq 2{\widehat{\mathfrak{R}}_{\mathcal{S}}(\Delta)} + 3M \sqrt{\frac{\log{2}/{\delta}}{2N}}  \\ 
&\quad\leq \frac{8}{\sqrt{N}} + \frac{192 \eta \overline{\mathcal{R}}_{\bm{\Xi}}}{\sqrt{Nd}} \sqrt{\log\left(\left(\frac{11 \eta \overline{\mathcal{R}}_{\bm{\Xi}}\sqrt{N}}{\sqrt{d}}+7\right)NW\right)} \log (\sqrt{N}B_{\Delta})  + 3M \sqrt{\frac{\log{2}/{\delta}}{2N}},
\end{aligned}
\end{equation}$$ with probability at least $1-\delta$.

Suppose that the remainder of the proof is conditioned on the event [\[eq:Radebnd1\]](#eq:Radebnd1){reference-type="eqref" reference="eq:Radebnd1"}. We will now show the upper bounds on $B_{\Delta}$, $M$, and $\mathop{\mathrm{\overline{\mathcal{R}}_{\bm{\Xi}}}}$ in [\[eq:Radebnd1\]](#eq:Radebnd1){reference-type="eqref" reference="eq:Radebnd1"}.

#### Bound on $B_{{\Delta}}$:

Using $\ell_{2}$ lipschitzness of $D(f_{\bm{\Theta}}; \bm{x}_i)$ in [9](#prop:lip_KL){reference-type="ref+Label" reference="prop:lip_KL"} gives $$\left|D(f_{\bm{\Theta}};\bm{x}_i)\right| \leq  \eta \|f_{\bm{\Theta}}(\bm{x}_i)\|_2, \quad \forall f_{\bm{\Theta}}\in \mathcal{F}_{\bm{\Xi}},\quad \forall\bm{x}_i \in \mathcal{S}.$$ This directly gives us $$\begin{equation}
\label{eq:BDbnd}
\begin{aligned}
B_{\Delta} = \sup_{D(f_{\bm{\Theta}})\in \Delta}\sqrt{\frac{1}{N}\sum_{i=1}^N \left(D(f_{\bm{\Theta}} ; \bm{x}_i)\right)^2}&\leq \sqrt{\frac{\eta^2}{N}\sum_{i=1}^N  \sup_{\bm{\Theta}\in \bm{\Xi}}\|f_{\bm{\Theta}}(\bm{x}_i)\|_2^2 }  \\ 
&\leq \eta  B_L,
\end{aligned}
\end{equation}$$ where $B_L$ is defined in [\[eq:bndBL\]](#eq:bndBL){reference-type="eqref" reference="eq:bndBL"}.

#### Bound on $M$:

Upper bound on $M$ can be obtained in a similar way as the bound on $B_{\Delta}$ in [\[eq:BDbnd\]](#eq:BDbnd){reference-type="eqref" reference="eq:BDbnd"}: $$\begin{equation}
  \label{eq:Mbnd}
  M=\sup_{\bm{\Theta}\in \bm{\Xi}, \bm{x}\in \mathcal{X}} |D(f_{\bm{\Theta}}; \bm{x})| \leq \eta \sup_{\bm{\Theta}\in \bm{\Xi}, \bm{x}\in \mathcal{X}}\|f_{\bm{\Theta}}(\bm{x})\|_2 \leq \eta B_L.
\end{equation}$$

#### Bound on $\mathop{\mathrm{\overline{\mathcal{R}}_{\bm{\Xi}}}}$:

In the proof of [@hieu2025generalization Theorem 1], Hieu et al. show an upper bound on $\mathop{\mathrm{\overline{\mathcal{R}}_{\bm{\Xi}}}}$ which is $$\begin{equation}
\label{eq:bndRA}
  \mathop{\mathrm{\overline{\mathcal{R}}_{\bm{\Xi}}}}\leq \sup_{\bm{x} \in \mathcal{X}} \|\bm{x}\|_2 \prod_{\ell=1}^L \sigma_{\ell}\rho_\ell \left(\sum_{\ell'=1}^L (a_{\ell'}/ \rho_{\ell'})^{2/3} \right)^{3/2}.
\end{equation}$$

For the sake of simplicity, we denote $B_x=\sup_{\bm{x} \in \mathcal{X}} \|\bm{x}\|_2$. Putting the bounds [\[eq:BDbnd\]](#eq:BDbnd){reference-type="eqref" reference="eq:BDbnd"}, [\[eq:Mbnd\]](#eq:Mbnd){reference-type="eqref" reference="eq:Mbnd"}, and [\[eq:bndRA\]](#eq:bndRA){reference-type="eqref" reference="eq:bndRA"} into [\[eq:Radebnd1\]](#eq:Radebnd1){reference-type="eqref" reference="eq:Radebnd1"} provides

$$\begin{equation*}
  \label{eq:Radebnd2}
\begin{aligned}
  &\sup_{f_{\bm{\Theta}}\in\mathcal{F}_{\bm{\Xi}}}\left|\mathop{\mathrm{\mathbb{E}}}_{\bm{x}\sim\mathcal{D}} \left[D(f_{\bm{\Theta}}; \bm{x})\right] - \frac{1}{N} \sum_{i=1}^N  D(f_{\bm{\Theta}}(\bm{x}_i))\right|\\ 
  &\leq  \tilde{\mathcal{O}}\left(\frac{\eta}{\sqrt{N}} \sqrt{\log (W)}B_x \prod_{\ell=1}^L \sigma_{\ell} \rho_{\ell} \left(\sum_{\ell'=1}^L (a_{\ell'}/ \rho_{\ell'})^{3/2} \right)^{{3}/{2}} + \eta B_x \prod_{\ell=1}^L(\rho_{\ell} \sigma_{\ell})\sqrt{\frac{\log(1/\delta)}{N}} \right) \\ 
  &\leq \tilde{\mathcal{O}}\left(\frac{\eta B_x\prod_{\ell=1}^L \sigma_{\ell} \rho_{\ell}}{\sqrt{N}}\left[\sqrt{\log (W)} \left(\sum_{\ell'=1}^L (a_{\ell'}/ \rho_{\ell'})^{3/2} \right)^{{3}/{2}} \vee \sqrt{\log(1/\delta)} \right]\right).
\end{aligned}
\end{equation*}$$

# Discussion on the approximation in Section [3.1](#subsec:elbo){reference-type="ref" reference="subsec:elbo"} {#discussion-on-the-approximation-in-section-subsecelbo}

## Discussion on [\[eq:dist_x_z\]](#eq:dist_x_z){reference-type="eqref" reference="eq:dist_x_z"} {#app:recon_approx}

The key step in our decoder-free ELBO maximization is the approximation $$\begin{align}
    \mathbb{E}_{q_\theta({\boldsymbol z}|{\boldsymbol x})}\bigl[\log p({\boldsymbol x}|{\boldsymbol z})\bigr] 
    & \approx \mathbb{E}_{q_\theta({\boldsymbol z}|{\boldsymbol x})q_\theta({\boldsymbol z}'|{\boldsymbol x})}\bigl[\log p({\boldsymbol z}'| {\boldsymbol z})\bigr]
\end{align}$$

#### Lower-bound view.

As shown in Lemma [1](#lem:recon_app_jensen){reference-type="ref" reference="lem:recon_app_jensen"}, this approximation admits a lower bound up to an additive constant independent of ${\boldsymbol z}$: $$\begin{align}
    \mathbb{E}_{q_\theta({\boldsymbol z}|{\boldsymbol x})} [ \log p({\boldsymbol x}|{\boldsymbol z}) ] \geq \mathbb{E}_{q_\theta({\boldsymbol z}|{\boldsymbol x})q_\theta({\boldsymbol z}’|{\boldsymbol x})}[\log p({\boldsymbol z}’|{\boldsymbol z})] + {\rm const}.
\end{align}$$ Consequently, maximizing the right-hand side with respect to $\theta$ implicitly maximizes the reconstruction term $\mathbb{E}_{q_\theta(\mathbf{z}\mid\mathbf{x})}\!\big[\log p(\mathbf{x}\mid\mathbf{z})\big]$, which is the objective of ELBO maximization. Moreover, using [\[eq:recon_infoNCE\]](#eq:recon_infoNCE){reference-type="eqref" reference="eq:recon_infoNCE"} (see Section [3.1](#subsec:elbo){reference-type="ref" reference="subsec:elbo"}), the surrogate is negatively related to InfoNCE: $$\begin{align}
    \mathbb{E}_{q_\theta(\mathbf{z}\mid\mathbf{x})}\!\big[\log p(\mathbf{x}\mid\mathbf{z})\big]
    & \approx - I_{\mathrm{NCE}}(\mathbf{x};\mathbf{x}'),
\end{align}$$ so minimizing the InfoNCE loss increases the reconstruction term.

#### Change-of-variables view.

Another perspective on the reconstruction approximation [\[eq:dist_x_z\]](#eq:dist_x_z){reference-type="eqref" reference="eq:dist_x_z"} comes from a change of variables. Let $g$ be an invertible, differentiable mapping such that ${\boldsymbol x}= g({\boldsymbol z}')$. Then, by the change-of-variables formula, $$\begin{align}
    p({\boldsymbol x}\mid {\boldsymbol z})
    = p({\boldsymbol z}' \mid {\boldsymbol z})\,\bigl|{\hbox{det}}J_{g^{-1}}({\boldsymbol x})\bigr|
    = p({\boldsymbol z}' \mid {\boldsymbol z})\,\bigl|{\hbox{det}}J_{g}({\boldsymbol z}')\bigr|^{-1},
\end{align}$$ where $J_{g}$ and $J_{g^{-1}}$ denote the Jacobians of $g$ and $g^{-1}$, respectively, and ${\boldsymbol z}' = g^{-1}({\boldsymbol x})$. Taking logarithms yields $$\begin{align}
\label{eq:recon_app_cov}
\log p({\boldsymbol x}\mid {\boldsymbol z})
= \log p({\boldsymbol z}' \mid {\boldsymbol z}) + \log\bigl|{\hbox{det}}J_{g^{-1}}({\boldsymbol x})\bigr|
= \log p({\boldsymbol z}' \mid {\boldsymbol z}) - \log\bigl|{\hbox{det}}J_{g}({\boldsymbol z}')\bigr|,
\end{align}$$ where the second term depends only on ${\boldsymbol x}$ (equivalently, on ${\boldsymbol z}'$) and is independent of ${\boldsymbol z}$.

**Sufficient condition (tightness).** If, in addition to invertibility, $g$ is *volume-preserving*, i.e., $\bigl|{\hbox{det}}J_{g^{-1}}({\boldsymbol x})\bigr| \equiv 1$ (equivalently, $\bigl|{\hbox{det}}J_{g}({\boldsymbol z}')\bigr|\equiv 1$) on the data manifold, then the additive term in [\[eq:recon_app_cov\]](#eq:recon_app_cov){reference-type="eqref" reference="eq:recon_app_cov"} vanishes and we obtain the tight equality $\log p({\boldsymbol x}\mid {\boldsymbol z})=\log p({\boldsymbol z}' \mid {\boldsymbol z})$. More generally, when $\bigl|{\hbox{det}}J_{g^{-1}}({\boldsymbol x})\bigr|$ is approximately constant over the data manifold, the additive term acts as (approximately) a constant shift independent of ${\boldsymbol z}$, yielding a tight surrogate for optimization.

This assumption is plausible in practice under the commonly observed *dimension-collapse* phenomenon: the embeddings ${\boldsymbol z}'$ have effective rank (intrinsic dimension) much smaller than the ambient embedding dimension yet retain nearly all task-relevant information about the features ${\boldsymbol x}$. When the feature and embedding manifolds have (approximately) the same intrinsic dimension and $g$ behaves near-isometrically between them, the Jacobian determinant varies weakly, making the surrogate in [\[eq:recon_app_cov\]](#eq:recon_app_cov){reference-type="eqref" reference="eq:recon_app_cov"} tight in practice.

## Gaussian KL Surrogate for Projected-Normal KL {#app:kl_surrogate}

We study the tightness of the bound in [\[eq:kl_bound\]](#eq:kl_bound){reference-type="eqref" reference="eq:kl_bound"}, repeated here: $$\begin{align}
D\bigl(\mathcal{N}(\mu,K)\,\|\,\mathcal{N}(0,I_d)\bigr)
\;\ge\;
D\bigl(\mathcal{P}\mathcal{N}(\mu,K)\,\|\,\mathrm{Unif}(\mathcal{S}^{d-1})\bigr).
\end{align}$$ Before analyzing tightness, we note several practical benefits of using the Gaussian KL as a surrogate for the projected-normal KL:

- **Closed form.** It is trivial to implement and numerically stable.

- **Aligned optima.** The Gaussian KL and projected-normal KL share the same minimizer (e.g., at $\mu=0$ and $K=I_d$), so optimizing the surrogate steers the model toward the same optimum.

- **Efficiency.** Unlike Monte Carlo or $k$-NN estimators needed for the projected-normal KL, the Gaussian KL requires no sampling.

Moreover, the KL term acts only as a regularizer, whereas InfoNCE directly drives semantic similarity; thus modest approximation error in the KL has limited effect on downstream performance.

We assess tightness by comparing the closed-form Gaussian KL with an estimated projected-normal KL using a divergence estimator [@what2009divergence] in two settings: synthetic data and CIFAR-10 under VCL training.

#### KL gap on synthetic data.

We approximate $D\bigl(\mathcal{P}\mathcal{N}(\mu,K)\,\|\,\mathrm{Unif}(\mathcal{S}^{d-1})\bigr)$ numerically using $10^5$ samples in dimension $d=128$ for random $(\mu,K)$ draws, with $\mu \sim \mathcal{N}(0,I_d)$ and $$\begin{align}
    K = \tfrac{1}{d} A A^\top + 0.1\,I_d, \quad A_{ij}\sim \mathcal{N}(0,0.5)\ \ \forall i,j.
\end{align}$$ We employ the $k$-nearest-neighbor divergence estimator [@what2009divergence] with $k=1$, compute both the Gaussian KL (analytically) and the projected-normal KL (using the estimator) on the same samples, and repeat over 20 random trials to reduce variance.

Table [2](#tab:kl_gap_synth){reference-type="ref" reference="tab:kl_gap_synth"} reports the gap between the two KLs on synthetic data: the average absolute gap is approximately $9.49$ (about a $10\%$ relative difference). Thus, the Gaussian KL surrogate closely tracks the projected-normal KL while retaining the practical advantages noted above.

::: {#tab:kl_gap_synth}
           G-KL    PN-KL   Gap (G-KL$-$PN-KL)   Ratio (G-KL$/$PN-KL)
  ------ -------- ------- -------------------- ----------------------
  mean    106.86   97.37          9.49                  0.91
  std      9.56    7.63            \-                    \-

  : Gaussian KL (G-KL) vs. projected normal KL (PN-KL) on synthetic data.
:::

#### KL gap on CIFAR-10.

Beyond the synthetic study, we measure the gap during VCL training on CIFAR-10 using the same experimental settings (Appendix [9.1](#app:exp_detail){reference-type="ref" reference="app:exp_detail"}); results are shown in Figure [5](#fig:kl_comp_cifar10){reference-type="ref" reference="fig:kl_comp_cifar10"}. After only a few epochs, the Gaussian KL and the projected-normal KL closely track each other. This indicates that minimizing the Gaussian-KL surrogate effectively minimizes the projected-normal KL---the quantity we aim to reduce---while retaining the practical advantages of the surrogate.

<figure id="fig:kl_comp_cifar10" data-latex-placement="h">

<figcaption>Tracking Gaussian KL (G-KL) and projected normal KL (PN-KL) during VCL training on CIFAR-10. (a) G-KL vs. PN-KL; (b) Absolute gap, <span class="math inline">|G-KL − PN-KL|</span>. This shows that minimizing Gaussian KL leads to minimizing projected normal KL.</figcaption>
</figure>

# Experiments {#app:experiments}

## Training Details and Hyperparameters {#app:exp_detail}

#### Datasets and preprocessing.

Experiments are conducted on CIFAR-10 [@krizhevsky2009learning], CIFAR-10C [@hendrycks2019robustness], CIFAR-10H [@peterson2019human], CIFAR-100 [@krizhevsky2009learning], STL-10 [@coates2011analysis], Tiny-ImageNet [@le2015tiny], and Caltech-256 [@griffin2007caltech]. Following SimCLR, we sample two views per image via random resized crop (image size $32\!\times\!32$ and scale $[0.2,1.0]$), horizontal flip ($p{=}0.5$), color jitter (brightness/contrast/saturation/hue $=0.4$, applied with $p{=}0.8$), Gaussian blur (kernel size $9$), and random grayscale ($p{=}0.2$). Inputs are normalized with dataset-specific means/standard deviations.

#### Architectures.

Encoders are ResNet-18, with embedding dimension $d{=}128$.

#### Optimization.

We use AdamW [@loshchilov2018decoupled] with base LR $10^{-2}$ (encoder and head), weight decay $10^{-4}$, batch size $B{=}512$, and $T{=}500$ epochs for pretraining and $T=100$ for training linear classifier. Temperature for InfoNCE loss is $\tau{=}0.07$. We set $m{=}1$ posterior samples per view for VSimCLR and VSupCon by default (ablation in Table [\[tab:classification_aux\]](#tab:classification_aux){reference-type="ref" reference="tab:classification_aux"}). No momentum encoder or queue is used; all negatives are in-batch. For training stability, we clip the posterior log-variance ($\log\boldsymbol{\sigma^2}$) to $[-5,5]$ to bound variances, and clip gradient global norm at $1.0$.

## Additional Results on Dimension Collapse {#app:dim_collapse}

In addition to the singular spectrum of VCL embeddings on CIFAR-10 and CIFAR-100 in Figure [\[fig:emb_dim\]](#fig:emb_dim){reference-type="ref" reference="fig:emb_dim"}, Figure [6](#fig:additional_dim_collapse){reference-type="ref" reference="fig:additional_dim_collapse"} reports results on Caltech-256 and Tiny-ImageNet. In both datasets, VCL mitigates the dimension-collapse phenomenon commonly observed in contrastive learning.

<figure id="fig:additional_dim_collapse" data-latex-placement="h">

<figcaption>Singular‐value spectrum of the embedding covariance on Cartech-256 and Tiny-ImageNet. VSimCLR mitigates dimensional collapse on both datasets. </figcaption>
</figure>

::: {#tab:logdet_by_class}
    Index Class          $\log{\hbox{det}}(K)$
  ------- ------------ -----------------------
        0 airplane                    -182.207
        1 automobile                  -181.691
        2 bird                        -183.713
        3 cat                         -191.317
        4 deer                        -184.969
        5 dog                         -185.432
        6 frog                        -182.125
        7 horse                       -179.331
        8 ship                        -185.991
        9 truck                       -188.179

  : Log‐determinant of average posterior covariance $K$ for each CIFAR‐10 class.
:::

## Distributional Contrastive Loss {#app:DistNCE}

In addition to the contrastive loss on embeddings, it is worthwhile to contrast the posterior distributions within the VCL framework. Specifically, we aim to pull together the posteriors corresponding to different augmentations of the same input and to push apart posteriors from distinct inputs. To incorporate this into VCL, we introduce the *DistNCE* loss, a contrastive objective over posterior parameters, defined as $$\begin{align}
\label{eq:dist_nce}
D_{\mathrm{DistNCE}}(\theta)
&= -\mathbb{E}\left[\log \frac{\exp\bigl(s(\theta,\theta^+)\bigr)}
{\displaystyle\sum_{j}\exp\bigl(s(\theta,\theta_j)\bigr)}\right],
\end{align}$$ where $\theta$ denotes the posterior parameters $(\boldsymbol{\mu},K)$, $\theta^+$ is the positive‐pair parameter for the same input, and $\{\theta_j\}_{j\neq +}$ are negative‐pair parameters from other inputs. The expectation is taken over the joint distribution $p(\theta,\theta^+)\prod_{j\neq +}p(\theta_j)$.

Moreover, we increase the number of posterior samples used for the InfoNCE loss. Specifically, we draw $m$ samples $\{\mathbf{z}^{(k)}\}_{k=1}^m$ from each posterior, resulting in an $m$-fold increase in effective batch size, and compute the InfoNCE loss over this enlarged set of embeddings. The classification results are reported in Table [\[tab:classification_aux\]](#tab:classification_aux){reference-type="ref" reference="tab:classification_aux"}.

We also evaluate the performance of the asymmetric lower bound [\[eq:asym_bound\]](#eq:asym_bound){reference-type="eqref" reference="eq:asym_bound"} (denoted ASYM) in place of the symmetrized objective [\[eq:VCL_obj\]](#eq:VCL_obj){reference-type="eqref" reference="eq:VCL_obj"}. These results are also shown in Table [\[tab:classification_aux\]](#tab:classification_aux){reference-type="ref" reference="tab:classification_aux"}.

From these experiments, we did not observe any significant differences when applying DistNCE [\[eq:dist_nce\]](#eq:dist_nce){reference-type="eqref" reference="eq:dist_nce"}, using the asymmetric loss, or sampling multiple embeddings per posterior. Based on these findings, we proceed with the basic VCL variants from the main text for all subsequent experiments.

:::::: table*
::::: center
:::: small
::: sc
+------------------------------------------------------------------------------------------------------+---------------+
| **Method**                                                                                           | STL10         |
+:=====================================================================================================+:=====:+:=====:+
| 2-3                                                                                                  | Top1  | Top5  |
+------------------------------------------------------------------------------------------------------+-------+-------+
| SimCLR                                                                                               | 60.44 | 95.80 |
+------------------------------------------------------------------------------------------------------+-------+-------+
| VSimCLR ($m=1$)                                                                                      | 60.11 | 92.00 |
+------------------------------------------------------------------------------------------------------+-------+-------+
| VSimCLR ($m=4$)                                                                                      | 57.86 | 88.29 |
+------------------------------------------------------------------------------------------------------+-------+-------+
| VSimCLR ($m=16$)                                                                                     | 59.13 | 92.85 |
+------------------------------------------------------------------------------------------------------+-------+-------+
| VSimCLR ($m=64$)                                                                                     | 56.91 | 86.63 |
+------------------------------------------------------------------------------------------------------+-------+-------+
| VSimCLR with DistNCE [\[eq:dist_nce\]](#eq:dist_nce){reference-type="eqref" reference="eq:dist_nce"} | 36.54 | 80.25 |
+------------------------------------------------------------------------------------------------------+-------+-------+
| VSimCLR (asym)                                                                                       | 57.38 | 88.78 |
+------------------------------------------------------------------------------------------------------+-------+-------+
| SupCon                                                                                               | 75.88 | 75.88 |
+------------------------------------------------------------------------------------------------------+-------+-------+
| VSupCon ($m=1$)                                                                                      | 75.76 | 96.99 |
+------------------------------------------------------------------------------------------------------+-------+-------+
| VSupCon ($m=4$)                                                                                      | 74.35 | 97.14 |
+------------------------------------------------------------------------------------------------------+-------+-------+
| VSupCon ($m=16$)                                                                                     | 76.11 | 98.39 |
+------------------------------------------------------------------------------------------------------+-------+-------+
| VSupCon ($m=64$)                                                                                     | 77.96 | 98.44 |
+------------------------------------------------------------------------------------------------------+-------+-------+
:::
::::
:::::
::::::

## Effect of KL Regularizer on Classification {#app:KL_classification}

As shown in Table [\[tab:classification\]](#tab:classification){reference-type="ref" reference="tab:classification"}, VSupCon exhibits reduced classification accuracy on some datasets, whereas VSimCLR remains stable. We attribute this degradation to two factors:

1.  VSimCLR's objective coincides with the VCL objective in [\[eq:VCL_obj\]](#eq:VCL_obj){reference-type="eqref" reference="eq:VCL_obj"}, but VSupCon's does not, creating a mismatch that can impede proper ELBO maximization.

2.  SupCon optimizes embeddings directly for classification; adding a KL term can conflict with this objective.

We therefore hypothesize that weakening the KL regularizer improves VSupCon's accuracy. To test this, we scale the KL term by $\beta\in\{1, 10^{-1}, 10^{-2}, 10^{-3}\}$, $$\begin{align}
    \mathcal{L}^{\mathrm{vsup}}(\beta)
    & = \mathcal{L}^{\mathrm{sup}} + \beta D_{\mathrm{KL}}\!\bigl(q_\theta({\boldsymbol z}\mid{\boldsymbol x})\,\|\,p({\boldsymbol z})\bigr),
\end{align}$$ and evaluate the resulting embeddings. In Table [\[tab:beta_classification\]](#tab:beta_classification){reference-type="ref" reference="tab:beta_classification"}, as expected, smaller $\beta$ (i.e., a weaker KL effect) yields higher accuracy. Thus, for pure classification tasks, SupCon may not benefit from a VCL variant unless the KL weight is carefully tuned.

:::::: table*
::::: center
:::: small
::: sc
  $\beta$    **Top-1 accuracy**   **Top-5 accuracy**
  --------- -------------------- --------------------
  1                47.90                72.34
  0.1              47.24                71.90
  0.01             50.35                73.27
  0.001            51.34                73.09
:::
::::
:::::
::::::

## Implications of Distributional Embeddings {#app:implication}

Distributional (probabilistic) embeddings provide useful capabilities, including uncertainty quantification and probability-based distances between samples and classes. We analyze them along three axes: uncertainty, typicality, and out-of-distribution (OOD) behavior.

<figure id="fig:lin_reg_trace" data-latex-placement="h">
<embed src="figures/Entropy_Trace_LinRegression.pdf" style="width:45.0%" />
<figcaption>Relationship between posterior dispersion and label ambiguity. Each point plots the trace of <span class="math inline"><em>K</em></span> (<span class="math inline">${\hbox{tr}}(K)$</span>) against the entropy of human‐annotated class probabilities from CIFAR-10H <span class="citation" data-cites="peterson2019human"></span>, with a first‐order linear fit (red line). Similar to the result in Figure <a href="#fig:lin_reg" data-reference-type="ref" data-reference="fig:lin_reg">[fig:lin_reg]</a>, the dispersion is negatively correlated with label ambiguity.</figcaption>
</figure>

#### Posterior covariance vs. uncertainty.

As shown in Figure [3](#fig:examples){reference-type="ref" reference="fig:examples"}, different samples exhibit varying degrees of posterior dispersion (e.g., the log-determinant of the covariance, $\log{\hbox{det}}(K)$), which can serve as an uncertainty measure. To examine how uncertainty and posterior covariance are related, we conduct experiments on two benchmark datasets, CIFAR-10H [@peterson2019human] and CIFAR-10C [@hendrycks2019robustness]:

- **CIFAR-10H:** The test set provides soft labels [@ishida2023is; @jeong2023demystifying; @jeong2024data] aggregated from multiple annotators. Using these soft labels, we compute the per-sample label entropy as a measure of uncertainty about the underlying class.

- **CIFAR-10C:** The test set provides systematically corrupted images with multiple corruption types and severities (higher severity $=$ stronger corruption), which induces greater label ambiguity and thus higher uncertainty.

Beyond comparing $\log{\hbox{det}}(K)$ with label entropy in Figure [\[fig:lin_reg\]](#fig:lin_reg){reference-type="ref" reference="fig:lin_reg"}, we also compare the trace of $K$ (denoted $\mathrm{tr}(K)$) against label entropy in Figure [7](#fig:lin_reg_trace){reference-type="ref" reference="fig:lin_reg_trace"}. In both cases, we observe a *negative* slope under a first-order linear fit. This indicates that VSimCLR assigns **lower** posterior dispersion to inputs with greater label uncertainty. Conversely, inputs that humans classify unambiguously---i.e., prototypical class examples---exhibit posteriors with **larger** dispersion, suggesting their latent representations span a broader region of the class-specific embedding space; ambiguous or outlier inputs yield **smaller** dispersion, reflecting more concentrated latent distributions.

A similar pattern appears in Figures [4](#fig:cifar10c_vsim){reference-type="ref" reference="fig:cifar10c_vsim"} and [8](#fig:cifar10c_vsup){reference-type="ref" reference="fig:cifar10c_vsup"}, which relate $\log{\hbox{det}}(K)$ to corruption severity on CIFAR-10C. We train VSimCLR and VSupCon on CIFAR-10 and evaluate their embeddings on CIFAR-10C. Because higher severity entails stronger corruption and greater label ambiguity, these figures further support the finding that posterior covariance dispersion is negatively correlated with uncertainty. Tables [4](#tab:cifar10c_vsim_logdet){reference-type="ref" reference="tab:cifar10c_vsim_logdet"} and [5](#tab:cifar10c_vsup_logdet){reference-type="ref" reference="tab:cifar10c_vsup_logdet"} report the mean $\log{\hbox{det}}(K)$ for each corruption type and severity level.

<figure id="fig:cifar10c_vsup">

<figcaption><span class="math inline">$\log {\hbox{det}}(K)$</span> of <strong>VSupCon</strong> embeddings on CIFAR-10C <span class="citation" data-cites="hendrycks2019robustness"></span> under different corruption types and severities. “Severity” denotes the corruption level. The observed negative correlation between <span class="math inline">$\log {\hbox{det}}(K)$</span> and severity is consistent with our finding that more uncertain samples exhibit smaller posterior covariance dispersion. Exact <span class="math inline">$\log {\hbox{det}}(K)$</span> values are in Table <a href="#tab:cifar10c_vsup_logdet" data-reference-type="ref" data-reference="tab:cifar10c_vsup_logdet">5</a>.</figcaption>
</figure>

::: {#tab:cifar10c_vsim_logdet}
  **Corruption**        **Severity 1**   **Severity 2**   **Severity 3**   **Severity 4**   **Severity 5**
  ------------------- ---------------- ---------------- ---------------- ---------------- ----------------
  gaussian_noise               -187.74          -189.85          -192.23          -193.05          -193.70
  shot_noise                   -187.49          -188.11          -190.18          -190.95          -191.97
  impulse_noise                -188.25          -190.71          -192.61          -194.66          -194.82
  speckle_noise                -187.59          -188.64          -189.21          -189.93          -190.48
  defocus_blur                 -184.41          -183.84          -182.67          -187.67          -186.76
  glass_blur                   -192.35          -191.76          -192.03          -194.36          -193.98
  motion_blur                  -185.83          -187.53          -189.88          -189.78          -191.94
  zoom_blur                    -185.95          -183.85          -183.86          -183.75          -185.07
  gaussian_blur                -184.43          -182.83          -182.11          -183.47          -191.56
  snow                         -186.92          -189.86          -190.48          -193.08          -193.89
  frost                        -188.43          -190.13          -192.08          -192.16          -193.85
  fog                          -185.61          -187.61          -189.65          -193.37          -204.82
  brightness                   -184.89          -185.43          -186.17          -187.16          -189.70
  saturate                     -186.40          -191.14          -185.02          -186.36          -187.87
  spatter                      -186.32          -188.43          -191.12          -188.88          -191.03
  contrast                     -185.67          -188.03          -189.84          -192.59          -200.25
  elastic_transform            -185.66          -185.12          -184.95          -189.66          -195.31
  pixelate                     -185.10          -186.44          -187.62          -188.58          -189.46
  jpeg_compression             -182.94          -183.30          -183.73          -184.38          -185.28

  : Average $\log{\hbox{det}}K$ of VSimCLR embeddings on CIFAR-10C for each corruption type and severity (higher severity $=$ stronger corruption).
:::

::: {#tab:cifar10c_vsup_logdet}
  **Corruption**        **Severity 1**   **Severity 2**   **Severity 3**   **Severity 4**   **Severity 5**
  ------------------- ---------------- ---------------- ---------------- ---------------- ----------------
  gaussian_noise                -98.85          -105.28          -109.87          -111.50          -112.42
  shot_noise                    -95.76           -99.39          -106.47          -108.50          -110.77
  impulse_noise                 -96.94          -103.24          -109.20          -117.34          -120.23
  speckle_noise                 -95.73          -101.21          -103.87          -107.95          -110.44
  defocus_blur                  -91.95           -91.90           -92.33           -93.94           -97.03
  glass_blur                   -111.32          -111.29          -109.63          -118.74          -117.08
  motion_blur                   -93.95           -96.48          -100.86          -100.96          -105.21
  zoom_blur                     -93.66           -92.94           -93.67           -94.06           -96.29
  gaussian_blur                 -91.95           -92.31           -93.14           -94.40           -98.17
  snow                          -95.28          -100.62          -100.32          -101.30          -103.04
  frost                         -93.98           -96.23          -100.71          -101.33          -105.15
  fog                           -92.33           -93.25           -95.34           -98.54          -109.05
  brightness                    -92.04           -92.06           -92.16           -92.40           -93.11
  saturate                      -93.05           -93.80           -92.14           -92.82           -94.02
  spatter                       -93.86           -97.46          -100.59          -100.27          -106.63
  contrast                      -92.14           -92.54           -93.10           -94.30          -101.31
  elastic_transform             -95.01           -94.65           -94.96          -100.26          -106.89
  pixelate                      -93.06           -94.88           -96.53          -101.58          -106.43
  jpeg_compression              -95.47           -98.31           -99.28          -100.59          -102.32

  : Average $\log{\hbox{det}}K$ of VSupCon embeddings on CIFAR-10C for each corruption type and severity (higher severity $=$ stronger corruption).
:::

This counterintuitive observation---that typical (i.e., common) samples exhibit larger posterior dispersion---parallels the concurrent findings of Guth et al. [@guth2025learning], albeit under different settings: (i) **Quantity:** we analyze latent-space posterior dispersion via $\log{\hbox{det}}K$, whereas they study input-space marginal density $p(x)$; (ii) **Observation:** typical samples have larger $\log{\hbox{det}}K$ (ours), while they have lower $p(x)$ (theirs). Although these quantities live in different spaces, both results indicate that typical samples are not the highest-density points. In our case, typical images yield larger dispersion and atypical images smaller dispersion; since dispersion is inversely related to peak density, our result is consistent with Guth et al. Hence, in both settings, "typical" $\neq$ "highest-density." Consequently, posterior dispersion serves as a useful uncertainty signal; see Table [1](#tab:label_scarcity){reference-type="ref" reference="tab:label_scarcity"} for an application under label scarcity.

<figure id="fig:mu_K" data-latex-placement="t">

<figcaption>Norm of the posterior mean <span class="math inline">∥<strong>μ</strong>∥</span> versus the log‐determinant of the covariance <span class="math inline">$\log{\hbox{det}}(K)$</span>, averaged per class. Both <span class="math inline"><strong>μ</strong></span> and <span class="math inline"><em>K</em></span> are computed by averaging over all samples belonging to the same class.</figcaption>
</figure>

#### Class-wise average posterior parameters.

Figure [9](#fig:mu_K){reference-type="ref" reference="fig:mu_K"} reports class-wise averages of the posterior parameters---the mean norm $\|\boldsymbol{\mu}\|$ and the covariance dispersion $\log{\hbox{det}}K$---for VSimCLR and VSupCon. Classes exhibit distinct dispersion profiles. Despite being trained independently, the two methods yield similar class-wise patterns in both quantities: for example, the *cat* and *dog* classes show comparatively lower $\|\boldsymbol{\mu}\|$ and $\log{\hbox{det}}K$, whereas *truck* attains the largest $\|\boldsymbol{\mu}\|$. Table [3](#tab:logdet_by_class){reference-type="ref" reference="tab:logdet_by_class"} provides detailed per-class $\log{\hbox{det}}K$ values.

<figure id="fig:ood" data-latex-placement="h">

<figcaption>Posterior parameters of CIFAR-10 and SVHN datasets. We use the same encoder of VSimCLR trained with CIFAR-10. </figcaption>
</figure>

#### Posterior on in-distribution vs. out-of-distribution.

We compare per-sample posterior parameters under VSimCLR for in-distribution (ID; CIFAR-10) versus out-of-distribution (OOD; SVHN [@netzer2011reading]) inputs. VSimCLR is trained on the CIFAR-10 training set, after which we extract $(\boldsymbol{\mu}, K)$ on the CIFAR-10 and SVHN test sets. Figure [10](#fig:ood){reference-type="ref" reference="fig:ood"} plots the pairs $\bigl(\|\boldsymbol{\mu}\|,\ \log{\hbox{det}}K\bigr)$ for each dataset; black markers denote dataset-wise means. While the mean values $\operatorname{avg}(\|\boldsymbol{\mu}\|)$ and $\operatorname{avg}(\log{\hbox{det}}K)$ are similar across CIFAR-10 and SVHN, the SVHN points exhibit substantially greater spread (dispersion) across samples, indicating a broader posterior-parameter distribution for OOD data.

[^1]: Although we adopt the SimCLR augmentation scheme, our method applies to any contrastive framework.

[^2]: In Appendix [9.3](#app:DistNCE){reference-type="ref" reference="app:DistNCE"}, we present additional results exploring various VSimCLR design choices.

[^3]: $\log{\hbox{det}}K$ quantifies the *dispersion of the posterior in embedding space*, which reflects *typicality* rather than label uncertainty. Larger values correspond to more "typical" samples with many latent realizations consistent with the data manifold, whereas smaller values indicate more "unique" or outlier samples with tightly concentrated posteriors. A generative analogy may help understanding: if an outlier image had an extremely large posterior variance, then samples drawn from the prior would reproduce that outlier far too often---contradicting its rarity. Hence, larger variance corresponds to "typical" not "uncertain" inputs.
