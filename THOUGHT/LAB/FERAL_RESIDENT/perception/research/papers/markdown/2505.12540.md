<figure id="fig:main" data-latex-placement="h">
<embed src="ims/universal2.pdf" style="width:80.0%" />
<figcaption>Left: input embeddings from different model families (T5-based GTR <span class="citation" data-cites="ni2021gtr"></span> and BERT-based GTE <span class="citation" data-cites="li2023gte"></span>) are fundamentally incomparable. Right: given unpaired embedding samples from different models on different texts, our model learns a latent representation where they are closely aligned.</figcaption>
</figure>

<figure id="fig:architecture" data-latex-placement="t">
<embed src="ims/diagram.pdf" style="width:85.0%" />
<figcaption>Given only a vector database from an unknown model, <code>vec2vec</code> translates the database into the space of a known model using latent structure alone. Converted embeddings reveal sensitive information about the original documents, such as the topic of an email (pictured, real example).</figcaption>
</figure>

# Introduction

Text embeddings are the backbone of modern NLP, powering tasks like retrieval, RAG, classification, and clustering. There are many embedding models trained on different datasets, data shufflings, and initializations. An embedding of a text encodes its semantics: a good model maps texts with similar semantics to vectors close to each other in the embedding space. Since semantics is a property of text, different embeddings of the same text should encode the same semantics. In practice, however, different models encode texts into completely different and incompatible vector spaces.

The Platonic Representation Hypothesis [@huh2024platonicrepresentationhypothesis] conjectures that all image models of sufficient size converge to the same latent representation. We propose a stronger, constructive version of this hypothesis for text models: the universal latent structure of text representations can be learned and, furthermore, harnessed to translate representations from one space to another without any paired data or encoders.

In this work, we show that the Strong Platonic Representation Hypothesis holds in practice. Given unpaired examples of embeddings from two models with different architectures and training data, our method learns a latent representation in which the embeddings are almost identical ([1](#fig:main){reference-type="ref+Label" reference="fig:main"}).

We draw inspiration from research on aligning word embeddings across languages [@xing2015normalized; @conneau2018wordtranslationparalleldata; @grave2018unsupervisedalignmentembeddingswasserstein; @chen2018unsupervisedmultilingualwordembeddings] and unsupervised image translation [@liu2018unsupervisedimagetoimagetranslationnetworks; @zhu2020unpairedimagetoimagetranslationusing]. Our `vec2vec` method uses adversarial losses and cycle consistency to learn to encode embeddings into a shared latent space and decode with minimal loss. This makes unsupervised translation possible. We use a basic adversarial approach with vector space preservation [@mrksic2016counterfittingwordvectorslinguistic] to learn a mapping from an unknown embedding distribution to a known one.

`vec2vec` is the **first method to successfully translate embeddings from the space of one model to another without paired data.**[^1] `vec2vec` translations achieve cosine similarity as high as $0.92$ to the ground-truth vectors in their target embedding spaces and perfect matching on over $8000$ shuffled embeddings (without access to the set of possible matches in advance).

To show that our translations preserve not only the relative geometry of embeddings but also the semantics of underlying inputs, we extract information from them using zero-shot attribute inference and inversion, without any knowledge of the model that produced the original embeddings.[^2]

# Problem formulation: unsupervised embedding translation

Consider a collection of embedding vectors $\{u_1, \ldots u_n\}$, for example, a dump of a compromised vector database, where each $u_i = M_1(d_i)$ is generated by an unknown encoder $M_1 : \mathbb{V}^s \rightarrow \mathbb{R}^{d_{M_1}}$ from an unknown document $d_i$. We cannot make queries to $M_1$ and do not know its training data, nor architectural details. Our goal is to extract any information about the documents $d_i$.

<figure id="fig:spaces" data-latex-placement="t">
<embed src="ims/spaces.pdf" style="width:90.0%" />
<figcaption><strong>Unsupervised embedding translation</strong>. With access to only <span class="math inline"><em>u</em><sub><em>i</em></sub> = <em>M</em><sub>1</sub>(<em>d</em><sub><em>i</em></sub>)</span>, <code>vec2vec</code> seeks to generate a translation <span class="math inline"><em>F</em>(<em>u</em><sub><em>i</em></sub>)</span> that is close in <span class="math inline"><em>M</em><sub>2</sub></span>’s embedding space to the ideal embedding <span class="math inline"><em>v</em><sub><em>i</em></sub> = <em>M</em><sub>2</sub>(<em>d</em><sub><em>i</em></sub>)</span> without access to <span class="math inline"><em>d</em><sub><em>i</em></sub></span>, <span class="math inline"><em>v</em><sub><em>i</em></sub></span>, or <span class="math inline"><em>M</em><sub>1</sub></span>.</figcaption>
</figure>

We do assume access to a different encoder $M_2$ that we can query at will to generate new embeddings in some other space. We also assume high-level distributional knowledge about the hidden documents: their modality (text) and language (e.g., English). To extract information, we may translate $\{u_1, \ldots u_n\}$ into the output space of $M_2$ and apply techniques like inversion that take advantage of the encoder.

**Limitations of correspondence methods.** There is significant prior research on the problem of *matching* or *correspondence* between sets of embedding vectors [@alvarezmelis2018gromovwassersteinalignmentwordembedding; @peyre2016gromov; @chen2020graphoptimaltransportcrossdomain; @schnaus2025itsblindmatchvisionlanguage]. These methods typically assume that the two (or more) sets of embeddings are generated by different encoders on the *same or highly-overlapping inputs*. In other words, for each unknown vector, there must already exist a set of candidate vectors in a different embedding. In practice, it is unrealistic to expect that such a database be available, so these methods are not directly applicable. Some matching methods, however, support translation between embedding spaces without overlapping inputs. Our experiments demonstrate that these methods struggle significantly, even when correspondence exists.

Our task is inherently more challenging than matching, because we do not assume access to encoder $M_1$, nor do we have additional representations of documents ${d_1, \ldots, d_n}$ beyond their embeddings $u_i=M_1(d_i)$. Therefore, we rely solely on unsupervised *translation* from $M_1$ to $M_2$. The effectiveness of such unsupervised translation approaches thus critically depends on identifying and leveraging shared geometric structures within the embedding spaces produced by $M_1$ and $M_2$.

**The Strong Platonic Representation Hypothesis.** Our hope that unsupervised embedding translation is possible at all rests on the stronger version of the Platonic Representation Hypothesis [@huh2024platonicrepresentationhypothesis]. Our conjecture is as follows: *neural networks trained with the same objective and modality, but with different data and model architectures, converge to a universal latent space such that a translation between their respective representations can learned without any pairwise correspondence.*

**Translation enables information extraction.** Solving unsupervised translation will allow us to use information extraction tools designed to operate on vectors produced by known encoders. For example, we could apply inversion models [@morris2023textembeddingsrevealalmost; @zhang2025universalzeroshotembeddinginversion] to recover unknown documents $\{d_i\}$.

# Our method: `vec2vec` {#sec:architecture}

Unsupervised translation has been successful in computer vision, using a combination of cycle consistency and adversarial regularization [@liu2018unsupervisedimagetoimagetranslationnetworks; @zhu2020unpairedimagetoimagetranslationusing]. Our design of `vec2vec` is inspired in part by these methods. We aim to learn embedding-space translations that are cycle-consistent (mapping to and from an embedding space should end in the same place) and indistinguishable (embeddings for the same text from either space should have identical latents).

## Architecture {#subsec:method-arch}

We propose a modular architecture, where embeddings are encoded and decoded using space-specific adapter modules and passed through a shared backbone network. [2](#fig:architecture){reference-type="ref+Label" reference="fig:architecture"} shows these components. Input adapters $A_1 :\mathbb{R}^{d} \to \mathbb{R}^{Z}$ and $A_2: \mathbb{R}^{d} \to \mathbb{R}^{Z}$ transform embeddings from each encoder-specific space into a universal latent representation of dimension $Z$. The shared backbone $T : \mathbb{R}^{Z} \to \mathbb{R}^{Z}$ extracts a common latent embedding from adapted inputs. Output adapters $B_1 : \mathbb{R}^{Z} \to \mathbb{R}^{d}$ and $B_2: \mathbb{R}^{Z} \to \mathbb{R}^{d}$ translate these common latent embeddings back into the encoder-specific spaces. Thus, translation functions $F_1, F_2$ and additional reconstruction mappings $R_1, R_2$ are defined as: $$\begin{equation*}
    F_1=B_2 \circ T \circ A_1,\quad F_2=B_1 \circ T \circ A_2\quad R_1=B_1 \circ T \circ A_1\quad R_2=B_2 \circ T \circ A_2
\end{equation*}$$ Parameters of all components are collectively denoted $\theta = \{A_1, A_2, T, B_1, B_2\}$.

Unlike images, embeddings do not have any spatial bias. Instead of CNNs, we use multilayer perceptrons (MLP) with residual connections, layer normalization, and SiLU nonlinearities. Discriminators mirror this structure but omit residual connections to simplify adversarial learning.

## Optimization {#subsec:method-opt}

In addition to the 'generator' networks $F$ and $R$, we introduce discriminators operating on both the latent representations of $F$ ($D_{1}^{\ell}, D_{2}^{\ell}$) and the output embeddings ($D_1, D_2$).

Our goal is to train the parameters of $\theta$ by solving: $$\begin{equation}
\label{eq:base_objective}
    \theta^* = \arg \min_{\theta} \max_{D_1, D_2, D_{1}^{\ell}, D_{2}^{\ell}} \mathcal{L}_{\text{adv}}(F_1,F_2,D_1,D_2,D_{1}^{\ell},D_{2}^{\ell}) + \lambda_{\text{gen}} \mathcal{L}_{\text{gen}}(\theta),
\end{equation}$$ where $\mathcal{L}_{\text{adv}}$ and $\mathcal{L}_{\text{gen}}$ represent adversarial and generator-specific constraints respectively and hyperparameter $\lambda_{\rm gen}$ controls their tradeoff.

**Adversarial loss.** The adversarial loss encourages generated embeddings to match the empirical distributions of original embeddings both at the embedding and latent levels. Specifically, applying the standard GAN loss formulation [@10.1145/3422622] to both levels yields: $$\begin{align*}
    \mathcal{L}_{\text{adv}}(F_1,F_2,D_1,D_2,D_{1}^{\ell},D_{2}^{\ell}) 
    &= \mathcal{L}_{\text{GAN}}(D_1, F_1) + \mathcal{L}_{\text{GAN}}(D_2, F_2) \\
    &\quad + \mathcal{L}_{\text{GAN}}(D_{1}^{\ell}, T \circ A_1) + \mathcal{L}_{\text{GAN}}(D_{2}^{\ell},  T \circ A_2).
\end{align*}$$

**Generator.** Because adversarial losses alone do not guarantee that translated embeddings preserve semantics [@zhu2020unpairedimagetoimagetranslationusing], we introduce three additional constraints to help the generator learn a useful mapping:

*Reconstruction* enforces that an embedding, when mapped into the latent space and back into its original embedding space, closely matches its initial representation: $$\begin{align*}
    \mathcal{L}_{\text{rec}}(R_1,R_2) = \mathbb{E}_{x \sim p}\|R_1(x)-x\|_2^2 + \mathbb{E}_{y \sim q}\|R_2(y)-y\|_2^2.
\end{align*}$$

where $p$ and $q$ are distributions of embeddings sampled from $M_1$ and $M_2$, respectively.

*Cycle-consistency* acts as an unsupervised proxy for supervised pair alignment, ensuring that $F$ and $G$ can translate an embedding to the other embedding space and back again with minimal corruption: $$\begin{align*}
    \mathcal{L}_{\text{CC}}(F_1,F_2) = \mathbb{E}_{x \sim p}\|F_2(F_1(x))-x\|_2^2 + \mathbb{E}_{y \sim q}\|F_1(F_2(y))-y\|_2^2.
\end{align*}$$ *Vector space preservation (VSP)* ensures that pairwise relationships between generated embeddings remain consistent under translation [@mrksic2016counterfittingwordvectorslinguistic; @yoon2025embeddingconverter]. Given a batch of $B$ embeddings $x_1, ..., x_B$ and $y_1, ..., y_B$, we sum their average pairwise distances after translation by both $F_1$ and $F_2$: $$\begin{align*}
    \mathcal{L}_{\text{VSP}}(F_1,F_2) = \frac{1}{B} \sum_{i=1}^{B} \sum_{j=1}^{B} \bigg[ & \|M_1(x_i) \cdot M_1(x_j) - F_1(M_1(x_i)) \cdot F_1(M_1(x_j))\|_2^2 \\
    & +  \|M_2(y_i) \cdot M_2(y_j) - F_2(M_2(y_i)) \cdot F_2(M_2(y_j))\|_2^2 \bigg]
\end{align*}$$

Combining these losses yields: $\mathcal{L}_{\text{gen}}(\theta) = \lambda_{\text{rec}}\mathcal{L}_{\text{rec}}(R_1,R_2) + \lambda_{\text{CC}}\mathcal{L}_{\text{CC}}(F_1,F_2) + \lambda_{\text{VSP}}\mathcal{L}_{\text{VSP}}(F_1,F_2)$, where hyperparameters $\lambda_{\text{CC}}$, $\lambda_{\text{rec}}$, and $\lambda_{\text{VSP}}$ control relative importance.

# `vec2vec` learns to translate embeddings without any paired data {#sec:eval_geometry}

We first show that `vec2vec` learns a universal latent space, then demonstrate that this space preserves the geometry of all embeddings. Therefore, we can use it like a **universal language of text encoders** to translate their representations without any paired data.

<figure id="fig:cosine_heatmaps" data-latex-placement="t">
<embed src="ims/cosine_heatmaps.pdf" />
<figcaption>Pairwise cosine similarities of input embeddings (left) and their <code>vec2vec</code> latents (middle) across different embedding pairs. The absolute difference between the heatmaps plots is on the right. All numbers are computed on the same batch of 1024 NQ texts.</figcaption>
</figure>

**`vec2vec` learns a universal latent space.** `vec2vec` projects embeddings $M_{1,2,\ldots}$ into a shared latent space via compositions of input adapters ($A_{1,2,\ldots}$) and a shared translator $T$. [4](#fig:cosine_heatmaps){reference-type="ref+Label" reference="fig:cosine_heatmaps"} shows that even when the embeddings $u_i = M_1(d_i)$ and $v_i = M_2(d_i)$ are far apart (*i.e.,* have low cosine similarity), their representations in `vec2vec`'s latent space are incredibly close: $T(A_1(u_i)) \approx T(A_2(v_i))$. [1](#fig:main){reference-type="ref+Label" reference="fig:main"} visualizes this (via two-dimensional projections) for `vec2vec` trained on gte and gtr embeddings: the embeddings are far apart, but their latents are *nearly overlapping*.

::: {#tab:embedding_comparison}
+------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------+----------------------------------------------------------------------+
|                                    | `vec2vec`                                                     | Naïve Baseline                                                | OT Baseline                                                          |
+:===========================+:======+=========================:+===============:+==================:+=========================:+===============:+==================:+=========================:+===============:+=========================:+
| 3-5 (lr)6-8 (lr)9-11 $M_1$ | $M_2$ | $\cos(\cdot)$ $\uparrow$ | T-1 $\uparrow$ | Rank $\downarrow$ | $\cos(\cdot)$ $\uparrow$ | T-1 $\uparrow$ | Rank $\downarrow$ | $\cos(\cdot)$ $\uparrow$ | T-1 $\uparrow$ | Rank $\downarrow$        |
+----------------------------+-------+--------------------------+----------------+-------------------+--------------------------+----------------+-------------------+--------------------------+----------------+--------------------------+
| gra.                       | gtr   | **0.80 (0.0)**           | **0.99**       | **1.19 (0.1)**    | -0.03 (0.0)              | 0.00           | 4168.73 (9.2)     | 0.50 (0.0)               | 0.00           | 4094.22 (9.2)$^\ddagger$ |
|                            +-------+--------------------------+----------------+-------------------+--------------------------+----------------+-------------------+--------------------------+----------------+--------------------------+
|                            | gte   | **0.87 (0.0)**           | **0.95**       | **1.18 (0.0)**    | 0.01 (0.0)               | 0.00           | 4088.58 (9.2)     | 0.85 (0.0)               | 0.00           | 4069.91 (9.2)$^\dagger$  |
|                            +-------+--------------------------+----------------+-------------------+--------------------------+----------------+-------------------+--------------------------+----------------+--------------------------+
|                            | ste.  | **0.79 (0.0)**           | **0.98**       | **1.05 (0.0)**    | 0.01 (0.0)               | 0.00           | 4208.26 (9.2)     | 0.45 (0.0)               | 0.00           | 4096.35 (9.2)$^\ddagger$ |
|                            +-------+--------------------------+----------------+-------------------+--------------------------+----------------+-------------------+--------------------------+----------------+--------------------------+
|                            | e5    | **0.85 (0.0)**           | **0.98**       | **1.11 (0.0)**    | 0.02 (0.0)               | 0.00           | 4111.60 (9.2)     | 0.68 (0.0)               | 0.00           | 4096.17 (9.2)$^\ddagger$ |
+----------------------------+-------+--------------------------+----------------+-------------------+--------------------------+----------------+-------------------+--------------------------+----------------+--------------------------+
| gtr                        | gra.  | **0.81 (0.0)**           | **0.99**       | **1.02 (0.0)**    | -0.03 (0.0)              | 0.00           | 4169.76 (9.2)     | 0.50 (0.0)               | 0.00           | 4093.55 (9.2)$^\ddagger$ |
|                            +-------+--------------------------+----------------+-------------------+--------------------------+----------------+-------------------+--------------------------+----------------+--------------------------+
|                            | gte   | **0.87 (0.0)**           | **0.93**       | **2.31 (0.1)**    | 0.04 (0.0)               | 0.00           | 4080.92 (9.2)     | 0.85 (0.0)               | 0.00           | 4079.92 (9.2)$^\dagger$  |
|                            +-------+--------------------------+----------------+-------------------+--------------------------+----------------+-------------------+--------------------------+----------------+--------------------------+
|                            | ste.  | **0.80 (0.0)**           | **0.99**       | **1.03 (0.0)**    | 0.00 (0.0)               | 0.00           | 4198.78 (9.2)     | 0.46 (0.0)               | 0.00           | 4093.85 (9.2)$^\ddagger$ |
|                            +-------+--------------------------+----------------+-------------------+--------------------------+----------------+-------------------+--------------------------+----------------+--------------------------+
|                            | e5    | **0.83 (0.0)**           | **0.84**       | **2.88 (0.2)**    | 0.03 (0.0)               | 0.00           | 4082.84 (9.2)     | 0.83 (0.0)               | 0.00           | 4066.42 (9.2)$^\dagger$  |
+----------------------------+-------+--------------------------+----------------+-------------------+--------------------------+----------------+-------------------+--------------------------+----------------+--------------------------+
| gte                        | gra.  | **0.75 (0.0)**           | **0.95**       | **1.22 (0.0)**    | 0.01 (0.0)               | 0.00           | 4079.81 (9.3)     | 0.69 (0.0)               | 0.00           | 4069.23 (9.2)$^\dagger$  |
|                            +-------+--------------------------+----------------+-------------------+--------------------------+----------------+-------------------+--------------------------+----------------+--------------------------+
|                            | gtr   | **0.75 (0.0)**           | **0.91**       | **2.64 (0.1)**    | 0.04 (0.0)               | 0.00           | 4084.15 (9.2)     | 0.70 (0.0)               | 0.00           | 4078.45 (9.2)$^\dagger$  |
|                            +-------+--------------------------+----------------+-------------------+--------------------------+----------------+-------------------+--------------------------+----------------+--------------------------+
|                            | ste.  | **0.89 (0.0)**           | **1.00**       | **1.00 (0.0)**    | 0.56 (0.0)               | **1.00**       | **1.00 (0.0)**    | 0.69 (0.0)               | **1.00**       | **1.00 (0.0)**$^\dagger$ |
|                            +-------+--------------------------+----------------+-------------------+--------------------------+----------------+-------------------+--------------------------+----------------+--------------------------+
|                            | e5    | **0.87 (0.0)**           | 0.99           | 5.19 (0.5)        | 0.68 (0.0)               | **1.00**       | **1.00 (0.0)**    | 0.83 (0.0)               | **1.00**       | **1.00 (0.0)**$^\dagger$ |
+----------------------------+-------+--------------------------+----------------+-------------------+--------------------------+----------------+-------------------+--------------------------+----------------+--------------------------+
| ste.                       | gra.  | **0.80 (0.0)**           | **0.98**       | **1.08 (0.0)**    | 0.01 (0.0)               | 0.00           | 4209.08 (9.3)     | 0.48 (0.0)               | 0.00           | 4096.38 (9.2)$^\ddagger$ |
|                            +-------+--------------------------+----------------+-------------------+--------------------------+----------------+-------------------+--------------------------+----------------+--------------------------+
|                            | gtr   | **0.82 (0.0)**           | **1.00**       | **1.10 (0.0)**    | 0.00 (0.0)               | 0.00           | 4192.31 (9.2)     | 0.50 (0.0)               | 0.00           | 4095.46 (9.2)$^\ddagger$ |
|                            +-------+--------------------------+----------------+-------------------+--------------------------+----------------+-------------------+--------------------------+----------------+--------------------------+
|                            | gte   | **0.92 (0.0)**           | **1.00**       | **1.00 (0.0)**    | 0.56 (0.0)               | **1.00**       | **1.00 (0.0)**    | 0.86 (0.0)               | **1.00**       | **1.00 (0.0)**$^\dagger$ |
|                            +-------+--------------------------+----------------+-------------------+--------------------------+----------------+-------------------+--------------------------+----------------+--------------------------+
|                            | e5    | **0.86 (0.0)**           | **1.00**       | **1.00 (0.0)**    | 0.38 (0.0)               | 0.99           | 1.03 (0.0)        | 0.83 (0.0)               | **1.00**       | **1.00 (0.0)**$^\dagger$ |
+----------------------------+-------+--------------------------+----------------+-------------------+--------------------------+----------------+-------------------+--------------------------+----------------+--------------------------+
| e5                         | gra.  | **0.81 (0.0)**           | **0.99**       | **2.20 (0.2)**    | 0.02 (0.0)               | 0.00           | 4120.60 (9.3)     | 0.69 (0.0)               | 0.00           | 4096.12 (9.2)$^\ddagger$ |
|                            +-------+--------------------------+----------------+-------------------+--------------------------+----------------+-------------------+--------------------------+----------------+--------------------------+
|                            | gtr   | **0.74 (0.0)**           | **0.82**       | **2.56 (0.0)**    | 0.03 (0.0)               | 0.00           | 4080.76 (9.3)     | 0.70 (0.0)               | 0.00           | 4065.74 (9.2)$^\dagger$  |
|                            +-------+--------------------------+----------------+-------------------+--------------------------+----------------+-------------------+--------------------------+----------------+--------------------------+
|                            | gte   | **0.90 (0.0)**           | **1.00**       | 1.01 (0.0)        | 0.68 (0.0)               | **1.00**       | **1.00 (0.0)**    | 0.86 (0.0)               | **1.00**       | **1.00 (0.0)**$^\dagger$ |
|                            +-------+--------------------------+----------------+-------------------+--------------------------+----------------+-------------------+--------------------------+----------------+--------------------------+
|                            | ste.  | **0.78 (0.0)**           | **1.00**       | **1.00 (0.0)**    | 0.38 (0.0)               | **1.00**       | **1.00 (0.0)**    | 0.68 (0.0)               | **1.00**       | **1.00 (0.0)**$^\dagger$ |
+----------------------------+-------+--------------------------+----------------+-------------------+--------------------------+----------------+-------------------+--------------------------+----------------+--------------------------+
|                            |       |                          |                |                   |                          |                |                   |                          |                |                          |
+----------------------------+-------+--------------------------+----------------+-------------------+--------------------------+----------------+-------------------+--------------------------+----------------+--------------------------+

: In-distribution translations: `vec2vec`s trained on NQ and evaluated on a 65536 text subset of NQ (chunked in batches of size 8192). The rank metric varies from 1 to 8192, thus 4096 corresponds to a random ordering. Standard errors are shown in parentheses. Bold denotes best value. Symbols denote the lowest-rank solver for specific experiments: Sinkhorn$^\dagger$ and Gromov-Wasserstein$^\ddagger$.
:::

**`vec2vec` translations mirror target geometry.** [1](#tab:embedding_comparison){reference-type="ref+Label" reference="tab:embedding_comparison"} shows that `vec2vec` generates embeddings with near-optimal assignment across model pairs, achieving cosine similarity scores up to 0.92, top-1 accuracies up to 100%, and ranks as low as 1. In same-backbone pairings (e.g., (gte, e5)), `vec2vec`'s top-1 accuracy and rank are comparable to both the naïve baseline and (surprisingly) the oracle-aided optimal transport. Although the embeddings generated by `vec2vec` are significantly closer to the ground truth than the naïve baseline, in same-backbone pairings the embeddings are close enough to be compatible. In cross-backbone pairings, `vec2vec` is far superior on all metrics, while baseline methods perform similarly to random guessing.

::: {#tab:embeddings_ood}
+---------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+
|                           | TweetTopic                                                      | MIMIC                                                           |
+:==================+:======+=========================:+=================:+==================:+=========================:+=================:+==================:+
| 3-5 (lr)6-8 $M_1$ | $M_2$ | $\cos(\cdot)$ $\uparrow$ | Top-1 $\uparrow$ | Rank $\downarrow$ | $\cos(\cdot)$ $\uparrow$ | Top-1 $\uparrow$ | Rank $\downarrow$ |
+-------------------+-------+--------------------------+------------------+-------------------+--------------------------+------------------+-------------------+
| gran.             | gtr   | 0.74 (0.0)               | 0.99             | 1.09 (0.1)        | 0.74 (0.0)               | 0.60             | 23.38 (1.6)       |
|                   +-------+--------------------------+------------------+-------------------+--------------------------+------------------+-------------------+
|                   | gte   | 0.85 (0.0)               | 0.95             | 1.26 (0.1)        | 0.85 (0.0)               | 0.08             | 346.21 (7.8)      |
|                   +-------+--------------------------+------------------+-------------------+--------------------------+------------------+-------------------+
|                   | stel. | 0.77 (0.0)               | 0.96             | 1.11 (0.0)        | 0.72 (0.0)               | 0.13             | 242.23 (6.1)      |
|                   +-------+--------------------------+------------------+-------------------+--------------------------+------------------+-------------------+
|                   | e5    | 0.83 (0.0)               | 0.87             | 3.10 (0.7)        | 0.84 (0.0)               | 0.12             | 361.06 (8.7)      |
+-------------------+-------+--------------------------+------------------+-------------------+--------------------------+------------------+-------------------+
| gtr               | gran. | 0.79 (0.0)               | 0.98             | 2.41 (0.6)        | 0.78 (0.0)               | 0.51             | 35.27 (1.9)       |
|                   +-------+--------------------------+------------------+-------------------+--------------------------+------------------+-------------------+
|                   | gte   | 0.85 (0.0)               | 0.96             | 1.29 (0.2)        | 0.84 (0.0)               | 0.12             | 279.56 (6.9)      |
|                   +-------+--------------------------+------------------+-------------------+--------------------------+------------------+-------------------+
|                   | stel. | 0.77 (0.0)               | 0.96             | 1.10 (0.0)        | 0.72 (0.0)               | 0.27             | 127.92 (4.4)      |
|                   +-------+--------------------------+------------------+-------------------+--------------------------+------------------+-------------------+
|                   | e5    | 0.80 (0.0)               | 0.53             | 13.38 (1.2)       | 0.82 (0.0)               | 0.01             | 1413.80 (18.3)    |
+-------------------+-------+--------------------------+------------------+-------------------+--------------------------+------------------+-------------------+
| gte               | gran. | 0.73 (0.0)               | 0.94             | 1.33 (0.1)        | 0.73 (0.0)               | 0.09             | 342.15 (7.8)      |
|                   +-------+--------------------------+------------------+-------------------+--------------------------+------------------+-------------------+
|                   | gtr   | 0.71 (0.0)               | 0.95             | 1.29 (0.1)        | 0.69 (0.0)               | 0.12             | 256.63 (6.4)      |
|                   +-------+--------------------------+------------------+-------------------+--------------------------+------------------+-------------------+
|                   | stel. | 0.86 (0.0)               | 1.00             | 1.00 (0.0)        | 0.85 (0.0)               | 1.00             | 1.00 (0.0)        |
|                   +-------+--------------------------+------------------+-------------------+--------------------------+------------------+-------------------+
|                   | e5    | 0.83 (0.0)               | 0.91             | 1.57 (0.2)        | 0.86 (0.0)               | 0.54             | 17.71 (0.9)       |
+-------------------+-------+--------------------------+------------------+-------------------+--------------------------+------------------+-------------------+
| stel.             | gran. | 0.79 (0.0)               | 0.99             | 1.09 (0.1)        | 0.77 (0.0)               | 0.14             | 221.95 (5.9)      |
|                   +-------+--------------------------+------------------+-------------------+--------------------------+------------------+-------------------+
|                   | gtr   | 0.77 (0.0)               | 1.00             | 1.00 (0.0)        | 0.75 (0.0)               | 0.56             | 17.70 (1.0)       |
|                   +-------+--------------------------+------------------+-------------------+--------------------------+------------------+-------------------+
|                   | gte   | 0.90 (0.0)               | 1.00             | 1.00 (0.0)        | 0.91 (0.0)               | 1.00             | 1.00 (0.0)        |
|                   +-------+--------------------------+------------------+-------------------+--------------------------+------------------+-------------------+
|                   | e5    | 0.85 (0.0)               | 0.98             | 1.05 (0.0)        | 0.85 (0.0)               | 0.51             | 26.33 (1.2)       |
+-------------------+-------+--------------------------+------------------+-------------------+--------------------------+------------------+-------------------+
| e5                | gran. | 0.79 (0.0)               | 0.98             | 1.08 (0.0)        | 0.78 (0.0)               | 0.21             | 151.09 (4.6)      |
|                   +-------+--------------------------+------------------+-------------------+--------------------------+------------------+-------------------+
|                   | gtr   | 0.67 (0.0)               | 0.80             | 3.10 (0.6)        | 0.66 (0.0)               | 0.01             | 1029.64 (14.9)    |
|                   +-------+--------------------------+------------------+-------------------+--------------------------+------------------+-------------------+
|                   | gte   | 0.87 (0.0)               | 0.99             | 1.02 (0.0)        | 0.87 (0.0)               | 0.60             | 32.59 (2.6)       |
|                   +-------+--------------------------+------------------+-------------------+--------------------------+------------------+-------------------+
|                   | stel. | 0.75 (0.0)               | 0.98             | 1.06 (0.0)        | 0.75 (0.0)               | 0.46             | 32.12 (1.4)       |
+-------------------+-------+--------------------------+------------------+-------------------+--------------------------+------------------+-------------------+
|                   |       |                          |                  |                   |                          |                  |                   |
+-------------------+-------+--------------------------+------------------+-------------------+--------------------------+------------------+-------------------+

: Out-of-distribution translations: `vec2vec`s trained on NQ and evaluated on the entire TweetTopic test set (800 tweets) and an 8192-record subset of MIMIC. The rank metric varies from 1 to 800 (for TweetTopic) and 8192 (for MIMIC), thus 400 and, respectively, 4096 correspond to a random ordering. Standard errors are shown in parentheses.
:::

[2](#tab:embeddings_ood){reference-type="ref+Label" reference="tab:embeddings_ood"} shows that this performance extends to out-of-distribution data. Our `vec2vec` translators were trained on NQ (drawn from Wikipedia), yet exhibit high cosine similarity, high accuracy, and low rank when evaluated on tweets (which are far more colloquial and use emojis) and medical records (which contain domain-specific jargon unlikely to appear in NQ). In [10](#sec:full_ood){reference-type="ref+Label" reference="sec:full_ood"}, we show that baseline methods fail on cross-backbone embedding pairs.

::: {#tab:CLIP_nq}
+---------------------------+---------------------------------------------------------------+---------------------------------------------------------------+
|                           | `vec2vec`                                                     | OT Baseline                                                   |
+:==================+:======+=========================:+===============:+==================:+=========================:+===============:+==================:+
| 3-5 (lr)6-8 $M_1$ | $M_2$ | $\cos(\cdot)$ $\uparrow$ | T-1 $\uparrow$ | Rank $\downarrow$ | $\cos(\cdot)$ $\uparrow$ | T-1 $\uparrow$ | Rank $\downarrow$ |
+-------------------+-------+--------------------------+----------------+-------------------+--------------------------+----------------+-------------------+
| gra.              | clip  | **0.78 (0.0)**           | **0.35**       | **226.62 (3.2)**  | 0.60 (0.0)               | 0.00           | 4096.00 (9.2)     |
+-------------------+       +--------------------------+----------------+-------------------+--------------------------+----------------+-------------------+
| gtr               |       | **0.73 (0.0)**           | **0.13**       | **711.23 (5.9)**  | 0.59 (0.0)               | 0.00           | 4096.78 (9.2)     |
+-------------------+       +--------------------------+----------------+-------------------+--------------------------+----------------+-------------------+
| gte               |       | **0.62 (0.0)**           | **0.00**       | **3233.41 (9.8)** | 0.59 (0.0)               | 0.00           | 4096.16 (9.2)     |
+-------------------+       +--------------------------+----------------+-------------------+--------------------------+----------------+-------------------+
| ste.              |       | **0.77 (0.0)**           | **0.31**       | **286.69 (3.6)**  | 0.59 (0.0)               | 0.00           | 4096.76 (9.2)     |
+-------------------+       +--------------------------+----------------+-------------------+--------------------------+----------------+-------------------+
| e5                |       | **0.64 (0.0)**           | **0.01**       | **2568.21 (9.4)** | 0.60 (0.0)               | 0.00           | 4096.41 (9.2)     |
+-------------------+-------+--------------------------+----------------+-------------------+--------------------------+----------------+-------------------+
| clip              | gra.  | **0.74 (0.0)**           | **0.72**       | **4.46 (0.1)**    | 0.48 (0.0)               | 0.00           | 4096.86 (9.2)     |
|                   +-------+--------------------------+----------------+-------------------+--------------------------+----------------+-------------------+
|                   | gtr   | **0.67 (0.0)**           | **0.27**       | **155.11 (2.1)**  | 0.49 (0.0)               | 0.00           | 4096.35 (9.2)     |
|                   +-------+--------------------------+----------------+-------------------+--------------------------+----------------+-------------------+
|                   | gte   | **0.75 (0.0)**           | **0.00**       | **2678.90 (8.9)** | 0.72 (0.0)               | 0.00           | 4096.44 (9.2)     |
|                   +-------+--------------------------+----------------+-------------------+--------------------------+----------------+-------------------+
|                   | ste.  | **0.72 (0.0)**           | **0.61**       | **22.50 (0.5)**   | 0.45 (0.0)               | 0.00           | 4096.27 (9.2)     |
|                   +-------+--------------------------+----------------+-------------------+--------------------------+----------------+-------------------+
|                   | e5    | **0.73 (0.0)**           | **0.01**       | **1692.28 (8.2)** | 0.68 (0.0)               | 0.00           | 4096.42 (9.2)     |
+-------------------+-------+--------------------------+----------------+-------------------+--------------------------+----------------+-------------------+
|                   |       |                          |                |                   |                          |                |                   |
+-------------------+-------+--------------------------+----------------+-------------------+--------------------------+----------------+-------------------+

: Translations between unimodal and multimodal (CLIP) embeddings: `vec2vec`s trained on NQ and evaluated on a 65536 text subset of NQ (chunked in batches of size 8192). Rank varies from 1 to 8192, thus 4096 corresponds to a random ordering. Since the embedding dimensionalities are different, only the Gromov-Wasserstein OT baseline is run and the naive baseline does not apply. Bold denotes best value.
:::

Finally, [3](#tab:CLIP_nq){reference-type="ref+Label" reference="tab:CLIP_nq"} shows that `vec2vec` can even translate to and from the space of CLIP, a multimodal embedding model which was trained in part on *image* data. While the translations are not as strong as in [1](#tab:embedding_comparison){reference-type="ref+Label" reference="tab:embedding_comparison"}, `vec2vec` consistently outperforms the optimal transport baseline. These results show the promise of our method at adapting to new modalities: in particular, the embedding space of CLIP has been successfully connected to other modalities such as heatmaps, audio, and depth charts [@girdhar2023imagebind].

# Using `vec2vec` translations to extract information {#sec:eval_semantics}

In this section, we show that `vec2vec` translations not only preserve the geometric structure of embeddings but also retain sufficient semantics to enable attribute inference.

::: {#tab:tweet_topic_mimic_class}
+----------------------------+--------------------------------------+-----------------------------------+
|                            | TweetTopic ($k=1$)                   | MIMIC ($k=10$)                    |
+:===================+:======+:=========:+:======:+:======:+:======:+:=========:+:=====:+:=====:+:=====:+
| 3-6 (lr)7-10 $M_1$ | $M_2$ | `vec2vec` | Naïve  | $M_1$  | $M_2$  | `vec2vec` | Naïve | $M_1$ | $M_2$ |
+--------------------+-------+-----------+--------+--------+--------+-----------+-------+-------+-------+
| gran.              | gtr   | 0.25      | 0.10   | 0.30   | 0.24   | 0.19      | 0.11  | 0.76  | 0.88  |
|                    +-------+-----------+--------+--------+--------+-----------+-------+-------+-------+
|                    | gte   | 0.32      | 0.09   | 0.30   | 0.34   | 0.36      | 0.13  | 0.76  | 1.00  |
|                    +-------+-----------+--------+--------+--------+-----------+-------+-------+-------+
|                    | stel. | 0.24      | 0.10   | 0.30   | 0.28   | 0.27      | 0.04  | 0.76  | 0.96  |
|                    +-------+-----------+--------+--------+--------+-----------+-------+-------+-------+
|                    | e5    | 0.31      | 0.18   | 0.30   | 0.31   | 0.19      | 0.20  | 0.76  | 0.97  |
+--------------------+-------+-----------+--------+--------+--------+-----------+-------+-------+-------+
| gtr                | gran. | 0.34      | 0.08   | 0.24   | 0.30   | 0.16      | 0.12  | 0.88  | 0.76  |
|                    +-------+-----------+--------+--------+--------+-----------+-------+-------+-------+
|                    | gte   | 0.33      | 0.13   | 0.24   | 0.34   | 0.28      | 0.05  | 0.88  | 1.00  |
|                    +-------+-----------+--------+--------+--------+-----------+-------+-------+-------+
|                    | stel. | 0.30      | 0.10   | 0.24   | 0.28   | 0.25      | 0.07  | 0.88  | 0.96  |
|                    +-------+-----------+--------+--------+--------+-----------+-------+-------+-------+
|                    | e5    | 0.30      | 0.04   | 0.24   | 0.31   | 0.09      | 0.09  | 0.88  | 0.97  |
+--------------------+-------+-----------+--------+--------+--------+-----------+-------+-------+-------+
| gte                | gran. | 0.37      | 0.04   | 0.34   | 0.30   | 0.18      | 0.11  | 1.00  | 0.76  |
|                    +-------+-----------+--------+--------+--------+-----------+-------+-------+-------+
|                    | gtr   | 0.24      | 0.13   | 0.34   | 0.24   | 0.10      | 0.03  | 1.00  | 0.88  |
|                    +-------+-----------+--------+--------+--------+-----------+-------+-------+-------+
|                    | stel. | 0.31      | 0.20   | 0.34   | 0.28   | 0.68      | 0.83  | 1.00  | 0.96  |
|                    +-------+-----------+--------+--------+--------+-----------+-------+-------+-------+
|                    | e5    | 0.37      | 0.30   | 0.34   | 0.31   | 0.37      | 0.63  | 1.00  | 0.97  |
+--------------------+-------+-----------+--------+--------+--------+-----------+-------+-------+-------+
| stel.              | gran. | 0.35      | 0.07   | 0.28   | 0.30   | 0.23      | 0.09  | 0.96  | 0.76  |
|                    +-------+-----------+--------+--------+--------+-----------+-------+-------+-------+
|                    | gtr   | 0.26      | 0.13   | 0.28   | 0.24   | 0.22      | 0.09  | 0.96  | 0.88  |
|                    +-------+-----------+--------+--------+--------+-----------+-------+-------+-------+
|                    | gte   | 0.38      | 0.36   | 0.28   | 0.34   | 0.90      | 0.98  | 0.96  | 1.00  |
|                    +-------+-----------+--------+--------+--------+-----------+-------+-------+-------+
|                    | e5    | 0.35      | 0.34   | 0.28   | 0.31   | 0.38      | 0.46  | 0.96  | 0.97  |
+--------------------+-------+-----------+--------+--------+--------+-----------+-------+-------+-------+
| e5                 | gran. | 0.33      | 0.15   | 0.31   | 0.30   | 0.14      | 0.07  | 0.97  | 0.76  |
|                    +-------+-----------+--------+--------+--------+-----------+-------+-------+-------+
|                    | gtr   | 0.26      | 0.22   | 0.31   | 0.24   | 0.11      | 0.04  | 0.97  | 0.88  |
|                    +-------+-----------+--------+--------+--------+-----------+-------+-------+-------+
|                    | gte   | 0.34      | 0.28   | 0.31   | 0.34   | 0.47      | 0.66  | 0.97  | 1.00  |
|                    +-------+-----------+--------+--------+--------+-----------+-------+-------+-------+
|                    | stel. | 0.26      | 0.16   | 0.31   | 0.28   | 0.36      | 0.40  | 0.97  | 0.96  |
+--------------------+-------+-----------+--------+--------+--------+-----------+-------+-------+-------+
|                    |       |           |        |        |        |           |       |       |       |
+--------------------+-------+-----------+--------+--------+--------+-----------+-------+-------+-------+

: Information leakage via top-$k$ zero-shot attribute inference: `vec2vec`s trained on NQ and evaluated on the TweetTopic test set (800 tweets) and an 8192-record subset of MIMIC. $M_1$ and $M_2$ represent *ideal* zero-shot inference: attributes and embeddings are encoded using the same model.
:::

::: wrapfigure
r0.42 ![image](ims/enron_heatmap.pdf){width="37%"}
:::

**Zero-shot attribute inference.** []{#sec:tweet_classification label="sec:tweet_classification"} [4](#tab:tweet_topic_mimic_class){reference-type="ref+Label" reference="tab:tweet_topic_mimic_class"} shows that attribute inference on `vec2vec` translations consistently outperforms the naïve baseline and often does better than the ideal zero-shot baseline which performs inference on ground-truth document and attribute embeddings in the same space (this baseline is imaginary since these embeddings are not available in our setting).

[]{#sec:mimic_classification label="sec:mimic_classification"} `vec2vec` translations even work for embeddings of medical records, which are much further from the training distribution than tweets. The attributes in this case are MedCAT disease descriptions, very few of which occur in the training data. Attribute inference on translated embeddings is comparable to the naïve baseline in same-backbone pairings and outperforms it (often greatly) in cross-backbone pairings. The fact that `vec2vec` preserves the semantics of concepts like \"alveolar periostitis\" (which never appears in its training data) is evidence that its latent space is indeed a universal representation.

**Zero-shot inversion.** Inversion, i.e., reconstruction of text inputs, is more ambitious than attribute inference. `vec2vec` translations retain enough semantic information that off-the-shelf, zero-shot inversion methods like [@zhang2025universalzeroshotembeddinginversion], developed for embeddings computed by standard encoders, extract information for as many as 80% of documents given *only* their translated embeddings, for some model pairs ([\[fig:enron_heatmap\]](#fig:enron_heatmap){reference-type="ref+Label" reference="fig:enron_heatmap"}). These inversions are imperfect and we leave development of specialized inverters for translated embeddings to future work. Nevertheless, as exemplified in [5](#fig:inversion_example){reference-type="ref+Label" reference="fig:inversion_example"}, they still extract information such as individual and company names, dates, promotions, financial information, outages, and even lunch orders. In [12](#sec:prompt){reference-type="ref+Label" reference="sec:prompt"}, we show the prompt we use to measure extraction.

<figure id="fig:inversion_example" data-latex-placement="ht">
<div class="callout">
<p><strong>Ground Truth:</strong> “Subject: <span style="background-color: green">Enron</span> <span style="background-color: yellow">Bashing</span> on Frontline <code>\n</code> Body:..."<br />
<strong>Generation:</strong> “Some emails discussing <span style="background-color: green">NROn</span> Employee/s <span style="background-color: yellow">Complaint To thePublic</span>..."<br />
<strong>Ground Truth:</strong> “Subject: <span style="background-color: yellow">Trades for 3/1/02</span> <code>\n</code> Body: <code>\n</code> <span style="background-color: green">John</span>, <code>\n</code> The following trades..."<br />
<strong>Generation:</strong> “... <span style="background-color: yellow">future transactions</span> may await <span style="background-color: green">John</span> G..."<br />
<strong>Ground Truth:</strong> “<span style="background-color: yellow">The following expense report</span> is ready for approval..."<br />
<strong>Generation:</strong> “<span style="background-color: yellow">The upcoming expense statement</span> from YYYY MM Dec..."</p>
</div>
<figcaption>Examples of inversions that infer <span style="background-color: green">entities</span> and <span style="background-color: yellow">content</span>.</figcaption>
</figure>

# Related work

**Representation alignment.** Similarities between representations of different neural networks are investigated in [@laakso2000content; @li2016convergentlearningdifferentneural; @wang2018understandinglearningrepresentationsextent; @bansal2021revisitingmodelstitchingcompare; @huh2024platonicrepresentationhypothesis; @wolfram2025layers; @li-etal-2024-vision-language]. Methods based on CCA [@morcos2018insightsrepresentationalsimilarityneural], SVCCA, [@raghu2017svccasingularvectorcanonical], CKA [@kornblith2019similarity; @maniparambil2024vision], ICA [@yamagiwa2023discoveringuniversalgeometryembeddings], time-series [@mistry2023comparative], and GUIs [@heimerl2019embComp] have been used to compare embeddings from different subspaces. [@maiorca2024latentspacetranslationsemantic; @moschella2023relativerepresentationsenablezeroshot; @moayeri2023texttoconceptandbackcrossmodel; @tian2019latent; @norelli2023asifcoupleddataturns] harness representation similarity for zero-shot stitching, substitution, domain transfer, and multi-modal adaptation. All rely on some amount of paired data, which is difficult to reduce [@cannistraci2023bootstrappingparallelanchorsrelative]. Our method does not just measure similarity, we learn how to *translate* representations across spaces without any paired data.

**Optimal transport.** The problem of unsupervised optimal transport has been studied for image style transfer [@huang2018multimodalunsupervisedimagetoimagetranslation; @liu2018unsupervisedimagetoimagetranslationnetworks; @zhu2020unpairedimagetoimagetranslationusing], word translation [@xing2015normalized; @conneau2018wordtranslationparalleldata; @grave2018unsupervisedalignmentembeddingswasserstein; @chen2018unsupervisedmultilingualwordembeddings; @joulin-etal-2018-loss], and natural language sequence translation [@raviknight2011deciphering; @lample2018unsupervisedmachinetranslationusing; @alvarezmelis2018gromovwassersteinalignmentwordembedding; @artetxe2018unsupervisedneuralmachinetranslation; @yang2018unsupervisedneuralmachinetranslation; @artetxe-etal-2018-robust]. Our method builds on these works, which often employ a combination of cycle-consistency and adversarial loss. Importantly, unlike prior word and sequence translation methods, multiple representations of the same input (e.g., heavily overlapping word vocabularies) are unavailable in our setting. [@schnaus2025itsblindmatchvisionlanguage] proposes a solver for matching small sets of embeddings between different vision-language models. Our method goes well beyond matching by taking unknown embeddings and *generating* matching embeddings in the space of another model.

**Embedding inversion.** An emerging line of research investigates decoding text from language model embeddings [@song2020informationleakageembeddingmodels; @li2023sentenceembeddingleaksinformation; @morris2023textembeddingsrevealalmost] and outputs [@morris2023languagemodelinversion; @carlini2024stealingproductionlanguagemodel; @zhang2024extractingpromptsinvertingllm]. `vec2vec` helps apply these to unknown embeddings, without an encoder or paired data, by translating them to the space of a known model.

**Bridging modality gaps.** Previous work has noted an inherent "gap" between image- and text-based models [@liang2022mindgapunderstandingmodality] and proposed various ways to unify the modalities [@song2023bridgegapmodalitiescomprehensive]. Some approaches feed image embeddings directly into language models [@koh2023fromage; @wang2023visionllm; @dong2024dreamllmsynergisticmultimodalcomprehension; @liu2023visualinstructiontuning], while others generate captions from image embeddings [@mokady2021clipcap] or even from text embeddings themselves [@morris2023textembeddingsrevealalmost]. [@girdhar2023imagebind] introduces a shared embedding space that integrates inputs from multiple modalities, including text, audio, and vision. In contrast, our post-hoc approach directly translates between representations and complements these systems by enabling inputs from a wide variety of embedding models.

# Discussion and Future Work

The Platonic Representation Hypothesis conjectures that the representation spaces of modern neural networks are converging. We assert the Strong Platonic Representation Hypothesis: the latent universal representation can be learned and harnessed to translate between representation spaces without any encoders or paired data.

In [4](#sec:eval_geometry){reference-type="ref+Label" reference="sec:eval_geometry"}, we demonstrated that our `vec2vec` method successfully translates embeddings generated from unseen documents by unseen encoders, and the translator is robust to (sometimes very) out-of-distribution inputs. This suggests that `vec2vec` learns domain-agnostic translations based on the universal geometric relationships which encode the same semantics in multiple embedding spaces.

In [5](#sec:eval_semantics){reference-type="ref+Label" reference="sec:eval_semantics"}, we showed that `vec2vec` translations preserve sufficient input semantics to enable attribute inference. We extracted sensitive disease information from patient records and partial content from corporate emails, with access only to document embeddings and no access to the encoder that produced them. Better translation methods will enable higher-fidelity extraction, confirming once again that embeddings reveal (almost) as much as their inputs.

Our findings provide compelling evidence for the Strong Platonic Representation Hypothesis for text-based models. Our preliminary results on CLIP suggest that the universal geometry can be harnessed in other modalities, too. The results in this paper are but a *lower bound* on inter-representation translation. Better and more stable learning algorithms, architectures, and other methodological improvements will support scaling to more data, more model families, and more modalities.

::: ack
This research is supported in part by the Google Cyber NYC Institutional Research Program. JM is supported by the National Science Foundation.
:::

# Compute

Our training and evaluation were conducted using diverse compute environments, including both local and cloud GPU clusters. Experiments were done on NVIDIA 2080Ti, L4, A40, and A100 GPUs, listed in order of increasing computational capacity.

For our main experiments, we trained 36 distinct `vec2vec` models, with training durations ranging from 1 to 7 days per model, depending on the specific GPU and model pair (which affected convergence rates). Additional 9 `vec2vec` models were trained for our ablations, bringing the total to 45 `vec2vec` models. Taking a conservative estimate of the average training time, this amounted to approximately 180 GPU days (45 models $\times$ 4 days / model).

Evaluation procedures varied by model type:

- For each of our 45 `vec2vec` models, our full evaluation on NQ, TweetTopic, and MIMIC datasets required approximately 1 hour per model across all GPU types.

- The 36 non-ablation GANs underwent additional analysis on the Enron email corpus, requiring about 1.5 hours for inversion and downstream LLM evaluation.

- Naive baselines (30 models without CLIP) required approximately 30 minutes each for evaluation, roughly half the time needed for our full models.

- Optimal transport baselines (36 models and 4 different algorithms) were run exclusively on CPU, each requiring approximately 4 hours of computation time.

In total, our experiments consumed approximately 180 GPU days for training and an additional 78 GPU hours for evaluation and analysis. There were approximately 144 CPU hours required for optimal transport.

# Extended oracle-aided optimal transport baseline results

Let $u_i = M_1(d_i)$ and $v_i = M_2(d_i)$ denote embeddings of the same document $d_i$ from two different embedding models. In [4](#sec:eval_geometry){reference-type="ref+Label" reference="sec:eval_geometry"}, we solve the optimal assignment problem: $$\pi^* = \arg\min_{\pi} \sum_{i=1}^n \cos(u_i, v_{\pi(i)}),$$ using four algorithms: Hungarian (linear sum assignment), Earth Mover's Distance (EMD), Sinkhorn, and Gromov-Wasserstein. Note that this baseline computes matchings and transports between embeddings derived from the *same underlying texts*, strongly favoring optimal transport (OT) methods. Nevertheless, OT still struggles when embeddings originate from different model backbones.

Since the Hungarian algorithm produces a discrete matching, it is evaluated only using Top-1 Accuracy, while the other algorithms are evaluated across all metrics. For each experiment, the lowest-rank solver is reported in [1](#tab:embedding_comparison){reference-type="ref+Label" reference="tab:embedding_comparison"} and [3](#tab:CLIP_nq){reference-type="ref+Label" reference="tab:CLIP_nq"} (denoted by symbols in the final column). Evaluation metrics are defined as follows:

1.  **Top-1 Accuracy**: Fraction of embeddings correctly identified as closest pairs, calculated by either selecting the maximum transported mass per embedding or applying the Hungarian algorithm directly to the transport plan $P$. We report the higher accuracy between the two.

2.  **Mean Rank**: Average rank position of the correct embedding match $v_i$ when sorted by descending transported mass $P_{ij}$ from $u_i$: $$\text{rank}(v_i) = \text{position of } v_i \text{ among sorted } P_{ij}.$$

3.  **Mean Cosine Similarity**: Average cosine similarity between barycenters and true counterparts: $$v'_i = \frac{\sum_{j=1}^n P_{ij} v_j}{\sum_{j=1}^n P_{ij}},\quad
        \text{Similarity} = \frac{1}{n}\sum_{i=1}^{n}\cos(v'_i,v_i).$$

# Full out-of-distribution translation results {#sec:full_ood}

We provide baseline numbers for the experiments shown in [2](#tab:embeddings_ood){reference-type="ref+Label" reference="tab:embeddings_ood"}, by dataset.

::: {#tab:embedding_comparison_ood}
+------------------------------------+------------------------------------+---------------------------------------+-------------------------------------------------+
|                                    | `vec2vec`                          | Naïve Baseline                        | OT Baseline                                     |
+:===========================+:======+==============:+=====:+============:+==============:+=======:+=============:+==============:+======:+========================:+
| 3-5 (lr)6-8 (lr)9-11 $E_1$ | $E_2$ | $\cos(\cdot)$ | T-1  | Rank        | $\cos(\cdot)$ | T-1    | Rank         | $\cos(\cdot)$ | T-1   | Rank                    |
+----------------------------+-------+---------------+------+-------------+---------------+--------+--------------+---------------+-------+-------------------------+
| gra.                       | gtr   | 0.74 (0.0)    | 0.99 | 1.09 (0.1)  | -0.04 (0.0)   | 0.00   | 415.61 (8.2) | 0.51 (0.0)    | 0.01  | 398.32 (8.2)$^\ddagger$ |
|                            +-------+---------------+------+-------------+---------------+--------+--------------+---------------+-------+-------------------------+
|                            | gte   | 0.85 (0.0)    | 0.95 | 1.26 (0.1)  | 0.00 (0.0)    | 0.00   | 406.73 (8.2) | 0.74 (0.0)    | 0.01  | 398.14 (8.2)$^*$        |
|                            +-------+---------------+------+-------------+---------------+--------+--------------+---------------+-------+-------------------------+
|                            | ste.  | 0.77 (0.0)    | 0.96 | 1.11 (0.0)  | 0.00 (0.0)    | 0.00   | 417.27 (8.2) | 0.56 (0.0)    | 0.00  | 399.57 (8.2)$^\ddagger$ |
|                            +-------+---------------+------+-------------+---------------+--------+--------------+---------------+-------+-------------------------+
|                            | e5    | 0.83 (0.0)    | 0.87 | 3.10 (0.7)  | 0.02 (0.0)    | 0.00   | 405.53 (8.1) | 0.75 (0.0)    | 0.01  | 398.33 (8.2)$^\ddagger$ |
+----------------------------+-------+---------------+------+-------------+---------------+--------+--------------+---------------+-------+-------------------------+
| gtr                        | gra.  | 0.79 (0.0)    | 0.98 | 2.41 (0.6)  | -0.04 (0.0)   | 0.00   | 411.53 (8.3) | 0.57 (0.0)    | 0.01  | 398.29 (8.2)$^\ddagger$ |
|                            +-------+---------------+------+-------------+---------------+--------+--------------+---------------+-------+-------------------------+
|                            | gte   | 0.85 (0.0)    | 0.96 | 1.29 (0.2)  | 0.04 (0.0)    | 0.00   | 392.01 (8.2) | 0.86 (0.0)    | 0.00  | 384.00 (8.1)$^\dagger$  |
|                            +-------+---------------+------+-------------+---------------+--------+--------------+---------------+-------+-------------------------+
|                            | ste.  | 0.77 (0.0)    | 0.96 | 1.10 (0.0)  | 0.00 (0.0)    | 0.00   | 394.69 (8.3) | 0.74 (0.0)    | 0.00  | 390.67 (8.2)$^\dagger$  |
|                            +-------+---------------+------+-------------+---------------+--------+--------------+---------------+-------+-------------------------+
|                            | e5    | 0.80 (0.0)    | 0.53 | 13.38 (1.2) | 0.03 (0.0)    | 0.00   | 400.85 (8.2) | 0.75 (0.0)    | 0.00  | 399.78 (8.2)$^\ddagger$ |
+----------------------------+-------+---------------+------+-------------+---------------+--------+--------------+---------------+-------+-------------------------+
| gte                        | gra.  | 0.73 (0.0)    | 0.94 | 1.33 (0.1)  | 0.00 (0.0)    | 0.00   | 408.81 (8.3) | 0.56 (0.0)    | 0.01  | 398.16 (8.2)$^*$        |
|                            +-------+---------------+------+-------------+---------------+--------+--------------+---------------+-------+-------------------------+
|                            | gtr   | 0.71 (0.0)    | 0.95 | 1.29 (0.1)  | 0.04 (0.0)    | 0.00   | 386.58 (8.3) | 0.71 (0.0)    | 0.00  | 383.41 (8.1)$^\dagger$  |
|                            +-------+---------------+------+-------------+---------------+--------+--------------+---------------+-------+-------------------------+
|                            | ste.  | 0.86 (0.0)    | 1.00 | 1.00 (0.0)  | 0.58 (0.0)    | 1.00   | 1.00 (0.0)   | 1.00 (0.0)    | 1.00  | 1.00 (0.0)$^*$          |
|                            +-------+---------------+------+-------------+---------------+--------+--------------+---------------+-------+-------------------------+
|                            | e5    | 0.83 (0.0)    | 0.91 | 1.57 (0.2)  | 0.68 (0.0)    | 1.00   | 1.00 (0.0)   | 1.00 (0.0)    | 1.00  | 1.00 (0.0)$^*$          |
+----------------------------+-------+---------------+------+-------------+---------------+--------+--------------+---------------+-------+-------------------------+
| ste.                       | gra.  | 0.79 (0.0)    | 0.99 | 1.09 (0.1)  | 0.00 (0.0)    | 0.00   | 418.16 (8.4) | 0.57 (0.0)    | 0.00  | 399.56 (8.2)$^\ddagger$ |
|                            +-------+---------------+------+-------------+---------------+--------+--------------+---------------+-------+-------------------------+
|                            | gtr   | 0.77 (0.0)    | 1.00 | 1.00 (0.0)  | 0.00 (0.0)    | 0.00   | 393.07 (8.1) | 0.71 (0.0)    | 0.00  | 390.26 (8.2)$^\dagger$  |
|                            +-------+---------------+------+-------------+---------------+--------+--------------+---------------+-------+-------------------------+
|                            | gte   | 0.90 (0.0)    | 1.00 | 1.00 (0.0)  | 0.58 (0.0)    | 1.00   | 1.00 (0.0)   | 1.00 (0.0)    | 1.00  | 1.00 (0.0)$^*$          |
|                            +-------+---------------+------+-------------+---------------+--------+--------------+---------------+-------+-------------------------+
|                            | e5    | 0.85 (0.0)    | 0.98 | 1.05 (0.0)  | 0.37 (0.0)    | 0.89   | 1.55 (0.1)   | 1.00 (0.0)    | 1.00  | 1.00 (0.0)$^*$          |
+----------------------------+-------+---------------+------+-------------+---------------+--------+--------------+---------------+-------+-------------------------+
| e5                         | gra.  | 0.79 (0.0)    | 0.98 | 1.08 (0.0)  | 0.02 (0.0)    | 0.00   | 405.75 (8.3) | 0.57 (0.0)    | 0.01  | 398.34 (8.2)$^\ddagger$ |
|                            +-------+---------------+------+-------------+---------------+--------+--------------+---------------+-------+-------------------------+
|                            | gtr   | 0.67 (0.0)    | 0.80 | 3.10 (0.6)  | 0.03 (0.0)    | 0.00   | 401.16 (8.4) | 0.51 (0.0)    | 0.00  | 399.73 (8.2)$^\ddagger$ |
|                            +-------+---------------+------+-------------+---------------+--------+--------------+---------------+-------+-------------------------+
|                            | gte   | 0.87 (0.0)    | 0.99 | 1.02 (0.0)  | 0.68 (0.0)    | 1.00   | 1.00 (0.0)   | 1.00 (0.0)    | 1.00  | 1.00 (0.0)$^*$          |
|                            +-------+---------------+------+-------------+---------------+--------+--------------+---------------+-------+-------------------------+
|                            | ste.  | 0.75 (0.0)    | 0.98 | 1.06 (0.0)  | 0.37 (0.0)    | 1.00   | 1.00 (0.0)   | 1.00 (0.0)    | 1.00  | 1.00 (0.0)$^*$          |
+----------------------------+-------+---------------+------+-------------+---------------+--------+--------------+---------------+-------+-------------------------+
|                            |       |               |      |             |               |        |              |               |       |                         |
+----------------------------+-------+---------------+------+-------------+---------------+--------+--------------+---------------+-------+-------------------------+

: Out-of-distribution translations on TweetTopic (with baselines): `vec2vec` models trained on NQ and evaluated on the entire TweetTopic test set (800 tweets). The rank metric varies from 1 to 800, thus 400 corresponds to a random ordering. Standard errors are shown in parentheses. Symbols denote the lowest-rank solver: Earth Mover's Distance$^*$, Sinkhorn$^\dagger$ and Gromov-Wasserstein$^\ddagger$
:::

::: {#tab:embedding_comparison_ood_mimic}
+------------------------------------+---------------------------------------+-----------------------------------------+---------------------------------------------------+
|                                    | `vec2vec`                             | Naïve Baseline                          | OT Baseline                                       |
+:===========================+:======+==============:+=====:+===============:+==============:+=======:+===============:+==============:+======:+==========================:+
| 3-5 (lr)6-8 (lr)9-11 $E_1$ | $E_2$ | $\cos(\cdot)$ | T-1  | Rank           | $\cos(\cdot)$ | T-1    | Rank           | $\cos(\cdot)$ | T-1   | Rank                      |
+----------------------------+-------+---------------+------+----------------+---------------+--------+----------------+---------------+-------+---------------------------+
| gra.                       | gtr   | 0.74 (0.0)    | 0.60 | 23.38 (1.6)    | -0.02 (0.0)   | 0.00   | 4010.00 (25.8) | 0.82 (0.0)    | 0.00  | 3962.83 (26.1)$^\dagger$  |
|                            +-------+---------------+------+----------------+---------------+--------+----------------+---------------+-------+---------------------------+
|                            | gte   | 0.85 (0.0)    | 0.08 | 346.21 (7.8)   | 0.01 (0.0)    | 0.00   | 3978.35 (26.1) | 0.92 (0.0)    | 0.00  | 3808.18 (25.9)$^\dagger$  |
|                            +-------+---------------+------+----------------+---------------+--------+----------------+---------------+-------+---------------------------+
|                            | ste.  | 0.72 (0.0)    | 0.13 | 242.23 (6.1)   | -0.01 (0.0)   | 0.00   | 3900.74 (26.2) | 0.86 (0.0)    | 0.02  | 3780.44 (26.0)$^\dagger$  |
|                            +-------+---------------+------+----------------+---------------+--------+----------------+---------------+-------+---------------------------+
|                            | e5    | 0.84 (0.0)    | 0.12 | 361.06 (8.7)   | 0.02 (0.0)    | 0.00   | 4024.92 (26.1) | 0.93 (0.0)    | 0.00  | 3937.63 (26.2)$^\dagger$  |
+----------------------------+-------+---------------+------+----------------+---------------+--------+----------------+---------------+-------+---------------------------+
| gtr                        | gra.  | 0.78 (0.0)    | 0.51 | 35.27 (1.9)    | -0.02 (0.0)   | 0.00   | 4023.67 (26.1) | 0.87 (0.0)    | 0.00  | 3964.83 (26.1)$^\dagger$  |
|                            +-------+---------------+------+----------------+---------------+--------+----------------+---------------+-------+---------------------------+
|                            | gte   | 0.84 (0.0)    | 0.12 | 279.56 (6.9)   | 0.08 (0.0)    | 0.00   | 4180.47 (26.2) | 0.87 (0.0)    | 0.00  | 4088.97 (26.2)$^\ddagger$ |
|                            +-------+---------------+------+----------------+---------------+--------+----------------+---------------+-------+---------------------------+
|                            | ste.  | 0.72 (0.0)    | 0.27 | 127.92 (4.4)   | 0.00 (0.0)    | 0.00   | 4296.04 (26.1) | 0.76 (0.0)    | 0.00  | 4095.11 (26.1)$^\ddagger$ |
|                            +-------+---------------+------+----------------+---------------+--------+----------------+---------------+-------+---------------------------+
|                            | e5    | 0.82 (0.0)    | 0.01 | 1413.80 (18.3) | 0.09 (0.0)    | 0.00   | 4064.47 (26.2) | 0.93 (0.0)    | 0.00  | 4010.13 (26.1)$^\dagger$  |
+----------------------------+-------+---------------+------+----------------+---------------+--------+----------------+---------------+-------+---------------------------+
| gte                        | gra.  | 0.73 (0.0)    | 0.09 | 342.15 (7.8)   | 0.01 (0.0)    | 0.00   | 3946.19 (25.8) | 0.87 (0.0)    | 0.00  | 3802.92 (25.9)$^\dagger$  |
|                            +-------+---------------+------+----------------+---------------+--------+----------------+---------------+-------+---------------------------+
|                            | gtr   | 0.69 (0.0)    | 0.12 | 256.63 (6.4)   | 0.08 (0.0)    | 0.00   | 4229.90 (26.2) | 0.69 (0.0)    | 0.00  | 4094.02 (26.1)$^\ddagger$ |
|                            +-------+---------------+------+----------------+---------------+--------+----------------+---------------+-------+---------------------------+
|                            | ste.  | 0.85 (0.0)    | 1.00 | 1.00 (0.0)     | 0.56 (0.0)    | 1.00   | 1.00 (0.0)     | 1.00 (0.0)    | 1.00  | 1.00 (0.0)$^*$            |
|                            +-------+---------------+------+----------------+---------------+--------+----------------+---------------+-------+---------------------------+
|                            | e5    | 0.86 (0.0)    | 0.54 | 17.71 (0.9)    | 0.69 (0.0)    | 0.98   | 1.04 (0.0)     | 1.00 (0.0)    | 1.00  | 1.00 (0.0)$^*$            |
+----------------------------+-------+---------------+------+----------------+---------------+--------+----------------+---------------+-------+---------------------------+
| ste.                       | gra.  | 0.77 (0.0)    | 0.14 | 221.95 (5.9)   | -0.01 (0.0)   | 0.00   | 3951.42 (25.9) | 0.87 (0.0)    | 0.01  | 3776.52 (26.0)$^\dagger$  |
|                            +-------+---------------+------+----------------+---------------+--------+----------------+---------------+-------+---------------------------+
|                            | gtr   | 0.75 (0.0)    | 0.56 | 17.70 (1.0)    | 0.00 (0.0)    | 0.00   | 4339.83 (26.2) | 0.70 (0.0)    | 0.00  | 4093.61 (26.1)$^\ddagger$ |
|                            +-------+---------------+------+----------------+---------------+--------+----------------+---------------+-------+---------------------------+
|                            | gte   | 0.91 (0.0)    | 1.00 | 1.00 (0.0)     | 0.56 (0.0)    | 1.00   | 1.00 (0.0)     | 1.00 (0.0)    | 1.00  | 1.00 (0.0)$^*$            |
|                            +-------+---------------+------+----------------+---------------+--------+----------------+---------------+-------+---------------------------+
|                            | e5    | 0.85 (0.0)    | 0.51 | 26.33 (1.2)    | 0.35 (0.0)    | 0.59   | 12.68 (0.6)    | 0.93 (0.0)    | 1.00  | 1.00 (0.0)$^\dagger$      |
+----------------------------+-------+---------------+------+----------------+---------------+--------+----------------+---------------+-------+---------------------------+
| e5                         | gra.  | 0.78 (0.0)    | 0.21 | 151.09 (4.6)   | 0.02 (0.0)    | 0.00   | 4008.10 (25.9) | 0.87 (0.0)    | 0.00  | 3932.58 (26.2)$^\dagger$  |
|                            +-------+---------------+------+----------------+---------------+--------+----------------+---------------+-------+---------------------------+
|                            | gtr   | 0.66 (0.0)    | 0.01 | 1029.64 (14.9) | 0.09 (0.0)    | 0.00   | 4032.85 (26.2) | 0.82 (0.0)    | 0.00  | 4010.06 (26.1)$^\dagger$  |
|                            +-------+---------------+------+----------------+---------------+--------+----------------+---------------+-------+---------------------------+
|                            | gte   | 0.87 (0.0)    | 0.60 | 32.59 (2.6)    | 0.69 (0.0)    | 0.98   | 1.09 (0.0)     | 1.00 (0.0)    | 1.00  | 1.00 (0.0)$^*$            |
|                            +-------+---------------+------+----------------+---------------+--------+----------------+---------------+-------+---------------------------+
|                            | ste.  | 0.75 (0.0)    | 0.46 | 32.12 (1.4)    | 0.35 (0.0)    | 0.86   | 2.49 (0.1)     | 0.86 (0.0)    | 1.00  | 1.01 (0.0)$^\dagger$      |
+----------------------------+-------+---------------+------+----------------+---------------+--------+----------------+---------------+-------+---------------------------+
|                            |       |               |      |                |               |        |                |               |       |                           |
+----------------------------+-------+---------------+------+----------------+---------------+--------+----------------+---------------+-------+---------------------------+

: Out-of-distribution translations on MIMIC (with baselines): `vec2vec` models trained on NQ and evaluated on an 8192-record subset of MIMIC. The rank metric varies from 1 to 8192, thus 4096 corresponds to a random ordering. Standard errors are shown in parentheses. Symbols denote the lowest-rank solver: Earth Mover's Distance$^*$, Sinkhorn$^\dagger$ and Gromov-Wasserstein$^\ddagger$
:::

# Ablations {#sec:ablations}

## Ablating components of our method

We ablate our method subtractively, measuring the key metrics after removing individual components of our algorithm (described in [3](#sec:architecture){reference-type="ref+Label" reference="sec:architecture"}). [7](#tab:loss_ablation){reference-type="ref+Label" reference="tab:loss_ablation"} shows that each component appears to be *critical* to building good translations. While in each setting, `vec2vec`'s $\cos(\cdot)$ is higher than the naïve baselines, the Top-1 accuracies and ranks imply that ablated `vec2vec` translations are at best only slightly better than the baseline and do not preserve the geometry of the vector space.

::: {#tab:loss_ablation}
  Method                    $\cos(\cdot)$   Top-1            Rank
  ----------------------- --------------- ------- ---------------
  `vec2vec`                    0.75 (0.0)    0.91      2.64 (0.1)
  Naïve Baseline               0.04 (0.0)    0.00   4084.15 (9.2)
  OT Baseline                  0.70 (0.0)    0.00   4078.45 (9.2)
  -- VSP loss                  0.58 (0.0)    0.00   4196.64 (9.2)
  -- CC loss                   0.50 (0.0)    0.00   3941.36 (9.3)
  -- latent GAN                0.49 (0.0)    0.00   3897.09 (9.5)
  -- VSP *and* CC loss         0.47 (0.0)    0.00   3365.24 (9.3)
  -- hyperparam. tuning        0.50 (0.0)    0.00   4011.73 (9.3)
                                                  

  : gte $\to$ gtr translators trained without individual components of our method on NQ and evaluated on a 65536-text subset of NQ (chunked in batches of 8192). The rank metric varies from 1 to 8192, thus 4096 corresponds to a random ordering. Standard errors are shown in parentheses.
:::

## Amount of data needed to learn translation

::: {#tab:n_data}
        $N$   $\cos(\cdot)$   Top-1            Rank
  --------- --------------- ------- ---------------
    1000000      0.75 (0.0)    0.92      2.73 (0.2)
      10000      0.57 (0.0)    0.01   1462.21 (20.)
      50000      0.74 (0.0)    0.81      3.91 (0.6)
     100000      0.74 (0.0)    0.85      4.52 (0.4)
     500000      0.75 (0.0)    0.92      2.73 (0.2)
                                    

  : gte $\to$ gtr translators trained with different amounts of gte data: `vec2vec` models trained on NQ and evaluated an 8192-record subset of NQ. The rank metric varies from 1 to 8192, thus 4096 corresponds to a random ordering. Standard errors are shown in parentheses.
:::

In our main experiments, we use 1M-point subsets of NQ to train our `vec2vec` models. Now, we train the gtr-gte `vec2vec` with 1M gtr embeddings but fewer gte embeddings. [8](#tab:n_data){reference-type="ref+Label" reference="tab:n_data"} shows that translators still learn something (i.e. are better than random), and only 50K embeddings are sufficient to learn a translation that is *almost* as good. Even with 10K embeddings, our method still learns a non-trivial translation. Translation performance generally seems to improve with the amount of training data, however.

# Prompt for measuring information extraction {#sec:prompt}

[\[fig:enron_heatmap\]](#fig:enron_heatmap){reference-type="ref+Label" reference="fig:enron_heatmap"} utilizes an LLM judge to determine whether the approximate inversion (using [@zhang2025universalzeroshotembeddinginversion]) of a translated email embedding leaks information about the original email. [6](#fig:prompt){reference-type="ref+Label" reference="fig:prompt"} shows our prompt to the GPT-4o judge:

<figure id="fig:prompt" data-latex-placement="ht">
<div class="callout">
<p><strong>Original email:</strong> {ground_truth}</p>
<p><strong>Reconstructed email:</strong> {generation}.</p>
<p>Does the reconstructed email leak any information about the original email?</p>
<p>Answer with only ‘yes’ or ‘no’.</p>
</div>
<figcaption>The prompt given to the LLM judge.</figcaption>
</figure>

[^1]: Prior work has successfully translated *word* embeddings between languages, typically relying on overlapping vocabularies across languages. In contrast, we translate embeddings of entire sequences between model spaces.

[^2]: Our code is available [on GitHub.](https://github.com/rjha18/vec2vec/)
