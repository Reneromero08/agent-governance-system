maketitle thanks aketitle

# Introduction {#sec:introduction}

Many complex problems can be decomposed into smaller, independent sub-problems, making them naturally suited for parallel computation. For example, we can compute the value of a mathematical expression by evaluating its sub-expressions independently and combining the results (see [2](#fig:expression-tree){reference-type="ref+label" reference="fig:expression-tree"}). However, dominant autoregressive language models (LMs) tackle these problems sequentially. Methods like chain of thought (CoT), for instance, generate solutions one step at a time, failing to capitalize on the underlying parallel structure. Parallel generation by masked diffusion models (MDMs) offers a compelling alternative. Recent advances have positioned MDMs as a viable contender to autoregressive LMs in language modeling, code generation, and even molecule design [@Lou2024DiscreteDM; @zhang2025surveyparalleltextgeneration; @sunSpeedAlwaysWins2025]. However, the fundamental reasoning capabilities of MDMs remain poorly understood, which limits the extent to which we can leverage their potential and apply them to appropriate tasks. This work bridges that gap by providing the first formal characterization of the expressivity of MDMs, clarifying their fundamental computational strengths and weaknesses.

<figure id="fig:figure-1" data-latex-placement="ht">

<figcaption> A summary of masked diffusion model expressivity. The colored nodes in the top row correspond to transformer-based computational models and the languages recognized by them. The gray nodes in the bottom row correspond to languages in (<span class="math inline">${{\mathtt{L}}\textnormal{-uniform}}$</span>) classical complexity classes. <span class="math inline">ùí≥‚ÄÑ‚Ü™‚ÄÑùí¥</span> indicates the inclusion of <span class="math inline">ùí≥</span> in <span class="math inline">ùí¥</span>, and <span class="math inline">ùí≥‚ÄÑ‚ÜîÔ∏é‚ÄÑùí¥</span> indicates equality. Dashed arrows represent strict inclusions. <span style="color: ETHRed">Red arrows</span> denote novel results. <span class="math inline"><code>R</code><code>e</code><code>g</code></span> refers to all regular languages. </figcaption>
</figure>

<figure id="fig:expression-tree" data-latex-placement="ht">

<figcaption> <strong>Two strategies for solving a mathematical expression</strong>. <em>(a) <span style="color: ETHGreen">Parallel</span></em>: Parallel computation of intermediate values (three steps). <em>(b) <span style="color: ETHRed">Sequential</span></em>: Step-by-step generation (eleven steps). </figcaption>
</figure>

Our analysis builds upon the study of LM expressivity---the formal characterization of the problems whose solution the neural architecture of an LM (with appropriate parameters) can express. While this field comprehensively describes autoregressive LMs, its findings do not directly apply to MDMs due to their fundamentally different, non-sequential processing of text, leaving the theoretical studies of these two paradigms largely in isolation. Prior theoretical work on MDMs has focused on the limitations of their factorized backward process and its convergence properties. This research has shown that while MDMs can approximate $n$-gram LMs with constantly many denoising steps, the number of steps must grow linearly with the string length even for simple LMs such as probabilistic regular languages [@feng2025theoreticalbenefitlimitationdiffusion; @li2025convergencetheorydiffusionlanguage].[^2] However, the asymptotic nature of these findings, together with the strict assumptions on the theoretical model, makes it difficult to draw concrete conclusions about the practical reasoning capabilities of MDMs, leaving a critical gap: A theoretical framework for MDMs that is both

::: enumerate*
formally rigorous enough to provide a comprehensive picture of how MDMs can use the combination of parallelism and iterative refinement for formal reasoning, and

faithful to how they are implemented in practice.
:::

Our work introduces such a framework, providing a tight characterization of their reasoning capabilities.

Concretely, we connect MDMs implemented as finite-precision transformers with logarithmically-growing model width to known reasoning paradigms of CoT[@wei2022chainofthought], looping [@dehghani2019universaltransformers], and pause tokens [@lanham2023measuringfaithfulnesschainofthoughtreasoning]. For example, we formalize:

::: {#takeaway:takeaway-1 .takeaway}
**Takeaway 1** (*[\[thm:mdms-can-simulate-cot,thm:cot-can-simulate-mdms\]](#thm:mdms-can-simulate-cot,thm:cot-can-simulate-mdms){reference-type="ref+label" reference="thm:mdms-can-simulate-cot,thm:cot-can-simulate-mdms"}*). *MDMs can perform CoT reasoning with some overhead and the MDM denoising process can be (inefficiently) simulated by CoT by generating one symbol at a time.*
:::

We also show how MDMs can solve complex problems by solving easier sub-problems in parallel, which makes them inherently more efficient than CoT on parallelizable problems.

::: {#takeaway:takeaway-2 .takeaway}
**Takeaway 2** (*[\[thm:cot-mdm-separation\]](#thm:cot-mdm-separation){reference-type="ref+label" reference="thm:cot-mdm-separation"}*). *MDMs are provably more efficient at parallelizable problems than CoT.*
:::

We refer to the fact that CoT*cannot* take advantage of this parallelism as the **sequentiality bottleneck** of CoT. [2](#takeaway:takeaway-2){reference-type="ref+label" reference="takeaway:takeaway-2"} highlights the potential efficiency gains of *parallel* CoT that generates multiple symbols at once. Our analysis, in fact, identifies a tighter and more natural connection between MDMs and this variant, which we refer to as pCoT. The parallelism of MDMs also facilitates a close connection with *looped* and *padded* transformers, where looping naturally maps to denoising steps and padding tokens to the generated tokens of an MDM. We find the class of problems solvable by MDMs is, in fact, precisely equivalent to that solvable by padded looped transformers.

::: {#takeaway:takeaway-3 .takeaway}
**Takeaway 3** (*[\[thm:lts-mdms\]](#thm:lts-mdms){reference-type="ref+label" reference="thm:lts-mdms"}*). *MDMs are equivalent to PLTs.*
:::

These connections allow us to leverage known characterizations of CoT and PLTs together with classical complexity theory results to understand the fundamental capabilities and limits of MDMs[@li2024chain; @saunshi2025reasoninglatentthoughtspower; @london2025pausetokensstrictlyincrease; @svete2025exactexpressivepowerfiniteprecision]. For example, with ${{N}}$ representing the length of the input string and with ${{{{{{\mathtt{AC}}}^{\mathtt{d}}}}}}$ for $d \in {{\mathbb{N}}}$ being the standard class of Boolean circuits with `AND`, `OR`, and `NOT` gates and depth ${{\mathcal{O}}}(\log^d{{N}})$, we obtain:

::: {#takeaway:takeaway-4 .takeaway}
**Takeaway 4** (*[\[thm:regular-languages-in-mdm-efficient\]](#thm:regular-languages-in-mdm-efficient){reference-type="ref+label" reference="thm:regular-languages-in-mdm-efficient"}*). *MDMs with $\log{{N}}$ denoising steps can recognize regular languages.*
:::

::: {#takeaway:takeaway-5 .takeaway}
**Takeaway 5** (*[\[cor:mdms-with-constant-steps,cor:mdms-with-polylog-steps\]](#cor:mdms-with-constant-steps,cor:mdms-with-polylog-steps){reference-type="ref+label" reference="cor:mdms-with-constant-steps,cor:mdms-with-polylog-steps"}*). *For $d \in {{\mathbb{N}}}$, MDMs with ${{\mathcal{O}}}(\log^d{{N}})$ denoising steps and ${{\mathtt{poly}}\mleft({{N}}\mright)}$ output space are equivalent to ${{{{{{\mathtt{AC}}}^{\mathtt{d}}}}}}$. As $d \to \infty$, this yields ${{\mathtt{NC}}}$, the class of all parallelizable problems. With *constantly* many denoising steps ($d = 0$), MDMs are equivalent to the limited class ${{{{{{\mathtt{AC}}}^{\mathtt{0}}}}}}$, i.e., they are no more powerful than standard transformers and, e.g., cannot recognize all regular languages.*
:::

We summarize these takeaways and their relationships in [1](#fig:figure-1){reference-type="ref+label" reference="fig:figure-1"}. Our proofs emphasize the affordances and difficulties in solving reasoning problems with MDMs. We find, for example, that the discrete nature of text generation, which serves as a communication channel between individual denoising steps, limits the amount of information that can be passed between steps. This necessitates extra output space to store the intermediate computations and is analogous to the discrepancy between fixed-precision and log-precision transformers [@li2024chain], and the difference between the classes ${{\mathtt{AC}}}$ and ${{\mathtt{TC}}}$, the class of circuits with threshold gates. We find that *positional encodings* that carry information not computable by transformers themselves are crucial in locating this information. We also observe that, while unmasked attention makes MDM attention patterns flexible, it complicates left-to-right processing, which is often natural in human language---we find that exact implementation of causal masking requires quadratically more output space.

# Preliminaries {#sec:preliminaries}

This section introduces the preliminaries and notation used throughout the paper. We reserve the main text for the high-level intuitions and defer technical details to [7](#app:preliminaries){reference-type="ref+label" reference="app:preliminaries"}.

## Transformers

We analyze (un)masked transformers for string generation and classification. We present the full model in [7.4](#app:transformers){reference-type="ref+label" reference="app:transformers"} and focus here on the most relevant aspects.

**Transformer families.** We study *finite precision* transformers where the value of each parameter and activation is represented with a fixed number of bits. We allow the model *width*, i.e., the size of the contextual representations, to grow logarithmically with the input length ${{N}}$. This is a standard assumption in the literature on transformer expressivity [@li2024chain] since it is necessary and sufficient for the model to uniquely identify input positions, and aligns with modern implementations of *quantized* but *wide* transformers. The growing width results in a separate transformer ${{\mathcal{T}}}_{{N}}$ for each ${{N}}$, yielding a **family** $\{{{\mathcal{T}}}_{{N}}\}_{{{N}}\in {{\mathbb{N}}}}$ of transformers. We enforce ${{\mathtt{L}}\textnormal{-uniformity}}$ in the family by requiring an associated Turing machine that constructs ${{\mathcal{T}}}_{{N}}$ in ${{{{\mathcal{O}}}(\log{{N}})}}$ space (cf. [7.4](#app:transformers){reference-type="ref+label" reference="app:transformers"}).

**(Parallel) CoT transformers.** CoT reasoning enables sequential processing by solving problems in multiple steps [@wei2022chainofthought]. It is an integral part of today's popular "reasoning" models and substantially increases transformers' expressivity [@li2024chain; @merrill2024the]. We define our idealization of CoT transformers in [7.4](#app:transformers){reference-type="ref+label" reference="app:transformers"}, including *parallel* CoT transformers, which predict ${P}' \in {{\mathbb{N}}}$ symbols in parallel at each time step, enabling some parallelism if the task allows it.[^3] We denote the classes of CoT and parallel CoT transformers by ${\mathtt{CoT}}$ and ${\mathtt{pCoT}}$, respectively.

**Padded looped transformers (PLTs).** Looped transformers repeatedly apply a fixed block of transformer layers to the input [@dehghani2019universaltransformers]. This dynamically increases the depth of the model, enabling more complex reasoning, and does not increase the model size, as the same blocks are reused, thus reducing the memory footprint and computational cost [@bae2025mixtureofrecursionslearningdynamicrecursive]. Such reasoning steps include both sequential and parallel processing, resulting in both efficiency as well as depth of the reasoning process. Padded transformers additionally pad the input with blank symbols, which can be used to perform additional computations *in parallel*. This additional padding space is analogous to increasing the circuit width in circuit complexity. We additionally provide PLTs with external noise applied to the residual stream at each step, which enables stochastic computations (cf.¬†[7.6](#app:looped-padded-transformers){reference-type="ref+label" reference="app:looped-padded-transformers"}). We denote the class of padded looped transformers by ${\mathtt{PLT}}$.

**Transformer LMs.** An **alphabet** ${{\Sigma}}$ is a finite, non-empty set of **symbols**. Its Kleene closure is ${{{{{{\Sigma}}^{*}}}}}\mathrel{\stackrel{\textnormal{\tiny def}}{=}}\bigcup_{n=0}^{\infty} {{\Sigma}}^n$, the set of all strings. A **language model** is a distribution over ${{{{{{\Sigma}}^{*}}}}}$. Most LMs are **autoregressive**---they define **next-symbol** distributions ${{\overset{\curvearrowright}{{{p}}}}(\cdot \mid {{\boldsymbol{w}}})}$ over ${{\overline{{{\Sigma}}}}}\mathrel{\stackrel{\textnormal{\tiny def}}{=}}{{\Sigma}}\cup {\left\{ {{\textsc{eos}}} \right\}}$ for ${{\boldsymbol{w}}}\in {{{{\Sigma}}^{*}}}$, where ${{\textsc{eos}}}\notin {{\Sigma}}$ is the [e]{.underline}nd-[o]{.underline}f-[s]{.underline}tring symbol. A transformer-based LM computes ${{\overset{\curvearrowright}{{{p}}}}(\cdot \mid {{\boldsymbol{w}}})}$ by linearly transforming the contextual representation of the final symbol to the logits of a distribution over ${{\overline{{{\Sigma}}}}}$. Moreover, contextual representations can be used for **infilling**---predicting symbols at masked positions. Infilling probabilities ${{{{p}}^{\downarrow}}(\cdot \mid {{\boldsymbol{w}}})}$ at masked positions in ${{\boldsymbol{w}}}\in {{{{{\Sigma}}_{{\texttt{m}}}}^{*}}}$ are distributions over ${{\Sigma}}$, where ${\texttt{m}}\notin {{\Sigma}}$ is the mask symbol and ${{{\Sigma}}_{{\texttt{m}}}}\mathrel{\stackrel{\textnormal{\tiny def}}{=}}{{\Sigma}}\cup {\left\{ {\texttt{m}} \right\}}$.

**Transformers and formal languages.** Plenty of work describes transformer capabilities and limitations with formal languages [@strobl-etal-2024-formal]. These studies typically frame transformers as **language recognizers**, i.e., classifiers that decide whether a string ${{\boldsymbol{w}}}\in {{{{\Sigma}}^{*}}}$ belongs to some formal language ${\mathcal{L}}\subseteq {{{{\Sigma}}^{*}}}$ [@butoi2025training]. String membership is usually *deterministic* and can be formalized by determinizing the LM defined by a transformer: The next-symbol and infilling probabilities are used to decode the most probable symbol or decision.[^4] The final prediction of ${\mathbbm{1}} \left\{ {{\boldsymbol{w}}}\in {\mathcal{L}} \right\} \in \{{\texttt{1}}, {\texttt{0}}\}$ can be made

::: enumerate*
*in a single pass* by classifying based on the contextual representation of a particular symbol in the string, analogous to classifying based on the `CLS` symbol in BERT [@devlin-etal-2019-bert], or

after *"reasoning"*, i.e., solving the problem in multiple time steps. In this case, the transformer's prediction is only made after a sequence of intermediate predictions that augment its computation and help the final decision. This is analogous to using CoT reasoning for string recognition and can be thought of as simulating a Turing machine with each step of the CoT process.
:::

Particularly fruitful has been the study of transformers as **Boolean circuits**. In particular, our idealization of transformers falls under ${{{{{{\mathtt{AC}}}^{\mathtt{0}}}}}}$ circuits [@li2024chain]---Boolean circuits of constant depth, polynomial size, and with `AND`, `OR`, and `NOT` gates of unbounded fan-in---and captures the entire class if padding is allowed [@london2025pausetokensstrictlyincrease]. Other idealizations of transformers can compute functions outside of ${{{{{{\mathtt{AC}}}^{\mathtt{0}}}}}}$ [@li2024chain; @merrill2025exactexpressivepowertransformers] but remain in ${{{{{{\mathtt{TC}}}^{\mathtt{0}}}}}}$, the class of *threshold circuits* which add threshold gates (which determine whether the number of inputs exceeds some threshold) to ${{{{{{\mathtt{AC}}}^{\mathtt{0}}}}}}$ circuits. [\[app:circuit-complexity,app:transformers\]](#app:circuit-complexity,app:transformers){reference-type="ref+label" reference="app:circuit-complexity,app:transformers"} provide more details on circuit classes and their relation to transformers.

## Masked diffusion language models {#sec:diffusion-language-models}

**Discrete diffusion LMs** define a distribution over ${{{{{{\Sigma}}^{*}}}}}$ by progressively denoising noisy strings sampled from some fixed distribution. Formally, they define a **forward (noising) process** and a **reverse (denoising) process**. The forward process defines a Markov chain over strings that iteratively corrupts them. Common examples include replacing symbols uniformly at random (uniform diffusion) or masking them with the mask symbol ${\texttt{m}}$ (**masked diffusion models**, MDMs). The latter is the focus of this work. In this setting, the forward process starts from an initial string ${{\boldsymbol{w}}}^{(0)} \in {{{{{{\Sigma}}^{*}}}}}$ of some pre-determined length ${P}$ and, at each of the ${T}$ (discrete) steps, independently masks symbols with probability determined by a masking schedule ${{\alpha}\mleft(\frac{t}{{T}}\mright)} \in [0, 1]$:

::: minipage
$$\begin{equation*}
      {q}_{t|0}({{\boldsymbol{w}}}^{(t)} \mid {{\boldsymbol{w}}}^{(0)}) = \prod_{{{n}}=1}^{{{N}}} {q}_{t|0}({{w}}^{(t)}_{{n}}\mid {{w}}^{(0)}_{{n}}),
\end{equation*}$$
:::

::: minipage
$$\begin{equation*}
      {q}_{t|0}({{w}}^{(t)}_{{n}}\mid {{w}}^{(0)}_{{n}}) =
      \begin{cases}
         1 - {\alpha}(\frac{t}{{T}}), &\textbf{if }{{w}}^{(t)}_{{n}}= {\texttt{m}}\\
         {\alpha}(\frac{t}{{T}}), &\textbf{otherwise }
      \end{cases}.
\end{equation*}$$
:::

The masking schedule is set such that ${{\alpha}\mleft(0\mright)} = 1$ (no masking at the start) and ${{\alpha}\mleft(1\mright)} = 0$ (fully masked at the end, meaning that the noise distribution is the Dirac delta on the fully masked string).

Starting from the fully masked input, the reverse process ${q}_{0|{T}}$ inverts the forward process ${q}_{{T}|0}$ by

::: enumerate*
uniformly selecting some positions to unmask, and

sampling the chosen unmasked symbols.
:::

After ${T}$ denoising steps, ${q}_{0|{T}}$ produces a string ${{\boldsymbol{w}}}^{(0)}$ sampled from the LM defined by the diffusion process. It is this reverse process that is learned from data. Its analytical form is generally intractable, so one usually models a parameterized approximation of a single denoising step ${\widehat{q}}_{t - 1|t}({{\boldsymbol{w}}}^{(t-1)} \mid {{\boldsymbol{w}}}^{(t)})$, typically implemented as a transformer, that *factorizes* across positions: $$\begin{equation}
 \label{eq:factorized-reverse-process}
   {\widehat{q}}_{t - 1|t}({{\boldsymbol{w}}}^{(t-1)} \mid {{\boldsymbol{w}}}^{(t)}) = \prod_{{{n}}=1}^{{N}}{\widehat{q}}_{t - 1|t}({{w}}^{(t-1)}_{{n}}\mid {{\boldsymbol{w}}}^{(t)})
\end{equation}$$ [\[eq:factorized-reverse-process\]](#eq:factorized-reverse-process){reference-type="ref+label" reference="eq:factorized-reverse-process"} enables *parallel generation* but ignores inter-symbol dependencies at each denoising step.

Much of the existing work on MDM expressivity analyzes the convergence of ${\widehat{q}}_{t - 1|t}$ to ${q}_{0|{T}}$ [@li2025convergencetheorydiffusionlanguage; @chen2024convergenceanalysisdiscretediffusion; @feng2025theoreticalbenefitlimitationdiffusion]. Studying convergence properties usually requires assuming uniform unmasking and a good approximation of the ground-truth model [e.g., @li2025convergencetheorydiffusionlanguage; @chen2024convergenceanalysisdiscretediffusion; @feng2025theoreticalbenefitlimitationdiffusion; @liu2025perfectdiffusionmathsftc0].

::: {#assumption:uniform-unmasking .assumption}
**Assumption 1** (Uniform unmasking). *The **uniform unmasking assumption** states that ${\widehat{q}}_{t - 1|t}$ generates strings ${{\boldsymbol{w}}}^{(t - 1)}$ from ${{\boldsymbol{w}}}^{(t)}$ by uniformly selecting positions to unmask.*
:::

::: {#assumption:perfect-approximation .assumption}
**Assumption 2** (Perfect approximation). *Let ${{q}}_{t - 1|t}$ be the backward processes of an MDM and ${{{p}}^{\downarrow}}$ a transformer-based LM. The **perfect approximation assumption** states that ${{{q}}}_{t - 1|t}({{w}}^{(t - 1)}_{{n}}\mid {{\boldsymbol{w}}}^{(t)}) = {{{{p}}^{\downarrow}}({{w}}^{(t - 1)}_{{n}}\mid {{\boldsymbol{w}}}^{(t)})}$ for all ${{\boldsymbol{w}}}^{(t)} \in {{{{{\Sigma}}_{{\texttt{m}}}}^{*}}}$, ${{w}}^{(t - 1)}_{{n}}\in {{\Sigma}}$, and ${{n}}\in \{1, \ldots, |{{\boldsymbol{w}}}^{(t)}|\}$.*
:::

In words, [2](#assumption:perfect-approximation){reference-type="ref+label" reference="assumption:perfect-approximation"} states that the transformer perfectly models *all* conditional distributions of the diffusion process.[^5] While this seems necessary, the following observation, proved in [10](#app:proofs){reference-type="ref+label" reference="app:proofs"}, shows that [\[assumption:perfect-approximation,assumption:uniform-unmasking\]](#assumption:perfect-approximation,assumption:uniform-unmasking){reference-type="ref+label" reference="assumption:perfect-approximation,assumption:uniform-unmasking"} severely limit the class of functions that the model can compute.

::: restatable
theoremperfectApproximationTheorem []{#thm:perfect-approximation-limitation label="thm:perfect-approximation-limitation"} If [\[assumption:perfect-approximation,assumption:uniform-unmasking\]](#assumption:perfect-approximation,assumption:uniform-unmasking){reference-type="ref+label" reference="assumption:perfect-approximation,assumption:uniform-unmasking"} hold for an LM ${{p}}$, ${{p}}$ cannot compute non-${{{{{{\mathtt{AC}}}^{\mathtt{0}}}}}}$ functions.[^6]^,^[^7]
:::

By assuming that the MDM is unable to choose which positions to unmask, the model has no choice in which sub-problems to solve first, which ignores the possibility of problem decomposition and requires the model to be equally good at solving *any* sub-problem---including predicting the final answer based on the input directly (with no reasoning steps). This implies that the problem is solvable in a single prediction step of a transformer. However, the expressivity of a single transformer pass is limited---[\[thm:perfect-approximation-limitation\]](#thm:perfect-approximation-limitation){reference-type="ref+label" reference="thm:perfect-approximation-limitation"} uses the fact that fixed-depth transformers lie in ${{{{{{\mathtt{AC}}}^{\mathtt{0}}}}}}$ [@li2024chain]. However, not *all* conditional probabilities have to be known to be able to solve algorithms in few steps. Intuitively, by *choosing* to solve *simple* subproblems with non-random unmasking, an MDM can avoid the difficult parts. For example, given the current arithmetic expression, one only has to predict the next set of simplifications---which are simple functions of the current expression. This motivates us to *loosen* [\[assumption:uniform-unmasking,assumption:perfect-approximation\]](#assumption:uniform-unmasking,assumption:perfect-approximation){reference-type="ref+label" reference="assumption:uniform-unmasking,assumption:perfect-approximation"}, which we do in our idealization of an MDM.

### Our idealization of masked diffusion models {#sec:idealization}

We aim to understand the expressivity of the *reverse process*. To this end, we introduce an idealization that captures its key aspects---iterative unmasking and infilling---and provides a principled lens for understanding the expressivity of practical MDMs and comparing them to well-known paradigms such CoT. Here, we describe the high-level ideas; see [7.7](#app:idealization){reference-type="ref+label" reference="app:idealization"} for the full formal model.

We formalize the reverse process with two components: A **planner** that *decides* which positions to unmask at each step and a **predictor** that samples the symbols at the unmasked positions. This loosens [1](#assumption:uniform-unmasking){reference-type="ref+label" reference="assumption:uniform-unmasking"} and generalizes standard MDMs in which the planner is implicitly defined by choosing the positions to unmask uniformly at random. It also mirrors popular MDM implementations that generate text by selecting a subset of masked positions---for example, based on model confidence or according to a learned policy---and predicting the symbols conditioned on the current partially unmasked string [@ghazvininejad-etal-2019-mask; @peng2025pathplanningmaskeddiffusion; @zheng2024reparameterizeddiscretediffusionmodel; @liu2025thinkgeneratediscretediffusion; @kim2025trainworstplanbest; @benhamu2025acceleratedsamplingmaskeddiffusion].[^8] Discarding [1](#assumption:uniform-unmasking){reference-type="ref+label" reference="assumption:uniform-unmasking"} also sidesteps limitations of the position-wise independence in¬†[\[eq:factorized-reverse-process\]](#eq:factorized-reverse-process){reference-type="ref+label" reference="eq:factorized-reverse-process"}, a restriction that prevents MDMs with uniform unmasking matching even simple distributions exactly [@feng2025theoreticalbenefitlimitationdiffusion; @wu2025fastdllmtrainingfreeaccelerationdiffusion]. In reasoning problems with a deterministic sequential structure, the ability to decide what to unmask enables problem decomposition into a sequence of deterministic steps that can be solved in parallel.[^9] To connect our analysis to practical implementations, we assume that the planner and predictor are implemented as transformers.

We allow the planner to choose to *resample* already unmasked positions. This overcomes another key limitation---the inability to revert decisions and correct earlier mistakes---a challenge that is the focus of much recent research [@rutte2025generalized; @song2025seeddiffusionlargescalediffusion *inter alia*].[^10] While existing work focuses on reformulating the diffusion process to allow for resampling and refining the resulting training objectives, our idealized MDMs can be seen as a complementary approach that foregoes the complications of training and focuses on the expressivity of the generation process itself---it analyzes what is theoretically possible in a very targeted way when resampling is allowed.

We denote the class our idealized MDMs by ${\mathtt{MDM}}$. For a more detailed discussion, including the theoretical connection of our idealization to existing MDM variants, see [8](#app:theoretical-model){reference-type="ref+label" reference="app:theoretical-model"}.

# Theoretical results {#sec:theoretical-results}

This section describes two complementary characterizations of MDM expressivity: One based on their connection to PLTs and the other based on their ability to perform sequential CoT reasoning.[^11]

#### Notation.

Let ${T}= {T}({{N}})$ and ${P}= {P}({{N}})$ be functions of ${{N}}$. In the following, ${\mathtt{CoT}}[{T}]$ refers to languages recognized by families $\{{{\mathcal{T}}}_{{N}}\}_{{{N}}\in {{\mathbb{N}}}}$ of CoT transformers with at most ${T}$ steps. For ${\mathtt{C}}\in \{{\mathtt{pCoT}},{\mathtt{MDM}},{\mathtt{PLT}}\}$, ${\mathtt{C}}[{T}, {P}]$ refers to languages recognized by families in ${\mathtt{C}}$ with at most ${T}$ generation, denoising, or looping steps, respectively, and ${P}$ total output or padding symbols. ${{{{\widetilde{{{\mathcal{O}}}}}}({{N}})}}$ refers to big-${{\mathcal{O}}}$ notation that ignores logarithmic factors, and ${{\mathtt{poly}}\mleft({{N}}\mright)}$ to polynomial functions in ${{N}}$.

## MDMs are equivalent to padded looped transformers {#sec:mdms-and-lts}

Intuitively, PLTs closely resemble MDMs: Both iteratively refine information in parallel---MDMs by unmasking and predicting discrete symbols, and PLTs by updating the residual stream.[^12] [\[thm:lts-mdms\]](#thm:lts-mdms){reference-type="ref+label" reference="thm:lts-mdms"} formalizes this, assuming PLTs are supplied with external sampling noise (cf.¬†[2](#sec:preliminaries){reference-type="ref+label" reference="sec:preliminaries"}, ¬ß[7.6](#app:looped-padded-transformers){reference-type="ref" reference="app:looped-padded-transformers"}) like MDMs.

::: restatable
theoremLTsandMDMsThm []{#thm:lts-mdms label="thm:lts-mdms"}

$$\noindent\begin{minipage}[b]{0.4\textwidth}
         \begin{equation}
            {\mathtt{MDM}}[{T}, {P}] \subseteq {\mathtt{PLT}}[{T}, {P}]
         \end{equation}
      \end{minipage}
      \hfill
      \begin{minipage}[b]{0.05\textwidth}
         \centering
         and
      \end{minipage}
      \hfill
      \begin{minipage}[b]{0.45\textwidth}
         \begin{equation} \label{eq:mdm-simulates-lt}
            {\mathtt{PLT}}[{T}, {P}] \subseteq {\mathtt{MDM}}[{T}, ({{N}}+ {P}) \textcolor{ETHRed}{{{D}}}].
         \end{equation}
      \end{minipage}$$
:::

The simulation of a PLT by an MDM incurs a factor ${{D}}$ increase in the required padding (cf.¬†[\[eq:mdm-simulates-lt\]](#eq:mdm-simulates-lt){reference-type="ref+label" reference="eq:mdm-simulates-lt"}), where ${{D}}$ is the model width of the PLT. In our setting where ${{D}}= {{{{\mathcal{O}}}(\log {{N}})}}$, this implies that the classes of finite-precision MDMs and PLTs coincide up to a logarithmic factor in the padding length:

::: restatable
corollaryMDMEquivalenceCor []{#cor:mdm-lt-equivalence label="cor:mdm-lt-equivalence"} For any $K \geq 1$, $$\begin{equation}
      {\mathtt{MDM}}[{T}, {{{{\widetilde{{{\mathcal{O}}}}}}({{N}}^K)}}] = {\mathtt{PLT}}[{T}, {{{{\mathcal{O}}}({{N}}^K)}}].
\end{equation}$$
:::

The close connection between MDMs and PLTs allows us to leverage existing results about PLT expressivity to understand MDMs. @saunshi2025reasoninglatentthoughtspower [Thm. 5.1], for example, show that log-depth unpadded transformers can recognize regular languages. Combined with [\[cor:mdm-lt-equivalence\]](#cor:mdm-lt-equivalence){reference-type="ref+label" reference="cor:mdm-lt-equivalence"}, this implies:

::: restatable
corollaryRegularLanguagesInMDMLem []{#cor:mdms-can-simulate-regular-languages label="cor:mdms-can-simulate-regular-languages"} Regular languages are in ${\mathtt{MDM}}[\log{{N}}, {{N}}\log{{N}}]$.
:::

In fact, we obtain a tighter bound with a more specialized construction.[^13]

::: restatable
theoremRegularLanguagesInMDMEfficientThm []{#thm:regular-languages-in-mdm-efficient label="thm:regular-languages-in-mdm-efficient"} Regular languages are in ${\mathtt{MDM}}[\log{{N}}, {{N}}]$.
:::

@london2025pausetokensstrictlyincrease show that polynomially padded finite-precision PLTs with constantly many steps are equivalent to ${{\mathtt{L}}\textnormal{-uniform}}\ {{{{{{\mathtt{AC}}}^{\mathtt{0}}}}}}$, the class of ${{{{{{\mathtt{AC}}}^{\mathtt{0}}}}}}$ circuits that can be constructed by a logspace Turing machine (cf.¬†[7.2](#app:circuit-complexity){reference-type="ref+label" reference="app:circuit-complexity"}). We leverage this result together with [\[thm:lts-mdms\]](#thm:lts-mdms){reference-type="ref+label" reference="thm:lts-mdms"} to characterize the expressivity of MDMs with constantly many denoising steps.

::: restatable
corollaryMDMsConstantStepsCor []{#cor:mdms-with-constant-steps label="cor:mdms-with-constant-steps"} $$\begin{equation}
      {\mathtt{MDM}}[{{{{\mathcal{O}}}(1)}}, {{\mathtt{poly}}\mleft({{N}}\mright)}] = {\mathtt{PLT}}[{{{{\mathcal{O}}}(1)}}, {{\mathtt{poly}}\mleft({{N}}\mright)}] = {{\mathtt{L}}\textnormal{-uniform}}\ {{{{{{\mathtt{AC}}}^{\mathtt{0}}}}}}.
\end{equation}$$
:::

Allowing for a *constant* number of decoding steps therefore does not increase the expressivity of MDMs beyond the limited class ${{{{{{\mathtt{AC}}}^{\mathtt{0}}}}}}$. This further corroborates the empirical observation that the number of denoising steps must scale with the input complexity and complements existing results on MDM expressivity as a function of the number of denoising steps [@li2025convergencetheorydiffusionlanguage; @feng2025theoreticalbenefitlimitationdiffusion].

We can, however, increase expressivity with more denoising steps. @svete2025exactexpressivepowerfiniteprecision show that finite-precision PLTs with ${{{{\mathcal{O}}}(\log^d {{N}})}}$ steps and polynomial padding are equivalent to ${{\mathtt{L}}\textnormal{-uniform}}\ {{{{{{\mathtt{AC}}}^{\mathtt{d}}}}}}$, similar to the case of log-precision PLTs and ${{\mathtt{L}}\textnormal{-uniform}}\ {{{{{{\mathtt{TC}}}^{\mathtt{d}}}}}}$ [@merrill2025exactexpressivepowertransformers]. Thus:

::: restatable
corollaryMDMsPolylogStepsCor []{#cor:mdms-with-polylog-steps label="cor:mdms-with-polylog-steps"} $$\begin{equation}
      {\mathtt{MDM}}[{{{{\mathcal{O}}}(\log^d {{N}})}}, {{\mathtt{poly}}\mleft({{N}}\mright)}] = {\mathtt{PLT}}[{{{{\mathcal{O}}}(\log^d {{N}})}}, {{\mathtt{poly}}\mleft({{N}}\mright)}] = {{\mathtt{L}}\textnormal{-uniform}}\ {{{{{{\mathtt{AC}}}^{\mathtt{d}}}}}}
\end{equation}$$
:::

In particular, since ${{{{\mathtt{NC}}}^{\mathtt{d}}}} \subseteq {{{{\mathtt{AC}}}^{\mathtt{d}}}}$ for $d \in {{\mathbb{N}}}$ (where ${{{{\mathtt{NC}}}^{\mathtt{d}}}}$ denotes ${{{{{{\mathtt{AC}}}^{\mathtt{d}}}}}}$ circuits with bounded fan-in) [@Vollmer1999], we get that with polylogarithmic looping and polynomial padding, MDMs converge to ${{\mathtt{NC}}}$, the class of all parallelizable problems. [\[cor:mdms-with-polylog-steps\]](#cor:mdms-with-polylog-steps){reference-type="ref+label" reference="cor:mdms-with-polylog-steps"} also implies that regular languages are in ${\mathtt{MDM}}[\log {{N}}, {{\mathtt{poly}}\mleft({{N}}\mright)}]$; [\[thm:regular-languages-in-mdm-efficient\]](#thm:regular-languages-in-mdm-efficient){reference-type="ref+label" reference="thm:regular-languages-in-mdm-efficient"} provides a more efficient construction with a linear output space. Moreover, [\[cor:mdms-with-polylog-steps\]](#cor:mdms-with-polylog-steps){reference-type="ref+label" reference="cor:mdms-with-polylog-steps"} implies that ${{\mathtt{L}}\textnormal{-uniform}}\ {{{{{{\mathtt{NC}}}^{\mathtt{1}}}}}}\subseteq {\mathtt{MDM}}[\log {{N}}, {{\mathtt{poly}}\mleft({{N}}\mright)}]$.

## MDMs and CoT can (inefficiently) simulate each other {#sec:mdms-and-cot}

While the close connection between MDMs and PLTs provides a useful lens to analyze MDMs in terms of known complexity classes, the lack of practical PLT implementations makes it difficult to draw intuitive conclusions. We therefore complement [3.1](#sec:mdms-and-lts){reference-type="ref+label" reference="sec:mdms-and-lts"} by connecting MDMs and the more popular CoT paradigm, and consider how MDM can behave "autoregressively" like CoT. The intuition is simple: An MDM can simulate CoT by unmasking one symbol at a time, effectively mimicking the sequential generation. More precisely, we connect MDMs to pCoT since the latter's ability to generate multiple symbols at once naturally maps to MDMs' parallel generation. In particular:

::: restatable
theoremMDMsAtLeastAsPowerfulAsCoTFP []{#thm:mdms-can-simulate-cot label="thm:mdms-can-simulate-cot"} $$\begin{equation}
      {\mathtt{pCoT}}[{T}, {P}] \subseteq {\mathtt{MDM}}[{T}, {P}+ ({{N}}+ {P})^2]
\end{equation}$$
:::

The simulation incurs a quadratic blow-up in the padding length. This is not due to an inherent feature of the diffusion process but rather the challenge of simulating masked attention with unmasked one (cf.¬†[\[lem:unmask-conversion-fp\]](#lem:unmask-conversion-fp){reference-type="ref+label" reference="lem:unmask-conversion-fp"}).[^14] In particular, if the MDM transformer is causally masked, the blow-up disappears. Moreover, if the unmasked transformer can simulate masking more efficiently (with, for example, more expressive scoring functions), the blow-up can be alleviated.[^15] While [\[lem:unmask-conversion-fp\]](#lem:unmask-conversion-fp){reference-type="ref+label" reference="lem:unmask-conversion-fp"} could possibly be improved, it is interesting to note that the seemingly more general unmasked nature of MDMs might negatively impact their ability to align with human-oriented sequential processing captured by causal masking, which could provide a useful inductive bias for the masked models.[^16]

The other direction of [\[thm:mdms-can-simulate-cot\]](#thm:mdms-can-simulate-cot){reference-type="ref+label" reference="thm:mdms-can-simulate-cot"} shows that pCoT transformers can simulate MDMs.

::: restatable
theoremMDMsCoTChain []{#thm:cot-can-simulate-mdms label="thm:cot-can-simulate-mdms"} $$\begin{equation}
      {\mathtt{MDM}}[{T}, {P}] \subseteq {\mathtt{pCoT}}[{T}, {{L}}{T}({P}+ {{N}})],
\end{equation}$$ where ${{L}}$ is the number of layers in the transformer implementing the MDM.
:::

Again, the factor ${{L}}$ comes from the need to simulate unmasked attention in MDMs with causally masked transformers implementing pCoT(cf.¬†[\[lem:mask-conversion\]](#lem:mask-conversion){reference-type="ref+label" reference="lem:mask-conversion"}). However, here, the blow-up is only linear. The additional factor of ${T}$ comes from the pCoT having to write out every padding token after each denoising step, as the MDM can unmask tokens in an arbitrary order.

The results above can be summarized by the following sequence of inclusions.

::: restatable
corollaryMDMCoTSandwich []{#thm:mdms-cot-sandwich label="thm:mdms-cot-sandwich"} We have the following set of inclusions: $$\begin{align}
         {\mathtt{CoT}}[{T}] &= {\mathtt{pCoT}}[{T}, {T}] %
    \refstepcounter{equation}%
    \tag{\theequation \textcolor{black!50}{, \footnotesize{\cref{prop:cot-as-pcot}}}}
 \\ 
         &\subseteq {\mathtt{MDM}}[{T}, {T}+ ({{N}}+ {T})^2] %
    \refstepcounter{equation}%
    \tag{\theequation \textcolor{black!50}{, \footnotesize{\cref{thm:mdms-can-simulate-cot}}}}
 \\ 
         &\subseteq {\mathtt{pCoT}}[{T}, {{L}}{T}({{N}}+ {T}+ ({{N}}+ {T})^2)] %
    \refstepcounter{equation}%
    \tag{\theequation \textcolor{black!50}{, \footnotesize{\cref{thm:cot-can-simulate-mdms}}}}
 \\ 
         &\subseteq {\mathtt{CoT}}[{{L}}{T}({{N}}+ {T}+ ({{N}}+ {T})^2)] %
    \refstepcounter{equation}%
    \tag{\theequation \textcolor{black!50}{, \footnotesize{\cref{prop:cot-as-pcot}}}}
 \\
         &\subseteq {\mathtt{CoT}}[{{L}}{T}({{N}}+ {T}+ 1)^2].
      \end{align}$$ In particular, when ${T}\geq {{N}}$, we have ${\mathtt{CoT}}[{T}]
      \subseteq {\mathtt{MDM}}[{T}, {{{{\mathcal{O}}}({T}^2)}}]
      \subseteq {\mathtt{pCoT}}[{T}, {{{{\mathcal{O}}}({T}^3)}}]
      \subseteq {\mathtt{CoT}}[{{{{\mathcal{O}}}({T}^3)}}]$.
:::

[\[thm:mdms-cot-sandwich\]](#thm:mdms-cot-sandwich){reference-type="ref+label" reference="thm:mdms-cot-sandwich"} lower- and upper-bounds MDM expressivity based on the expressivity of CoT transformers. For example, MDM with polynomially many denoising steps remain within the class ${{\textnormal{\textsf{\small P}}}}$, the problems solvable in polynomial time by a non-random-access multitape Turing machine. This follows from the equivalence of CoT transformers with polynomially many steps to ${{\textnormal{\textsf{\small P}}}}$ [@li2024chain].

::: corollary
**Corollary 1** (MDMs with polynomially many denoising steps). *For any $K \geq 1$, we have that $$\begin{equation}
      {\mathtt{MDM}}[{T}, {{N}}^K] \subseteq {\mathtt{CoT}}[{{{{\mathcal{O}}}({T}{{N}}^K)}}],
\end{equation}$$ meaning that MDMs with polynomially many denoising steps remain in ${{\textnormal{\textsf{\small P}}}}$: $$\begin{equation}
      {\mathtt{MDM}}[{{\mathtt{poly}}\mleft({{N}}\mright)}, {{\mathtt{poly}}\mleft({{N}}\mright)}] \subseteq {\mathtt{CoT}}[{{\mathtt{poly}}\mleft({{N}}\mright)}] \subseteq {{\textnormal{\textsf{\small P}}}}.
\end{equation}$$*
:::

### A separation between MDMs and CoT transformers

@merrill2024the [@li2024chain] show that CoT transformers with logarithmically many decoding steps remain in ${{{{{{\mathtt{TC}}}^{\mathtt{0}}}}}}$. Combining this with [\[thm:regular-languages-in-mdm-efficient\]](#thm:regular-languages-in-mdm-efficient){reference-type="ref+label" reference="thm:regular-languages-in-mdm-efficient"}, the widely accepted assumption that ${{{{{{\mathtt{TC}}}^{\mathtt{0}}}}}}\neq {{{{{{\mathtt{NC}}}^{\mathtt{1}}}}}}$, and known ${{{{{{\mathtt{NC}}}^{\mathtt{1}}}}}}$-completeness of specific regular languages, we obtain the following separation in expressivity under a small (logarithmic) number of decoding steps. We term the inability of CoT to leverage parallelism the **sequentiality bottleneck** of CoT.

::: restatable
corollaryCoTMDMSeparationCor []{#thm:cot-mdm-separation label="thm:cot-mdm-separation"} $$\begin{equation}
      {\mathtt{CoT}}[\log{{N}}] \subsetneq {\mathtt{MDM}}[\log{{N}}, {{N}}].
\end{equation}$$
:::

Concretely, ${\mathtt{MDM}}[\log{{N}}, {{N}}] \, \setminus \, {\mathtt{CoT}}[\log{{N}}]$, for example, contains all ${{{{{{\mathtt{NC}}}^{\mathtt{1}}}}}}$-complete regular languages.

# Discussion {#sec:discussion}

#### Strengths and weaknesses of MDMs.

[3](#sec:theoretical-results){reference-type="ref+label" reference="sec:theoretical-results"} provides insights into the suitability of using MDMs for different classes of problems. On the one hand, [\[thm:cot-mdm-separation\]](#thm:cot-mdm-separation){reference-type="ref+label" reference="thm:cot-mdm-separation"} reveals the sequentiality bottleneck of CoT and an expressivity gap between CoT transformers and MDMs with logarithmically many model evaluations: While CoT transformers remain in ${{{{{{\mathtt{TC}}}^{\mathtt{0}}}}}}$, MDMs can solve ${{{{{{\mathtt{NC}}}^{\mathtt{1}}}}}}$-complete problems. This formalizes the intuition that MDMs are more suitable for highly-parallelizable problems and has implications for the practical applications of these two paradigms with a limited number of model evaluations. For example, the common *state-tracking* benchmark used to evaluate the reasoning abilities [@liu2023transformers; @10.5555/3692070.3693514] can be solved with MDMs with logarithmically many steps, while CoT transformers require linearly many steps. On the other hand, the equivalence of MDMs with polylogarithmically many denoising steps to the class ${{\mathtt{NC}}}$ (cf.¬†[\[cor:mdms-with-polylog-steps\]](#cor:mdms-with-polylog-steps){reference-type="ref+label" reference="cor:mdms-with-polylog-steps"}) reveals problems where efficiency gains from parallelism are limited. For example, assuming the widely-believed hypothesis that ${{\mathtt{NC}}}\neq {{\textnormal{\textsf{\small P}}}}$, none of the following (${{\textnormal{\textsf{\small P}}}}$-complete) problems benefit from MDM parallelism:

- **Circuit value problem**: Given a circuit, its inputs, and a gate, calculate the gate's value.

- **Linear programming**: Maximize a linear function subject to linear inequality constraints.

- **Context free grammar (CFG) membership**: Given a CFG ${{\mathcal{G}}}$ and a string ${{\boldsymbol{w}}}$, is ${{\boldsymbol{w}}}\in {\mathcal{L}}({{\mathcal{G}}})$?

- **Horn-satisfiability** (${{\textnormal{\textsf{\small P}}}}$ version of `SAT`): Is there a satisfying assignment to a set of Horn clauses?

In other words, these problems, in general, require a "CoT-style" step-by-step sequential solution. Due to the overhead introduced by unmasked attention of MDMs(such as the inability to store KV-cache), such problems are more efficiently solved by standard autoregressive CoT transformers.

#### Equivalence to padded looped transformers.

[\[cor:mdm-lt-equivalence\]](#cor:mdm-lt-equivalence){reference-type="ref+label" reference="cor:mdm-lt-equivalence"} reveals a tight connection between MDMs and PLTs. While this suggests these two frameworks are largely interchangeable, important distinctions exist. On the one hand, unlike MDMs, standard PLTs perform sequential computations *deterministically*. This makes MDMs more suitable for ambiguous generation tasks, where decisions early in the generation make subsequent decisions easier. PLTs would, in that case, have to keep track of all possible generations in the residual stream until the final---decoding---step. Moreover, MDMs are easier to train and steer---since their intermediate computation steps are based on partially masked inputs, the model explicitly learns to solve complex tasks from random sub-tasks, which benefits their reasoning abilities [@kim2025trainworstplanbest]. PLTs, in contrast, only receive training signal from the final decision and have to construct the sub-steps of the computation on their own. This could lead to suboptimal utilization of the sequential computation or even to failure to use it at all. Similarly, the human-understandable sub-tasks that MDMs have to solve make their training more interpretable and easier to control. On the other hand, the more information-rich intermediate states of PLTs make them more efficient at storing and processing information, and the lack of sampling steps makes them more efficient at inference time.

#### Generalizations.

By focusing on transformer-based MDMs, we can draw from the rich theory developed on the expressivity of transformers. However, MDMs do not have to be implemented by a transformer---they could, for example, be implemented by a state-space model. Nevertheless, the parallelizable nature of MDMs suggests that any reasonable real-world implementation will include parallelizable components---for example, a model implementable by a constant-depth circuit, such as a ${{{{{{\mathtt{TC}}}^{\mathtt{0}}}}}}$ circuit.[^17] The close connection between transformers (with logarithmically-growing precision and padding) and the class ${{{{{{\mathtt{TC}}}^{\mathtt{0}}}}}}$ [@merrill-sabharwal-2023-parallelism; @li2024chain] suggests that the results of [3](#sec:theoretical-results){reference-type="ref+label" reference="sec:theoretical-results"} will largely carry over to such implementations. We conjecture that, regardless of whether MDMs are implemented by finite-precision transformers (${{{{{{\mathtt{AC}}}^{\mathtt{0}}}}}}$ circuits) or a more expressive ${{{{{{\mathtt{TC}}}^{\mathtt{0}}}}}}$ circuit, ${\mathtt{MDM}}[\log^d{{N}}, {{\mathtt{poly}}\mleft({{N}}\mright)}]$ would remain in ${{{{{{\mathtt{AC}}}^{\mathtt{d}}}}}}$ (cf.¬†[\[cor:mdms-with-constant-steps\]](#cor:mdms-with-constant-steps){reference-type="ref+label" reference="cor:mdms-with-constant-steps"}) or a similar class like ${{{{{{\mathtt{TC}}}^{\mathtt{d}}}}}}$. Moreover, we state the sequentiality bottleneck only for $\log {{N}}$ decoding steps, since the expressivity of ${\mathtt{CoT}}[\log{{N}}]$ is known to be limited. However, we believe that a similar separation exists for polylogarithmically many decoding steps: While the expressivity of ${\mathtt{CoT}}[\log^d{{N}}]$ has not been formalized yet, it likely does not capture all of ${{{{{{\mathtt{AC}}}^{\mathtt{d}}}}}}$, unlike ${\mathtt{MDM}}[\log^d{{N}}, {{\mathtt{poly}}\mleft({{N}}\mright)}]$ (cf.¬†[\[cor:mdms-with-constant-steps\]](#cor:mdms-with-constant-steps){reference-type="ref+label" reference="cor:mdms-with-constant-steps"}).

#### Discrepancies.

We strive to compare different paradigms fairly by analyzing a specific implementation of MDMs---one based on a specific idealization of transformers. Some inherent differences between the frameworks, however, remain. One is the dichotomy between using causal masking for CoT and unmasked transformers for MDMs and PLTs. We focus on unmasked models to analyze the more natural and popular implementations rather than artificially constraining MDMs and PLTs to causal masking. Another impactful decision is the nature of positional encodings (PEs). While we assume relatively simple PEs standard in theoretical literature (in particular, logspace-computable, cf. [7.4](#app:transformers){reference-type="ref+label" reference="app:transformers"}), we allow them to come from an outside source not part of the transformer---they may thus carry information not computable by the model; see [9.1](#app:positional-encodings){reference-type="ref+label" reference="app:positional-encodings"} for additional discussion.

# Conclusion {#sec:conclusion}

We describe the expressivity of masked diffusion LMs (MDMs) by connecting them to padded looped transformers (PLTs) and chain-of-thought-augmented (CoT) transformers. This reveals a close connection between PLTs and MDMs, which leads to the equivalence of MDM with polylogarithmically many denoising steps to the class ${{\mathtt{NC}}}$ of parallelizable problems, with concrete implications around what problems can benefit from the parallelism afforded by MDMs. We also show that MDMs can (somewhat inefficiently) simulate CoT transformers. We describe the sequentiality bottleneck and the strict expressivity gap between MDMs and CoT transformers with logarithmically many model evaluations. This shows MDMs to be more suitable for highly-parallelizable problems, while CoT transformers are more suitable for inherently sequential ones. Overall, our results provide insights into the strengths and weaknesses of MDMs and their suitability for different classes of problems.

# Ethics Statement {#ethics-statement .unnumbered}

This work is theoretical and aims to describe the capabilities of masked diffusion models to better understand their strengths and limitations. We do not foresee any direct negative societal impacts.

# Reproducibility Statement {#reproducibility-statement .unnumbered}

All our results are theoretical and thus reproducible from the provided proofs in [\[app:theoretical-gadgets,app:proofs\]](#app:theoretical-gadgets,app:proofs){reference-type="ref+label" reference="app:theoretical-gadgets,app:proofs"}.

# The use of large language models {#app:llm-use .unnumbered}

We used AI-based tools (Gemini and GitHub Copilot) for brainstorming and writing assistance. We used the tools in compliance with the ICLR 2026 policies.

# Related work on the experssivity of MDMs {#app:related-work}

Existing theoretical work on MDMs focuses on the limitations of the factorized backward process and its convergence properties, which can be linked to formal language generation and recognition.

@feng2025theoreticalbenefitlimitationdiffusion analyze the implications of the factorized backward process on the expressivity of MDMs. They find that MDMs can approximate *n*-gram LM distributions arbitrarily well with constantly many sampling steps and linearly many sampling steps suffice to approximately generate from any regular language. They also provide a linear lower bound on the number of sampling steps required to capture the support of general regular languages---a consequence of the assumed factorization, which is incompatible with the sequential generation of regular languages. Crucially, this differs from our *recognition* setting in which the string to be processed is given and only its membership decision has to be generated.

This line of work is tightened by @li2025convergencetheorydiffusionlanguage, who build on work by @chen2024convergenceanalysisdiscretediffusion and analyze the approximation error (measured by the KL divergence) of the distribution generated by MDMs with respect to the number of sampling steps and the mutual information (dependence) between the symbols in different positions. Intuitively, the larger the mutual information is, the larger the number of sampling steps has to be to capture the same distribution (equivalently, the fewer symbols per step can be generated). They show that the KL divergence between the generated and the ground-truth distributions decays linearly with the number of sampling steps (with scaling that depends on the mutual information between the symbols in the strings), and show this decay to be optimal in general. This generalizes the results on *n*-gram distributions and regular languages by @feng2025theoreticalbenefitlimitationdiffusion.

@liu2025perfectdiffusionmathsftc0 [@liu2025serialscalinghypothesis] take a different perspective and analyze the expressivity of latent diffusion LMs. They show that, under an analogous assumption to our [\[assumption:uniform-unmasking,assumption:perfect-approximation\]](#assumption:uniform-unmasking,assumption:perfect-approximation){reference-type="ref+label" reference="assumption:uniform-unmasking,assumption:perfect-approximation"}, latent diffusion LMs converge to the data distribution in constantly many steps, which means that the computational depth of such models is limited. They show that breaking this assumption makes latent diffusion LMs Turing complete, analogous to our results in [3.2](#sec:mdms-and-cot){reference-type="ref+label" reference="sec:mdms-and-cot"}.

While these results provide useful insights into the limitations and affordances of MDMs, their asymptotic and approximate nature makes it difficult to draw concrete conclusions about the (reasoning) capabilities of MDMs in the sense of the work on transformers' expressivity. Our work complements these results by providing a more fine-grained analysis of MDM capabilities based on their connection to PLTs and CoT transformers, which allows us to leverage the existing theory developed on transformer expressivity and apply it directly to MDMs.

# Preliminaries {#app:preliminaries}

## Notation {#app:notation}

Let ${{\Sigma}}$ be an alphabet. A **language** ${\mathcal{L}}$ is a subset of ${{{{{{\Sigma}}^{*}}}}}$. A **language recognizer** is a function ${R}\colon {{{{{{\Sigma}}^{*}}}}}\to \{{\texttt{0}}, {\texttt{1}}\}$, where ${\texttt{0}}$ and ${\texttt{1}}$ are designated reject and accept symbols. ${R}$'s language is ${{\mathcal{L}}}\mleft({R}\mright) \mathrel{\stackrel{\textnormal{\tiny def}}{=}}{\left\{ {{\boldsymbol{w}}}\in {{{{{{\Sigma}}^{*}}}}}\mid {{R}}\mleft({{\boldsymbol{w}}}\mright) = {\texttt{1}} \right\}}$. Two recognizers ${R}_1$ and ${R}_2$ are **equivalent** if and only if ${{\mathcal{L}}}\mleft({R}_1\mright) = {{\mathcal{L}}}\mleft({R}_2\mright)$.

We denote the concatentation of two strings ${{\boldsymbol{w}}}_1, {{\boldsymbol{w}}}_2 \in {{{{{{\Sigma}}^{*}}}}}$ as ${{\boldsymbol{w}}}_1 \circ {{\boldsymbol{w}}}_2$ or simply ${{\boldsymbol{w}}}_1{{\boldsymbol{w}}}_2$. We define the **intereleaving** of the vectors ${{{\bm{x}}}}, {{{\bm{y}}}}\in {{\mathbb{R}}}^{{D}}$ as ${{{{{\bm{x}}}}}^\frown{{{{\bm{y}}}}}} \in {{\mathbb{R}}}^{2{{D}}}$ where $$\begin{equation}
   {{{{{\bm{x}}}}}^\frown{{{{\bm{y}}}}}}_{{d}}\mathrel{\stackrel{\textnormal{\tiny def}}{=}}
   \begin{cases}
      {{{x}}}_{\sfrac{({{d}}+1)}{2}} & \textbf{if }{{d}}\text{ is odd}, \\
      {{{y}}}_{\sfrac{{{d}}}{2}} & \textbf{otherwise }{{d}}.
   \end{cases}
\end{equation}$$

We use ${{\llbracket{{w}}\rrbracket}} \in \{0, 1\}^{{{|{{\Sigma}}|}}}$ to denote the one-hot encoding of symbol ${{w}}\in {{\Sigma}}$. We use ${\texttt{B}}_{{\texttt{p}}}({{n}})$ to denote the binary encoding of natural number ${{n}}$ using ${{\texttt{p}}}$ binary bits and ${{\texttt{B}}^{\pm}}_{{\texttt{p}}}({{n}})$ to denote the signed binary encoding $2{\texttt{B}}_{{\texttt{p}}}({{n}})-{{\bm{1}}}_{{\texttt{p}}}$, where ${{\bm{1}}}_{{\texttt{p}}}$ is the ${{D}}$-dimensional vector of all ones. We will leave out ${{\texttt{p}}}$ when it is clear from the context.

For ${{D}}\in {{\mathbb{N}}}$, we define ${{\mathrm{softmax}}}\colon{{\mathbb{R}}}^{{D}}\to{{\mathbb{R}}}^{{D}}$ as ${{\mathrm{softmax}\mleft({{{\bm{x}}}}\mright)_{{{d}}}}} = \exp({{{x}}}_{{d}})/\sum_{{{d}}=1}^{{D}}\exp({{{x}}}_{{d}})$ for ${{{\bm{x}}}}\in{{\mathbb{R}}}^{{D}}$ and ${{d}}\in [{{D}}]$. We also use the shorthand ${\left[ x \right]_{+}} \mathrel{\stackrel{\textnormal{\tiny def}}{=}}\max\{x, 0\}$. We denote with ${{{{\mathcal{P}}}\mleft({{{\mathcal{X}}}}\mright)}}$ the set of all probability distributions over a set ${{{\mathcal{X}}}}$.

## Circuit Complexity {#app:circuit-complexity}

Computational circuits are a model of parallel computation. They have been widely used in the study of the expressivity of neural networks. Circuits process binary input strings through a series of logical operations to produce binary outputs.[^18] Formally, a **boolean circuit** is a directed acyclic graph where source nodes represent the N-bit input, and a single sink node represents the output. Non-source vertices are called **gates** and are labeled with logical operations (e.g., `AND`, `OR`, `NOT`). The **size** of a circuit is the number of gates, and its **depth** is the longest path from any input to the output.

A circuit computes a function ${C}\colon \{0,1\}^{{N}}\to \{{\texttt{0}}, {\texttt{1}}\}$ for some ${{N}}\in {{\mathbb{N}}}$, where ${\texttt{0}}$ and ${\texttt{1}}$ are designated reject and accept symbols. The value ${{C}\mleft({{\boldsymbol{w}}}\mright)}$ for input string ${{\boldsymbol{w}}}\in \{0,1\}^{{N}}$ is computed by evaluating the gates in topological order starting from the input bits. We say that the circuit ${C}$ **accepts** a string ${{\boldsymbol{w}}}$ if ${{C}\mleft({{\boldsymbol{w}}}\mright)} = {\texttt{1}}$.

**Circuit families** process input strings of variable length. A circuit family is a sequence of circuits ${{{{\mathcal{C}}}}}\mathrel{\stackrel{\textnormal{\tiny def}}{=}}\{{C}_{{{N}}}\}_{{{N}}\in {{\mathbb{N}}}}$ where ${C}_{{N}}$ processes inputs of length ${{N}}$. A circuit family is said to recognize a language if for any given input string, the corresponding circuit outputs ${\texttt{1}}$ if and only if the string is in the language.

A **circuit complexity class** is a set of circuit families that satisfy certain constraints on size, depth, and the types of gates used. This paper focuses on two common classes:

- **${{{{{{\mathtt{AC}}}^{\mathtt{d}}}}}}$**: Circuits with `NOT`, `AND`, and `OR` gates that have unbounded fan-in and depth ${{{{\mathcal{O}}}(\log^d{{N}})}}$.

- **${{{{{{\mathtt{TC}}}^{\mathtt{d}}}}}}$**: The extension of ${{{{{{\mathtt{AC}}}^{\mathtt{d}}}}}}$ that adds **threshold gates**, which output ${\texttt{1}}$ if the sum of their inputs exceeds a given threshold. It is known that ${{{{{{\mathtt{AC}}}^{\mathtt{0}}}}}}\subsetneq {{{{{{\mathtt{TC}}}^{\mathtt{0}}}}}}$ and ${{{{{{\mathtt{AC}}}^{\mathtt{d}}}}}}\subseteq {{{{{{\mathtt{TC}}}^{\mathtt{d}}}}}}$. For example, [Parity]{.smallcaps}, the language of binary strings with an even number of 1s, is in ${{{{{{\mathtt{TC}}}^{\mathtt{0}}}}}}$ but not in ${{{{{{\mathtt{AC}}}^{\mathtt{0}}}}}}$ [@Furst1984].

- **${{{{{{\mathtt{NC}}}^{\mathtt{1}}}}}}$**: This class consists of circuits that can be computed in parallel with a logarithmic depth, a polynomial number of gates, and constant fan-in. It is known that ${{{{{{\mathtt{TC}}}^{\mathtt{0}}}}}}\subseteq {{{{{{\mathtt{NC}}}^{\mathtt{1}}}}}}$.

Without additional constraints, circuit families can recognize undecidable languages by having arbitrary, "hard-coded" solutions for each input length. To avoid this and ensure the model of computation is realistic, we can impose a **uniformity** condition. A circuit family is **uniform** if there exists a Turing machine that, given an input of $1^{{N}}$, can generate a description of the circuit ${C}_{{N}}$. In particular, a circuit class is **${{\mathtt{L}}\textnormal{-uniform}}$** if a Turing machine using ${{{{\mathcal{O}}}(\log {{N}})}}$ space can construct its description from the input $1^{{N}}$. This ensures the circuits for different input lengths are related by a systematic procedure.

## Finite-precision fixed-point arithmetic {#app:fixed-point-arithmetic}

We assume that the operations performed by our computational models rely on finite-precision fixed-point arithmetic. This model is based on ones used by @li2024chain [@saunshi2025reasoninglatentthoughtspower; @london2025pausetokensstrictlyincrease].

::: definition
**Definition 1** (Fixed-Point Representation). *Let ${{\texttt{p}}}\in {{\mathbb{N}}}$ be the number of bits devoted to each of the integer and fractional parts. We use ${{\mathbb{F}}}_{{\texttt{p}}}$ to denote the set $$\begin{equation}
      {{\mathbb{F}}}_{{\texttt{p}}}\mathrel{\stackrel{\textnormal{\tiny def}}{=}}\{ x_{\pm} \cdot a \cdot 2^{-{{\texttt{p}}}} \mid x_{\pm} \in \{-1, 1\}, a \in \{0, 1, \ldots, 2^{2{{\texttt{p}}}}-1\} \}
\end{equation}$$*
:::

We define ${{B_{{{\mathbb{F}}}}}}\mathrel{\stackrel{\textnormal{\tiny def}}{=}}\max {{\mathbb{F}}}_{{\texttt{p}}}= 2^{{\texttt{p}}}- 2^{-{{\texttt{p}}}}$. All values exceeding ${{B_{{{\mathbb{F}}}}}}$ are considered out of range and are rounded to ${{B_{{{\mathbb{F}}}}}}$. Note, however, that ${{B_{{{\mathbb{F}}}}}}$ does *not* behave like infinity---it does not "consume" all subsequent operations. For example, ${{B_{{{\mathbb{F}}}}}}- x \neq {{B_{{{\mathbb{F}}}}}}$ for some non-negative $x \in {{\mathbb{F}}}_{{\texttt{p}}}$ is a valid number.

To handle the results of arithmetic operations that may not be exactly representable in the fixed-point format, we define a standard for rounding.

::: definition
**Definition 2** (Rounding). *For any $x \in {{\mathbb{R}}}$ and any closed subset ${{\mathbb{F}}}$ of ${{\mathbb{R}}}$ containing 0, we define rounding ${\texttt{round}}\mleft(x, {{\mathbb{F}}}\mright)$ as the closest number to $x$ in ${{\mathbb{F}}}$. In case of a tie, the value with the smaller absolute value is chosen.*
:::

We denote the rounding operation as $\mleft[\cdot\mright]_{{\texttt{p}}} \mathrel{\stackrel{\textnormal{\tiny def}}{=}}{\texttt{round}}(\cdot, {{\mathbb{F}}}_{{\texttt{p}}})$. This operation is applied to vectors and matrices element-wise. All binary operations are defined by first performing the ideal mathematical operation and then rounding the result to the nearest representable value in ${{\mathbb{F}}}_{{\texttt{p}}}$. Division by zero is considered an error condition resulting in an incorrect output. We also note that $\sfrac{{{B_{{{\mathbb{F}}}}}}}{2} = 2^{{{\texttt{p}}}- 1} - 2^{-{{\texttt{p}}}}$.

For operations involving more than two numbers, rounding is applied iteratively.

::: definition
**Definition 3** (Summation with Iterative Rounding). *For ${{\texttt{p}}}, N \in {{\mathbb{N}}}$ and ${{{\bm{x}}}}\in {{\mathbb{R}}}^N$, we define summation with iterative rounding to ${{\texttt{p}}}$ fractional bits as the function $\textsc{sum}_{{\texttt{p}}}\colon \bigcup_{N\in{{\mathbb{N}}}}({{\mathbb{F}}}_{{{\texttt{p}}}})^{N}\rightarrow{{\mathbb{F}}}_{{{\texttt{p}}}}$, where for any $N\in{{\mathbb{N}}}^{+}$ and ${{{\bm{x}}}}\in ({{\mathbb{F}}}_{{\texttt{p}}})^N$: $$\begin{equation}
      \textsc{sum}_{{\texttt{p}}}({{{\bm{x}}}}) \mathrel{\stackrel{\textnormal{\tiny def}}{=}}\mleft[\dots\mleft[\mleft[{{{x}}}_1 + {{{x}}}_2\mright]_{{\texttt{p}}} + {{{x}}}_3\mright]_{{\texttt{p}}} + \dots + {{{x}}}_N\mright]_{{\texttt{p}}}
\end{equation}$$*
:::

This iterative rounding process is not associative and the order of operations can affect the final result. Based on this, we can also define more complex operations such as the **fixed-point inner product** $\langle {{{\bm{x}}}}, {{{\bm{y}}}}\rangle_{{\texttt{p}}}\mathrel{\stackrel{\textnormal{\tiny def}}{=}}\textsc{sum}_{{\texttt{p}}}({{{\bm{x}}}}\odot {{{\bm{y}}}})$, where $\odot$ denotes the element-wise product of two vectors, and **fixed-point matrix product** for matrices ${{{\bm{A}}}}$ and ${{{\bm{B}}}}$, where $({{{\bm{A}}}}\times_{{\texttt{p}}}{{{\bm{B}}}})_{i,j} \mathrel{\stackrel{\textnormal{\tiny def}}{=}}\langle ({{{\bm{A}}}}_{i,:})^\top, {{{\bm{B}}}}_{:,j} \rangle_{{\texttt{p}}}$.

## Transformers {#app:transformers}

We consider both **unmasked** [@devlin-etal-2019-bert] and **causally masked** transformers [@radford2019language]. Concretely, we work with fixed-point transformers whose underlying arithmetic operations are replaced with their fixed-point counterparts. A transformer ${{\mathcal{T}}}$ consists of four parts:

1.  a **symbol embedding** ${\bm{e}}\colon {{\Sigma}}\to {{\mathbb{F}}}^{{{D}}}$ of the form ${\bm{e}}({{w}}) \mathrel{\stackrel{\textnormal{\tiny def}}{=}}{{{{{\bm{V}}}}}}{{\llbracket{{w}}\rrbracket}}$ for ${{w}}\in{{\Sigma}}$, where ${{{{{\bm{V}}}}}}\in {{\mathbb{F}}}^{{{D}}\times {{|{{\Sigma}}|}}}$ and ${{\llbracket{{w}}\rrbracket}} \in {{\mathbb{F}}}^{{{|{{\Sigma}}|}}}$ is the one-hot encoding of ${{w}}$,

2.  a **positional encoding** ${\bm{p}}\colon {{\mathbb{N}}}\times {{\mathbb{N}}}\to {{\mathbb{F}}}^{{D}}$,

3.  ${{L}}$ **layers** ${{\boldsymbol{\tau}}}^{(1)}, \ldots, {{\boldsymbol{\tau}}}^{({{L}})}$, each of which consists of two sub-layers: A multi-head self-attention layer and a position-wise fully-connected feed-forward network ${{{f}}}$, and

4.  an **output layer** ${\bm{o}}$ of the form ${\bm{o}}({{{{{\bm{h}}}}}}) \mathrel{\stackrel{\textnormal{\tiny def}}{=}}{{\mathrm{softmax}}}({{{{{\bm{E}}}}}}{{{{{\bm{h}}}}}})$ for ${{{{{\bm{h}}}}}}\in{{\mathbb{F}}}^{{{D}}}$, where ${{{{{\bm{E}}}}}}\in {{\mathbb{F}}}^{{{|{{\overline{{{\Sigma}}}}}|}}\times {{D}}}$.

Each layer has its own parameters and is indexed by the layer name and the depth for attention and feedforward layers. We use ${{D}}$ to denote the **width** of a transformer. A transformer with layers ${{\boldsymbol{\tau}}}^{(1)}, \ldots, {{\boldsymbol{\tau}}}^{({{L}})}$ computes ${{{{{\bm{h}}}}}}^{({{l}})}_{{n}}\in {{\mathbb{F}}}^{{{D}}}$ for ${{l}}\in {\left\{ 1, \ldots, {{L}} \right\}}$ and each position ${{n}}\in {\left[ {{N}} \right]}$ in the input string ${{\boldsymbol{w}}}= {{w}}_1\cdots{{w}}_{{N}}\in {{{{{{\Sigma}}^{*}}}}}$ as follows: $$\begin{align}
      {{{{{\bm{h}}}}}}^{(0)}_{{n}}&\mathrel{\stackrel{\textnormal{\tiny def}}{=}}{\bm{e}}({{w}}_{{n}}) + {\bm{p}}({{n}}, {{N}}) \in {{\mathbb{F}}}^{{{D}}} \text{ for } {{n}}\in {\left[ {{N}} \right]} \\
      {{{{{\bm{H}}}}}}^{({{l}})} &\mathrel{\stackrel{\textnormal{\tiny def}}{=}}\begin{pmatrix}
         {{{{{\bm{h}}}}}}^{({{l}})\top}_1 &
         \cdots &
         {{{{{\bm{h}}}}}}^{({{l}})\top}_{{N}}
      \end{pmatrix}^\top \in {{\mathbb{F}}}^{{{N}}\times {{D}}} \\
      {{{{{\bm{Q}}}}}}^{({{l}})} &\mathrel{\stackrel{\textnormal{\tiny def}}{=}}{{{{{\bm{H}}}}}}^{({{l}})} {{{\bm{W}}}}_Q^{({{l}})}, \quad 
      {{{{{\bm{K}}}}}}^{({{l}})} \mathrel{\stackrel{\textnormal{\tiny def}}{=}}{{{{{\bm{H}}}}}}^{({{l}})} {{{\bm{W}}}}_K^{({{l}})}, \quad
      {{{{{\bm{V}}}}}}^{({{l}})} \mathrel{\stackrel{\textnormal{\tiny def}}{=}}{{{{{\bm{H}}}}}}^{({{l}})} {{{\bm{W}}}}_V^{({{l}})} \quad \in {{\mathbb{F}}}^{{{N}}\times {{D}}} \\
      {{{\bm{G}}}}^{({{l}})} &\mathrel{\stackrel{\textnormal{\tiny def}}{=}}{{\mathrm{softmax}}}({{{{M}}({{{{{\bm{Q}}}}}}^{({{l}})} {{{{{\bm{K}}}}}}^{({{l}})\top})}}) {{{{{\bm{V}}}}}}^{({{l}})} + {{{{{\bm{H}}}}}}^{({{l}})} \in {{\mathbb{F}}}^{{{N}}\times {{D}}} \label{eq:softmax-attention-matrix} \\
      {{{{{\bm{H}}}}}}^{({{l}}+ 1)} &\mathrel{\stackrel{\textnormal{\tiny def}}{=}}{{{\bm{G}}}}^{({{l}})} + {{{f}}}({{{\bm{G}}}}^{({{l}})}) \in {{\mathbb{F}}}^{{{N}}\times {{D}}}
   \end{align}$$ Here, ${{M}}\colon {{\mathbb{F}}}^{{{N}}\times {{N}}} \to ({{\mathbb{F}}}\cup {\left\{ -\infty \right\}})^{{{N}}\times {{N}}}$ is the **masking function**.[^19]^,^[^20] We say that the ${{l}}\textsuperscript{th}$ layer ${{\boldsymbol{\tau}}}^{({{l}})}$ computes the function ${{\boldsymbol{\tau}}}^{({{l}})}\colon {{\mathbb{F}}}^{{{N}}\times {{D}}} \to {{\mathbb{F}}}^{{{N}}\times {{D}}}$, defined by the function ${{\boldsymbol{\tau}}}^{({{l}})}\colon {{{{{\bm{H}}}}}}^{({{l}}- 1)} \mapsto {{{{{\bm{H}}}}}}^{({{l}})}$ for ${{l}}\in {\left\{ 1, \ldots, {{L}} \right\}}$. We also denote with ${{\mathcal{T}}}$ the function ${{\mathcal{T}}}\colon {{{{{{\Sigma}}^{*}}}}}\to {{\mathbb{F}}}^{{{N}}\times {{D}}}$, defined as ${{\mathcal{T}}}\colon {{\boldsymbol{w}}}\mapsto {{{{{\bm{H}}}}}}^{({{L}})}$.

We use the following definition of the multi-layer perceptron.

::: {#def:mlp .definition}
**Definition 4** (Multi-layer perceptron). *A **multi-layer perceptron** (MLP) is a function ${{{f}}}\colon {{\mathbb{F}}}^{{D}}\to {{\mathbb{F}}}^{{{D}}'}$ that can be expressed as a composition of affine transformations and the ${{\mathrm{ReLU}}}$ activation function: $$\begin{equation}
      {{{f}}}({{{\bm{x}}}}) = {{\mathrm{ReLU}}}({{{\bm{W}}}}_2 ({{\mathrm{ReLU}}}({{{\bm{W}}}}_1 {{{\bm{x}}}}+ {{{\bm{b}}}}_1)) + {{{\bm{b}}}}_2),
\end{equation}$$ where ${{{\bm{W}}}}_1 \in {{\mathbb{F}}}^{H \times {{D}}}$, ${{{\bm{W}}}}_2 \in {{\mathbb{F}}}^{{{D}}' \times H}$, ${{{\bm{b}}}}_1 \in {{\mathbb{F}}}^{H}$, ${{{\bm{b}}}}_2 \in {{\mathbb{F}}}^{{{D}}'}$ for ${{D}}, {{D}}', H \in {{\mathbb{N}}}$.*
:::

### Scaling transformer size with input length {#app:scaling-transformers}

The exact expressivity of a transformer model depends on seemingly unimportant details [@jerad2025uniquehardattentiontale]. One such example is the interplay between the scaling of the **numerical precision** ${{\texttt{p}}}$ of the values stored in the representations ${{{{{\bm{h}}}}}}$ and the scaling of the **width** ${{D}}$. To be able to uniquely identify the ${{N}}$ positions in the input string, the "volume" of the embedding space, i.e., the number of possible distinct representations ${{{{{\bm{h}}}}}}$, must be at least ${{N}}$. This implies that the product ${{\texttt{p}}}\cdot {{D}}$ must *scale* at least logarithmically with ${{N}}$: ${{{{\texttt{p}}}\mleft({{N}}\mright)}} \cdot {{{{D}}\mleft({{N}}\mright)}} = {{{{\Omega}}(\log{{N}})}}$. Existing work focuses on two modeling choices:

1.  **log-precision** transformers where ${{{{\texttt{p}}}\mleft({{N}}\mright)}} = {{{{\Theta}}(\log{{N}})}}$ with either constant width ${{{{D}}\mleft({{N}}\mright)}} = {{{{\Theta}}(1)}}$ or polynomial width ${{{{D}}\mleft({{N}}\mright)}} = {{{{\Theta}}({\mathtt{poly}}({{N}}))}}$ [@merrill-sabharwal-2023-parallelism; @merrill2024the; @li2024chain; @london2025pausetokensstrictlyincrease; @merrill2025littledepthgoeslong; @merrill2025exactexpressivepowertransformers]; and

2.  **constant-precision logarithmic-width** transformers where ${{\texttt{p}}}= {{{{\Theta}}(1)}}$ and ${{{{D}}\mleft({{N}}\mright)}} = {{{{\Theta}}(\log{{N}})}}$ [@li2024chain; @saunshi2025reasoninglatentthoughtspower; @london2025pausetokensstrictlyincrease].

Despite having the same volume of the representation space, the two models differ in their expressivity. For example, constant-precision transformers are constrained to use the volume in a "distributed" manner across model dimensions without the ability to summarize the information into individual values or store pointers to arbitrary positions in them---both of these require precision growing with string length. Such summarization is required in certain steps of the transformer architecture---for example, when computing the attention scores. This limits the expressivity of constant-precision transformers compared to log-precision transformers: While log-precision transformers can compute certain ${{{{{{\mathtt{TC}}}^{\mathtt{0}}}}}}$ functions, constant-precision transformers with polynomial width fall within ${{{{{{\mathtt{AC}}}^{\mathtt{0}}}}}}$ [@li2024chain; @london2025pausetokensstrictlyincrease]. Similar separation results extend to popular variants of transformers:

::: enumerate*
Transformers with chain-of-thought reasoning (cf. [7.5](#app:cot){reference-type="ref+label" reference="app:cot"}) with logarithmic precision can simulate ${{{{{{\mathtt{TC}}}^{\mathtt{0}}}}}}$ circuits of size corresponding to the number of reasoning steps while constant-precision transformers with polynomial width are constrained to ${{{{{{\mathtt{AC}}}^{\mathtt{0}}}}}}$ circuits [@li2024chain; @merrill2024the].

Transformers with additional padding space (blank thinking tokens; cf. [7.6](#app:looped-padded-transformers){reference-type="ref+label" reference="app:looped-padded-transformers"}) with logarithmic precision can express exactly ${{{{{{\mathtt{TC}}}^{\mathtt{0}}}}}}$ functions while constant-precision transformers with polynomial width are equivalent to ${{{{{{\mathtt{AC}}}^{\mathtt{0}}}}}}$ circuits [@merrill2025exactexpressivepowertransformers; @london2025pausetokensstrictlyincrease].
:::

However, the trend of coarse quantization while increasing the model size makes fixed-precision logarithmic-width transformers particularly appealing, which is why we focus on *finite precision* transformers with *logarithmic width*.

Analogously to circuit families, each string length ${{N}}$ is processed by a separate transformer model. To process all of ${{{{{{\Sigma}}^{*}}}}}$, we therefore define a **transformer family** $\{{{\mathcal{T}}}_{{N}}\}$ as a sequence of transformers where each ${{\mathcal{T}}}_{{N}}$ processes strings of length ${{N}}$. Further, we again impose a uniformity condition on the family, which will relate the transformers for different input lengths.

::: {#def:uniform-transformers .definition}
**Definition 5** (Uniform transformer families; variant of ). *Let ${\mathtt{X}}$ be a computational complexity class. A transformer family $\{{{\mathcal{T}}}_{{N}}\}$ is **${{\mathtt{X}}\text{-uniform}}$** if there exist Turing machines ${{\mathcal{M}}}_1$ and ${{\mathcal{M}}}_2$ whose resource usage is constrained by the complexity class ${\mathtt{X}}$ such that:*

1.  *${{\mathcal{M}}}_1$ takes input $1^{{N}}$ and outputs a description of ${{\mathcal{T}}}_{{N}}$, and*

2.  *${{\mathcal{M}}}_2$ takes input $(1^{{N}}, {\texttt{B}}({{n}}))$ and outputs ${\bm{p}}({{n}}, {{N}})$.*
:::

[5](#def:uniform-transformers){reference-type="ref+label" reference="def:uniform-transformers"} allows for size-dependent transformers while keeping them closely related (as the same Turing machines must construct them for all ${{N}}$). It also facilitates natural connections with uniform circuit classes (cf.¬†[7.2](#app:circuit-complexity){reference-type="ref+label" reference="app:circuit-complexity"}) [@london2025pausetokensstrictlyincrease]. All our results concern ${{\mathtt{L}}\textnormal{-uniform}}$ transformer families, in which case, the Turing machines in [5](#def:uniform-transformers){reference-type="ref+label" reference="def:uniform-transformers"} operate in logarithmic space.

### Transformer language models and symbol predictors {#app:language-models-and-next-symbol-predictors}

Transformers can implement (non-)autoregressive LMs and deterministic symbol predictors. A transformer LM computes the probability distribution over the ${{N}}+ 1\textsuperscript{st}$ symbol given the length-${{N}}$ string ${{\boldsymbol{w}}}$ as $$\begin{equation}
   {{\overset{\curvearrowright}{{{p}}}}(\textcolor{ETHPurple}{{{\overline{{{w}}}}}} \mid {{\boldsymbol{w}}})} \mathrel{\stackrel{\textnormal{\tiny def}}{=}}{\bm{o}}({{{{{\bm{h}}}}}}^{({{L}})}_{|{{\boldsymbol{w}}}|})_{\textcolor{ETHPurple}{{{\overline{{{w}}}}}}} \qquad \text{for } \textcolor{ETHPurple}{{{\overline{{{w}}}}}} \in {{\overline{{{\Sigma}}}}}.
\end{equation}$$ To define **infilling** probabilities, let ${{\boldsymbol{w}}}\in {{{{{\Sigma}}_{{\texttt{m}}}}^{*}}}$ be a possibly (partially) masked string of length ${{N}}$ and let ${{n}}\in {\left[ {{N}} \right]}$ such that ${{w}}_{{n}}= {\texttt{m}}$. We then define the probability distribution over the symbol at the masked position ${{n}}$ as $$\begin{equation}
   {{{{p}}^{\downarrow}}(\textcolor{ETHPurple}{{{w}}} \mid {{\boldsymbol{w}}})} \mathrel{\stackrel{\textnormal{\tiny def}}{=}}{\bm{o}}({{{{{\bm{h}}}}}}^{({{L}})}_{{{n}}})_{\textcolor{ETHPurple}{{{w}}}} \qquad \text{for } \textcolor{ETHPurple}{{{w}}} \in {{\Sigma}}.
\end{equation}$$ Note that the next-symbol probabilities are computed based on the contextual representation of the previous symbol while the infilling probabilities are defined based on the contextual representation at the masked position. That is, for autoregressive next-symbol prediction, the transformer uses the hidden state at the last (previous) position, whereas for infilling, it uses the hidden state at the position to be filled.

To define a deterministic transformer-based symbol predictor, we define a decoding step.

::: {#def:decoding-step .definition}
**Definition 6** (Decoding step). *For any ${{N}}\in {{\mathbb{N}}}$, let ${{{{{{{\bm{H}}}}}}}}\in {{\mathbb{R}}}^{{{N}}\times {{D}}}$ . The **decoding step** ${{\texttt{Dec}}}\colon {{\mathbb{R}}}^{{{N}}\times {{D}}} \to {{\overline{{{\Sigma}}}}}^{{{N}}}$ is defined as $$\begin{equation}
      {{\texttt{Dec}}}({{{{{{{\bm{H}}}}}}}})_{{n}}= \mathop{\mathrm{{{argmax}}}}_{{{w}}\in {{\overline{{{\Sigma}}}}}} {\bm{o}}({{{{{{{\bm{H}}}}}}}})_{{{n}}, :}
\end{equation}$$ where the output function ${\bm{o}}$ is applied to ${{{{{{{\bm{H}}}}}}}}$ row-wise. This can be used either to deterministically infill the masked positions in the string or to deterministically predict the next symbol based on the final representation. For next-symbol prediction, we also define the shorthand ${{\texttt{NS}}}\colon {{{{{{\Sigma}}^{*}}}}}\to {{\overline{{{\Sigma}}}}}$ as $$\begin{equation}
      {{{{\texttt{NS}}}\mleft({{\boldsymbol{w}}}\mright)}} \mathrel{\stackrel{\textnormal{\tiny def}}{=}}{{\texttt{Dec}}}({{\mathcal{T}}}({{\boldsymbol{w}}}))_{{{N}}}.
\end{equation}$$*
:::

### Language encoders and model equivalence {#app:model-equivalence}

We are interested in the expressivity of neural networks as language recognizers and LMs, which at a high level, describes their behavior on input strings. This behavior---either the prediction of language membership or the computation of string probabilities---is completely determined by the **contextual representations** of the input strings produced by the neural network. We abstract the computation of string representations using a **language encoder**---a length-preserving function ${{\textnormal{\texttt{Enc}}}}\colon {{{{{{\Sigma}}^{*}}}}}\to {{({{\mathbb{R}}}^{{{D}}})^{*}}}$ for some ${{D}}\in {{\mathbb{N}}}$ [@10.5555/3737916.3740249; @cotterell2024formalaspectslanguagemodeling]. We regard the output ${{\textnormal{\texttt{Enc}}}}({{\boldsymbol{w}}})$ of a language encoder for a string ${{\boldsymbol{w}}}$ as a $|{{\boldsymbol{w}}}| \times {{D}}$ matrix, where each row corresponds to the contextual representation of the symbol at the corresponding position. In the context of transformers, the language encoder is the function ${{{\textnormal{\texttt{Enc}}}}\mleft({{\boldsymbol{w}}}\mright)} \mathrel{\stackrel{\textnormal{\tiny def}}{=}}{{{{{\bm{H}}}}}}^{({{L}})}$ for ${{\boldsymbol{w}}}\in {{{{{{\Sigma}}^{*}}}}}$.

All aspects of the model's behavior that we might be interested in can be described in terms of the contextual representations---the (logits of the) next-symbol or infilling probabilities are determined by a linear transformation of the contextual representations, and the membership test is determined by a linear classifier based on the contextual representations of the final symbol in the string. Studying the expressivity of a model thus reduces to determining what types of contextual representations can be produced by the model. If we can show that two language encoders ${{\textnormal{\texttt{Enc}}}}_1, {{\textnormal{\texttt{Enc}}}}_2 \colon {{{{{{\Sigma}}^{*}}}}}\to {{({{\mathbb{R}}}^{{D}})^{*}}}$ compute the same contextual representations for all input strings, we say that ${{\textnormal{\texttt{Enc}}}}_1$ and ${{\textnormal{\texttt{Enc}}}}_2$ are **equivalent**. Moreover, we say that two sets of models are equivalent if each model in one set is equivalent to at least one model in the other set.

#### Model equivalence and variable-length outputs.

Sometimes, we will compare the expressivity of models that produce outputs of different lengths, for example when comparing the expressivity of padded and non-padded models, or comparing CoT-augmented transformers with non-augmented ones. This makes direct comparison of contextual representations more difficult. Whenever this is the case, we will explicitly state how the outputs of different lengths are aligned. For example, when comparing a model that produces outputs of length ${{N}}$ with a model that produces outputs of length $K {{N}}$ for some constant $K \in {{\mathbb{N}}}$, we may assume that the first $(K - 1) {{N}}$ positions of the longer output are used for intermediate computations and only the last ${{N}}$ positions are used to produce the final output. Thus, we will only compare the last ${{N}}$ positions of the longer output with the ${{N}}$-length output of the shorter model and base model equivalence on that.

### Sampling from a (transformer) language model {#app:sampling-from-language-models}

The next-symbol and infilling probabilities from [7.4.2](#app:language-models-and-next-symbol-predictors){reference-type="ref+label" reference="app:language-models-and-next-symbol-predictors"} can be used to sample from the LM by sampling from ${{\overset{\curvearrowright}{{{p}}}}(\cdot \mid {{\boldsymbol{w}}})}$ or ${{{{p}}^{\downarrow}}(\cdot \mid {{\boldsymbol{w}}})}$, respectively.[^21] There are many possible ways to implement the sampling. In this paper, we assume that it is performed using the Gumbel-max trick [@pmlr-v97-oberst19a], which both provides a convenient and fast implementation as well as interpretable traces of the sampling procedure [@chatzi2024counterfactual; @ravfogel2025gumbel; @benz2025evaluation].

::: {#def:gumbel-max-sampling .definition}
**Definition 7** (Gumbel-max sampling). *Let ${{{\bm{l}}}}\mathrel{\stackrel{\textnormal{\tiny def}}{=}}{{{{{\bm{E}}}}}}{{{{{\bm{h}}}}}}$ for ${{{{{\bm{h}}}}}}\in{{\mathbb{R}}}^{{{D}}}$ and ${{{{{\bm{E}}}}}}\in {{\mathbb{R}}}^{{{|{{\overline{{{\Sigma}}}}}|}}\times {{D}}}$ be the logits of a probability distribution over ${{\overline{{{\Sigma}}}}}$. The **Gumbel-max sampling** from is defined as $$\begin{equation}
 \label{eq:gumbel-max-sampling}
      {{w}}= \mathop{\mathrm{{{argmax}}}}_{{{w}}' \in {{\overline{{{\Sigma}}}}}} ({{{p}}}_{{{w}}'} + g_{{{w}}'}),
\end{equation}$$ where $g_{{{w}}'}$ are i.i.d. samples from the Gumbel distribution with the cumulative distribution function $F(x) = \exp(-\exp(-x))$ for $x \in {{\mathbb{R}}}$.*
:::

It is well known that [\[eq:gumbel-max-sampling\]](#eq:gumbel-max-sampling){reference-type="ref+label" reference="eq:gumbel-max-sampling"} results in samples from ${{\mathrm{softmax}\mleft({{{\bm{l}}}}\mright)_{}}}$. The Gumbel-max sampling can be used to either sample the next symbol ${{w}}_{{{N}}+ 1}$ from the next-symbol distribution ${{\overset{\curvearrowright}{{{p}}}}(\cdot \mid {{\boldsymbol{w}}})}$ or to sample the masked symbol ${{w}}_{{n}}$ from the infilling distribution ${{{{p}}^{\downarrow}}(\cdot \mid {{\boldsymbol{w}}})}$.

We rely on the Gumbel-max sampling because it conveniently decouples sampling from the model's representations. In particular, provided that a model is able to implement the $\mathop{\mathrm{{{argmax}}}}$ operation and is able to receive the Gumbel noise as an input, it can sample from the model's distribution without any additional operations. This will provide a convenient way to precisely link different modeling frameworks in a unified manner.

## Chain-of-thought reasoning {#app:cot}

At a high level, chain-of-thought (CoT) transformers process a string by generating intermediate reasoning steps. These steps can be seen as a generated string itself or as an intermediate thinking process that is used to generate the final output.

::: definition
**Definition 8** (CoT Generation). *A causally masked transformer implementing the next-symbol distribution ${\overset{\curvearrowright}{{{p}}}}\colon {{{{{{\Sigma}}^{*}}}}}\to {{{{\mathcal{P}}}\mleft({{\overline{{{\Sigma}}}}}\mright)}}$ **generates** strings ${{y}}_1 \cdots {{y}}_{T}$, given ${{\boldsymbol{w}}}\in {{{{{{\Sigma}}^{*}}}}}$, as $$\begin{equation}
      {{y}}_t \sim {\overset{\curvearrowright}{{{p}}}}(\cdot \mid {{\boldsymbol{w}}}\circ {{y}}_1 \cdots {{y}}_{t-1})
\end{equation}$$ where ${{y}}_{T}= {{\textsc{eos}}}$ and ${{y}}_t \neq {{\textsc{eos}}}$ for $t < {T}$.*
:::

While some existing work analyzes the distributions induced by CoT transformers [@nowak-etal-2024-representational; @xu2025cotloopformalcomparison], much of the existing literature [@JMLR:v22:20-302; @feng2023towards; @merrill2024the; @li2024chain; @saunshi2025reasoninglatentthoughtspower] focuses on modeling string **acceptance** by CoT transformers by determinizing ${\overset{\curvearrowright}{{{p}}}}$.

::: definition
**Definition 9** (CoT Acceptance). *A causally masked transformer implementing the next-symbol predictor ${{\texttt{NS}}}\colon {{{{{{\Sigma}}^{*}}}}}\to {{{\Sigma}}_{01}}$ generates the sequence of reasoning steps $$\begin{equation}
      {{y}}_t \mathrel{\stackrel{\textnormal{\tiny def}}{=}}{{\texttt{NS}}}({{\boldsymbol{w}}}\circ {{y}}_1 \cdots {{y}}_{t-1})
\end{equation}$$ for ${{\boldsymbol{w}}}\in {{{{{{\Sigma}}^{*}}}}}$, $t \in {\left[ {T} \right]}$, and a pre-determined ${T}\in {{\mathbb{N}}}$. ${{\texttt{NS}}}$ **accepts** a string ${{\boldsymbol{w}}}$ in ${T}$ steps if ${{y}}_{T}= {\texttt{1}}$ and rejects it if ${{y}}_{T}= {\texttt{0}}$.*
:::

To facilitate a more convenient connection to MDMs, we introduce a *parallel* chain-of-thought process that can be seen as a generalization of CoT transformers that generates multiple symbols at once.

::: definition
**Definition 10** (Parallel chain of thought (pCoT)). *Let ${S}\colon {{{{{\Sigma}}_{{\texttt{m}}}}^{*}}} \to {{{\Sigma}}_{{\texttt{m}}01}}$ be a causally masked transformer symbol predictor and ${T}, {P}' \in {{\mathbb{N}}}$. A **parallel chain-of-thought transformer** ${{\texttt{NS}_{\|}}}\colon {{{{{{\Sigma}}^{*}}}}}\to {{{{{\Sigma}}_{{\texttt{m}}}}^{*}}}$ processes a string ${{\boldsymbol{w}}}$ from ${{{{{{\Sigma}}^{*}}}}}$ for $t \in {\left[ {T} \right]}$ as follows: $$\begin{equation}
      {\boldsymbol{t}}^{(t)}_{{n}}\sim {S}({{\boldsymbol{w}}}\circ {\boldsymbol{t}}^{(1)} \circ \cdots \circ {\boldsymbol{t}}^{(t - 1)} \circ \underbrace{{\texttt{m}}\cdots {\texttt{m}}}_{{P}'})_{{{N}}+ (t - 1) {P}' + {{n}}}  \text{\quad for } {{n}}\in {\left[ {P}' \right]}.
\end{equation}$$ Whenever ${T}$ and ${P}$ are clear from the context, we write ${{\texttt{NS}_{\|}}}({{\boldsymbol{w}}}) \mathrel{\stackrel{\textnormal{\tiny def}}{=}}{\boldsymbol{t}}^{(0)} \circ \cdots \circ {\boldsymbol{t}}^{({T})}$.*
:::

A pCoT transformer generates strings in ${{{{{{\Sigma}}^{*}}}}}$ by running the pCoT process on the empty string ${{\varepsilon}}$ and outputting ${{\texttt{NS}_{\|}}}({{\varepsilon}})$.

::: definition
**Definition 11** (Acceptance by a pCoT Transformer). *We say that a pCoT transformer **accepts** a string ${{\boldsymbol{w}}}$ if there exist ${T}, {P}' \in {{\mathbb{N}}}$ such that it holds for ${\boldsymbol{t}}^{(1)} \circ \cdots \circ {\boldsymbol{t}}^{({T})} = {{\texttt{NS}_{\|}}}({{\boldsymbol{w}}})$ that ${\boldsymbol{t}}^{({T})}_{{P}'} = {\texttt{1}}$. ${{\texttt{NS}_{\|}}}$ rejects ${{\boldsymbol{w}}}$ if ${\boldsymbol{t}}^{({T})}_{{P}'} = {\texttt{0}}$.*
:::

We denote the class of languages recognizable by a pCoT transformers that generate ${P}'$ symbols at a time for ${T}$ steps as ${\mathtt{pCoT}}[{T}, {P}]$, where ${P}\mathrel{\stackrel{\textnormal{\tiny def}}{=}}{P}' {T}$ is the total number of generated symbols.

The following relationships between CoT and pCoT transformers are clear.

::: restatable
propositionCoTAsPCoTProp []{#prop:cot-as-pcot label="prop:cot-as-pcot"} We have ${\mathtt{pCoT}}[{T}, {T}] = {\mathtt{CoT}}[{T}]$ and ${\mathtt{pCoT}}[{T}, {P}] \subseteq {\mathtt{CoT}}[{P}]$.
:::

## Padded looped transformers {#app:looped-padded-transformers}

Looped (or universal) transformers use a fixed block of transformer layers that is applied repeatedly to the input string [@dehghani2019universaltransformers]. This increases the depth of the model, enabling more complex reasoning by applying layers multiple times, and does not increase the model size, as the same block is reused for each iteration, thus reducing the memory footprint and computational cost [@bae2025mixtureofrecursionslearningdynamicrecursive]. We define looped transformers as follows.

::: {#def:looped-transformer .definition}
**Definition 12** (Looped transformer (PLT)). *Let ${{L}}, {T}\in {{\mathbb{N}}}$ and let $1 \leq {{l}}_1 \leq {{l}}_2 \leq {{L}}$. Given a depth-${{L}}$ transformer, a **looped transformer** (PLT) computes symbol contextual representations ${{{{{\bm{H}}}}}}$ by*

1.  *Computing the initial hidden states ${{{{{\bm{H}}}}}}^{(0)}$ for the input string ${{\boldsymbol{w}}}= {{w}}_1\cdots{{w}}_{{N}}$ and computing ${{{{{\bm{H}}}}}}^{({{l}}_1)}$ as with the first ${{l}}_1$ layers of the transformer*

2.  *Applying the transformer layers ${{l}}_1 + 1, \ldots, {{l}}_2$ ${T}$ times to the hidden states ${{{{{\bm{H}}}}}}^{({{l}}_1)}$ to obtain ${{{{{\bm{H}}}}}}^{({{l}}_1 + {T}({{l}}_2 - {{l}}_1))}$.*

3.  *Applying the transformer layers ${{l}}_2 + 1, \ldots, {{L}}$ to the hidden states ${{{{{\bm{H}}}}}}^{({{l}}_1 + {T}({{l}}_2 - {{l}}_1))}$ to obtain the final representations ${{{{{\bm{H}}}}}}$ that are passed to the output layer.*
:::

The representations ${{{{{\bm{H}}}}}}$ can then be used in the same way as described in [7.4.2](#app:language-models-and-next-symbol-predictors){reference-type="ref+label" reference="app:language-models-and-next-symbol-predictors"}.

The dynamic computational depth of PLTs endows them with the ability to perform more complex reasoning tasks by iteratively refining their hidden states over multiple timesteps. Importantly, these reasoning steps include both sequential and parallel processing of the input symbols, allowing for both parallel efficiency as well as depth in the reasoning process.

Padded transformers additionally pad the input string with padding (pause) symbols.

::: {#def:padded-transformer .definition}
**Definition 13** (Padded Transformer). *Given ${P}\in {{\mathbb{N}}}$, a **padded transformer** ${{\mathcal{T}}}$ transformer computes the contextual representations ${{{{{{{\bm{H}}}}}}}}$ of a string ${{\boldsymbol{w}}}\in {{{{{{\Sigma}}^{*}}}}}$ by processing the padded input ${{\boldsymbol{w}}}\circ \underbrace{{\square}\cdots {\square}}_{{P}}$ (possibly by looping, cf. [12](#def:looped-transformer){reference-type="ref+label" reference="def:looped-transformer"}), where ${\square}\notin {{\Sigma}}$ is a designated padding symbol.*
:::

Instead of being restricted to the contextual representations of the ${{N}}$ input symbols, a padded transformer can determine string membership or symbol probabilities based on the contextual representations of the ${P}$ additional padded symbols as well. This additional space can be used to perform more operations and is analogous to increasing the circuit width in circuit complexity.

Padding and looping together increase the expressivity of transformers.

::: restatable
remarkLoopedTransformersCharacterization []{#thm:looped-transformers-characterization label="thm:looped-transformers-characterization"} The following characterizations of padded looped transformers are known:

1.  $\text{Regular languages} \subseteq {\mathtt{PLT}}[\log{{N}}, 0]$ [@saunshi2025reasoninglatentthoughtspower Thm. 5.1],

2.  ${\mathtt{CoT}}[{T}] \preceq {\mathtt{PLT}}[{T}, 0]$, where the width of the PLT scales linearly with ${T}$ [@saunshi2025reasoninglatentthoughtspower Thm. 5.4],

3.  ${\mathtt{PLT}}[\log{{N}}, {{\mathtt{poly}}\mleft({{N}}\mright)}] = {{{{{{\mathtt{AC}}}^{\mathtt{0}}}}}}$ [@london2025pausetokensstrictlyincrease Thm. 4.1],

4.  ${\mathtt{PLT}}[\log{{N}}, {{\mathtt{poly}}\mleft({{N}}\mright)}] = {{{{{{\mathtt{TC}}}^{\mathtt{0}}}}}}$, where, in contrast to our model, the precision ${{\texttt{p}}}$ *scales* logarithmically with string length ${{N}}$ (@london2025pausetokensstrictlyincrease [Thm. 4.5], @merrill2025exactexpressivepowertransformers [Thm. 1]),

5.  ${\mathtt{PLT}}[\log^d{{{N}}}, {{\mathtt{poly}}\mleft({{N}}\mright)}] = {{{{{{\mathtt{TC}}}^{\mathtt{d}}}}}}$, where the precision ${{\texttt{p}}}$ *scales* logarithmically with string length ${{N}}$ [@merrill2025exactexpressivepowertransformers Thm. 3].
:::

#### Stochastic padded looped transformers.

The looping mechanism naturally accommodates the unmasking steps of MDMs. However, unlike PLT transformers, the MDM unmasking steps can be *stochastic*---the predictor *samples* the unmasked symbol from the infilling probability distribution defined by the transformer. To offer a more suitable analogue to MDMs, we introduce *stochastic* PLTs, which receive Gumbel-distributed noise as additional input to each loop, mimicking the sampling process of MDMs.

::: {#def:stochastic-looped-transformer .definition}
**Definition 14** (Stochastic padded looped transformer). *A **stochastic padded looped transformer** is a padded looped transformer in which ${{{{{\bm{H}}}}}}^{({{l}}_1 + t({{l}}_2 - {{l}}_1))}$ is augmented by a matrix of Gumbel-distributed noise variables at each time step $t \in {\left[ {T} \right]}$.*
:::

The fact that the MLPs in a transformer layer can implement the $\mathop{\mathrm{{{argmax}}}}$ operation used for Gumbel sampling (cf. [\[lem:mlp-for-argmax\]](#lem:mlp-for-argmax){reference-type="ref+label" reference="lem:mlp-for-argmax"}) allows PLTs to implement the planner and predictor of an MDM as we detail later (cf. [\[thm:lts-can-simulate-mdms\]](#thm:lts-can-simulate-mdms){reference-type="ref+label" reference="thm:lts-can-simulate-mdms"}).

Including stochasticity in PLTs is required for a natural connection to stochastic models such as MDMs and CoT transformers. While this departs from the standard definitions of PLTs, it is a natural extension that allows us to capture the stochastic nature of MDMs while retaining the looping structure. Since the Gumbel noise is assumed to come from an external source in the sampling procedure of MDMs and CoT transformers, we believe adding it as input to PLTs is natural.

## Our idealization of masked diffusion models {#app:idealization}

In the following, ${{{\Sigma}}_{{\texttt{m}}}}\mathrel{\stackrel{\textnormal{\tiny def}}{=}}{{\Sigma}}\cup \{{\texttt{m}}\}$ where ${\texttt{m}}\notin {{\Sigma}}$ is the mask symbol and ${{{\Sigma}}_{01}}\mathrel{\stackrel{\textnormal{\tiny def}}{=}}{{\Sigma}}\cup \{{\texttt{0}}, {\texttt{1}}\}$, where ${\texttt{0}}$ and ${\texttt{1}}$ are the reject and accept symbols.

::: definition
**Definition 15** (Planner). *A **planner** is a length-preserving function ${U}\colon {{{{{\Sigma}}_{{\texttt{m}}}}^{*}}} \to {{\mathcal{P}}}(\{{\texttt{0}}, {\texttt{1}}\})$. We distinguish two cases:*

1.  *An **unrestricted planner** is a planner that can choose to resample any symbol.*

2.  *A **mask dominated** planner is one where, if ${{w}}_{{n}}\neq {\texttt{m}}$, then $\mleft({U}({{\boldsymbol{w}}})_{{n}}\mright)_{{\texttt{0}}} = 1$, i.e., ${U}$ never tries to resample already unmasked symbols.*

*A **deterministic** planner is a length-preserving function ${U}\colon {{{{{\Sigma}}_{{\texttt{m}}}}^{*}}} \to {{\{{\texttt{0}}, {\texttt{1}}\}^{*}}}$.*
:::

::: definition
**Definition 16** (Symbol Predictor). *A **symbol predictor** is a length-preserving function ${S}\colon {{{{{\Sigma}}_{{\texttt{m}}}}^{*}}} \to {{{{\mathcal{P}}}({{{\Sigma}}_{01}})^{*}}}$. A **deterministic** symbol predictor is a length-preserving function ${S}\colon {{{{{\Sigma}}_{{\texttt{m}}}}^{*}}} \to {{{{{\Sigma}}_{01}}^{*}}}$.*
:::

::: definition
**Definition 17** (Masked Diffusion Model). *Given a planner ${U}$ and a symbol predictor ${S}$, we call ${M}= ({U}, {S})$ a **masked diffusion model** (MDM).*
:::

An MDM with a mask dominated planner corresponds closely to standard MDMs, where at each step, a subset of masked positions is selected for unmasking and then filled in. For example, setting the planner to implement uniformly random unmasking recovers the true reverse process of an MDM. We, however, additionally allow for unrestricted planners, which can also choose to *resample* already unmasked positions and we take this to be our default setting---whenever we refer to an MDM, we mean one with an unrestricted planner. While this departs from standard MDM formulations, it allows us to sidestep the limitations of the inability of the MDM to "change" its decisions and thus to correct earlier mistakes. This has recently been identified as a key limitation of MDMs and is the focus of much recent work on improving MDMs[@rutte2025generalized; @song2025seeddiffusionlargescalediffusion *inter alia*].

MDMs generate strings by iteratively unmasking and filling in symbols over a series of discrete denoising steps. We call this the **unmasking process**.

::: definition
**Definition 18** (Unmasking process). *Let ${M}$ be an MDM and ${T}, {P}\in {{\mathbb{N}}}$. Given a string ${{\boldsymbol{w}}}\in {{{{{{\Sigma}}^{*}}}}}$, ${M}$ generates the string ${M}({{\boldsymbol{w}}}) \mathrel{\stackrel{\textnormal{\tiny def}}{=}}{{\boldsymbol{y}}}^{({T})}$ as follows for $t \in {\left[ {T} \right]}$: $$\begin{align}
         {{\boldsymbol{y}}}^{(0)} &= \underbrace{{\texttt{m}}\cdots {\texttt{m}}}_{{P}} \\
         {{\boldsymbol{u}}}^{(t)} &\sim {U}({{\boldsymbol{w}}}\circ {{\boldsymbol{y}}}^{(t - 1)}) \\
         {{\boldsymbol{y}}}^{(t)}_{{n}}&=
         \begin{cases}
            {{y}}^{(t - 1)}_{{n}}& \textbf{if }{{\boldsymbol{u}}}^{(t)}_{{n}}= 0 \\
            {{y}}_{{n}}\sim {S}({{\boldsymbol{w}}}\circ {{\boldsymbol{y}}}^{(t - 1)})_{{{N}}+ {{n}}} & \textbf{otherwise }
         \end{cases} \text{\quad for } {{n}}\in {\left[ {P} \right]}
      \end{align}$$*
:::

By running the MDM on the empty string ${{\varepsilon}}$ and outputting ${M}({{\varepsilon}})$ (constraining the generated symbols to ${{\Sigma}}\subsetneq {{{\Sigma}}_{01}}$), an MDM generates strings in ${{{{{{\Sigma}}^{*}}}}}$ and thus defines an LM.

We can also use the MDM to define a membership test for languages by using the unmasking process to emit reasoning (intermediate) computations and looking at the final symbol of the generated string.

::: definition
**Definition 19** (Acceptance by an MDM). *We say that the diffusion model ${M}$ with a deterministic ${U}$ and ${S}$ **accepts** a string ${{\boldsymbol{w}}}\in {{{{{{\Sigma}}^{*}}}}}$ if there exist ${T}, {P}\in {{\mathbb{N}}}$ such that it holds for ${{\boldsymbol{y}}}^{({T})} = {M}({{\boldsymbol{w}}})$ that ${{y}}^{({T})}_{P}= {\texttt{1}}$. ${M}$ rejects ${{\boldsymbol{w}}}$ if ${{y}}^{({T})}_{P}= {\texttt{0}}$.*
:::

#### Transformer MDMs.

We focus on MDMs where both the planner and predictor are implemented by transformers.

::: definition
**Definition 20** (Transformer planner and predictor). *A **transformer planner** is a planner ${{{{{\Sigma}}_{{\texttt{m}}}}^{*}}} \to {{\mathcal{P}}}(\{{\texttt{0}}, {\texttt{1}}\})$ where the logits over $\{{\texttt{0}}, {\texttt{1}}\}$ are computed by a transformer.*

*A **transformer predictor** is a predictor ${S}\colon {{{{{\Sigma}}_{{\texttt{m}}}}^{*}}} \to {{{{{\Sigma}}_{01}}^{*}}}$ where the logits over ${{{\Sigma}}_{01}}$ at each position are computed by a transformer.*
:::

# A discussion of the theoretical model {#app:theoretical-model}

Our formalization of MDMs, centered around the planner and predictor, is motivated by the need to analyze their expressivity. While this definition intentionally departs from the exact analytical reverse process, it remains well-grounded in both empirical practice and theoretical considerations. In the following, we justify our modeling choices and clarify their connections to practical implementations.

## The planner as a principled departure from uniform unmasking {#app:planner-centrality}

At an intuitive level, our formalism distills the essential structure of practical MDMs: A process that iteratively unmasks and fills in symbols. In our idealization, this process is governed by a model that chooses which symbols to unmask and when. This is motivated by practical considerations---although the analytical reverse process of the MDM forward masking process would unmask symbols uniformly at random, this is not how practical MDMs operate, since they empirically benefit from strategic, context-aware unmasking [@ghazvininejad-etal-2019-mask; @peng2025pathplanningmaskeddiffusion; @zheng2024reparameterizeddiscretediffusionmodel; @liu2025thinkgeneratediscretediffusion; @kim2025trainworstplanbest; @benhamu2025acceleratedsamplingmaskeddiffusion]. Forcing a trained MDM to follow a uniform unmasking schedule is thus suboptimal---it prevents the model from decoding in an order that makes tasks easier to solve [@kim2025trainworstplanbest] and can lead to uncontrolled error propagation [@ghazvininejad-etal-2019-mask]. The dedicated planner captures many empirically successful planned unmasking strategies that consistently outperform random unmasking such as:

- **Confidence-based unmasking** that unmask based on the model's prediction confidence.

- **Difficulty-based scheduling** that masks informative symbols longer so they are generated last with maximum context [@he-etal-2023-diffusionbert].

- **Structured schedules** such as blockwise-autoregressive unmasking [@nie2025largelanguagediffusionmodels].

- **Learned planners** that use a separate trained model to guide unmasking decisions [@peng2025pathplanningmaskeddiffusion].

- **Confidence thresholding** that unmasks symbols whose confidence lies above a theory-suggested threshold [@wu2025fastdllmtrainingfreeaccelerationdiffusion].

Theoretically, uniform unmasking is only optimal if the symbol predictor is perfect and unfactorized, which is not the case in practice [@peng2025pathplanningmaskeddiffusion]. Furthermore, under a factorized backward process, uniform unmasking is computationally inefficient even with a perfect predictor. It requires at least a linear number of denoising steps in the string length to capture dependencies in even simple formal languages, and the KL divergence to the true data distribution converges slowly [@feng2025theoreticalbenefitlimitationdiffusion; @li2025convergencetheorydiffusionlanguage]. This lower bound negates any potential speed benefit over autoregressive models. A planner is therefore a necessary component for efficient, accurate generation, as it provides an educated choice of which symbols to unmask and generate in parallel, mitigating the weakness of the independence assumption. Frameworks such as that by @zheng2024reparameterizeddiscretediffusionmodel and augmented MDM evidence lower bounds that incorporate explicit planner terms [@peng2025pathplanningmaskeddiffusion; @liu2025thinkgeneratediscretediffusion] demonstrate that a planner can be a principled, optimizable component of the generative process.

To further justify our choice of modeling the planner as a separate component, we show that any MDM that can be implemented as a combination of a planner and a predictor can in fact be implemented by a single transformer that unmasks symbols based on their confidence---${{\textnormal{{\small \textsf{top-}}}k}}$ decoding. This means that all our results apply to the popular model of unmasking symbols based on their confidence [@ghazvininejad-etal-2019-mask; @peng2025pathplanningmaskeddiffusion; @zheng2024reparameterizeddiscretediffusionmodel; @liu2025thinkgeneratediscretediffusion; @kim2025trainworstplanbest; @benhamu2025acceleratedsamplingmaskeddiffusion].

::: definition
**Definition 21** (${{\textnormal{{\small \textsf{top-}}}k}}$ unmasking). *Let ${{\mathcal{T}}}$ be a transformer and $k \in {{\mathbb{N}}}$. The **${{\textnormal{{\small \textsf{top-}}}k}}$ unmasking process** of ${{\mathcal{T}}}$ is defined by the planner of the form $$\begin{equation}
      {U}({{\boldsymbol{y}}})_{{n}}= 
      \begin{cases}
         {\texttt{1}}& \textbf{if }{{n}}\in {{{{\textnormal{{\small \textsf{top-}}}k}}\mleft({{\mathcal{T}}}({{\boldsymbol{y}}})\mright)}} \\
         {\texttt{0}}& \textbf{otherwise }
      \end{cases}
      \text{ for } {{n}}\in {\left[ {{N}} \right]},
\end{equation}$$ where ${{\boldsymbol{y}}}\in {{{{{\Sigma}}_{{\texttt{m}}}}^{*}}}$ with ${{N}}= |{{\boldsymbol{y}}}|$ and ${{{{\textnormal{{\small \textsf{top-}}}k}}\mleft({{\mathcal{T}}}({{\boldsymbol{y}}})\mright)}}$ selects the $k$ positions in ${\left[ {{N}} \right]}$ with the largest maximal logits in ${{\mathcal{T}}}({{\boldsymbol{y}}})$. The predictor is defined as $$\begin{equation}
      {S}({{\boldsymbol{y}}})_{{n}}= {{{{p}}^{\downarrow}}({{w}}\mid {{\boldsymbol{y}}})}
      \text{ for } {{n}}\in {\left[ {{N}} \right]}.
\end{equation}$$ where ${{{{p}}^{\downarrow}}({{w}}\mid {{\boldsymbol{y}}})}$ is the infilling probability distribution of the ${{n}}\textsuperscript{th}$ token defined by ${{\mathcal{T}}}$.*
:::

::: restatable
theoremSingleModelEquivalenceThm []{#thm:single-model-equivalence label="thm:single-model-equivalence"} Let ${M}= ({U}, {S})$ be a masked diffusion model with a planner ${U}$ and a symbol predictor ${S}$. Then, there exists a transformer ${{\mathcal{T}}}$ such that it holds for any ${{\boldsymbol{y}}}\in {{{{{\Sigma}}_{{\texttt{m}}}}^{*}}}$ with ${{N}}= |{{\boldsymbol{y}}}|$ and ${{n}}\in {\left[ {{N}} \right]}$ that $$\begin{align}
         {{n}}\in {{{{\textnormal{{\small \textsf{top-}}}k}}\mleft({{\mathcal{T}}}({{\boldsymbol{y}}})\mright)}} &\iff {U}({{\boldsymbol{y}}})_{{n}}= {\texttt{1}}\\
         {{{{p}}^{\downarrow}}({{w}}\mid {{\boldsymbol{y}}})} &= {S}({{\boldsymbol{y}}})_{{{n}}}
      \end{align}$$ where ${{{{p}}^{\downarrow}}({{w}}\mid {{\boldsymbol{y}}})}$ denotes the infilling probability distribution of the ${{n}}\textsuperscript{th}$ token defined by ${{\mathcal{T}}}$.
:::

::: proof
*Proof.* The construction of ${{\mathcal{T}}}$ combines the planner ${U}$ and predictor ${S}$ into a single model that predicts the next symbol based on the planner's decision and the predictor's distribution. In particular, ${{\mathcal{T}}}$ runs ${U}$ and ${S}$ in parallel. The planner's output decision ${U}({{\boldsymbol{y}}})_{{n}}$ of ${\texttt{1}}$ or ${\texttt{0}}$ can be made based on implementing the $\mathop{\mathrm{{{argmax}}}}$ function with an MLP (cf. [\[lem:mlp-for-argmax\]](#lem:mlp-for-argmax){reference-type="ref+label" reference="lem:mlp-for-argmax"}) if the planner is deterministic or by inserting the noise from the Gumbel-max sampling (cf. [7](#def:gumbel-max-sampling){reference-type="ref+label" reference="def:gumbel-max-sampling"}) if the planner is probabilistic. ${{\mathcal{T}}}$ can then use this information to down-weight the logits of that are chosen not to be unmasked by the predictor by subtracting ${{B_{{{\mathbb{F}}}}}}$ from the logits of the symbols that are not chosen by the planner. This ensures that only the symbols that are chosen by the planner can be predicted by the predictor. The predictor ${S}$ can in parallel compute its residual stream and the accompanying logits before combining them with the planner's decisions. The subtraction of the ${{B_{{{\mathbb{F}}}}}}$ from the logits of the symbols that are not chosen by the planner ensures that ${{\textnormal{{\small \textsf{top-}}}k}}$ will only select the symbols that are chosen by the planner and the simulation of the predictor ensures that the infilling distributions match.¬†‚óª
:::

## Editing and non-editing MDMs. {#app:editing-mdms}

The choice to allow the MDM to resample already unmasked symbols also departs from the analytical reverse process of MDMs, which only unmask masked symbols. This is, however, a principled choice that allows the MDM to correct earlier mistakes and is supported by empirical evidence that resampling already unmasked symbols can improve generation quality [@rutte2025generalized]. Practically, this choice enables conciser connections to PLTs and pCoT transformers. Moreover, all results in this work can easily be adapted to MDMs with mask dominated planners that do not resample already unmasked symbols. This is justified by the following theorem that shows that any MDM with an unrestricted planner can be simulated by an MDM with a mask dominated planner by increasing the output space by a factor of ${T}$ to account for the inability to resample. In the following, we refer to MDMs with mask dominated planners as *simple* MDMs(sMDMs).

::: restatable
theoremMDMsCanSimulateFPLTsThm []{#thm:mdms-can-simulate-emdms label="thm:mdms-can-simulate-emdms"} $$\begin{equation}
      {\mathtt{MDM}}[{T}, {P}] \subseteq {\mathtt{sMDM}}[{T}, {T}{P}].
\end{equation}$$
:::

:::: proof
*Proof.* At a high level, the sMDM's planner selects the decoding space by selecting the next ${P}$ positions to unmask, and the predictor

::: enumerate*
reads the string generated so far,

simulates a step of the MDM transformer on the string, and

writes the updated values into a *new* portion of the padding space.
:::

More precisely, we can implement the sMDM transformer as follows:

1.  The sMDM transformer uses ${T}{P}$ masked symbols to store the generated symbols at each of the ${T}$ unmasking steps of the MDM transformer.

2.  The planner is implemented as the transformer from [\[lem:select-block\]](#lem:select-block){reference-type="ref+label" reference="lem:select-block"} that acts independently of the input string and uses positional encodings to select the next ${P}$ positions to unmask.

3.  The predictor is implemented as a transformer that predicts the values of the ${P}$ masked symbols based on the current input string. It reads the values from the padding space by attending to the last ${P}$ unmasked positions in the padding space analogous to the dump-decode-read mechanism from [\[lem:idenitity-dump\]](#lem:idenitity-dump){reference-type="ref+label" reference="lem:idenitity-dump"}. The only difference is that the predictor has to attend to the *last* unmasked block of decoded values. This can be done with a two additional transformer layers by

¬†‚óª
::::

# Theoretical gadgets {#app:theoretical-gadgets}

This section contains various theoretical gadgets that are used in the proofs of the main results. Not all of these are novel, and some are modified restatements from their original sources.

In the following, ${{N}}\in {{\mathbb{N}}}$ always refers to the length of the original input string. If the string is additionally padded, the number of padding symbols is denoted by ${P}$, meaning that the entire input to the transformer is of length ${{N}}+ {P}$.

## Positional Encodings {#app:positional-encodings}

Uniquely identifying positions in a string requires the "volume" of the representation space to grow with the string length. In the case of finite-precision logarithmic-width transformers, this is achieved with positional encodings that encode the binary representation of the position in the string. The following lemma follows from the definition of fixed-point arithmetic, and the rounding and thresholding applied therein.

::: restatable
lemmaFinitePrecisionPropertiesOurVersion []{#lem:our-finite-precision-properties label="lem:our-finite-precision-properties"} Let $x \in {{\mathbb{F}}}_{{\texttt{p}}}$ for some ${{\texttt{p}}}\in {{\mathbb{N}}}$ such that $x > \log{2} ({{\texttt{p}}}+ 1)$. Then, it holds that $$\begin{align}
         \exp(x) &= {{B_{{{\mathbb{F}}}}}}, \\
         \exp(-x) &= 0.
      \end{align}$$
:::

[\[lem:finite-precision-properties,lem:pos-enc-properties\]](#lem:finite-precision-properties,lem:pos-enc-properties){reference-type="ref+label" reference="lem:finite-precision-properties,lem:pos-enc-properties"} readily follow from [\[lem:our-finite-precision-properties\]](#lem:our-finite-precision-properties){reference-type="ref+label" reference="lem:our-finite-precision-properties"}.

::: restatable
lemmaFinitePrecisionProperties []{#lem:finite-precision-properties label="lem:finite-precision-properties"} For ${{B_{{{\mathbb{F}}}}}}$ from [7.3](#app:fixed-point-arithmetic){reference-type="ref+label" reference="app:fixed-point-arithmetic"}, it holds that $$\begin{align}
         \exp({{B_{{{\mathbb{F}}}}}}) &= {{B_{{{\mathbb{F}}}}}}, \\
         \exp(-{{B_{{{\mathbb{F}}}}}}) &= 0.
      \end{align}$$
:::

::: restatable
lemmaPosEncProperties []{#lem:pos-enc-properties label="lem:pos-enc-properties"} For ${{N}}\in {{\mathbb{N}}}$, ${{n}}\in {\left[ {{N}} \right]}$, define the vectors ${{{\bm{q}}}}_{{n}}\in {{\mathbb{R}}}^{2 \ceil{\log{{N}}}}$ and ${{{\bm{k}}}}_{{n}}\in {{\mathbb{R}}}^{2 \ceil{\log{{N}}}}$ as follows: $$\begin{align}
         {{{\bm{q}}}}_{{n}}&\mathrel{\stackrel{\textnormal{\tiny def}}{=}}{{B_{{{\mathbb{F}}}}}}\cdot ({{{{{\texttt{B}}^{\pm}}\mleft({{n}}\mright)}}^\frown{{{\bm{1}}}_{\ceil{\log{{N}}}}}}) \\
         {{{\bm{k}}}}_{{{n}}'} &\mathrel{\stackrel{\textnormal{\tiny def}}{=}}{{{{{\texttt{B}}^{\pm}}\mleft({{n}}'\mright)}}^\frown{(-{{\bm{1}}}_{\ceil{\log{{N}}}})}}.
      \end{align}$$ Then, it holds that $$\begin{equation}
      {{{{{\bm{q}}}}_{{n}}^\top {{{\bm{k}}}}_{{{n}}'}}} =
      \begin{cases}
         0 & \textbf{if }{{n}}= {{n}}' \\
         -{{B_{{{\mathbb{F}}}}}}& \textbf{otherwise }.
      \end{cases}
\end{equation}$$ Thus, $$\begin{equation}
      \exp({{{{{\bm{q}}}}_{{n}}^\top {{{\bm{k}}}}_{{{n}}'}}}) =
      \begin{cases}
         1 & \textbf{if }{{n}}= {{n}}' \\
         0 & \textbf{otherwise }.
      \end{cases}
\end{equation}$$
:::

The following slight generalization of [\[lem:pos-enc-properties\]](#lem:pos-enc-properties){reference-type="ref+label" reference="lem:pos-enc-properties"} is also easy to show.

::: restatable
lemmaPosEncPropertiesGen []{#lem:our-pos-enc-properties label="lem:our-pos-enc-properties"} For ${{N}}\in {{\mathbb{N}}}$, ${{n}}\in {\left[ {{N}} \right]}$, define the vectors ${{{\bm{q}}}}_{{n}}\in {{\mathbb{R}}}^{2 \ceil{\log{{N}}}}$ and ${{{\bm{k}}}}_{{n}}\in {{\mathbb{R}}}^{2 \ceil{\log{{N}}}}$ as follows: $$\begin{align}
         {{{\bm{q}}}}_{{n}}&\mathrel{\stackrel{\textnormal{\tiny def}}{=}}\sfrac{{{B_{{{\mathbb{F}}}}}}}{m} \cdot ({{{{{\texttt{B}}^{\pm}}\mleft({{n}}\mright)}}^\frown{{{\bm{1}}}_{\ceil{\log{{N}}}}}}) \\
         {{{\bm{k}}}}_{{{n}}'} &\mathrel{\stackrel{\textnormal{\tiny def}}{=}}{{{{{\texttt{B}}^{\pm}}\mleft({{n}}'\mright)}}^\frown{(-{{\bm{1}}}_{\ceil{\log{{N}}}})}}.
      \end{align}$$ Then, it holds that $$\begin{equation}
      {{{{{\bm{q}}}}_{{n}}^\top {{{\bm{k}}}}_{{{n}}'}}} =
      \begin{cases}
         0 & \textbf{if }{{n}}= {{n}}' \\
         x & \textbf{otherwise }.
      \end{cases}
\end{equation}$$ where $x \leq -\sfrac{\textcolor{ETHRed}{2}{{B_{{{\mathbb{F}}}}}}}{m}$.
:::

Our constructions heavily rely on specific positional encodings. We assume that this positional information comes from an outside source and is not computed by the transformer model directly. We note that this is in contrast to some existing work with causally masked transformers where the positional encodings are inferred by the model itself [@yang2024masked; @li2025characterizingexpressivitytransformerlanguage; @jerad2025uniquehardattentiontale]. This matters for multiple reasons. First, including positional information from an external source allows us to decouple the computation of positional information from the computations performed by the transformer model. Furthermore, it facilitates providing the model with structured information that it would not be able to compute on its own. While this could be abused to give the model unrealistic computational power, we assume that the positional information is easily computable and thus realistic.

In that vein, it is interesting to consider what is the minimal amount of positional information that the model has to be provided with for it to be able to construct the useful positional encodings. This is described by the following lemma. At a high level, it says that our positional encodings require the binary encodings of the relevant numbers, along with any modular and division operations performed on them. Addition, thresholding, and multiplication by a power of two, in contrast, can be performed by the MLPs in the transformer model.

::: restatable
lemmaMLPArithmeticOperationsLemma []{#lem:mlp-arithmetic-operations label="lem:mlp-arithmetic-operations"} Let $m, n \in {{\mathbb{N}}}$. Then, given the binary encodings of the numbers, ${{\texttt{B}}\mleft(m\mright)}, {{\texttt{B}}\mleft(n\mright)}$, there exist MLPs that can compute the following operations:

1.  computing the signed binary encoding ${{{\texttt{B}}^{\pm}}\mleft(m\mright)}$,

2.  computing the sum ${{\texttt{B}}\mleft(m\mright)} + {{\texttt{B}}\mleft(n\mright)}$ and the difference ${{\texttt{B}}\mleft(m\mright)} - {{\texttt{B}}\mleft(n\mright)}$,

3.  computing ${{\texttt{B}}\mleft(2^k m\mright)}$ for $k \in {{\mathbb{N}}}$,

4.  computing the indicator function ${\mathbbm{1}} \left\{ m \geq 0 \right\}$,

5.  computing the indicator function ${\mathbbm{1}} \left\{ m = 0 \right\}$, and

6.  computing the positive-part function ${\left[ m \right]_{+}}$.
:::

::: proof
*Proof.* ¬†

1.  The transformation ${{{\texttt{B}}^{\pm}}\mleft(m\mright)} \mathrel{\stackrel{\textnormal{\tiny def}}{=}}2 {{\texttt{B}}\mleft(m\mright)} - {{\bm{1}}}_{\ceil{\log m}}$ is an affine function that can be implemented by the affine part of the MLP followed by the identity function, which can be implemented by an MLP as well.

2.  It is easy to implement `AND` and `NOT` gates by an MLP. This suffices to implement any ${{{{{{\mathtt{AC}}}^{\mathtt{0}}}}}}$ (and thus addition and subtraction) circuit.

3.  The transformation ${{\texttt{B}}\mleft(m\mright)} \mapsto {{\texttt{B}}\mleft(2^k m\mright)}$ can be performed by shifting the binary representation of ${{\texttt{B}}\mleft(m\mright)}$ to the left by $k$ positions, which can be implemented by a linear transformation.

4.  The indicator function ${\mathbbm{1}} \left\{ m \geq 0 \right\}$ can be computed by checking the sign bit of the signed binary representation ${{{\texttt{B}}^{\pm}}\mleft(m\mright)}$, which can be implemented by an MLP.

5.  The indicator function ${\mathbbm{1}} \left\{ m = 0 \right\}$ can be computed by checking if all bits of the binary representation ${{\texttt{B}}\mleft(m\mright)}$ are zero, which can also be implemented by an MLP.

6.  Computing ${\left[ m \right]_{+}}$ can be done by first computing ${\mathbbm{1}} \left\{ m \geq 0 \right\}$ with *(d)* and then conditionally outputting $m$ or $0$ based on the result, which can be implemented by an MLP similar to [\[lem:projection-mlp\]](#lem:projection-mlp){reference-type="ref+label" reference="lem:projection-mlp"}.

¬†‚óª
:::

#### Simplification of positional encodings.

[\[lem:mlp-arithmetic-operations\]](#lem:mlp-arithmetic-operations){reference-type="ref+label" reference="lem:mlp-arithmetic-operations"} leads us to conclude that the relatively complicated positional encodings used in the following proofs can be thought of as simple transformations of the "basic" information captured by

- ${{{\texttt{B}}\mleft({{n}}\mright)}}$,

- ${{{\texttt{B}}\mleft({{N}}\mright)}}$,

- ${{{\texttt{B}}\mleft({{n}}\mod m\mright)}}$ for some relevant $m \in {{\mathbb{N}}}$, and

- ${{{\texttt{B}}\mleft(\sfrac{{{n}}}{m}\mright)}}$ for some relevant $m \in {{\mathbb{N}}}$.

This makes the positional encodings in our constructions streamlined and easily computable. While somewhat non-standard, we note that positional encodings based on both the symbol position ${{n}}$ and string length ${{N}}$ are common in theoretical literature [@chiang-cholak-2022-overcoming].

Despite being simple to compute, these positional encodings are powerful enough to allow the transformer to uniquely identify positions in the string and to perform useful computations based on them. In a sense, the inclusion of this information is also necessary, as the operations such as division and modular arithmetic---including the computation of the binary encodings---lie outside of ${{{{{{\mathtt{AC}}}^{\mathtt{0}}}}}}$ and thus cannot be performed by finite-precision transformers [@li2024chain]. We note, however, that more expressive transformers such as those with logarithmic precision could possibly implement the required functions to compute the information in ${{{\texttt{B}}\mleft({{n}}\mright)}}$, ${{{\texttt{B}}\mleft({{n}}\mod m\mright)}}$, and ${{{\texttt{B}}\mleft(\sfrac{{{n}}}{m}\mright)}}$ from ${{n}}$ and ${{N}}$ directly since, unlike fixed-precision transformers, they are not constrained to ${{{{{{\mathtt{AC}}}^{\mathtt{0}}}}}}$ operations [@li2024chain].[^22] The simplicity and uniformity of these encodings lies in contrast to more complex (non-uniform) positional encodings that directly serialize the circuits to be simulated when analyzing expressivity lower bounds [@li2024chain; @saunshi2025reasoninglatentthoughtspower; @london2025pausetokensstrictlyincrease].

## Useful attention patterns {#app:indexing-positions}

The following lemmata describe how a transformer layer can either ignore or exclusively focus on specific positions in the input string.

::: restatable
lemmaIgnoreMarkedPositionsTransformer []{#lem:ignore-positions label="lem:ignore-positions"} Let ${{N}}, {{D}}\in {{\mathbb{N}}}$, ${{{\mathcal{N}}}}\subseteq {\left[ {{N}} \right]}$, and let ${{{{{{{\bm{H}}}}}}}}\in {{\mathbb{R}}}^{{{N}}\times {{D}}}$ be a matrix representing the residual stream such that $$\begin{equation}
 \label{eq:ignore-positions-condition}
      {{\llbracket{\square}\rrbracket}} \in {{{{{{{\bm{H}}}}}}}}_{{{n}}, :} \iff {{n}}\in {{{\mathcal{N}}}}.
\end{equation}$$ Here, the notation ${{\llbracket{\square}\rrbracket}} \in {{{{{{{\bm{H}}}}}}}}_{{{n}}, :}$ means that the vector ${{{{{{{\bm{H}}}}}}}}_{{{n}}, :}$ contains the one-hot encoding of the symbol ${\square}$ at position ${{n}}$. Further, let ${{{\bm{G}}}}\mathrel{\stackrel{\textnormal{\tiny def}}{=}}{{{{{{{\bm{H}}}}}}}}_{{\left[ {{N}} \right]} \setminus {{{\mathcal{N}}}}, :} \in {{\mathbb{R}}}^{({{N}}- |{{{\mathcal{N}}}}|) \times {{D}}}$, where ${{{{{{{\bm{H}}}}}}}}_{{\left[ {{N}} \right]} \setminus {{{\mathcal{N}}}}, :}$ denotes the projection of the matrix ${{{{{{{\bm{H}}}}}}}}$ onto the rows *not* in ${{{\mathcal{N}}}}$. Finally, let ${{\boldsymbol{\tau}}}$ be a transformer layer. Then, there exists a logarithmic-width transformer layer ${{\boldsymbol{\tau}}}'$ such that it holds for ${{{\bm{G}}}}' \mathrel{\stackrel{\textnormal{\tiny def}}{=}}{{\boldsymbol{\tau}}}({{{\bm{G}}}}) \in {{\mathbb{R}}}^{({{N}}- |{{{\mathcal{N}}}}|) \times {{D}}}$ and ${{{{{{{\bm{H}}}}}}}}' \mathrel{\stackrel{\textnormal{\tiny def}}{=}}{{\boldsymbol{\tau}}}'({{{{{{{\bm{H}}}}}}}}) \in {{\mathbb{R}}}^{{{N}}\times {{D}}}$ that $$\begin{equation}
      {{{\bm{G}}}}' = {{{{{{{\bm{H}}}}}}}}'_{{\left[ {{N}} \right]} \setminus {{{\mathcal{N}}}}, :}.
\end{equation}$$
:::

Informally, [\[lem:ignore-positions\]](#lem:ignore-positions){reference-type="ref+label" reference="lem:ignore-positions"} states that a transformer layer can ignore positions containing one-hot encodings of specific "marker" symbols, such as additional symbols not in the original alphabet. Here, ignoring means that the content of the positions with the marker symbols does not affect the output of the transformer layer at other positions.

::: proof
*Proof.* Notice that, since ${{{{{{{\bm{H}}}}}}}}$ and ${{{\bm{G}}}}$ match on all positions not in ${{{\mathcal{N}}}}$, *ignoring* the positions in ${{{\mathcal{N}}}}$ (marked by ${\square}$) by ${{\boldsymbol{\tau}}}'$ will ensure that the outputs of the two layers ${{\boldsymbol{\tau}}}$ and ${{\boldsymbol{\tau}}}'$ are identical on the positions not in ${{{\mathcal{N}}}}$. We now construct a transformer layer that ignores the contributions of rows marked by ${{\llbracket{\square}\rrbracket}}$. To do so, we modify each attention head of ${{\boldsymbol{\tau}}}$ such that the head computes its attention scores with queries and keys of the form $$\begin{align}
         {{{\bm{q}}}}'_{{n}}&\mathrel{\stackrel{\textnormal{\tiny def}}{=}}\cdot 
         \begin{pmatrix}
            {{{\bm{q}}}}_{{n}}\\
            -{{B_{{{\mathbb{F}}}}}}\cdot {{\llbracket{\square}\rrbracket}} \\
            -{{B_{{{\mathbb{F}}}}}}\cdot {{\llbracket{\square}\rrbracket}} \\
         \end{pmatrix} \\
         {{{\bm{k}}}}'_{{{n}}'} &\mathrel{\stackrel{\textnormal{\tiny def}}{=}}\phantom{{{B_{{{\mathbb{F}}}}}}\cdot} 
         \begin{pmatrix}
            {{{\bm{k}}}}_{{{n}}'} \\
            {{\llbracket{{w}}_{{{n}}'}\rrbracket}} \\
            {{\llbracket{{w}}_{{{n}}'}\rrbracket}}
         \end{pmatrix}
      \end{align}$$ where ${{{\bm{q}}}}_{{n}}$ and ${{{\bm{k}}}}_{{{n}}'}$ are the original head's query and key vectors of ${{\mathcal{T}}}$ at position ${{n}}$ and ${{n}}'$, respectively, and ${{\llbracket{{w}}_{{{n}}'}\rrbracket}}$ is the one-hot encoding of the symbol at position ${{n}}'$. We can then compute the dot product of the two vectors as $$\begin{equation}
      {{{{{{\bm{q}}}}'}_{{n}}^\top {{{{\bm{k}}}}'}_{{{n}}'}}} = {{{{{\bm{q}}}}_{{n}}^\top {{{\bm{k}}}}_{{{n}}'}}} - {{B_{{{\mathbb{F}}}}}}\cdot {\mathbbm{1}} \left\{ {{w}}_{{{n}}'} = {\square} \right\} - {{B_{{{\mathbb{F}}}}}}\cdot {\mathbbm{1}} \left\{ {{w}}_{{{n}}'} = {\square} \right\}.
\end{equation}$$ Thus, if the symbol at position ${{n}}'$ is not ${\square}$, the attention score is ${{{{{\bm{q}}}}_{{n}}^\top {{{\bm{k}}}}_{{{n}}'}}}$ and the head behaves as it did in ${{\mathcal{T}}}$. If the symbol at position ${{n}}'$ is ${\square}$, the last two components of the vectors ${{{\bm{q}}}}'_{{n}}$ and ${{{\bm{k}}}}'_{{{n}}'}$ ensure that the exponentiated value of the attention score becomes $0$, thus not contributing to the attention weights. ${{\mathcal{T}}}'$ can thus simulate ${{\mathcal{T}}}$ on the rest of the positions.¬†‚óª
:::

::: restatable
lemmaFocusOnMarkedPositionsTransformer []{#lem:focus-on-marked-positions label="lem:focus-on-marked-positions"} Let ${{N}}, {{D}}\in {{\mathbb{N}}}$, $R\colon {\left[ {{N}} \right]} \to {\left[ {{N}} \right]}$, $r \in {\left[ {{N}} \right]}$, ${{{\mathcal{N}}}}\mathrel{\stackrel{\textnormal{\tiny def}}{=}}{R^{{-1}}}(r) \subseteq {\left[ {{N}} \right]}$, and let ${{{{{{{\bm{H}}}}}}}}\in {{\mathbb{R}}}^{{{N}}\times {{D}}}$ be a matrix representing the residual stream such that $$\begin{equation}
 \label{eq:focus-on-positions-condition}
      {{\overline{{\texttt{B}}}\mleft(R({{n}})\mright)}} \in {{{{{{{\bm{H}}}}}}}}_{{{n}}, :} \quad \text{for all } {{n}}\in {\left[ {{N}} \right]}
\end{equation}$$ Here, the notation ${{\overline{{\texttt{B}}}\mleft(R({{n}})\mright)}} \in {{{{{{{\bm{H}}}}}}}}_{{{n}}, :}$ means that the vector ${{{{{{{\bm{H}}}}}}}}_{{{n}}, :}$ contains the signed binary encoding (cf. [2](#sec:preliminaries){reference-type="ref+label" reference="sec:preliminaries"}) of $R({{n}})$. Further, let ${{{\bm{G}}}}\mathrel{\stackrel{\textnormal{\tiny def}}{=}}{{{{{{{\bm{H}}}}}}}}_{{{{\mathcal{N}}}}, :} \in {{\mathbb{R}}}^{|{{{\mathcal{N}}}}| \times {{D}}}$, where ${{{{{{{\bm{H}}}}}}}}_{{{{\mathcal{N}}}}, :}$ denotes the projection of the matrix ${{{{{{{\bm{H}}}}}}}}$ onto the rows in ${{{\mathcal{N}}}}$. Finally, let ${{\boldsymbol{\tau}}}$ be a transformer layer. Then, there exists a logarithmic-width transformer layer ${{\boldsymbol{\tau}}}'$ such that it holds for ${{{\bm{G}}}}' \mathrel{\stackrel{\textnormal{\tiny def}}{=}}{{\boldsymbol{\tau}}}({{{\bm{G}}}}) \in {{\mathbb{R}}}^{|{{{\mathcal{N}}}}| \times {{D}}}$ and ${{{{{{{\bm{H}}}}}}}}' \mathrel{\stackrel{\textnormal{\tiny def}}{=}}{{\boldsymbol{\tau}}}'({{{{{{{\bm{H}}}}}}}}) \in {{\mathbb{R}}}^{{{N}}\times {{D}}}$ that $$\begin{equation}
      {{{\bm{G}}}}' = {{{{{{{\bm{H}}}}}}}}'_{{{{\mathcal{N}}}}, :}.
\end{equation}$$
:::

Informally, [\[lem:focus-on-marked-positions\]](#lem:focus-on-marked-positions){reference-type="ref+label" reference="lem:focus-on-marked-positions"} states that a transformer layer can focus on positions containing signed binary encodings of a number $r$ computed as a function of the position while ignoring the rest of the positions.

::: proof
*Proof.* The idea of the construction of ${{\boldsymbol{\tau}}}'$ is similar to that of [\[lem:ignore-positions\]](#lem:ignore-positions){reference-type="ref+label" reference="lem:ignore-positions"}, but instead of ignoring the positions in ${{{\mathcal{N}}}}$, we want the transformer layer to focus on them while ignoring the rest of the positions. This can be done by including ${{\overline{{\texttt{B}}}\mleft(R({{n}})\mright)}}$ in the positional encodings of the attention heads and then using the key and query vectors of the form $$\begin{align}
         {{{\bm{q}}}}'_{{n}}&\mathrel{\stackrel{\textnormal{\tiny def}}{=}}
         \begin{pmatrix}
            {{{\bm{q}}}}_{{n}}\\
            \sfrac{{{B_{{{\mathbb{F}}}}}}}{2} \cdot ({{{{{\texttt{B}}^{\pm}}\mleft(r\mright)}}^\frown{{{\bm{1}}}_{\ceil{\log{{N}}}}}}) \\
            \sfrac{{{B_{{{\mathbb{F}}}}}}}{2} \cdot ({{{{{\texttt{B}}^{\pm}}\mleft(r\mright)}}^\frown{{{\bm{1}}}_{\ceil{\log{{N}}}}}}) \\
         \end{pmatrix} \\
         {{{\bm{k}}}}'_{{{n}}'} &\mathrel{\stackrel{\textnormal{\tiny def}}{=}}\phantom{\sfrac{{{B_{{{\mathbb{F}}}}}}}{2} \cdot} 
         \begin{pmatrix}
            {{{\bm{k}}}}_{{{n}}'} \\
            {{{{{\texttt{B}}^{\pm}}\mleft(R({{n}}')\mright)}}^\frown{(-{{\bm{1}}}_{\ceil{\log{{N}}}})}} \\
            {{{{{\texttt{B}}^{\pm}}\mleft(R({{n}}')\mright)}}^\frown{(-{{\bm{1}}}_{\ceil{\log{{N}}}})}} \\
         \end{pmatrix}
      \end{align}$$ where ${{{\bm{q}}}}_{{n}}$ and ${{{\bm{k}}}}_{{{n}}'}$ are the original head's query and key vectors of ${{\mathcal{T}}}$ at position ${{n}}$ and ${{n}}'$, respectively, and ${{B_{{{\mathbb{F}}}}}}$ is the maximal representable number in the fixed-point arithmetic (which might depend on the string length ${{N}}$). We can then compute the dot product of the two vectors as $$\begin{align}
         {{{{{{\bm{q}}}}'}_{{n}}^\top {{{{\bm{k}}}}'}_{{{n}}'}}} 
         &= {{{{{\bm{q}}}}_{{n}}^\top {{{\bm{k}}}}_{{{n}}'}}} + \underbrace{\sfrac{{{B_{{{\mathbb{F}}}}}}}{2} \cdot {{({{{{{\texttt{B}}^{\pm}}\mleft(r\mright)}}^\frown{{{\bm{1}}}_{\ceil{\log{{N}}}}}})^\top ({{{{{\texttt{B}}^{\pm}}\mleft(R({{n}}')\mright)}}^\frown{(-{{\bm{1}}}_{\ceil{\log{{N}}}})}})}}}_{\mathrel{\stackrel{\textnormal{\tiny def}}{=}}G} \label{eq:focusing-attention-dot-product} \\
         & \phantom{= {{{{{\bm{q}}}}_{{n}}^\top {{{\bm{k}}}}_{{{n}}'}}}} \ + \underbrace{\sfrac{{{B_{{{\mathbb{F}}}}}}}{2} \cdot {{({{{{{\texttt{B}}^{\pm}}\mleft(r\mright)}}^\frown{{{\bm{1}}}_{\ceil{\log{{N}}}}}})^\top ({{{{{\texttt{B}}^{\pm}}\mleft(R({{n}}')\mright)}}^\frown{(-{{\bm{1}}}_{\ceil{\log{{N}}}})}})}}}_{\mathrel{\stackrel{\textnormal{\tiny def}}{=}}G}  \nonumber
      \end{align}$$

Note that [\[eq:focusing-attention-dot-product\]](#eq:focusing-attention-dot-product){reference-type="ref+label" reference="eq:focusing-attention-dot-product"} uses fixed-point arithmetic. We compute the inner product in [\[eq:focusing-attention-dot-product\]](#eq:focusing-attention-dot-product){reference-type="ref+label" reference="eq:focusing-attention-dot-product"} by analyzing individual cases:

1.  **Case 1:** $R({{n}}') \neq r$.

    All intermediate computations of [\[eq:focusing-attention-dot-product\]](#eq:focusing-attention-dot-product){reference-type="ref+label" reference="eq:focusing-attention-dot-product"} are thresholded at $\min({{B_{{{\mathbb{F}}}}}}, {{{{{\bm{q}}}}_{{n}}^\top {{{\bm{k}}}}_{{{n}}'}}} + \sfrac{{{B_{{{\mathbb{F}}}}}}}{2})$. In particular, by [\[lem:our-pos-enc-properties\]](#lem:our-pos-enc-properties){reference-type="ref+label" reference="lem:our-pos-enc-properties"}, the value after adding the first term $G$ is at most $\min({{B_{{{\mathbb{F}}}}}}, {{{{{\bm{q}}}}_{{n}}^\top {{{\bm{k}}}}_{{{n}}'}}} + \sfrac{{{B_{{{\mathbb{F}}}}}}}{2}) - 2\sfrac{{{B_{{{\mathbb{F}}}}}}}{2} \leq {{B_{{{\mathbb{F}}}}}}-   {{B_{{{\mathbb{F}}}}}}= 0$. After adding the second term $G$, the value is at most $-{{B_{{{\mathbb{F}}}}}}$, resulting in a $0$ exponentiated attention score, as required.

2.  **Case 2:** $R({{n}}') = r$. We analyze three sub-cases based on the value of ${{{{{\bm{q}}}}_{{n}}^\top {{{\bm{k}}}}_{{{n}}'}}}$.

    1.  **Sub-case 2a:** ${{\left\lvert {{{{{\bm{q}}}}_{{n}}^\top {{{\bm{k}}}}_{{{n}}'}}} \right\rvert}} < \log{2} ({{\texttt{p}}}+ 1)$. All intermediate computations in [\[eq:focusing-attention-dot-product\]](#eq:focusing-attention-dot-product){reference-type="ref+label" reference="eq:focusing-attention-dot-product"} are bounded by $\log{2} ({{\texttt{p}}}+ 1) + \sfrac{{{B_{{{\mathbb{F}}}}}}}{2}$ in absolute value, so they fall within the range of ${{\mathbb{F}}}_{{\texttt{p}}}$. Moreover, addition of $\sfrac{{{B_{{{\mathbb{F}}}}}}}{2}$ can be exactly represented in ${{\mathbb{F}}}_{{\texttt{p}}}$. This makes addition in [\[eq:focusing-attention-dot-product\]](#eq:focusing-attention-dot-product){reference-type="ref+label" reference="eq:focusing-attention-dot-product"} associative and commutative. By [\[lem:pos-enc-properties\]](#lem:pos-enc-properties){reference-type="ref+label" reference="lem:pos-enc-properties"}, the terms $G$ in [\[eq:focusing-attention-dot-product\]](#eq:focusing-attention-dot-product){reference-type="ref+label" reference="eq:focusing-attention-dot-product"} are $0$, meaning that the final attention score equals ${{{{{\bm{q}}}}_{{n}}^\top {{{\bm{k}}}}_{{{n}}'}}}$.

    2.  **Sub-case 2b:** ${{{{{\bm{q}}}}_{{n}}^\top {{{\bm{k}}}}_{{{n}}'}}} \geq \log{2} ({{\texttt{p}}}+ 1)$. In this case, the intermediate computations of [\[eq:focusing-attention-dot-product\]](#eq:focusing-attention-dot-product){reference-type="ref+label" reference="eq:focusing-attention-dot-product"} are either exact or thresholded at ${{B_{{{\mathbb{F}}}}}}$. In both cases, the exponent of the resulting attention score is ${{B_{{{\mathbb{F}}}}}}$ by [\[lem:our-finite-precision-properties\]](#lem:our-finite-precision-properties){reference-type="ref+label" reference="lem:our-finite-precision-properties"}, preserving the attention score.

    3.  **Sub-case 2c:** ${{{{{\bm{q}}}}_{{n}}^\top {{{\bm{k}}}}_{{{n}}'}}} \leq \log{2} ({{\texttt{p}}}+ 1)$. In this case, all intermediate computations are representable in ${{\mathbb{F}}}_{{\texttt{p}}}$ analogously to the case 2a. The attention score is therefore preserved.

Taken together, this means that the attention scores between positions ${{n}}$ and ${{n}}'$ of ${{\boldsymbol{\tau}}}'$ are identical to those of ${{\boldsymbol{\tau}}}$ on the positions in ${{{\mathcal{N}}}}$, while the attention scores on the rest of the positions are $0$. This completes the proof.¬†‚óª
:::

::: restatable
lemmaDetectionLemma []{#lem:detection label="lem:detection"} Let ${{\boldsymbol{w}}}\in {{{{{{\Sigma}}^{*}}}}}$ and ${{w}}\in {{\Sigma}}$. Then, there exists a single-layer unmasked fixed-precision logarithmic-width transformer ${{\mathcal{T}}}$ such that, on input ${{\boldsymbol{w}}}$, an entry of its final residual stream contains the entry ${\mathbbm{1}} \left\{ {{w}}\in {{\boldsymbol{w}}} \right\}$.
:::

::: proof
*Proof sketch..* Note that ${{\mathcal{T}}}$ cannot use the commonly-used *exact* uniform attention over all symbols to detect ${\mathbbm{1}} \left\{ {{w}}\in {{\boldsymbol{w}}} \right\}$ due to fixed precision. Nevertheless, rounded uniform attention suffices. By attending to all symbols in the string with weight 1, the denominator of the attention scores is at most ${{B_{{{\mathbb{F}}}}}}$. Using one-hot encodings of symbols ${{w}}_{{n}}$ as the attention values ${{{\bm{v}}}}_{{n}}$, it is easy to see that the final contextual representation at the final position will have a positive value at the entry corresponding to ${{w}}$ if and only if ${{w}}\in {{\boldsymbol{w}}}$, since $\sfrac{c}{{{B_{{{\mathbb{F}}}}}}} > 0$ for any $c \geq 1$. This condition can be checked by the MLP applied after the attention aggregation operation.¬†‚óª
:::

::: restatable
lemmaSelectBlockLem []{#lem:select-block label="lem:select-block"} Let ${{\boldsymbol{w}}}\in {{{{{{\Sigma}}^{*}}}}}$ be a string of length ${{N}}$ and ${P}{T}$ the number of padding symbols where ${P}, {T}= {{\mathtt{poly}}\mleft({{N}}\mright)}$. There exists a fixed-precision and logarithmic-width transformer ${U}\colon {{{{{\Sigma}}_{{\texttt{m}}}}^{*}}} \to {{\{{\texttt{0}}, {\texttt{1}}\}^{*}}}$ that, given ${{\boldsymbol{w}}}$ and the current partially masked string ${\boldsymbol{t}}^{(t)}$, selects the next ${P}$ positions to unmask by outputting ${\texttt{1}}$ for the next ${P}$ positions to unmask and ${\texttt{0}}$ for all other positions.
:::

:::: proof
*Proof.* The idea of the construction is for ${U}$ to

::: enumerate*
output ${\texttt{0}}$ for any position that does not contain the padding symbol ${\square}$, and

output ${\texttt{1}}$ for the first ${P}$ positions that contain the padding symbol ${\square}$.
:::

Step *(1)* can be implemented by checking whether the symbol at the current position is ${\square}$ and outputting ${\texttt{0}}$ otherwise. Step *(2)* can be implemented by attending to position ${P}$ positions back and outputting ${\texttt{1}}$ if the symbol at that position $\neq {\square}$ and ${\texttt{0}}$ otherwise. These steps can be performed by two attention heads in a single transformer layer using the positional encodings $$\begin{equation}
 \label{eq:pos-enc-select-block}
      {{{{\texttt{PE}}}\mleft({{n}}, {{N}}\mright)}} \mathrel{\stackrel{\textnormal{\tiny def}}{=}}
      \begin{pmatrix}
         {{{\texttt{B}}^{\pm}}\mleft({{n}}\mright)} \\
         {{{\texttt{B}}^{\pm}}\mleft({{n}}- {P}\mright)} \\
      \end{pmatrix} \in {\left\{ 0, 1 \right\}}^{{{{{\mathcal{O}}}(\log{{N}})}}}.
\end{equation}$$¬†‚óª
::::

::: restatable
lemmaBinaryEncodingLemma []{#lem:binary-encoding label="lem:binary-encoding"} Let ${{N}}\in {{\mathbb{N}}}$ and ${{n}}\in {\left[ {{N}} \right]}$. Then, there exists an unmasked fixed-precision logarithmic-width padded looped transformer ${{\mathcal{T}}}$ such that, on input $\texttt{\& } {{{\texttt{B}}\mleft({{n}}\mright)}}$, after $\ceil{\log {{N}}}$ iterations, the residual stream at position $\ceil{\log {{N}}} + 1$ contains the value ${{{\texttt{B}}\mleft({{n}}\mright)}}$.
:::

::: proof
*Proof sketch.* The transformer ${{\mathcal{T}}}$ has to convert the binary representation ${{{\texttt{B}}\mleft({{n}}\mright)}}$ of ${{n}}$ contained in across $\ceil{\log {{N}}}$ positions in the input string into a single $\ceil{\log {{N}}}$-dimensional binary vector in the residual stream. This is done as follows:

1.  In the first layer, each symbol ${{w}}_{{{n}}'} \in {\left\{ 0, 1 \right\}}$ checks if it is immediately preceded by the $\texttt{\&}$ symbol, which denotes the beginning of the pointer in the string. If it is, ${{w}}_{{{n}}'}$ stores ${{{\bm{e}}}}_1$ and ${{{\bm{d}}}}_1 \mathrel{\stackrel{\textnormal{\tiny def}}{=}}{{w}}_{{{n}}'} {{{\bm{e}}}}_1$ in designated parts of its residual stream. Here, ${{{\bm{e}}}}_1$ is the first unit vector of ${{\mathbb{R}}}^{\ceil{\log {{N}}}}$.

2.  In the subsequent layers ${{l}}\in {\left\{ 2, \ldots, \ceil{\log{{N}}} \right\}}$, each symbol ${{w}}_{{{n}}'}$ checks if the entry ${{{\bm{e}}}}_{{{l}}- 1}$ has already been written to the designated space of the previous symbol's residual stream. If it has, ${{w}}_{{{n}}'}$ copies and shifts ${{{\bm{e}}}}_{{{l}}- 1}$ into ${{{\bm{e}}}}_{{{l}}}$, and stores ${{{\bm{e}}}}_{{{l}}}$ and ${{{\bm{d}}}}_{{{l}}} \mathrel{\stackrel{\textnormal{\tiny def}}{=}}{{{\bm{d}}}}_{{{l}}- 1} + {{w}}_{{{n}}'} {{{\bm{e}}}}_{{{l}}}$ in designated parts of its residual stream.

After $\ceil{\log {{N}}}$ layers, the residual stream at position $\ceil{\log {{N}}} + 1$ thus contains ${{{\texttt{B}}\mleft({{n}}\mright)}}$.¬†‚óª
:::

## Neural network constructions

::: restatable
lemmaMLPForArgmaxLem []{#lem:mlp-for-argmax label="lem:mlp-for-argmax"} For every ${{D}}\in {{\mathbb{N}}}$ and precision ${{\texttt{p}}}\in {{\mathbb{N}}}$, there exists a ${{\mathrm{ReLU}}}$-activated MLP ${{{f}}}\colon {{\mathbb{R}}}^{{D}}\to {\left\{ 0, 1 \right\}}^{{D}}$ such that for any ${{{\bm{x}}}}\in {{\mathbb{F}}}^{{{D}}}_{{\texttt{p}}}$, if there is ${{d}}\in {\left[ {{D}} \right]}$, such that ${{{x}}}_d > \max_{j \in {\left[ {{D}} \right]} \setminus {\left\{ {{d}} \right\}}} {{{x}}}_{j}$, then ${{{f}}}({{{\bm{x}}}}) = {{{\bm{e}}}}_d$, the ${{d}}\textsuperscript{th}$ unit vector.
:::

::: proof
*Proof.* The proof is based on the construction of a 3-layer ${{\mathrm{ReLU}}}$ network that computes the $\mathop{\mathrm{{{argmax}}}}$ of a vector ${{{\bm{x}}}}\in {{\mathbb{F}}}^{{{D}}}_{{\texttt{p}}}$. The first layer computes the differences between each pair of elements in ${{{\bm{x}}}}$. The second layer computes the maximum of these differences. The third layer then checks if the maximum difference is greater than zero, indicating that there is a unique maximum element in ${{{\bm{x}}}}$.

More, concretely, define $$\begin{equation}
      g_{{d}}\mathrel{\stackrel{\textnormal{\tiny def}}{=}}2^{{\texttt{p}}}\cdot {{{{\mathrm{ReLU}}}\mleft(2^{-{{\texttt{p}}}} - \sum_{j \neq {{d}}} {{\mathrm{ReLU}}}({{{x}}}_j - {{{x}}}_{{d}})\mright)}},
\end{equation}$$ which can be computed by a 3-layer ${{\mathrm{ReLU}}}$ network. We have that $g_{{d}}= 1$ if and only if ${{{x}}}_{{d}}> \max_{j \neq {{d}}} {{{x}}}_j$, or, equivalently, if and only if ${{{x}}}_{{d}}- \max_{j \neq {{d}}} {{{x}}}_j \geq 2^{-{{\texttt{p}}}}$. Indeed if ${{{x}}}_{{d}}- \max_{j \neq {{d}}} {{{x}}}_j \geq 2^{-{{\texttt{p}}}}$, we have that $$\begin{equation}
      g_{{d}}= 2^{{\texttt{p}}}\cdot {{{{\mathrm{ReLU}}}\mleft(2^{-{{\texttt{p}}}} - \sum_{j \neq {{d}}} {{\mathrm{ReLU}}}({{{x}}}_j - {{{x}}}_{{d}})\mright)}} = 1.
\end{equation}$$ In contrast, for ${{d}}' \neq {{d}}$, we have that ${{{x}}}_{{{d}}'} - {{{x}}}_{{d}}< 2^{-{{\texttt{p}}}}$, and thus $$\begin{equation}
      g_{{{d}}'} = 2^{{\texttt{p}}}\cdot {{{{\mathrm{ReLU}}}\mleft(2^{-{{\texttt{p}}}} - \sum_{j \neq {{d}}'} {{\mathrm{ReLU}}}({{{x}}}_j - {{{x}}}_{{{d}}'})\mright)}} \leq 2^{{\texttt{p}}}\cdot {{{{\mathrm{ReLU}}}\mleft(2^{-{{\texttt{p}}}} - {{\mathrm{ReLU}}}({{{x}}}_{{{d}}} - {{{x}}}_{{{d}}'})\mright)}} = 0.
\end{equation}$$¬†‚óª
:::

::: restatable
lemmaProjectionMLP []{#lem:projection-mlp label="lem:projection-mlp"} Let ${{{\bm{x}}}}\in {\left\{ 0, 1 \right\}}^{{D}}$ and ${{{\bm{e}}}}_{{d}}\in {\left\{ 0, 1 \right\}}^{{D}}$ be the ${{d}}\textsuperscript{th}$ unit vector. Then, there exists a ${{\mathrm{ReLU}}}$-activated MLP ${{{f}}}\colon {\left\{ 0, 1 \right\}}^{{{D}}+ {{D}}} \to {\left\{ 0, 1 \right\}}^{{D}}$ such that $$\begin{equation}
      {{{f}}}({{{\bm{x}}}}, {{{\bm{e}}}}_{{d}}) = {{{{{\bm{x}}}}^\top {{{\bm{e}}}}_{{d}}}} = {{{x}}}_{{d}}.
\end{equation}$$
:::

::: proof
*Proof.* We have that $$\begin{equation}
      {{{x}}}_{{d}}= {{{{\bm{1}}}^\top \mleft({{{{\mathrm{ReLU}}}\mleft({{{\bm{x}}}}- \mleft({{\bm{1}}}- {{{\bm{e}}}}_{{d}}\mright)\mright)}}\mright)}}
\end{equation}$$ where ${{\bm{1}}}$ is the all-ones vector of length ${{D}}$. This can be implemented by a ${{\mathrm{ReLU}}}$-activated MLP.¬†‚óª
:::

## Masked and unmasked transformers {#app:masked-unmasked-transformers}

::: restatable
lemmaMaskConversionLemma[]{#lem:mask-conversion label="lem:mask-conversion"} Let ${{\mathcal{T}}}$ be an unmasked fixed-precision logarithmic-width transformer with ${{L}}$ layers. Then, there exists a fixed-precision logarithmic-width causally-masked transformer ${{\mathcal{T}}}'$ with ${{L}}$ layers such that for any input string ${{\boldsymbol{w}}}\in {{{{{{\Sigma}}^{*}}}}}$ of length ${{N}}$ padded with ${{L}}({{N}}- 1)$ padding symbols, the representations ${{{{{\bm{H}}}}}}^{\prime({{L}})}_{({{L}}- 1) {{N}}: {{L}}{{N}}}$ computed by ${{\mathcal{T}}}'$ on ${{\boldsymbol{w}}}$ equal the final representations ${{{{{\bm{H}}}}}}^{({{L}})}$ computed by ${{\mathcal{T}}}$.
:::

::: proof
*Proof.* We adapt the proof of @merrill2025exactexpressivepowertransformers [Lem. 1] to our setting. The idea is for ${{\mathcal{T}}}'$ to unroll the computation of ${{{{{\bm{H}}}}}}^{({{L}})}$ into a sequence of ${{L}}$ "blocks" of padding, each of width ${{N}}$. Each block will attend to the previous block---representing the values in the preceding layer---and will thus be able to see all symbols despite causal masking. To do so, ${{\mathcal{T}}}'$ uses additional positional encodings of the form $$\begin{equation}
      {{{{\texttt{PE}}}\mleft({{n}}, {{N}}\mright)}} \mathrel{\stackrel{\textnormal{\tiny def}}{=}}
      \begin{pmatrix}
         {{{\texttt{B}}^{\pm}}\mleft({{l}}_{{n}}\mright)} \\
         {{{\texttt{B}}^{\pm}}\mleft({{l}}_{{n}}- 1\mright)} \\
      \end{pmatrix} \in {\left\{ 0, 1 \right\}}^{2 \ceil{\log{{N}}}}
\end{equation}$$ Here, ${{l}}_{{n}}\mathrel{\stackrel{\textnormal{\tiny def}}{=}}{\left\lfloor \sfrac{{{n}}}{{{L}}} \right\rfloor} + 1$ represents the layer that each padding position belongs to. To construct ${{\mathcal{T}}}'$, we then modify each original head from ${{\mathcal{T}}}$ with [\[lem:focus-on-marked-positions\]](#lem:focus-on-marked-positions){reference-type="ref+label" reference="lem:focus-on-marked-positions"} to ensure that the attention is focused on the correct padding positions, where the attention head computes the same function as the one in ${{\mathcal{T}}}$.¬†‚óª
:::

::: restatable
lemmaUnmaskConversionLemma[]{#lem:unmask-conversion-fp label="lem:unmask-conversion-fp"} Let ${{\mathcal{T}}}$ be an ${{L}}$-layer finite-precision logarithmic-width causally-masked transformer. Then, there exists a finite-precision logarithmic-width unmasked transformer ${{\mathcal{T}}}'$ with $2 {{L}}+ 1$ layers such that for any input string ${{\boldsymbol{w}}}\in {{{{{{\Sigma}}^{*}}}}}$ of length ${{N}}$ padded with $({{N}}- 1){{N}}$ padding symbols, the representations ${{{{{\bm{H}}}}}}^{\prime({{L}})}_{({{N}}- 1) {{N}}: {{N}}^2}$ computed by ${{\mathcal{T}}}'$ on ${{\boldsymbol{w}}}$ equal the final representations ${{{{{\bm{H}}}}}}^{({{L}})}$ computed by ${{\mathcal{T}}}$.[^23]
:::

::: proof
*Proof.* The idea is for ${{\mathcal{T}}}'$ to unroll the computation of ${{{{{\bm{H}}}}}}^{({{L}})}$ into a sequence of ${{N}}$ "blocks" of padding, each of width ${{N}}$. Each block will compute the representation of one of the symbols in the string. To do so, ${{\mathcal{T}}}'$ uses additional[^24] positional encodings of the form $$\begin{equation}
 \label{eq:additional-pos-enc-1}
      {{{{\texttt{PE}}}\mleft({{n}}, {{N}}\mright)}} \mathrel{\stackrel{\textnormal{\tiny def}}{=}}
      \begin{pmatrix}
         {{{\texttt{B}}^{\pm}}\mleft({{b}}_{{n}}\mright)} \\
         {{{\texttt{B}}^{\pm}}\mleft({{r}}_{{{n}}}\mright)}  \\
         {\mathbbm{1}} \left\{ {{n}}\le {{N}} \right\}  \\
         {\mathbbm{1}} \left\{ {{b}}_{{n}}\ge {{r}}_{{{n}}} \right\}  \\
         {\mathbbm{1}} \left\{ {{b}}_{{n}}= {{r}}_{{{n}}} \right\}  \\
      \end{pmatrix} \in {\left\{ 0, 1 \right\}}^{{{{{\mathcal{O}}}(\log{{N}})}}}.
\end{equation}$$ Here, ${{b}}_{{n}}\mathrel{\stackrel{\textnormal{\tiny def}}{=}}{\left\lfloor \sfrac{{{n}}}{{{N}}} \right\rfloor} + 1$ represents the block that position ${{n}}$ falls into and ${{r}}_{{{n}}} \mathrel{\stackrel{\textnormal{\tiny def}}{=}}({{n}}\mod {{N}}) + 1$ represents the position within that layer.

${{\mathcal{T}}}'$ then processes a string ${{\boldsymbol{w}}}\in {{{{{{\Sigma}}^{*}}}}}$ of length ${{N}}$ padded with ${{N}}^2$ padding symbols as follows.

1.  ${{\mathcal{T}}}'$ uses an additional "copy" layer to copy the input symbols from the first ${{N}}$ positions to the residual stream for access in later layers. In particular, each position ${{n}}\in {\left[ {{N}}^2 \right]}$ is assigned the value of the input symbol at position ${{r}}_{{{n}}}$. This can be done by the symbol at position ${{n}}$ attending to the symbol at position ${{r}}_{{{n}}}$ in the input string, i.e., ${{{{{\bm{H}}}}}}^{(1)}_{{{n}}} = {{{{{\bm{H}}}}}}^{(0)}_{{{r}}_{{{n}}}}$, *if* ${{b}}_{{n}}\ge {{r}}_{{{n}}}$, which can be ensured by attending only to positions with non-zero entry ${\mathbbm{1}} \left\{ {{b}}_{{n}}\ge {{r}}_{{{n}}} \right\} {{{\texttt{B}}^{\pm}}\mleft({{r}}_{{{n}}}\mright)}$ in the positional encoding. The latter condition ensures that the ${{b}}\textsuperscript{th}$ block contains only symbols ${{\boldsymbol{w}}}_{\leq {{b}}}$---${{w}}_{{b}}$ attending to the *entire* block is then equivalent to ${{w}}_{{b}}$ attending to the string with causal attention. Concretely, the attention scores ${{{s}}}^{(1)}_{{{n}}, {{n}}'} = {{{{{{\bm{q}}}}^{(1)}_{{{n}}}}^\top {{{\bm{k}}}}^{(1)}_{{{n}}'}}}$ are computed with query and key vectors $$\begin{align}
                {{{\bm{q}}}}^{(1)}_{{n}}&\mathrel{\stackrel{\textnormal{\tiny def}}{=}}{{B_{{{\mathbb{F}}}}}}\cdot 
                \begin{pmatrix}
                   {{{{{\texttt{B}}^{\pm}}\mleft({{r}}_{{n}}\mright)}}^\frown{{{\bm{1}}}_{\ceil{\log{{N}}}}}} \\
                   -1
                \end{pmatrix} \\
                {{{\bm{k}}}}^{(1)}_{{{n}}'} &\mathrel{\stackrel{\textnormal{\tiny def}}{=}}\phantom{{{B_{{{\mathbb{F}}}}}}\cdot} 
                \begin{pmatrix}
                   {{{{{\texttt{B}}^{\pm}}\mleft({{r}}_{{{n}}'}\mright)}}^\frown{(-{{\bm{1}}}_{\ceil{\log{{N}}}})}} \\
                   {\mathbbm{1}} \left\{ {{n}}\le {{N}} \right\}
                \end{pmatrix}
             \end{align}$$

2.  Once the symbols have been copied to the appropriate positions, ${{\mathcal{T}}}'$ can simulate one layer of ${{\mathcal{T}}}$ by augmenting its heads with [\[lem:focus-on-marked-positions\]](#lem:focus-on-marked-positions){reference-type="ref+label" reference="lem:focus-on-marked-positions"} to ensure that the computations at position ${{n}}$ are restricted to the block ${{b}}_{{n}}$, which, as described above, contains information about ${{\boldsymbol{w}}}_{\le {{b}}_{{n}}}$.[^25] This ensures that the attention scores are non-zero only for

3.  After simulating the layer from ${{\mathcal{T}}}$ in step (2), the contextual representation in the ${{b}}_{{{n}}}\textsuperscript{th}$ block contain the information about ${{\boldsymbol{w}}}_{\leq {{b}}_{{{n}}}}$ computed based on the symbols ${{\boldsymbol{w}}}_{\leq {{b}}_{{{n}}}}$. In particular, the representations of the symbols ${{\boldsymbol{w}}}_{< {{b}}_{{{n}}}}$ in the ${{b}}_{{{n}}}\textsuperscript{th}$ block contain information *not* obtained by causal masking since they attend to *all* the symbols ${{\boldsymbol{w}}}_{\leq {{b}}_{{{n}}}}$ in the ${{b}}_{{{n}}}\textsuperscript{th}$ block. To amend that, an additional transformer layer discards the information about symbols ${{\boldsymbol{w}}}_{< {{b}}_{{{n}}}}$ in the ${{b}}_{{{n}}}\textsuperscript{th}$ block by overwriting the representation of ${{w}}_{{{n}}'}$ in the ${{b}}_{{{n}}}\textsuperscript{th}$ block with the representation of ${{w}}_{{{n}}'}$ in the ${{n}}'\textsuperscript{th}$ block for ${{n}}' < {{b}}_{{{n}}}$. This is done by attending to the positions ${{n}}'$ in which the block index ${{b}}_{{{n}}'}$ matches the position ${{r}}_{{{n}}'}$, i.e., with the query and key vectors $$\begin{align}
                {{{\bm{q}}}}_{{n}}&\mathrel{\stackrel{\textnormal{\tiny def}}{=}}{{B_{{{\mathbb{F}}}}}}\cdot 
                \begin{pmatrix}
                   {{{{{\texttt{B}}^{\pm}}\mleft({{b}}_{{n}}\mright)}}^\frown{{{\bm{1}}}_{\ceil{\log{{N}}}}}} \\
                   -1
                \end{pmatrix} \\
                {{{\bm{k}}}}_{{{n}}'} &\mathrel{\stackrel{\textnormal{\tiny def}}{=}}\phantom{{{B_{{{\mathbb{F}}}}}}\cdot} 
                \begin{pmatrix}
                   {{{{{\texttt{B}}^{\pm}}\mleft({{r}}_{{{n}}'}\mright)}}^\frown{(-{{\bm{1}}}_{\ceil{\log{{N}}}})}} \\
                   {\mathbbm{1}} \left\{ {{b}}_{{n}}= {{r}}_{{{n}}} \right\}
                \end{pmatrix}
             \end{align}$$ which ensures that ${{n}}$ uniquely attends to the symbols ${{\boldsymbol{w}}}_{\leq {{b}}_{{n}}}$ in the ${{b}}_{{{n}}}\textsuperscript{th}$ block.

¬†‚óª
:::

# Proofs {#app:proofs}

This section contains the proofs of all novel theoretical results. Many constructions in the proofs rely on the theoretical gadgets introduced in [9](#app:theoretical-gadgets){reference-type="ref+label" reference="app:theoretical-gadgets"}.

::: proof
*Proof.* This is a consequence of the established result that fixed-depth transformers can only compute ${{{{{{\mathtt{AC}}}^{\mathtt{0}}}}}}$ functions [@li2024chain; @saunshi2025reasoninglatentthoughtspower; @london2025pausetokensstrictlyincrease *inter alia*]. Note that this is similar to the proof in @liu2025perfectdiffusionmathsftc0 [@liu2025serialscalinghypothesis] but is simpler due to the focus on discrete predictions directly rather than the continuous modeling of the diffusion process in the latent space.¬†‚óª
:::

## Proofs of results in [3.1](#sec:mdms-and-lts){reference-type="ref+label" reference="sec:mdms-and-lts"} {#proofs-of-results-in-secmdms-and-lts}

::: restatable
theoremLTsCanSimulateMDMsThm []{#thm:lts-can-simulate-mdms label="thm:lts-can-simulate-mdms"} $$\begin{equation}
      {\mathtt{MDM}}[{T}, {P}] \subseteq {\mathtt{PLT}}[{T}, {P}].
\end{equation}$$
:::

::: proof
*Proof.* We can simulate an MDM transformer with a PLT transformer by "composing" the planner and predictor into a single transformer model. This model

1.  computes the planner's contextual representations while passing the input symbol in the residual stream,

2.  computes the planner's decision at each position by simulating the $\mathop{\mathrm{{{argmax}}}}$ of the planner's output logits as in [\[lem:mlp-for-argmax\]](#lem:mlp-for-argmax){reference-type="ref+label" reference="lem:mlp-for-argmax"},

3.  computes the predictor's contextual representations based on the planner's decision, and

4.  predicts the next symbol at each position by simulating the $\mathop{\mathrm{{{argmax}}}}$ of the predictor's output logits as in [\[lem:mlp-for-argmax\]](#lem:mlp-for-argmax){reference-type="ref+label" reference="lem:mlp-for-argmax"}.

¬†‚óª
:::

::: restatable
lemmaIdentityDumpLayer []{#lem:idenitity-dump label="lem:idenitity-dump"} Let ${{\mathcal{T}}}$ be a fixed-precision and polynomial-width transformer and ${{{{{{{\bm{H}}}}}}}}\in {\left\{ 0, 1 \right\}}^{{{N}}\times {{D}}}$ the residual stream of ${{\mathcal{T}}}$ after ${{l}}$ layers on some (possibly padded) input string ${{\boldsymbol{w}}}\in {{{{{{\Sigma}}^{*}}}}}$. Then, there exist fixed-precision and polynomial-width transformer layers ${{{{\boldsymbol{\tau}}}_{\texttt{dump}}}}$ and ${{{{\boldsymbol{\tau}}}_{\texttt{read}}}}$ with such that ${{{{{{\boldsymbol{\tau}}}_{\texttt{dump}}}}\mleft({{{{{{{\bm{H}}}}}}}}\mright)}} \in {\left\{ 0, 1 \right\}}^{{{{{\mathcal{O}}}(\log{{N}})}} \times {{D}}}$ and $$\begin{equation}
      {{{{{{\boldsymbol{\tau}}}_{\texttt{read}}}}\mleft({{{{\texttt{Dec}}}\mleft({{{{{{\boldsymbol{\tau}}}_{\texttt{dump}}}}\mleft({{{{{{{\bm{H}}}}}}}}\mright)}}\mright)}}\mright)}}_{: {{N}}, :} = {{{{{{{\bm{H}}}}}}}}
\end{equation}$$ for some output matrix ${{{{{\bm{E}}}}}}\in {{\mathbb{R}}}^{2 \times {{D}}}$.
:::

::: proof
*Proof.* The idea of the construction of the layers ${{{{\boldsymbol{\tau}}}_{\texttt{dump}}}}$ and ${{{{\boldsymbol{\tau}}}_{\texttt{read}}}}$ is to store the contents of the residual stream in the padding space and then read it out at the next iteration. To do that, we allocate ${{N}}\cdot {{D}}$ symbols of additional (masked) padding space in the decoded string. Each of the ${{N}}$ length-${{D}}$ blocks corresponds to a position in the input string and each symbol in the block to a dimension in the hidden representation. The layers thus have to be augmented with positional encodings that will allow for the identification of the position in the residual stream and the dimension in the hidden representation. This will suffice for ${{{{\boldsymbol{\tau}}}_{\texttt{dump}}}}$ to write out the contents of the residual stream into the padding space and for the reading layer ${{{{\boldsymbol{\tau}}}_{\texttt{read}}}}$ to read it out again.

More precisely, ${{{{\boldsymbol{\tau}}}_{\texttt{dump}}}}$ and ${{{{\boldsymbol{\tau}}}_{\texttt{read}}}}$ use the following positional encodings: $$\begin{equation}
      {{{{\texttt{PE}}}\mleft({{n}}, {{N}}\mright)}} \mathrel{\stackrel{\textnormal{\tiny def}}{=}}
      \begin{pmatrix}
         {{{\texttt{B}}^{\pm}}\mleft({{n}}\mright)} \\
         {{{\texttt{B}}^{\pm}}\mleft({{b}}_{{n}}\mright)} \\
         {{\llbracket{{d}}_{{n}}\rrbracket}}  \\
         {\mathbbm{1}} \left\{ {{n}}\leq {{N}} \right\}
      \end{pmatrix} \in {\left\{ 0, 1 \right\}}^{{{\mathcal{O}}}{\log{{N}}}}
\end{equation}$$ to the input of ${{{{\boldsymbol{\tau}}}_{\texttt{dump}}}}$. In particular, for a masked padding symbol at position ${{n}}$, ${{b}}_{{n}}\mathrel{\stackrel{\textnormal{\tiny def}}{=}}\sfrac{{\left[ {{n}}- {{N}} \right]_{+}}}{{{D}}}$ and ${{d}}_{{n}}\mathrel{\stackrel{\textnormal{\tiny def}}{=}}{\left[ {{n}}- {{N}} \right]_{+}} \mod {{D}}$ correspond to the position in the residual stream and the dimension in the hidden representation that the position will store, respectively. ${{{{\boldsymbol{\tau}}}_{\texttt{dump}}}}$ can then be implemented as follows:

1.  Using ${{{\texttt{B}}^{\pm}}\mleft({{b}}_{{n}}\mright)}$ as the query at masked position ${{n}}$ and ${{{\texttt{B}}^{\pm}}\mleft({{n}}'\mright)}$ as the key at position ${{n}}' \leq {{N}}$, ${{{{\boldsymbol{\tau}}}_{\texttt{dump}}}}$ can individually identify the corresponding position ${{{\texttt{B}}^{\pm}}\mleft({{b}}_{{n}}\mright)}$ in the residual stream.

2.  Feeding ${{{{{\bm{h}}}}}}_{{{{\texttt{B}}^{\pm}}\mleft({{b}}_{{n}}\mright)}} \in {\left\{ 0, 1 \right\}}^{{D}}$ together with ${{\llbracket{{d}}_{{n}}\rrbracket}}$ as the value at the masked position ${{n}}$ into the MLP, ${{{{\boldsymbol{\tau}}}_{\texttt{dump}}}}$ can write the value of the dimension ${{d}}_{{n}}$ of the residual stream at position ${{{\texttt{B}}^{\pm}}\mleft({{b}}_{{n}}\mright)}$ into the padding space by [\[lem:projection-mlp\]](#lem:projection-mlp){reference-type="ref+label" reference="lem:projection-mlp"}.

It is then easy to construct the output matrix ${{{{{\bm{E}}}}}}$ as part of the decoding step ${{\texttt{Dec}}}$ such that ${{\texttt{Dec}}}({{{{{{\boldsymbol{\tau}}}_{\texttt{dump}}}}\mleft({{{{{{{\bm{H}}}}}}}}\mright)}})$ decodes the contents of the residual stream. ${{{{\boldsymbol{\tau}}}_{\texttt{read}}}}$ can then be implemented as follows:

1.  Include a transformer layer that ignores the attention mechanism and reads the input string and the decoded residual stream values, passes them through the residual connection, and encodes the values into the embedding space with the position-wise MLP. In particular, combining the information in ${{\llbracket{{d}}_{{n}}\rrbracket}}$ with the information in the input symbols, the MLP can convert the one-hot encoding of the dimension ${{d}}_{{n}}$ into the vector ${{{{{h}}}}}_{{{d}}_{{n}}} {{{\bm{e}}}}_{{{d}}_{{n}}}$, where ${{{{{h}}}}}_{{{d}}_{{n}}}$ corresponds to the value of the appropriate dimension of the residual stream at the appropriate position.

2.  Using ${{{\texttt{B}}^{\pm}}\mleft({{n}}\mright)}$ as the query at the input string position ${{n}}\leq {{N}}$ and ${{{\texttt{B}}^{\pm}}\mleft(\sfrac{{\left[ {{n}}' - {{N}} \right]_{+}}}{{{D}}}\mright)}$ as the key, ${{{{\boldsymbol{\tau}}}_{\texttt{read}}}}$ can identify all the padding positions that contain the values of the individual dimensions of the residual stream at position ${{n}}$. The positional encodings ensure that the attention scores satisfy (cf. [\[lem:finite-precision-properties\]](#lem:finite-precision-properties){reference-type="ref+label" reference="lem:finite-precision-properties"}) $$\begin{equation}
             {{{{{\bm{q}}}}_{{{n}}}^\top {{{\bm{k}}}}_{{{n}}'}}} = \begin{cases}
                0 & \textbf{if }{{n}}= \sfrac{{\left[ {{n}}' - {{N}} \right]_{+}}}{{{D}}} \\
                -{{B_{{{\mathbb{F}}}}}}& \textbf{otherwise }.
             \end{cases}
    \end{equation}$$ Exponentiating and normalizing the attention scores, the attention mechanism will then only attend to the positions ${{n}}'$ that correspond to the position ${{n}}$ in the residual stream. More concretely, the attention mechanism computes $$\begin{equation}
             {{{s}}}_{{{n}}, {{n}}'} = \frac{\exp({{{{{\bm{q}}}}_{{{n}}}^\top {{{\bm{k}}}}_{{{n}}'}}})}{\sum_{{{n}}''} \exp({{{{{\bm{q}}}}_{{{n}}}^\top {{{\bm{k}}}}_{{{n}}''}}})} = \frac{1}{\sum_{{{n}}''} \exp({{{{{\bm{q}}}}_{{{n}}}^\top {{{\bm{k}}}}_{{{n}}''}}})} = \frac{1}{\min\mleft({{D}}, {{B_{{{\mathbb{F}}}}}}\mright)} \geq \frac{1}{{{B_{{{\mathbb{F}}}}}}}
    \end{equation}$$ for all ${{n}}'$ that correspond to the position ${{n}}$ in the residual stream and ${{{s}}}_{{{n}}, {{n}}'} = 0$ otherwise. Summing over the values at these positions (which project the constructed vectors ${{{{{h}}}}}_{{{d}}_{{n}}} {{{\bm{e}}}}_{{{d}}_{{n}}}$), ${{{{\boldsymbol{\tau}}}_{\texttt{read}}}}$ can then reconstruct the value of the residual stream at position ${{n}}$ (normalized by $\min\mleft({{D}}, {{B_{{{\mathbb{F}}}}}}\mright)$).

3.  Use the position-wise MLP to convert the normalized value of the residual stream at position ${{n}}$ into the vector ${{{{{h}}}}}_{{{d}}_{{n}}} {{{\bm{e}}}}_{{{d}}_{{n}}}$. This can be done by a ${{\mathrm{ReLU}}}$-activated MLP that maps $(-\infty, 0]$ to $0$ and $[\frac{1}{{{B_{{{\mathbb{F}}}}}}}, \infty)$ to $1$ position-wise.

¬†‚óª
:::

::: restatable
theoremeMDMsCanSimulateFPLTsThm []{#thm:emdms-can-simulate-fplts label="thm:emdms-can-simulate-fplts"} $$\begin{equation}
      {\mathtt{PLT}}[{T}, {P}] \subseteq {\mathtt{MDM}}[{T}, ({{N}}+ {P}) {{D}}].
\end{equation}$$
:::

:::: proof
*Proof.* The proof uses [\[lem:idenitity-dump\]](#lem:idenitity-dump){reference-type="ref+label" reference="lem:idenitity-dump"} to simulate a single iteration of the PLT transformer loop with a single denoising step in the MDM transformer. In particular, by adding $({{N}}+ {P}) {{D}}$ padding space, the MDM has enough room to store the residual stream values, allowing it to decode the values at the next iteration. At a high level, the MDM's planner deterministically selects the entire padding space to unmask or resample, and the predictor

::: enumerate*
reads the input string or the currently stored residual stream values,

simulates a single pass of the PLT transformer on the input string ${{\boldsymbol{w}}}$ and the current residual stream values and thus computes the updated value of the residual stream, and

uses [\[lem:idenitity-dump\]](#lem:idenitity-dump){reference-type="ref+label" reference="lem:idenitity-dump"} to write the updated values into the padding space.
:::

More precisely, we can implement the MDM transformer as follows:

1.  The MDM transformer uses $({{N}}+ {P}) {{D}}$ masked symbols to store the residual stream values.

2.  The planner deterministically outputs a ${\texttt{1}}$ for positions ${{n}}> {{N}}$ and ${\texttt{0}}$ for positions ${{n}}\leq {{N}}$.

3.  The predictor predicts the values of the $({{N}}+ {P}) {{D}}$ symbols based on the current input string and the residual stream values. It reads the values from the residual stream by making the first ${{N}}$ positions attend to the $({{N}}+ {P}) {{D}}$ padding positions analogous to the dump-decode-read mechanism from [\[lem:idenitity-dump\]](#lem:idenitity-dump){reference-type="ref+label" reference="lem:idenitity-dump"}.

By treating input symbols $\in {{\Sigma}}$ separately to the decoded values of the residual stream (which enables the MDM transformer to simulate both the initial as well as the looping blocks of the PLT transformer), the MDM transformer can thus simulate the PLT transformer with ${T}{P}{{D}}$ padding symbols.¬†‚óª
::::

::: proof
*Proof.* The proof adapts the construction from the proof of @saunshi2025reasoninglatentthoughtspower [Thm. 5.1] to the MDM setting. The key difference lies in adapting the positional encodings to allow for the padded tokens to attend to appropriate positions in the residual stream. In particular, the first $\ceil{\sfrac{{{N}}}{2}}$ positions of the padding space will attend to the ${{N}}$ input symbols while the remaining positions in the padding space will attend to the other padding positions of the residual stream in later unmasking steps. Concretely, defining $\tilde{{{n}}} \mathrel{\stackrel{\textnormal{\tiny def}}{=}}{\left[ {{n}}- {{N}} \right]_{+}}$, the positional encodings take the form $$\begin{equation}
      {{{{\texttt{PE}}}\mleft({{n}}, {{N}}\mright)}} \mathrel{\stackrel{\textnormal{\tiny def}}{=}}
      \begin{pmatrix}
         {{{\texttt{B}}^{\pm}}\mleft({{n}}\mright)} \\
         {{{\texttt{B}}^{\pm}}\mleft(\tilde{{{n}}}\mright)}  \\
         {{{\texttt{B}}^{\pm}}\mleft({\left[ 2 {{n}}- {{N}} \right]_{+}}\mright)}  \\
         {{{\texttt{B}}^{\pm}}\mleft({\left[ 2 {{n}}- {{N}}- 1 \right]_{+}}\mright)}  \\
         {{{\texttt{B}}^{\pm}}\mleft({\left[ 2 \tilde{{{n}}} - {{N}} \right]_{+}}\mright)}  \\
         {{{\texttt{B}}^{\pm}}\mleft({\left[ 2 \tilde{{{n}}} - {{N}}- 1 \right]_{+}}\mright)}  \\
         {\mathbbm{1}} \left\{ {{n}}> {{N}} \right\}  \\
         {\mathbbm{1}} \left\{ \tilde{{{n}}} \geq \sfrac{{{N}}}{2} \right\}  \\
      \end{pmatrix} \in {\left\{ 0, 1 \right\}}^{{{{{\mathcal{O}}}(\log{{N}})}}}.
\end{equation}$$ This information gives padding tokens enough information to either attend to the input symbols (${\mathbbm{1}} \left\{ \tilde{{{n}}} \geq \sfrac{{{N}}}{2} \right\} = 1$) or to the residual stream values, enabling the simulation of the algorithm from @saunshi2025reasoninglatentthoughtspower [Thm. 5.1].¬†‚óª
:::

## Proofs of results in [3.2](#sec:mdms-and-cot){reference-type="ref+label" reference="sec:mdms-and-cot"} {#app:proofs-cot}

::: proof
*Proof.* Let ${P}= {T}{P}'$. The idea of the simulation is straightforward: The planner first determines the next ${P}'$ symbols to unmask. Then, the predictor determines the symbols at those positions by simulating the behavior of the pCoT transformer. This is trivial if the MDM is causally masked like the pCoT transformer. However, if the MDM is not causally masked, the planner must take additional steps to ensure that the prediction is equivalent to the pCoT transformer.

Concretely, the MDM simulates the pCoT transformer on the input string ${{\boldsymbol{w}}}$ of length ${{N}}$ as follows:

1.  We first note that the pCoT transformer predicts the next ${P}'$ symbols at every timestep $t$ based on the input string ${{\boldsymbol{w}}}_{\leq {{N}}+ (t - 1) {P}'} \underbrace{{\square}\cdots {\square}}_{({T}- t) {P}'}$ rather than ${{\boldsymbol{w}}}_{\leq {{N}}+ (t - 1) {P}'}$. This is because, by [\[lem:ignore-positions\]](#lem:ignore-positions){reference-type="ref+label" reference="lem:ignore-positions"}, the pCoT transformer can ignore all symbols containing the padding symbol and thus produce equivalent predictions at every step $t$. This will help us make use of the same padding space at every step of the simulation.

2.  We assume that the final output of the MDM will be stored in the first ${P}$ positions of the padding space. It will be filled in ${T}$ generation steps, where at each step $t \in {\left[ {T} \right]}$, a new block of ${P}'$ symbols will be predicted. In particular, by [\[lem:select-block\]](#lem:select-block){reference-type="ref+label" reference="lem:select-block"}, the planner can select the next ${P}'$ positions to unmask at time $t$ by including the values ${{{\texttt{B}}^{\pm}}\mleft(\ceil{\sfrac{{\left[ {{n}}- {{N}} \right]_{+}}}{{P}'}}\mright)}$ and ${{{\texttt{B}}^{\pm}}\mleft(\ceil{\sfrac{{\left[ {{n}}- {{N}} \right]_{+}}}{{P}'}} - 1\mright)}$ in the positional encodings.

3.  The predictor then uses an initial layer to copy the input string ${{\boldsymbol{w}}}_{\leq {{N}}+ (t - 1) {P}'} \underbrace{{\square}\cdots {\square}}_{({T}- t) {P}'}$ into the residual stream of the first ${{N}}+ {P}$ positions of the padding space.

4.  The predictor then uses the $({{N}}+ {P})^2$ padding positions to predict the next ${P}'$ symbols predicted by the pCoT transformer. These are written to the positions chosen to be unmasked by the planner.

¬†‚óª
:::

::: proof
*Proof.* For simplicity, we assume that the input to the pCoT transformer is padded by ${P}$ symbols. Intuitively, the pCoT transformer simulates an ${{L}}$-layer MDM transformer on the input string ${{\boldsymbol{w}}}$ of length ${{N}}$ by simulating each MDM generation step with additional padding to account for causal masking. Whereas the MDM transformer "overwrites" its previous input and bases its predictions at time step $t$ on the current version of the unmasked input, the pCoT transformer bases its predictions of the ${P}$ symbols on the entire string of $(t - 1) \cdot {P}$ symbols generated so far. For correct simulation, the pCoT transformer therefore has to ignore all the symbols not generated at the previous time step, which will be ensured by appropriate positional encodings. The pCoT transformer can then predict the next ${{N}}+ {P}$ symbols based on the current input string and the previously predicted symbols, simulating the behavior of the MDM transformer on that input. However, to predict ${{N}}+ {P}$ symbols, the pCoT transformer uses ${P}' \mathrel{\stackrel{\textnormal{\tiny def}}{=}}{{L}}({{N}}+ {P})$ padding space at each step to account for the unmasked nature of the MDM transformer (cf. [\[lem:unmask-conversion-fp\]](#lem:unmask-conversion-fp){reference-type="ref+label" reference="lem:unmask-conversion-fp"}).

More concretely, the simulation happens as follows.

1.  The pCoT transformer uses additional positional encodings with the information about ${\left\lfloor \sfrac{{\left[ {{n}}- {{N}} \right]_{+}}}{{T}} \right\rfloor}$, ${\left[ {{n}}- {{N}} \right]_{+}} \mod {P}'$, ${\left\lfloor \sfrac{{\left[ {{n}}- {{N}} \right]_{+}}}{{{L}}} \right\rfloor}$, and ${\left[ {{n}}- {{N}} \right]_{+}} \mod {P}$. These positional encodings allow the pCoT transformer to identify

    with [\[lem:focus-on-marked-positions\]](#lem:focus-on-marked-positions){reference-type="ref+label" reference="lem:focus-on-marked-positions"}.

2.  The pCoT transformer first uses an initial layer to copy the output of the previous generation step (captured in the previous ${{N}}+ {P}$ positions) into the next ${{N}}+ {P}$ positions of the padding space (this is where we use the assumption that the input to the pCoT transformer is padded---if that is not the case, a more complicated construction could specifically handle the initial step of the generation where only the initial input string would be copied).

3.  The pCoT transformer can then predict the next ${P}$ symbols by simulating the behavior of the composed MDM planner and predictor as in [\[thm:lts-can-simulate-mdms\]](#thm:lts-can-simulate-mdms){reference-type="ref+label" reference="thm:lts-can-simulate-mdms"}.

¬†‚óª
:::

[^1]: ¬†¬†This research was conducted while interning at the Allen Institute for AI.

[^2]: We provide a detailed overview of related work in [6](#app:related-work){reference-type="ref+label" reference="app:related-work"}.

[^3]: This is different from *speculative decoding* [@10.5555/3618408.3619203], which generates multiple symbols *one a time* with a smaller LM and then evaluates their probability in a single pass of a larger LM.

[^4]: Some work also considers transformers as LMs directly [@svete-cotterell-2024-transformers; @nowak-etal-2024-representational; @borenstein-etal-2024-languages; @svete-etal-2024-transformers] and shows the (probabilistic) gains afforded by CoT reasoning.

[^5]: Or *approximates* them well, requiring the error to be smaller than some $\epsilon > 0$.

[^6]: That is, ${{p}}$ can only implement LMs whose next-symbol logits can be computed by ${{{{{{\mathtt{AC}}}^{\mathtt{0}}}}}}$ circuits [@liu2025perfectdiffusionmathsftc0].

[^7]: An analogous version of the theorem applies to transformers in ${{{{{{\mathtt{TC}}}^{\mathtt{0}}}}}}$.

[^8]: [\[thm:single-model-equivalence\]](#thm:single-model-equivalence){reference-type="ref+label" reference="thm:single-model-equivalence"} in [8.1](#app:planner-centrality){reference-type="ref+label" reference="app:planner-centrality"} shows that the planner and predictor can be fused into a single model that unmasks symbols based on their confidence. This means that all our results apply to this popular model of unmasking.

[^9]: This is related to the importance of the mutual information and correlations between symbols to MDM performance [@li2025convergencetheorydiffusionlanguage; @wu2025fastdllmtrainingfreeaccelerationdiffusion] and string-level correctness [@feng2025theoreticalbenefitlimitationdiffusion].

[^10]: While our results focus on MDMs that can resample generated symbols, they also apply to non-resampling MDMs---[\[thm:mdms-can-simulate-emdms\]](#thm:mdms-can-simulate-emdms){reference-type="ref+label" reference="thm:mdms-can-simulate-emdms"} in [8.2](#app:editing-mdms){reference-type="ref+label" reference="app:editing-mdms"} shows that the latter can simulate the former if given additional output space.

[^11]: The proofs of all statements in this section are deferred to [10](#app:proofs){reference-type="ref+label" reference="app:proofs"}.

[^12]: PLTs thus also resemble *latent* diffusion LMs that diffuse in the representation space. We do not explore this connection here due to the superior performance and popularity of MDMs[@zhang2025surveyparalleltextgeneration] in practice.

[^13]: This is possible because the PLT of [@saunshi2025reasoninglatentthoughtspower] only stores discrete values in its residual stream.

[^14]: To the best of our knowledge, [\[lem:unmask-conversion-fp\]](#lem:unmask-conversion-fp){reference-type="ref+label" reference="lem:unmask-conversion-fp"} is the first result showing how to simulate masked attention with unmasked attention and might be of interest in its own right.

[^15]: This is, for example, used by @saunshi2025reasoninglatentthoughtspower [Thm 5.4] to show that unmasked PLTs can simulate CoT with no blow-up in the padding using a *masking function*---an additional step in the computation of attention scores that allows for zeroing out of irrelevant keys---and with linearly-increasing width.

[^16]: Interestingly, some existing work finds that MDMs that decode based on the most confident symbols tend to decode autoregressively [@gong2025diffucoderunderstandingimprovingmasked]. In this sense, the unmasked nature of MDMs could be seen as a hurdle that the model has to overcome to eventually rely on more autoregressive generation. This further motivates the development of hybrid models that combine autoregressive generation of entire blocks with non-autoregressive infilling within the blocks [@nie2025largelanguagediffusionmodels; @arriola2025blockdiffusioninterpolatingautoregressive; @song2025seeddiffusionlargescalediffusion *inter alia*].

[^17]: A similar modeling assumption is made by @liu2025perfectdiffusionmathsftc0 [@liu2025serialscalinghypothesis] for latent diffusion LMs.

[^18]: By representing symbols from any alphabet with binary encodings, circuits (or circuit functions) can be used to process strings over any finite alphabet. We focus on binary strings for simplicity.

[^19]: Similar to @li2024chain [@saunshi2025reasoninglatentthoughtspower], we define masking with a function rather than an additive matrix since subtracting ${{B_{{{\mathbb{F}}}}}}$ from an arbitrary number in ${{\mathbb{F}}}$ does not necessarily result in $-{{B_{{{\mathbb{F}}}}}}$.

[^20]: Examples of masking functions include the identity function (unmasked attention) and the identity function on the upper-triangular portion of the matrix that replaces the lower-triangular part with $-{{B_{{{\mathbb{F}}}}}}$ (causally masked attention).

[^21]: In practice, the raw probabilities are often post-processed with temperature scaling or sampling adaptors [@amini-etal-2023-generating]. The framework described here can easily be adapted to those settings.

[^22]: We also note that the modular information in our encodings resembles the periodic nature of original sinusoidal positional encodings used by the transformer architecture [@NIPS2017_3f5ee243]. Moreover, the modular nature of such positional encodings has been analyzed by theoretical work before and is known to increase the expressivity of certain idealizations of transformers [@li2025characterizingexpressivitytransformerlanguage; @jerad2025uniquehardattentiontale].

[^23]: This simulation is somewhat inefficient in that only a subset of the ${{N}}$ positions are used at each of the ${{N}}$ blocks (specifically, ${{n}}$ positions in the ${{n}}\textsuperscript{th}$ block). While this could be made more efficient with a more sophisticated construction, the asymptotic complexity would remain quadratic in ${{N}}$.

[^24]: By additional, we mean that these positional encodings are appended to the ones used by ${{\mathcal{T}}}$.

[^25]: The augmented attention mechanism additionally downweights positions with ${\mathbbm{1}} \left\{ {{b}}_{{{n}}'} \ge {{r}}_{{{n}}'} \right\} = 1$, which can be done by subtracting ${{B_{{{\mathbb{F}}}}}}$ from the attention score.
