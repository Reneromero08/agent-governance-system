# Introduction

<figure id="fig1" data-latex-placement="t">
<div class="center">
<embed src="main_texts/intro.pdf" style="width:100.0%" />
</div>
<figcaption>(a) Average two-way individual accuracy and pair accuracy of CLIP-ViT-L/14-336px and LLaVA-1.5-7B on various benchmarks <span class="citation" data-cites="kamath2023s hsieh2024sugarcrepe thrush2022winoground li2024naturalbench yarom2024you tong2024eyes"></span>. (b) CLIP and Generative MLLM architectures (using LLaVA-1.5 as an example) for fine-grained visual reasoning tasks. We observe that Generative MLLMs perform better in extracting and utilizing query-relevant information from the same vision encoder. </figcaption>
</figure>

Despite the success and widespread adoption of Contrastive Language-Image Pretraining (CLIP) [@radford2021learning], recent studies have pointed out that state-of-the-art CLIP models still fall short in various visual reasoning tasks, including Winoground [@thrush2022winoground], SugarCREPE [@hsieh2024sugarcrepe], and What'sUp [@kamath2023s]. These benchmarks require vision-language models (VLMs) to pair images and captions, which are carefully designed to test model capabilities of visio-linguistic compositional reasoning, spatial reasoning, or fine-grained detail understanding---areas beyond standard zero-shot classification on ImageNet. While CLIP excels at the latter, its performance in these visual reasoning tasks remains poor.

One plausible explanation for these shortcomings is the potential information loss during the encoding process of the CLIP vision encoder [@tong2024eyes]. For example, the encoder might behave like a bag-of-words model which only grasps the individual concepts in the image ("`mug`" and "`plate`" in Figure [1](#fig1){reference-type="ref" reference="fig1"}), but not the structural relationship ("`the mug is to the left of the plate`") [@yuksekgonul2023and].

In this work, we observe that the query-relevant visual information could still be preserved by CLIP vision encoder, but a better strategy is required to extract it: As shown in Figure [1](#fig1){reference-type="ref" reference="fig1"}, LLaVA-1.5-7B [@liu2024improved] with the *same* pretrained vision encoder, surpasses CLIP-ViT-L/14-336px by a large margin on many challenging visual reasoning benchmarks. Particularly, on spatial reasoning benchmark What'sUp, while CLIP's pair accuracy is lower than random chance (25%), LLaVA-1.5 achieves beyond 50% on all four subsets (Table [\[tab:comparison-1\]](#tab:comparison-1){reference-type="ref" reference="tab:comparison-1"}). More evidence of other Generative MLLMs on various benchmarks showing this phenomenon is presented in Section [2](#sec:compare){reference-type="ref" reference="sec:compare"}. These results indicate that these Generative MLLMs extract and utilize query-relevant information more effectively from the same CLIP vision encoder. Notably, the vision encoder remains unchanged throughout training, ensuring a fair comparison.

What is the driving force behind Generative MLLMs' extracting more visual information and achieving strong visual reasoning performance? How can it benefit and improve CLIP-like contrastive VLMs? In Section [3](#ablation){reference-type="ref" reference="ablation"}, we investigate these questions by conducting controlled experiments on various factors as follows:

- **Training data.** In Section [3.1](#ab1_data){reference-type="ref" reference="ab1_data"}, we observe little performance gain after directly finetuning CLIP on LLaVA-1.5's training data and hard negatives, indicating that training data is not the only contributor.

- **Token usage and position embedding.** In Section [3.2](#ab2_token){reference-type="ref" reference="ab2_token"}, we observe that using patch tokens instead of the \[CLS\] token of CLIP (as proposed in PACL [@mukhoti2023open]) brings improvement, and adding Rotary Position Embedding (RoPE) [@su2024roformer] yields higher pair accuracy. However, using multiple text tokens from the CLIP text encoder as SPARC did [@bica2024improving] does not help.

- **Language models.** In Section [3.3](#ab3_lm_choice){reference-type="ref" reference="ab3_lm_choice"}, we replace the CLIP text encoder with a stronger, LLM-converted model [@huang2024llm2clip], but it does not suffice to realize effective extraction and outperform random chance.

- **Architecture design for image-text alignment.** In Section [3.4](#ab4_vlm2vec){reference-type="ref" reference="ab4_vlm2vec"}, we find that text generation is not the only path to visual reasoning, as image-text matching through cosine similarity performed by contrastive VLMs can have strong performance on challenging benchmarks.

- **Training objective for image-text alignment.** In Section [3.4](#ab4_vlm2vec){reference-type="ref" reference="ab4_vlm2vec"}, we discover that finetuning with autoregressive loss is not necessary for deriving a VLM with fine-grained visual reasoning ability.

- **Question as prompt.** In Section [3.4](#ab4_vlm2vec){reference-type="ref" reference="ab4_vlm2vec"}, we also investigate the role of the question as a prompt for Generative MLLMs and find that, when fully fused with the image, it reweights the image tokens, significantly aiding in the extraction of relevant information and the enhancement of image embeddings.

In Section [4](#discussion_related_work){reference-type="ref" reference="discussion_related_work"}, we discuss the implications of our findings and their connection to prior work. Overall, we provide insights into VLM design and propose directions for improving contrastive VLMs.

# Comparing CLIP and Generative MLLMs' visual reasoning performance {#sec:compare}

We begin by introducing the task setup for the comparison. Using score-based evaluation, we notice a significant performance gap between CLIP and Generative MLLMs with the same vision encoder across several challenging visual reasoning benchmarks, highlighting the latter's stronger ability to extract and utilize visual information for reasoning.

## Task Setup {#background}

:::: table*
::: center
+:--------------------+:--------:+:--------:+:--------:+:--------:+:-------:+:-------:+:--------:+:--------:+
|                     | What'sUp Subset A                         | What'sUp Subset B                       |
+---------------------+---------------------+---------------------+-------------------+---------------------+
|                     | Left/Right          | On/Under            | Left/Right        | Front/Behind        |
+---------------------+----------+----------+----------+----------+---------+---------+----------+----------+
|                     | Indiv.   | Pairs    | Indiv.   | Pairs    | Indiv.  | Pairs   | Indiv.   | Pairs    |
+---------------------+----------+----------+----------+----------+---------+---------+----------+----------+
| CLIP-ViT-L/14-336px | 49.0     | 1.9      | 61.7     | 23.3     | 54.9    | 10.8    | 51.5     | 7.8      |
+---------------------+----------+----------+----------+----------+---------+---------+----------+----------+
| LLaVA-1.5-7B        | 96.6     | 93.2     | 76.2     | 52.4     | 98.5    | 97.1    | **76.0** | **52.9** |
+---------------------+----------+----------+----------+----------+---------+---------+----------+----------+
| Phi-3-V-3.8B        | 97.6     | 95.1     | 78.6     | 58.3     | **100** | **100** | 61.8     | 26.5     |
+---------------------+----------+----------+----------+----------+---------+---------+----------+----------+
| LLaMA-3-V-8B        | **98.1** | **96.1** | **81.1** | **64.1** | **100** | **100** | 73.0     | 47.1     |
+---------------------+----------+----------+----------+----------+---------+---------+----------+----------+
| Random chance       | 50.0     | 25.0     | 50.0     | 25.0     | 50.0    | 25.0    | 50.0     | 25.0     |
+---------------------+----------+----------+----------+----------+---------+---------+----------+----------+
:::
::::

:::: table*
::: center
                         Winoground   NaturalBench-R     MMVP     MMVP-VLM           
  --------------------- ------------ ---------------- ---------- ---------- -- -- -- --
  CLIP-ViT-L/14-336px       27.8           47.8          14.0       20.7             
  LLaVA-1.5-7B              39.8           52.2          36.0     **49.6**           
  Phi-3-V-3.8B              35.8           50.5          30.7       31.9             
  LLaMA-3-V-8B            **46.3**       **64.7**      **50.0**   **49.6**           
  Random chance             25.0           25.0          25.0       25.0             
:::
::::

:::: table*
::: center
+:----------------------+:----------:+:--------:+:--------------:+:--------------:+:--------:+:--------:+:--------:+:--------:+:-:+:-:+:-:+
|                       | SugarCREPE | SeeTrue  | **What'sUp A** | **What'sUp B** | **COCO-spatial**    | **GQA-spatial**     |   |   |   |
+-----------------------+------------+----------+----------------+----------------+----------+----------+----------+----------+---+---+---+
|                       |            |          |                |                | One-obj. | Two-obj. | One-obj. | Two-obj. |   |   |   |
+-----------------------+------------+----------+----------------+----------------+----------+----------+----------+----------+---+---+---+
| CLIP-ViT-L/14-224px   | 79.2       | 62.6     | 26.7           | 25.7           | 49.1     | 50.2     | 46.0     | 48.1     |   |   |   |
+-----------------------+------------+----------+----------------+----------------+----------+----------+----------+----------+---+---+---+
| CLIP-ViT-L/14-336px   | 80.0       | 63.0     | 28.9           | 27.2           | 48.9     | 51.1     | 46.6     | 49.1     |   |   |   |
+-----------------------+------------+----------+----------------+----------------+----------+----------+----------+----------+---+---+---+
| SigLIP-ViT-L/16-384px | 85.2       | 66.8     | 26.7           | 28.7           | 50.3     | 48.6     | 47.8     | 48.7     |   |   |   |
+-----------------------+------------+----------+----------------+----------------+----------+----------+----------+----------+---+---+---+
| EVA01-ViT-g-14        | 81.1       | 64.9     | 28.2           | 27.9           | 45.9     | 50.5     | 44.4     | 49.8     |   |   |   |
+-----------------------+------------+----------+----------------+----------------+----------+----------+----------+----------+---+---+---+
| LLaVA-1.5-7B          | 88.5       | 76.0     | **69.9**       | **65.4**       | 89.9     | **88.9** | 94.6     | **95.2** |   |   |   |
+-----------------------+------------+----------+----------------+----------------+----------+----------+----------+----------+---+---+---+
| Phi-3-V-3.8B          | 82.8       | 73.7     | 66.0           | 52.7           | 89.5     | 79.8     | 93.0     | 87.3     |   |   |   |
+-----------------------+------------+----------+----------------+----------------+----------+----------+----------+----------+---+---+---+
| LLaMA-3-V-8B          | **91.2**   | **80.7** | 66.7           | 58.6           | **91.9** | 78.9     | **95.3** | 91.4     |   |   |   |
+-----------------------+------------+----------+----------------+----------------+----------+----------+----------+----------+---+---+---+
| Random chance         | 50.0       | 50.0     | 25.0           | 25.0           | 50.0     | 50.0     | 50.0     | 50.0     |   |   |   |
+-----------------------+------------+----------+----------------+----------------+----------+----------+----------+----------+---+---+---+
:::
::::

This paper focuses on the image-text matching task in which VLMs are asked to choose from captions for a given image or vice versa.

**Benchmarks.** We use several challenging benchmarks, **Winoground** [@thrush2022winoground], **NaturalBench** [@li2024naturalbench], **SeeTrue** [@yarom2024you], **SugarCREPE** [@hsieh2024sugarcrepe], for assessing VLMs' compositionality. In Winoground, each test case has two image+text pairs with the same words in different order. For NaturalBench, we use the retrieval version (denoted as NaturalBench-R) in the same format as Winoground provided by @lin2024evaluating. SeeTrue consists of individual image-text pairs, while SugarCREPE has one image and two captions per test case. We use **MMVP(-VLM)** [@tong2024eyes] to test VLMs' ability to capture visual details like object existence, orientation, and counting. Since MMVP is not in paired image-text format, we manually convert it without altering content. We adopt **What'sUp A&B** with **COCO-spatial** and **GQA-spatial** [@kamath2023s] to evaluate VLMs' spatial reasoning. For What'sUp, each test case includes four captions (e.g., "`A dog left of/right of/on/under a table`") and corresponding images with minimal variation except for spatial relationships. We split each test case into two pairs---e.g., one pair contrasts "`left of`" versus "`right of`" with their ground truth images, and the other covers the remaining captions. This yields four benchmark subsets for A and B. COCO-spatial and GQA-spatial have one image and two captions per test case. More details are in Appendix [7.1](#appendix_benchmark){reference-type="ref" reference="appendix_benchmark"}.

**Models.** Our main comparison is between **CLIP-ViT-L/14-336px** [@radford2021learning] and Generative MLLMs that use its pretrained vision encoder and keep the weights frozen during training: **LLaVA-1.5-7B** [@liu2024improved], along with **Phi-3-V-3.8B** and **LLaMA-3-V-8B** [@{hanoona2024LLaVA++}]. In these MLLMs, the patch tokens from the CLIP vision encoder first pass through a two-layer MLP connector and are then used as input tokens for a generative language model which yields the token probability determining the model response. We also include results of **CLIP-ViT-L/14-224px**, **SigLIP-ViT-L/16-384px** [@zhai2023sigmoid], and **EVA01-ViT-g-14** [@sun2023eva] for reference since they are of interest and widely used [@tong2024cambrian].

**Evaluation protocol.** For CLIP-like contrastive VLMs, the matching score is the cosine similarity between its image embeddings and text embeddings. In prior works, Generative MLLMs are commonly evaluated by GPT-4 [@achiam2023gpt] or human evaluators on generated responses. However, human evaluators are expensive for thousands of model responses, and GPT-4 as the judge can be incorrect and affected by user prompts. To ensure a fair comparison, we choose to use a score-based evaluation method and adopt the VQAScore [@lin2024evaluating], defined as $$\begin{align*}
    P (\text{``Yes''}|\text{\texttt{image}, ``Does this figure show `{\texttt{text}}'? }\\
    \text{Please answer yes or no.''})
\end{align*}$$ The question template remains the same across different benchmarks. We present the comparison between VQAScore and response-based evaluation in Appendix [7.3](#appendix_response){reference-type="ref" reference="appendix_response"}.

**Evaluation metrics.** For SeeTrue, we report an average AUROC of three subsets. For other benchmarks, we use pair accuracy and individual accuracy when applicable. **Pair accuracy** [@tong2024eyes; @kamath2023s] requires correct matching for both images, and it only applies to benchmarks with two images in a test case. **Individual accuracy** refers to the accuracy of individual images. For MMVP(-VLM), we follow the original paper and use pair accuracy to represent the correct matching for both *captions* and individual accuracy for individual *captions* instead.

## Results {#blindness}

We present the comparison in Table [\[tab:comparison-1\]](#tab:comparison-1){reference-type="ref" reference="tab:comparison-1"}, [\[tab:comparison-2\]](#tab:comparison-2){reference-type="ref" reference="tab:comparison-2"}, and [\[tab:comparison-3\]](#tab:comparison-3){reference-type="ref" reference="tab:comparison-3"}. On these challenging benchmarks, Generative MLLMs outperform CLIP-ViT-L/14-336px with the same vision encoder, showing that (1) CLIP vision encoder has much query-relevant visual information not utilized by CLIP, and (2) Generative MLLMs can extract and align this information from the encoder more effectively. The performance gap is the most significant on **spatial reasoning**, where the CLIP models behave close to random chance for individual accuracy and lower than random chance for pair accuracy, but Generative MLLMs achieve high accuracies. We further find that the Generative MLLMs can even outperform XVLM [@zeng2021multi] specialized in spatial reasoning (See Appendix [8.4](#appendix_xvlm){reference-type="ref" reference="appendix_xvlm"}).

# Investigation of the Performance Gap {#ablation}

The gap observed in Section [2.2](#blindness){reference-type="ref" reference="blindness"} could be the result of various factors, ranging from model training to architecture. In this section, we try to dissect and examine which factors contribute to Generative MLLMs' success and cause CLIP's failure by controlled experiments. We focus on the performance gap on **What'sUp**, of which the test cases are tightly controlled and balanced. A road map of the experiments is illustrated in Figure [2](#roadmap){reference-type="ref" reference="roadmap"}.

<figure id="roadmap" data-latex-placement="t">
<div class="center">
<embed src="main_texts/roadmap.pdf" style="width:100.0%" />
</div>
<figcaption>An illustration for CLIP-like contrastive VLMs and the controlled experiments in Section <a href="#ablation" data-reference-type="ref" data-reference="ablation">3</a>. We first investigate the effect of training data by replacing them with LLaVA-1.5’s training data (<span class="math inline">$\text{\textcircled{\scriptsize 1}}$</span>). Then, we try different token usage for CLIP vision encoder and text encoder (<span class="math inline">$\text{\textcircled{\scriptsize 2}}$</span>) and discuss the influence of using stronger text encoders converted from LLMs (<span class="math inline">$\text{\textcircled{\scriptsize 3}}$</span>). Finally, we convert LLaVA-1.5 to contrastive VLMs (<span class="math inline">$\text{\textcircled{\scriptsize 4}}$</span>) to study the effect of the alignment architecture and training objective.</figcaption>
</figure>

## Training Data {#ab1_data}

:::: center
::: {#tab:ablation_data}
+:------------------+:---------:+:---------:+:---------:+:---------:+:-:+:-:+:-:+:-:+:-:+
|                   | What'sUp Subset A     | What'sUp Subset B     |   |   |   |   |   |
+-------------------+-----------+-----------+-----------+-----------+---+---+---+---+---+
|                   | Indiv.    | Pairs     | Indiv.    | Pairs     |   |   |   |   |   |
+-------------------+-----------+-----------+-----------+-----------+---+---+---+---+---+
| CLIP              | 49.0      | 1.9       | 54.9      | 10.8      |   |   |   |   |   |
+-------------------+-----------+-----------+-----------+-----------+---+---+---+---+---+
| + finetuning (ft) | 50.5      | 1.9       | 53.9      | 5.9       |   |   |   |   |   |
+-------------------+-----------+-----------+-----------+-----------+---+---+---+---+---+
| + ft + hard neg.  | 50.5      | 1.0       | 50.5      | 1.0       |   |   |   |   |   |
+-------------------+-----------+-----------+-----------+-----------+---+---+---+---+---+
| SigLIP            | 50.0      | 1.9       | 51.5      | 5.9       |   |   |   |   |   |
+-------------------+-----------+-----------+-----------+-----------+---+---+---+---+---+
| + finetuning (ft) | 49.0      | 1.0       | 51.0      | 3.9       |   |   |   |   |   |
+-------------------+-----------+-----------+-----------+-----------+---+---+---+---+---+
| + ft + hard neg.  | 50.0      | 0.0       | 50.0      | 0.0       |   |   |   |   |   |
+-------------------+-----------+-----------+-----------+-----------+---+---+---+---+---+
| EVA-CLIP          | 49.0      | 1.0       | 50.1      | 4.9       |   |   |   |   |   |
+-------------------+-----------+-----------+-----------+-----------+---+---+---+---+---+
| + finetuning (ft) | 50.0      | 4.9       | 48.5      | 2.0       |   |   |   |   |   |
+-------------------+-----------+-----------+-----------+-----------+---+---+---+---+---+
| + ft + hard neg.  | 50.0      | 1.9       | 48.0      | 2.0       |   |   |   |   |   |
+-------------------+-----------+-----------+-----------+-----------+---+---+---+---+---+
| Random chance     | 50.0      | 25.0      | 50.0      | 25.0      |   |   |   |   |   |
+-------------------+-----------+-----------+-----------+-----------+---+---+---+---+---+

: The two-way individual accuracy and pair accuracy results of CLIP-ViT-L/14-336px, SigLIP-ViT-L/16-384px, and EVA01-ViT-g-14 focusing on the **Left/Right** subsets of What'sUp after finetuning on LLaVA-1.5's training data with or without hard negative captions. After direct finetuning, the accuracies are still quite low.
:::
::::

First, we hypothesize that Generative MLLMs' visual information extraction ability benefits from training data. To check the effect of data, we use LLaVA-1.5's training data to finetune CLIP, SigLIP, and EVA-CLIP. We convert the datasets to the image-caption format (Details are deferred to Appendix [8.2](#appendix_data){reference-type="ref" reference="appendix_data"}). By default, we freeze the vision encoder during finetuning for strict ablation. Considering that contrastive learning relies on negative samples beyond data quality [@robinson2020contrastive; @kalantidis2020hard], we also construct hard negative captions by switching the related phrases to their opposite (e.g., replacing "`on the left`" with "`on the right`"). In this setting, the training objective follows NegCLIP [@yuksekgonul2023and].

Results are shown in Table [1](#tab:ablation_data){reference-type="ref" reference="tab:ablation_data"}. Finetuning on LLaVA-1.5's training data does not help these models, even with hard negatives. Still, their accuracy is around random chance. We also try to unlock the SigLIP vision encoder during finetuning, which does not increase the performance either (See results in Appendix [8.3](#appendix_unlock){reference-type="ref" reference="appendix_unlock"}). We experiment with XVLM [@zeng2021multi] and observe similar results in Appendix [8.4](#appendix_xvlm){reference-type="ref" reference="appendix_xvlm"}. This finding aligns with the previous failure on finetuning them on a much larger, preposition-focused subset of LAION [@kamath2023s], indicating that **data alone does not lead to stronger extraction ability.**

## Token Usage {#ab2_token}

:::: table*
::: center
+:--------------------------+:--------:+:--------:+:--------:+:--------:+:--------:+:--------:+:--------:+:--------:+
|                           | What'sUp Subset A                         | What'sUp Subset B                         |
+---------------------------+---------------------+---------------------+---------------------+---------------------+
|                           | Left/Right          | On/Under            | Left/Right          | Front/Behind        |
+---------------------------+----------+----------+----------+----------+----------+----------+----------+----------+
|                           | Indiv.   | Pairs    | Indiv.   | Pairs    | Indiv.   | Pairs    | Indiv.   | Pairs    |
+---------------------------+----------+----------+----------+----------+----------+----------+----------+----------+
| LLaVA-1.5-7B-LoRA         | **84.5** | **68.9** | **76.2** | **52.4** | **89.2** | **78.4** | **86.3** | **72.5** |
+---------------------------+----------+----------+----------+----------+----------+----------+----------+----------+
| `[CLS]`-LLaVA-1.5-7B-LoRA | 44.2     | 8.7      | 54.4     | 8.7      | 49.0     | 4.9      | 53.9     | 12.7     |
+---------------------------+----------+----------+----------+----------+----------+----------+----------+----------+
| Random chance             | 50.0     | 25.0     | 50.0     | 25.0     | 50.0     | 25.0     | 50.0     | 25.0     |
+---------------------------+----------+----------+----------+----------+----------+----------+----------+----------+
:::
::::

:::: table*
::: center
+:-------------------------------------+:--------:+:--------:+:--------:+:--------:+:--------:+:--------:+:--------:+:--------:+:-:+
|                                      | What'sUp Subset A                         | What'sUp Subset B                         |   |
+--------------------------------------+---------------------+---------------------+---------------------+---------------------+---+
|                                      | Left/Right          | On/Under            | Left/Right          | Front/Behind        |   |
+--------------------------------------+----------+----------+----------+----------+----------+----------+----------+----------+---+
|                                      | Indiv.   | Pairs    | Indiv.   | Pairs    | Indiv.   | Pairs    | Indiv.   | Pairs    |   |
+--------------------------------------+----------+----------+----------+----------+----------+----------+----------+----------+---+
| CLIP-ViT-L/14-336px                  | 49.0     | 1.9      | **61.7** | **23.3** | **54.9** | 10.8     | 51.5     | 7.8      |   |
+--------------------------------------+----------+----------+----------+----------+----------+----------+----------+----------+---+
| \+ Patch Tokens (PT)                 | 47.6     | 9.7      | 52.9     | 10.7     | 52.9     | 9.8      | 51.5     | 6.9      |   |
+--------------------------------------+----------+----------+----------+----------+----------+----------+----------+----------+---+
| \+ PT + RoPE                         | **54.9** | **22.3** | 46.1     | 13.6     | 52.0     | **20.6** | 45.6     | 12.7     |   |
+--------------------------------------+----------+----------+----------+----------+----------+----------+----------+----------+---+
| \+ PT + RoPE + Multiple Text Tokens  | 48.1     | 0.0      | 50.0     | 2.9      | 50.0     | 6.9      | 48.0     | 7.8      |   |
+--------------------------------------+----------+----------+----------+----------+----------+----------+----------+----------+---+
| \+ PT + RoPE + Stronger Text Encoder | 50.5     | 10.7     | 48.5     | 6.8      | 50.0     | 15.7     | **53.9** | **21.6** |   |
+--------------------------------------+----------+----------+----------+----------+----------+----------+----------+----------+---+
|                                      |          |          |          |          |          |          |          |          |   |
+--------------------------------------+----------+----------+----------+----------+----------+----------+----------+----------+---+
| Random chance                        | 50.0     | 25.0     | 50.0     | 25.0     | 50.0     | 25.0     | 50.0     | 25.0     |   |
+--------------------------------------+----------+----------+----------+----------+----------+----------+----------+----------+---+
:::
::::

**Patch tokens.** The output of the CLIP vision encoder consists of two parts: The **`[CLS]` token**, functioning as the global feature of the image, and the **patch tokens**, containing local information of image patches. We notice that these Generative MLLMs employ all 576 patch tokens from the CLIP-ViT-L/14-336px vision encoder, in contrast to CLIP using only the projected `[CLS]` token.

We first perform an ablation study on LLaVA-1.5: We change the input of its language model to use only the `[CLS]` token, train this "`[CLS]`-LLaVA-1.5" model from scratch (pretraining + finetuning) using LoRA [@hu2021lora], and observe that its spatial reasoning performance is significantly worse than our reproduced LLaVA-1.5-LoRA in Table [\[tab:ablation_cls_llava\]](#tab:ablation_cls_llava){reference-type="ref" reference="tab:ablation_cls_llava"}. This proves the importance of patch tokens to fine-grained visual reasoning: **Detailed information of images resides in these patch tokens.**

Inspired by this finding, we try incorporating patch tokens in standard CLIP models. We adopt the PACL method [@mukhoti2023open] as it proposes to train a vision embedder $e_v$ for patch tokens and a text embedder $e_t$ on top of the frozen CLIP model (consisting of vision encoder $f_v$ and text encoder $f_t$). For input image $\mathbf{x}$ and text $\mathbf{y}$, we calculate the image feature $\mathbf{v}(\mathbf{x})$ by $$\begin{align*}
    s(\mathbf{x}, \mathbf{y}) &= e_v(f_v(\mathbf{x}))\cdot e_t(f_t(\mathbf{y})) \\
    \mathbf{v}(\mathbf{x}) &= e_v(f_v(\mathbf{x}))^\top \cdot\text{sigmoid}(10\cdot s(\mathbf{x}, \mathbf{y}))
\end{align*}$$ In other words, $s(\mathbf{x}, \mathbf{y})$ determines the weight for each projected patch token based on the text, and $\mathbf{v}(\mathbf{x})$ is a weighted sum of all projected patch tokens. Then we use $\mathbf{v}(\mathbf{x})$ and $e_t(f_t(\mathbf{y}))$ as the image and text features for CLIP training with the original contrastive objective. During the evaluation, we use the average of projected patch tokens $e_v(f_v(\mathbf{x}))$ as the image feature and $e_t(f_t(\mathbf{y}))$ as the text feature. The results of training on LLaVA-1.5's data are shown in the second row of Table [\[tab:ablation_pacl\]](#tab:ablation_pacl){reference-type="ref" reference="tab:ablation_pacl"}. It brings higher pair accuracy to the Left/Right subset in Subset A.

**Position embeddings.** Considering that the average or weighted sum does not maintain the order/positional information of patch tokens, we add Rotary Position Embeddings (RoPE) [@su2024roformer] to $f_v(\mathbf{x})$ before passing it to the vision embedder $e_v$, since RoPE is applied to visual tokens in the language model of Generative MLLMs we study. In the third row of Table [\[tab:ablation_pacl\]](#tab:ablation_pacl){reference-type="ref" reference="tab:ablation_pacl"}, we find that this combination yields significantly higher pair accuracy on three subsets, showing that **part of the information comes from the order of patch tokens.** Nonetheless, the individual accuracy is not improved by much.

**Multiple text tokens.** Does using multiple text tokens of CLIP text encoder as well offer further performance gain? In Generative MLLMs, it is natural to use multiple visual tokens and text tokens, as they are concatenated as the input of an autoregressive language model. However, it is non-trivial to do so in contrastive VLMs. Therefore, we leverage the SPARC method [@bica2024improving] to implement the interaction between multiple visual tokens and text tokens for CLIP: We first obtain a weighted sum of patch tokens for each text token (named grouped visual tokens) and then perform local contrastive learning between grouped visual tokens and text tokens within each sample. The training objective is the sum of this local contrastive loss and the standard contrastive loss. For evaluation, we use the average of grouped visual tokens as the image feature and the average of text tokens as the text feature. Details are deferred to Appendix [8.5](#appendix_sparc){reference-type="ref" reference="appendix_sparc"}. Despite the complexity, this method does not help our task (See the fourth row of Table [\[tab:ablation_pacl\]](#tab:ablation_pacl){reference-type="ref" reference="tab:ablation_pacl"}). This failure might result from the hardness of training and the insufficiency of token interaction.

## Language Model {#ab3_lm_choice}

Previous research suggests that the CLIP text encoder fails to capture changed word orders, negation, and spatial or numerical details [@tong2024mass; @kamath2023text; @yuksekgonul2023and], while Generative MLLMs employ powerful pretrained LLMs, which is supposed to be stronger than the CLIP text encoder at reasoning.

Are pretrained LLMs the missing piece to effectively extracting visual information? We perform further experiments on finetuning CLIP with patch tokens and RoPE on LLaVA-1.5 training data but replacing the original CLIP text encoder with a stronger one provided by LLM2CLIP [@huang2024llm2clip]. This text encoder is converted from Llama-3-8B-Instruct [@dubey2024llama] by contrastive finetuning and is shown to bring performance boost to state-of-the-art CLIP models on benchmarks such as MS COCO [@lin2014microsoft]. We keep this text encoder and the CLIP vision encoder frozen during our finetuning. The results are shown in the fifth row of Table [\[tab:ablation_pacl\]](#tab:ablation_pacl){reference-type="ref" reference="tab:ablation_pacl"}, where we also attach the results of the original LLM2CLIP checkpoint of CLIP-ViT-L/14-336px for reference[^1]. We find that **a stronger text encoder does not suffice to effectively extract more information towards solving the task.**

:::: table*
::: center
+:--------------------------+:--------:+:--------:+:--------:+:--------:+:-------:+:-------:+:--------:+:--------:+
|                           | What'sUp Subset A                         | What'sUp Subset B                       |
+---------------------------+---------------------+---------------------+-------------------+---------------------+
|                           | Left/Right          | On/Under            | Left/Right        | Front/Behind        |
+---------------------------+----------+----------+----------+----------+---------+---------+----------+----------+
|                           | Indiv.   | Pairs    | Indiv.   | Pairs    | Indiv.  | Pairs   | Indiv.   | Pairs    |
+---------------------------+----------+----------+----------+----------+---------+---------+----------+----------+
| CLIP-ViT-L/14-336px       | 49.0     | 1.9      | 61.7     | 23.3     | 54.9    | 10.8    | 51.5     | 7.8      |
+---------------------------+----------+----------+----------+----------+---------+---------+----------+----------+
| LLaVA-1.5-7B-VLM2Vec-LoRA | **97.1** | **95.1** | **68.0** | **35.9** | **100** | **100** | **60.8** | **22.5** |
+---------------------------+----------+----------+----------+----------+---------+---------+----------+----------+
| w/o Question in Prompt    | 49.5     | 0.0      | 50.5     | 1.9      | 46.6    | 2.0     | 50.5     | 1.0      |
+---------------------------+----------+----------+----------+----------+---------+---------+----------+----------+
| Random chance             | 50.0     | 25.0     | 50.0     | 25.0     | 50.0    | 25.0    | 50.0     | 25.0     |
+---------------------------+----------+----------+----------+----------+---------+---------+----------+----------+
:::
::::

## Alignment Architecture, Training Objective, and Prompt {#ab4_vlm2vec}

A major difference between CLIP-like contrastive VLMs and LLaVA-like Generative MLLMs is how they align images and texts. However, it is hard to examine every factor involved separately: The alignment architecture of CLIP---cosine similarity between image embeddings and text embeddings---is bound to its training objective (contrastive loss) and contrastive VLM structure (dual encoders). On the other hand, it is plausible to hypothesize that contrastive VLMs cannot perform fine-grained visual reasoning since cosine similarity might be overly coarse-grained both for training and evaluation, compared with text generation and autoregressive loss used by Generative MLLMs.

We bypass this obstacle in comparison by converting a Generative MLLM to a CLIP-like contrastive VLM. On LLaVA-1.5-7B, we use the converting method proposed by VLM2Vec [@jiang2024vlm2vec]: Specifically, we take the last layer vector representation of the last output token of LLaVA-1.5-7B as the output embedding. In this way, we get the encoder for image+question(prompt) and pure text simultaneously since LLaVA-1.5 allows using one or zero images in the input. Following the original paper, we use the prompt templates: "`Represent the given image with the following question: {Question}`" while encoding the image if there is a question in the sample; "`Find the text that can answer the given query: {Question}`" when there is no image; and no additional prompt for encoding im- age+question of LCS-558K and the captions. Then, we finetune this encoder using contrastive loss and LoRA [@hu2021lora] on LLaVA-1.5's training data with the CLIP vision encoder frozen. Surprisingly, they exhibit strong performance without using a large batch size (256) in Table [\[tab:vlm2vec\]](#tab:vlm2vec){reference-type="ref" reference="tab:vlm2vec"} (LLaVA-1.5-7B-VLM2Vec-LoRA). The question used for evaluation is listed in Appendix [7.4](#appendix_eval_vlm2vec){reference-type="ref" reference="appendix_eval_vlm2vec"}. **This proves that text generation+autoregressive loss is not the only solution to fine-grained visual reasoning.**

What could be the key factor of the success of this contrastive LLaVA-1.5 compared with CLIP models, including the standard ones and ours with patch tokens plus RoPE in Section [3.2](#ab2_token){reference-type="ref" reference="ab2_token"}? We verify that the additional question added in the prompt when obtaining the image embeddings plays an important role here. When we change the prompt template to "`Represent the given image.`" without any question, the model performance degenerates to the standard CLIP performance as shown in the third row of Table [\[tab:vlm2vec\]](#tab:vlm2vec){reference-type="ref" reference="tab:vlm2vec"}. Therefore, we conclude that **the question greatly helps the extraction and utilization of visual information from the vision encoder.** The question helps to reweight the patch tokens according to the context. Without the question, the image embeddings remain the same regarding different tasks (e.g., coarse-grained classification like "`dog/cat`", versus fine-grained visual reasoning like "`dog to the left/right of the table`"), which could be suboptimal and cause difficulty in alignment.

# Discussion and Connection to Prior Work {#discussion_related_work}

In this section, we first discuss how our findings connect to the observations and conclusions in existing literature. Then, we list two directions for improving VLM's visual reasoning ability based on our results.

## Connection to Prior Work

Recent results of VLMs on various benchmarks for testing fine-grained visual reasoning ability (e.g., compositionality, spatial reasoning, counting) reveal that they fail to solve simple tasks unexpectedly and often ignore visual patterns in the image [@thrush2022winoground; @yuksekgonul2023and]. Researchers are actively exploring the root causes of such failures. @lin2024evaluating notices the advantage of Generative MLLMs over CLIP in image-text matching tasks. We observe the significant discrepancy in performance when controlling the vision encoder and thus focus on how Generative MLLMs could outperform CLIP-like contrastive VLMs with the same vision encoder.

**Vision encoder and token usage.** @tong2024eyes observes that the CLIP vision encoder could encode visually distinct images into highly similar embeddings, omitting essential information and thus resulting in low accuracy on tasks regarding the visual semantic difference. Hence, they suggest using features from multiple vision encoders, which is adopted by later works [@kar2024brave; @tong2024cambrian; @xu2024libra]. However, we observe that this part of information could be captured by the CLIP vision encoder but is not extracted or aligned properly. Similar to our observation, @koishigarina2025clip argues that CLIP is not bag-of-words uni-modally, and the real issue of CLIP's compositionality lies in poor cross-modal alignment. Besides, while @tong2024eyes only calculates the similarity between `[CLS]` tokens used by CLIP as evidence, we argue that detailed information is preserved in patch tokens and their positions.

**Text encoder.** @kamath2023text and @tong2024mass point out that the CLIP text encoder might discard relevant information during encoding so that the model could not discriminate images that differ in key aspects. Following previous efforts in converting LLM to an encoder [@behnamghader2024llm2vec], recent works explore using LLM-converted encoders as the text encoder for CLIP: LLM2CLIP [@huang2024llm2clip] finds that this practice boosts performance on several retrieval tasks on top of state-of-the-art CLIP models, but we observe its unsatisfying performance on What'sUp; @stone2024learning achieves high accuracy on challenging benchmarks for compositionality after large-scale pretraining, although they reported struggles on Left/Right spatial relations. We discover that a stronger text encoder is not enough for solving the fine-grained visual reasoning task.

**Training data and objective.** Data-centric methods for improving CLIP-like models include selecting or synthesizing higher-quality image-text pairs [@gadre2024datacomp; @nguyen2024improving; @zheng2024dreamlip], involving more negative samples by manual design [@yuksekgonul2023and; @paiss2023teaching] or larger batch size [@stone2024learning]. But @kamath2023s observes that CLIP cannot learn spatial relations even after training on a large amount of relevant data, suggesting that we might need inductive bias or denser supervision like XVLM [@zeng2021multi]. Others try applying autoregressive loss, such as Cap/CapPa [@tschannen2023image], or combining it with contrastive loss, like CoCa [@yu2022coca]. Inspired by VLM2Vec [@jiang2024vlm2vec], we train LLaVA-1.5-VLM2Vec and verify that task-specific inductive bias, additional supervision, manually designed hard negatives, or finetuning with autoregressive loss is not necessary for contrastive VLMs to learn spatial relations.

**Alignment architecture of contrastive VLMs.** Cross-modal alignment can be implemented by cross-modal matching through cosine similarity [@radford2021learning], matching by directly outputting a score [@li2023blip2], and generation (outputting a response) [@liu2024improved; @awadalla2023openflamingo]. The cross-modal contrasting is efficient, yet unable to perform complex reasoning like the generative models where Chain-of-Thought is applicable [@wei2022chain]. Nevertheless, our LLaVA-1.5-VLM2Vec experiments show that advanced techniques can ignite the potential of contrastive VLMs in visual information extraction and improve their visual reasoning performance.

## Discussion on Improving VLMs' Visual Reasoning Ability

**Promptable image embeddings boost performance on fine-grained tasks.** CLOC [@chen2024contrastive] formulates the idea of promptable embedding for regional understanding of images. They pass image embeddings and spatial hints to a prompter for obtaining region representations of images and perform localized contrastive training. In this way, when a grounding task only requires information from part of the image, the representation will not be distracted by other parts and thus lead to higher accuracy. VLM2Vec [@jiang2024vlm2vec] extends the spatial hints to general prompts and proposes a method for converting Generative MLLMs to encoders. Our ablation study of questions in prompts for LLaVA-1.5-VLM2Vec demonstrates the effectiveness of this technique.

**Effectively utilizing vision encoders offers benefits without pretraining new vision models.** Our results suggest that there is still room to enhance VLMs with a fixed, pretrained vision encoder by advanced extraction methods. We explore whether this also holds for Generative MLLMs in the Appendix [8.7](#appendix_m3id){reference-type="ref" reference="appendix_m3id"}: We try an alternative decoding algorithm on LLaVA-1.5-7B for attending more to the visual information, named Multi-Modal Mutual-Information Decoding [@favero2024multi], which leads to performance gain (+6%), on par with using interleaved visual tokens from multiple vision encoders (I-MoF [@tong2024eyes]). This result indicates that LLaVA-1.5 still misses some key information for query answering and has room for further improvement apart from using a better vision encoder.

# Conclusion

Our study first reveals that Generative MLLMs perceive fine-grained visual information more effectively using the same vision encoder than CLIP for visual reasoning tasks. Through controlled experiments, we find that patch tokens, position embeddings, and prompt-based image embeddings are key differences causing the gap; however, training data, multiple text tokens, and better text encoders are insufficient to bridge the gap. Additionally, text generation and finetuning with autoregressive loss are not mandatory for strong visual reasoning. These findings not only offer insights into VLM design but also provide practical guidelines for enhancing contrastive VLMs on visual reasoning.

# Limitations

First, for controlled experiments on data in Section [3.1](#ab1_data){reference-type="ref" reference="ab1_data"}, we do not train models from scratch or use larger batch sizes due to the limited computing resources, so the conclusion regarding data might be restricted.

Second, the number of visual reasoning benchmarks we study is restricted. Therefore, we hope that more comprehensive, unbiased, and visual-centric reasoning benchmarks for VLMs can be available in the future.

Third, we only study the comparison between CLIP-ViT-L/14-336px and the Generative MLLMs that use its vision encoder and explore the reasons behind their discrepancy. Our conclusion is thus restricted to them. We do not claim that all Generative MLLMs are better than contrastive VLMs in all cases. Nevertheless, it is interesting to compare other pairs of contrastive VLMs and Generative MLLMs, and we leave this for future work.

# Acknowledgments {#acknowledgments .unnumbered}

We thank Amita Kamath for the discussion about the What'sUp benchmark. We appreciate the comments and advice from Rui Xin and Scott Geng on our drafts. PWK is supported by the Singapore National Research Foundation and the National AI Group in the Singapore Ministry of Digital Development and Innovation under the AI Visiting Professorship Programme (award number AIVP-2024-001) and by the AI2050 program at Schmidt Sciences. SSD acknowledges the support of NSF DMS 2134106, NSF CCF 2212261, NSF IIS 2143493, NSF IIS 2229881, Sloan Fellowship, and the AI2050 program at Schmidt Sciences.

# Benchmarks and Additional Evaluations

<figure id="fig:example" data-latex-placement="t">
<div class="center">
<embed src="main_texts/Figure3.pdf" style="width:98.0%" />
</div>
<figcaption>Example test case and evaluation method for CLIP-like models on What’sUp benchmark. In our two-way evaluation on benchmarks with paired images, a test case consists of two similar images and two captions. The model chooses one caption for each image, and it gets one point in pair accuracy only if choosing correctly for both images. The choices of CLIP-like models are determined by <span class="math inline"><em>S</em><sub><em>C</em></sub></span>, cosine similarity between image and text embeddings.</figcaption>
</figure>

## Benchmark Information {#appendix_benchmark}

#### What'sUp.

The What'sUp benchmark [@kamath2023s] contains 820 images of pairs of household objects captured by the authors, 408 in Subset A and 412 in Subset B. For every object pair, all prepositions are present in the benchmark, and thus the images and captions are balanced, avoiding the bias in real-world images (e.g., a cup is usually on the table, not under the table). We corrected the mislabeled images in the GitHub Issues and reevaluated the pretrained VLMs. For CLIP and XVLM's evaluation, we refer to the official code provided by the What'sUp benchmark's authors in <https://github.com/amitakamath/whatsup_vlms>. The evaluation of SigLIP and EVA-CLIP directly follows the evaluation of CLIP in the official code. We offer an example in Figure [3](#fig:example){reference-type="ref" reference="fig:example"} to demonstrate how pair accuracy and individual accuracy are computed on benchmarks with paired images like What'sUp.

#### COCO-spatial and GQA-spatial.

@kamath2023s also selects validation sample from COCO [@lin2014microsoft] and GQA [@hudson2019gqa] targeting spatial relations (to the left of vs to the right of, above vs below). Each test case contains one image, one positive caption, and one negative caption. COCO-spatial has 2687 test cases, and GQA has 1451 test cases in total.

#### Winoground, NaturalBench, and SeeTrue.

Winoground [@thrush2022winoground] is a challenging benchmark consisting of 400 pairs of image-text pairs. It focuses on VLM's compositionality, with two images and two similar captions in one test case. A example of the captions is "`some plants surrounding a lightbulb`" vs "`a lightbulb surrounding some plants`." High pair accuracy requires VLM to match these images with their captions correctly at the same time. NaturalBench [@li2024naturalbench] is a benchmark for testing Generative MLLMs on compositionality with unbiased Yes/No answers. In one test case, there are two images with two questions, and each question has \"Yes\" as the answer for one image and \"No\" for the other image. We use the retrieval version of NaturalBench provided by [@lin2024evaluating]. SeeTrue [@yarom2024you] is an alignment bench that has 6930 human labels for whether a given image is paired with the text or not. We report the AUROC (Area Under the Receiver Operating Characteristic curve) instead of accuracy on SeeTrue. We use VQAScore's official code for evaluation on these benchmarks in <https://github.com/linzhiqiu/t2v_metrics>.

#### SugarCREPE.

SugarCREPE [@hsieh2024sugarcrepe] is designed for evaluating VLM's compositionality with grammatical, sensical, and fluent hard negatives. Each test case contains one image, one positive caption, and one negative caption. There are 7512 test cases in total.

#### MMVP(-VLM).

The MMVP benchmark contains 150 pairs of similar images, and the MMVP-VLM benchmark has 135 pairs of similar images, divided into nine categories. There is an overlap between the image pairs in these two benchmarks. We corrected the mislabeled images in the GitHub Issues and reevaluated the pretrained VLMs. Since MMVP is incompatible with CLIP, we convert its questions manually. We attach the converted version to the supplementary material for reference.

## Model Weight Information {#appendix_model}

We use public pretrained weights of LLaVA-1.5-7B (<https://huggingface.co/llava-hf/llava-1.5-7b-hf>) under the Meta LLaMA License Agreement and the weights of Phi-3-V-3.8B in <https://huggingface.co/MBZUAI/LLaVA-Phi-3-mini-4k-instruct> and LLaMA-3-V-8B in <https://huggingface.co/MBZUAI/LLaVA-Meta-Llama-3-8B-Instruct> provided by [@ranasinghe2024learning] under MIT License since they are trained with vision encoder frozen. For contrastive VLMs, we use OpenAI's pretrained CLIP-ViT-L/14-224px and CLIP-ViT-L/14-336px model under MIT License, SigLIP-ViT-L/16-384px pretrained on the WebLI dataset [@chen2022pali] and EVA01-ViT-g-14 pretrained on the LAION400M-s11b-b41k dataset [@schuhmann2021laion] under Apache 2.0 License provided in the OpenCLIP repository. In Table [\[tab:ablation_cls_llava\]](#tab:ablation_cls_llava){reference-type="ref" reference="tab:ablation_cls_llava"}, LLaVA-1.5-7B-LoRA is reproduced. In Table [\[tab:ablation_pacl\]](#tab:ablation_pacl){reference-type="ref" reference="tab:ablation_pacl"}, the checkpoint for LLM2CLIP is from <https://huggingface.co/microsoft/LLM2CLIP-Llama-3-8B-Instruct-CC-Finetuned> under Apache 2.0 License. We also use the text encoder and the adapter of this checkpoint in our experiments of using a stronger text encoder. In Table [\[tab:vlm2vec\]](#tab:vlm2vec){reference-type="ref" reference="tab:vlm2vec"}, the LLaVA-1.5-7B-VLM2Vec-LoRA is trained by ourselves with the vision encoder frozen using the VLM2Vec method [@jiang2024vlm2vec].

## Comparison between VQAScore and Response-Based Evaluation {#appendix_response}

We compare the score-based evaluation, VQAScore [@lin2024evaluating], and the standard response-based evaluation for Generative MLLMs on What'sUp. Response-based evaluation requires a question accompanied by a given image as the input, and the questions used for LLaVA-1.5's evaluation are listed in Table [\[tab:prompts\]](#tab:prompts){reference-type="ref" reference="tab:prompts"}. Then, the question is concatenated with the fixed prompt template ("`USER: <image>\n{question} ASSISTANT:`"). Considering the position bias in LLMs [@wang2024eliminating], we exchange the position of two prepositions in the question with 50% probability on COCO-spatial and GQA-spatial benchmarks for fair results. On the What'sUp benchmark, the orders are always the same for two images. Then, we use greedy decoding to ensure reproducibility and evaluate the outputs by keyword matching since we observe that the outputs of Generative MLLMs are quite structured, showing their strong instruction-following ability.

The reason why we use different commands after the main question (e.g., "`Answer left or right`", "`Choose from the two options`", and "`Give a short answer`") is that we find the LLaVA-1.5 model sensitive to such command. For instance, we try "`Answer on or under`" and "`Answer with under or on`" for the On/Under subset in What'sUp Subset A, and the model accuracy is quite low. For Phi-3-V-3.8B and LLaMA-3-V-8B, we try these prompts and pick the one with the highest accuracy. This is one of their limitations that deserves future research. However, we aim to show that they can extract such information, so we use the best prompt to showcase its ability.

The results are shown in Table [\[tab:response\]](#tab:response){reference-type="ref" reference="tab:response"}. We observe that the accuracy of LLaVA-1.5-7B is increased on On/Under and Front/Behind subsets. However, the performance of LLaMA-3-V-8B is worsened. Overall, they still surpass CLIP.

:::: table*
::: center
  Subset                            Question
  --------------------------------- ------------------------------------------------------------------------------------------------------------------------------------------------------------------
  What'sUp Subset A&B, Left/Right   `Is the (object 1) to the left of or to the right of the (object 2)? Answer left or right.`
  What'sUp Subset A, On/Under       `Is the (object 1) on or under the (object 2)? Choose from the two options.`
  What'sUp Subset B, Front/Behind   `Is the (object 1) in front of or behind the (object 2)? Answer front or behind.`
  What'sUp Subset A (four-way)      `Is the (object1) to the left of, to the right of, on, or under the (object2)? Choose from the four options.`
  What'sUp Subset B (four-way)      `Is the (object1) to the left of, to the right of, in front of, or behind the (object2)? Answer front, behind, left, or right.`
  COCO/GQA-spatial, One obj.        `Is the (object 1) on the (left/right/top/bottom) or on the (right/left/bottom/top)? Give a short answer.`
  COCO-spatial, Two obj.            `Is the (object 1) (to the left of/to the right of/above/below) a (object 2) or (to the right of/to the left of/below/above) a (object 2)? Give a short answer.`
  GQA-spatial, Two obj.             `Is the (object 1) to the (left/right/front/behind) of a (object 2) or to the (right/left/behind/front) of a (object 2)? Give a short answer.`
:::
::::

:::: table*
::: center
  Subset                            Question
  --------------------------------- ------------------------------------------------------------------------------------
  What'sUp Subset A&B, Left/Right   `Is the (object 1) to the left of or to the right of the (object 2)?`
  What'sUp Subset A, On/Under       `Is the (object 1) at the bottom of the (object2) or at the top of the (object2)?`
  What'sUp Subset B, Front/Behind   `Is the (object 1) in the back of the (object2) or in the front of the (object2)?`
:::
::::

:::: table*
::: center
+:--------------------+:-------:+:-------:+:--------:+:--------:+:-------:+:-------:+:--------:+:--------:+
|                     | What'sUp Subset A                       | What'sUp Subset B                       |
+---------------------+-------------------+---------------------+-------------------+---------------------+
|                     | Left/Right        | On/Under            | Left/Right        | Front/Behind        |
+---------------------+---------+---------+----------+----------+---------+---------+----------+----------+
|                     | Indiv.  | Pairs   | Indiv.   | Pairs    | Indiv.  | Pairs   | Indiv.   | Pairs    |
+---------------------+---------+---------+----------+----------+---------+---------+----------+----------+
| CLIP-ViT-L/14-336px | 49.0    | 1.9     | 61.7     | 23.3     | 54.9    | 10.8    | 51.5     | 7.8      |
+---------------------+---------+---------+----------+----------+---------+---------+----------+----------+
| LLaVA-1.5-7B        | 99.0    | 98.1    | 80.1     | 60.2     | **100** | **100** | **98.5** | **97.1** |
+---------------------+---------+---------+----------+----------+---------+---------+----------+----------+
| Phi-3-V-3.8B        | **100** | **100** | **85.4** | **70.9** | **100** | **100** | 56.9     | 13.7     |
+---------------------+---------+---------+----------+----------+---------+---------+----------+----------+
| LLaMA-3-V-8B        | 90.3    | 80.6    | 57.8     | 20.4     | 71.1    | 46.1    | 69.1     | 41.2     |
+---------------------+---------+---------+----------+----------+---------+---------+----------+----------+
:::
::::

## Evaluating VLM2Vec {#appendix_eval_vlm2vec}

For evaluation, we use the same question template as for training ("`Represent the given image with the following question: {Question}`"). We list the questions used for VLM2Vec's evaluation in Table [\[tab:vlm2vec_prompts\]](#tab:vlm2vec_prompts){reference-type="ref" reference="tab:vlm2vec_prompts"}. Similar to response-based evaluation for Generative MLLMs, we notice variance when using different questions. Here, we adopt the questions that lead to the best performance on the benchmarks.

In addition, we show that the benefit of using a question in the prompt generalizes beyond What'sUp. Here, we perform the same comparison as in Table [\[tab:vlm2vec\]](#tab:vlm2vec){reference-type="ref" reference="tab:vlm2vec"} on MMVP and MMVP-VLM. We use the original question of the benchmark in the prompt. For MMVP-VLM which does not have questions, we manually add an MMVP-like question to each test case without altering content or tuning the prompt. We attached these questions to the updated supplementary material. We use the same prompt format as What'sUp ("`Represent the given image with the following question: {Question}`" or "`Represent the given image.`" without any question). We observe similar results in Table [\[tab:vlm2vec-mmvp\]](#tab:vlm2vec-mmvp){reference-type="ref" reference="tab:vlm2vec-mmvp"}.

:::: table*
::: center
                                 MMVP     MMVP-VLM
  --------------------------- ---------- ----------
  CLIP-ViT-L/14-336px            14.0       20.7
  LLaVA-1.5-7B-VLM2Vec-LoRA    **30.0**   **37.8**
  w/o Question in Prompt         9.3        11.9
  Random chance                  25.0       25.0
:::
::::

# Supplementary Experimental Details and Results

## Hyperparameters {#appendix_hyperparameters}

Our code for training standard CLIP, SigLIP, and EVA-CLIP is based on <https://github.com/mlfoundations/open_clip> [@ilharco_gabriel_2021_5143773]. We finetune these models for five epochs with a learning rate of 5e-6 on the combination of converted LCS-558K plus converted DataMix-665K. We use 50 steps of warmup and AdamW optimizer with a cosine-annealing learning rate schedule. The batch size is 512, and we train the models on 4 GPUs. The training time is less than one day.

For the `[CLS]`-LLaVA-1.5-7B-LoRA and reproduced LLaVA-1.5-7B-LoRA in Table [\[tab:ablation_cls_llava\]](#tab:ablation_cls_llava){reference-type="ref" reference="tab:ablation_cls_llava"}, we use the official LLaVA code in <https://github.com/haotian-liu/LLaVA> released under the Apache 2.0 license. The batch size, learning rate, and other training settings are the same as described in LLaVA-1.5 paper [@liu2024improved].

For the experiments in Table [\[tab:ablation_pacl\]](#tab:ablation_pacl){reference-type="ref" reference="tab:ablation_pacl"}, we start from an implementation of PACL [@mukhoti2023open] in <https://github.com/NMS05/Patch-Aligned-Contrastive-Learning>. Since we only need to train the vision embedder and text embedder, we apply a larger batch size (4096) and train for 10 epochs on 8 GPUs on the combination of converted LCS-558K plus converted DataMix-665K. We use 0.1 as the fixed temperature, 1e-4 as the learning rate, and Adam as the optimizer. The training time is less than one day for all experiments. We adopt the data augmentation implemented in the codebase, except the `RandomHorizontalFlip`, which discourages models from learning about Left/Right spatial relations. When processing the captions, we follow the technique used in the codebase, which uses the original caption randomly with a probability of 0.5, and template+(a noun in the caption) otherwise. The templates are: "`a picture of {}.`", "`itap of {}.`", "`a photograph of {}.`", "`this picture contains {}.`", "`a good photo of {}.`". For experiments with a stronger text encoder, we do not apply this technique on DataMix-665K.

For LLaVA-1.5-7B-VLM2Vec-LoRA in Table [\[tab:vlm2vec\]](#tab:vlm2vec){reference-type="ref" reference="tab:vlm2vec"}, we refer to the VLM2Vec code in <https://github.com/TIGER-AI-Lab/VLM2Vec>. We use rank=8 for LoRA, 256 for the batch size, 1024 for maximum input token length, and 0.02 for the temperature. We train the model for only 900 steps on 4 GPUs for 40 hours on the combination of LCS-558K and DataMix-665K, with a linear learning rate schedule, 100 warmup steps, and 2e-5 as the learning rate. Although we do not train the model on full data, the model performance is remarkable on What'sUp.

## Converting LLaVA-1.5's Training Data {#appendix_data}

We use LLaVA-1.5's training data for all finetuning experiments we include in the paper. The DataMix-665K is under CC BY 4.0 License, while the LCS-558K is under LAION/CC/SBU License for images and BSD 3-Clause \"New\" or \"Revised\" License for BLIP-generated captions. They are datasets of English conversations.

We check the frequency of appearance of the following keywords in DataMix-665K and LCS-558K: `on the left`, `on the right`, `to the left`, `to the right`, `at the left`, `at the right`. In DataMix-665K, there are 12957 instances with at least one of the key phrases, among which 12658 have a paired image. For captions (ground truth answers), this number is 13473 since an instance is paired with a multi-turn conversation. In LCS-558K, there are 560 such instances and captions since each instance has only one question and one answer.

In our experiments in Section [3.1](#ab1_data){reference-type="ref" reference="ab1_data"} and Section [3.2](#ab2_token){reference-type="ref" reference="ab2_token"}, LCS-558K was converted from image-text pair format to conversation format, so we revert this process by using ground truth answer as the caption. Since DataMix-665K is in a multi-turn conversation format, we randomly pick one answer as the caption in each epoch. In Section [3.3](#ab3_lm_choice){reference-type="ref" reference="ab3_lm_choice"}, the new text encoder can encode long paragraphs, so we use the concatenation of all answers in the multi-turn conversation as the ground truth caption. In practice, we calculate the text embeddings of all possible captions using the text encoder of LLM2CLIP before training to save memory and time. In Section [3.4](#ab4_vlm2vec){reference-type="ref" reference="ab4_vlm2vec"}, we randomly choose one turn from the multi-turn conversation.

## Results of Unlocking Image Encoder {#appendix_unlock}

We try unlocking the image encoder during finetuning on the SigLIP-ViT-L/16-384px model. The results are in Table [\[tab:unlock_data\]](#tab:unlock_data){reference-type="ref" reference="tab:unlock_data"}. Still, the individual accuracy remains low. []{#appendix_ablation label="appendix_ablation"}

:::: table*
::: center
+:----------------------+:---------:+:---------:+:---------:+:---------:+:--------:+:--------:+:--------:+:--------:+:-:+
|                       | What'sUp Subset A     | What'sUp Subset B     | COCO-spatial        | GQA-spatial         |   |
+-----------------------+-----------+-----------+-----------+-----------+----------+----------+----------+----------+---+
|                       | Indiv.    | Pairs     | Indiv.    | Pairs     | One-obj. | Two-obj. | One-obj. | Two-obj. |   |
+-----------------------+-----------+-----------+-----------+-----------+----------+----------+----------+----------+---+
| SigLIP-ViT-L/16-384px | 50.0      | 1.9       | 51.5      | 5.9       | 48.7     | 50.2     | 51.2     | 47.0     |   |
+-----------------------+-----------+-----------+-----------+-----------+----------+----------+----------+----------+---+
| \+ finetuning (ft)    | 50.5      | 2.9       | 51.5      | 5.9       | 48.7     | 57.7     | 50.5     | 48.1     |   |
+-----------------------+-----------+-----------+-----------+-----------+----------+----------+----------+----------+---+
| \+ ft + hard neg.     | 50.0      | 3.9       | 47.1      | 2.0       | 52.3     | 47.0     | 51.8     | 52.7     |   |
+-----------------------+-----------+-----------+-----------+-----------+----------+----------+----------+----------+---+
| Random chance         | 50.0      | 25.0      | 50.0      | 25.0      | 50.0     | 50.0     | 50.0     | 50.0     |   |
+-----------------------+-----------+-----------+-----------+-----------+----------+----------+----------+----------+---+
:::
::::

## Evaluating and Finetuning XVLM {#appendix_xvlm}

:::: table*
::: center
+:--------------+:----------:+:----------:+:--------:+:--------:+:--------:+:--------:+:-:+:-:+:-:+:-:+:-:+
|               | What'sUp A | What'sUp B | COCO-spatial        | GQA-spatial         |   |   |   |   |   |
+---------------+------------+------------+----------+----------+----------+----------+---+---+---+---+---+
|               |            |            | One-obj. | Two-obj. | One-obj. | Two-obj. |   |   |   |   |   |
+---------------+------------+------------+----------+----------+----------+----------+---+---+---+---+---+
| XVLM-16M      | **50.0**   | 32.8       | 65.4     | 64.6     | **63.2** | **53.3** |   |   |   |   |   |
+---------------+------------+------------+----------+----------+----------+----------+---+---+---+---+---+
| \+ finetuning | 46.4       | **34.6**   | **66.8** | **65.2** | 61.3     | 51.2     |   |   |   |   |   |
+---------------+------------+------------+----------+----------+----------+----------+---+---+---+---+---+
| Random chance | 25.0       | 25.0       | 50.0     | 50.0     | 50.0     | 50.0     |   |   |   |   |   |
+---------------+------------+------------+----------+----------+----------+----------+---+---+---+---+---+
:::
::::

Observing the similar failure of the data-informed attempt, previous work concluded that even with relevant, high-quality data and hard negatives, denser supervision is likely required to let the model learn the basic spatial relations [@kamath2023s], as in XVLM [@zeng2021multi], a VLM with supervision at the bounding-box level. We attach XVLM-16M's performance in Table [\[tab:xvlm_data\]](#tab:xvlm_data){reference-type="ref" reference="tab:xvlm_data"}. We find that Generative MLLMs still beat XVLM-16M, while they do not incorporate downstream task-related inductive bias or denser supervision.

We explore finetuning XVLM on LLaVA-1.5's training data based on their official code (<https://github.com/zengyan-97/X-VLM>), but no improvement is observed in the results (the last row in Table [\[tab:xvlm_data\]](#tab:xvlm_data){reference-type="ref" reference="tab:xvlm_data"}). The image encoder is locked during finetuning. We use both contrastive learning loss and image-text matching loss to finetune the XVLM-16M model for five epochs with a learning rate of 1e-5 and a weight decay rate of 0.01. We use 10% steps of warmup and AdamW optimizer with a lambda learning rate schedule. The batch size is 128, and we train the model on 4 GPUs. The evaluation is performed through the image-text matching score.

## Implementation Details of PACL and SPARC {#appendix_sparc}

For PACL, the vision embedder applied on CLIP-ViT-L/14-336px is the sum of a one-layer linear projection and a two-layer nonlinear projection with GELU as the activation function. The input of the vision embedder is 576 1024-dimensional patch tokens after `LayerNorm` and `Dropout` (with probability = 0.1), and the output dimension is 768 for 576 tokens. The text embedder accepts one 768-dimensional text token and applies one-layer linear projection on it. All output embeddings are L2-normalized. The embedders used in SPARC experiments share the same model structures as in PACL. For experiments with the text encoder from LLM2CLIP, the input dimension of the text embedder is 1280 instead of 768, with other settings unchanged.

For RoPE, we refer to the implementation in the codebase for LLaMA in <https://github.com/huggingface/transformers>. It is applied before `LayerNorm`. Compared with learned or sinusoidal position embeddings, it maintains the relative positions of tokens. We choose to use it since it is applied in language models of Generative MLLMs listed in Section [2](#sec:compare){reference-type="ref" reference="sec:compare"}.

For SPARC, we follow the pseudocode in Appendix C of @bica2024improving. Specifically, the output of the vision embedder is 576 768-dimensional projected patch tokens, and the output of the text embedder is 77 768-dimensional projected text tokens (with padding). After multiplying them, we get a similarity matrix of size $77\times 576$. Following the SPARC paper, we first apply min-max normalization to the matrix and then sparsify it by zeroing out all matrix entries below the threshold $1/576$. We normalize the rows of the similarity matrix, multiply it with the patch tokens, and obtain 77 grouped visual tokens. The global representation of the image is the mean of these grouped visual tokens after L2-normalization, and we get the global representation of the text similarly. During inference, we calculate the cosine similarity between the two global representations. When training, we use the global representations for the standard contrastive loss and apply a local contrastive loss to contrast the 77 grouped visual tokens and 77 text tokens within each sample. In this way, we align the patch tokens to individual concepts represented by text tokens. Unlike the original implementation, we do not use a learnable temperature for contrastive losses.

## When Generative MLLMs are worse than CLIP

We also observe that in some cases, MLLMs have worse performance than CLIP (See Table [\[tab:failure\]](#tab:failure){reference-type="ref" reference="tab:failure"}). On EqBen-mini [@wang2023equivariant], their performance is close. On COCOCounterfactuals [@le2024coco], we notice that CLIP embeddings are involved in the construction of the benchmark as a metric, which could affect the comparison.

This phenomenon is also discussed in previous literature [@zhang2024visually; @geigle2024african], where they find that training on web-crawled data teaches CLIP many rare concepts, while Generative MLLMs are not sufficiently exposed to such data for image-text alignment.

:::: table*
::: center
                         EqBen-mini   COCOCounterfactuals                 
  --------------------- ------------ --------------------- -- -- -- -- -- --
  CLIP-ViT-L/14-336px     **40.0**         **87.7**                       
  LLaVA-1.5-7B              32.9             57.9                         
:::
::::

## Alternative Decoding for Generative MLLMs {#appendix_m3id}

:::: table*
::: center
                                            Indiv.     Pairs    
  --------------------------------------- ---------- ---------- --
  LLaVA-1.5-7B                               61.7       25.3    
  \+ M3ID [@favero2024multi]               **64.3**   **31.3**  
  LLaVA-1.5-13B + I-MoF [@tong2024eyes]       --      **31.3**  
  Random chance                              50.0       25.0    
:::
::::

We apply Multi-Modal Mutual-Information Decoding (M3ID) [@favero2024multi] on LLaVA-1.5 for response-based evaluation. For token in each decoding step $t$, M3ID computes the output probability with the image and without any input image, denoted as $\mathbf{l_c}$ and $\mathbf{l_u}$ respectively. The latter corresponds to the language priors of the answer to the given question. Then a correction term ($\mathbf{l_c}-\mathbf{l_u}$) is added to $\mathbf{l_c}$ with weight $\frac{1-\exp(-\lambda t)}{\exp(-\lambda t)}$ if the model is not highly confident with the token in step $t$ ($\max_k (l_c)_k < \log \alpha$ where $\alpha, \lambda$ are pre-defined hyperparameters). This correction prevents the VLM from omitting the visual input and relying on the language priors.

In Table [\[tab:decoding\]](#tab:decoding){reference-type="ref" reference="tab:decoding"}, this method achieves gain (+6%) relative to the baseline LLaVA-1.5-7B on MMVP. We note that this is on par with I-MoF with interleaved CLIP and DINO features) [@tong2024eyes]. This result suggests that LLaVA-1.5 did not attend to the visual input enough and thus might miss the key information for answering the query. A similar finding was described through the interpretability perspective on attention weights in @stan2024lvlm.

[^1]: The original LLM2CLIP is not a fair comparison as its implementation unfreezes the CLIP vision encoder during finetuning.
