maketitle thanks aketitle

**Abstract**

> In self-supervised contrastive learning, a widely-adopted objective function is InfoNCE, which uses the heuristic cosine similarity for the representation comparison, and is closely related to maximizing the Kullback--Leibler (KL)-based mutual information. In this paper, we aim at answering two intriguing questions: (1) Can we go beyond the KL-based objective? (2) Besides the popular cosine similarity, can we design a better similarity function? We provide answers to both questions by generalizing the KL-based mutual information to the $f$-**M**utual **I**nformation in **C**ontrastive **L**earning ($f$-MICL) using the $f$-divergences. To answer the first question, we provide a wide range of $f$-MICL objectives which share the nice properties of InfoNCE (e.g., alignment and uniformity), and meanwhile result in similar or even superior performance. For the second question, assuming that the joint feature distribution is proportional to the Gaussian kernel, we derive an **$f$-Gaussian similarity** with better interpretability and empirical performance. Finally, we identify close relationships between the $f$-MICL objective and several popular InfoNCE-based objectives. Using benchmark tasks from both vision and natural language, we empirically evaluate $f$-MICL with different $f$-divergences on various architectures (SimCLR, MoCo, and MoCo v3) and datasets. We observe that $f$-MICL generally outperforms the benchmarks and the best-performing $f$-divergence is task and dataset dependent.

startsection section1@-2.0ex plus -0.5ex minus -.2ex1.5ex plus 0.3ex minus0.2ex

Introduction Recent advances in self-supervised learning aim at learning similar representations from different augmented views of the same data sample. However, naively implementing this idea would easily make representations converge to some trivial constant (i.e., feature collapse) in practice. To address this problem, researchers propose new algorithms either from the *model architecture* perspective or the *training objective* perspective. The former method (e.g., [@byol; @siamsiam; @zhang2021zero]) applies techniques such as stop gradient or predictor module to create asymmetry in networks, while the latter method encourages the contrastiveness between similar (positive) and dissimilar (negative) sample pairs through the objective design.

In this paper, we intend to deepen our understanding of contrastive learning by generalizing the current objective design. To achieve self-supervised contrastive learning, existing objectives are proposed from different perspectives such as the mutual information (e.g., InfoNCE [@wu2018unsupervised; @oord2018representation; @ChenKNH20; @henaff2019data; @he2020momentum] ), the information redundancy (e.g., Barlow Twins [@barlow]), and the regularization (e.g., VICReg [@vicreg]). In particular, the InfoNCE objective is widely used, which aims to maximize the probability of picking a similar sample pair among a batch of sample pairs, and can be interpreted as a lower bound of the mutual information (MI) between two views of samples [@oord2018representation; @bachman2019learning; @tian2020contrastive; @TschannenDRGL20]. This is consistent with the well-known "InfoMax principle\" [@linsker1988self]. To measure the similarity between sample pairs, cosine similarity is usually adopted.

To attain the aforementioned goals of better understanding and generalizing contrastive learning, we here focus on the widely-adopted InfoNCE objective, and aim at two questions regarding it:

*(1) MI is essentially the Kullback--Leibler (KL) divergence between the joint distribution and the product of the marginal distributions. Is this KL-based objective optimal? If not, can we go beyond the KL-based objective?*

*(2) Besides the commonly used cosine similarity for measuring the distance between samples, can we provide a better similarity function with a theoretical basis?*

To answer the above two questions, we generalize the KL-based mutual information to the broader $f$-divergence family [@ali1966general; @csiszar1967information], and propose the benchmark of $f$-mutual information in contrastive learning ($f$-MICL). By searching through a wide range of $f$-divergences, we observe that the KL divergence is not always the best, and several other $f$-divergences in fact show similar or even superior performance in practice.

For the second question, while it is challenging to provide an answer based on the InfoNCE objective, it is possible to derive a proper similarity function under the $f$-MICL framework. By assuming that the joint feature distribution is proportional to the popularly-adopted Gaussian kernel, we propose a novel *$f$-Gaussian similarity* function that enjoys better empirical performance.

Finally, we show the generalization of $f$-MICL by drawing connections between the $f$-MICL objective and several popular InfoNCE-based objectives (e.g., SimCLR[@ChenKNH20], MoCo[@he2020momentum], and Alignment and Uniformity (AU) [@wang2020understanding]). We identify that those objectives are closely related to $f$-MICL: Alignment and Uniformity (AU) [@wang2020understanding] can be treated as a special case, and SimCLR [@ChenKNH20] and MoCo [@he2020momentum] are upper bounds for a transformed $f$-MICL. These results provide a different angle to better understand InfoNCE. Moreover, we show both theoretically and empirically that nice properties of InfoNCE (e.g., alignment and uniformity [@wang2020understanding]) can be naturally extended to $f$-MICL.

We summarize our main contributions as follows:

- Motivated by InfoNCE, we propose a general framework for contrastive learning by extending the MI to the general $f$-MI, which provides a wide range of objective choices.

- Instead of using heuristic similarity functions, we provide a novel similarity function, called *$f$-Gaussian similarity*, based on the convex conjugate and an assumption on the joint feature distribution.

- We identify close relationships between our $f$-MICL objective and several InfoNCE-based contrastive learning objectives.

- Empirically, we show that $f$-MICL achieves notable improvement over benchmarks on various datasets, and the best-performing $f$-divergence depends on the specific task and dataset. In addition, our proposed $f$-Gaussian similarity consistently outperforms the cosine similarity.

startsection section1@-2.0ex plus -0.5ex minus -.2ex1.5ex plus 0.3ex minus0.2ex

$f$-Mutual Information[]{#sec:prem label="sec:prem"}

To provide answers to the above two questions regarding the InfoNCE objective, we first extend the KL-based mutual information to the more general $f$-mutual information. The definition of the $f$-mutual information ($f$-MI) is as follows:

::: {#def:fMI .definition}
**Definition 1** (**$f$-mutual information, @csiszar1967information**). *Consider a pair of random variables $(X, Y)$ with joint density function $p(x,y)$ and marginal densities $p(x)$ and $p(y)$. The $f$-mutual information $I_f$ between $X$ and $Y$ is defined as $$\begin{align}
\begin{split}
&I_f(X;Y) := %D_{f}\left(p(x,y) \|p(x) p(y) \right) 
\int f\left(\frac{p(x, y)}{p(x)p(y)} \right) p(x)p(y) \cdot \mathrm{d}\lambda(x,y),
\end{split}
\end{align}$$ where $f : \mathbb{R}_+ \to \mathbb{R}$ is (closed) convex with $f(1) = 0$, and $\lambda$ is a dominating measure (e.g., Lebesgue).*
:::

Note that the $f$-MI is essentially the $f$-divergence between the joint distribution and the product of marginal distributions. It is well-known that the $f$-MI is non-negative and symmetric. Moreover, provided that $f$ is strictly convex, $I_f(X; Y) = 0$ iff $X$ and $Y$ are independent [@ali1966general].

Since it is challenging to provide an accurate estimation of the $f$-divergences in high dimensions, [@nguyen2010estimating] used the convex conjugate as a lower bound for the $f$-divergences. With this result we can lower bound $I_f(X;Y)$ as follows: $$\begin{align}
\label{eq:dual_form_fMI}
%\begin{split}
\textcolor{black}{I_f(X;Y) \geq} \sup_{s\in \mathcal{F}} \Big(\underset{(X, Y) \sim p(x,y)}{\mathbb{E}} s(X, Y) - \underset{(X, Y)\sim p(x)p(y)}{\mathbb{E}} f^*\circ s(X, Y) \Big),
%\end{split}
\end{align}$$ where $p(x,y)$ denotes the joint density, $p(x)p(y)$ stands for the product of marginal densities, and the symbol $\circ$ denotes function composition. The function $f^*(t) := \sup_{x\in \mathbb{R}_+}(x t - f(x))$ is known as the convex conjugate[^1] of $f$ and is *monotonically increasing*, and $s(\cdot)$ belongs to $\mathcal{F}$, a class of functions on $(x, y)$ that we can parameterize. Using results in [@nguyen2010estimating], one can show that [\[eq:dual_form_fMI\]](#eq:dual_form_fMI){reference-type="ref+label" reference="eq:dual_form_fMI"} is equal to $I_f(X;Y)$ if there exists $s_\star \in \mathcal{F}$ such that for any $(x, y) \in \textrm{supp}(p(x)) \times \textrm{supp}(p(y))$, where $\textrm{supp}(\cdot)$ denotes the support of a distribution, we have: $$\begin{align}
\label{eq:condition_T}
s_\star(x,y) = f'\left(\frac{p(x, y)}{p(x)p(y)}\right).
\end{align}$$ [In other words, plugging the optimal $s_\star(x,y)$ into [\[eq:dual_form_fMI\]](#eq:dual_form_fMI){reference-type="ref+label" reference="eq:dual_form_fMI"} we obtain equality.]{style="color: black"} In [\[tbl:choices_f_div\]](#tbl:choices_f_div){reference-type="ref+Label" reference="tbl:choices_f_div"} we list common choices of $f$-divergences, their conjugates, and the derivatives. We also include the composition $f^* \circ f'$ for later purposes (see Theorem [\[thm:uniformity\]](#thm:uniformity){reference-type="ref" reference="thm:uniformity"} below).

:::: table*
::: center
  **Divergence**     $f(u)$                           $f^*(t)$                                                     $f'(u)$                                      $f^*\circ f'(u)$        
  ------------------ -------------------------------- ------------------------------------------------------------ -------------------------------------------- ----------------------- --
  KL                 $u \log u$                       $\exp(t-1)$                                                  $\log u + 1$                                 $u$                     
  JS                 $\varphi(u)$                     $- \log(2-e^t)$                                              $\log 2 + \log \frac{u}{1+u}$                $-\log \frac{2}{1+u}$   
  Pearson $\chi^2$   $(u-1)^2$                        $t^2/4 + t$                                                  $2(u-1)$                                     $u^2 - 1$               
  SH                 $(\sqrt{u} - 1)^2$               $\frac{t}{1 - t}$                                            $1 - u^{-1/2}$                               $u^{1/2} - 1$           
  Tsallis-$\alpha$   $\tfrac{u^\alpha}{\alpha - 1}$   $(\tfrac{\alpha - 1}{\alpha}t)^{\tfrac{\alpha}{\alpha-1}}$   $\frac{\alpha}{\alpha - 1} u^{\alpha - 1}$   $u^\alpha$              
  VLC                $\frac{(u-1)^2}{u + 1}$          $4 - t - 4\sqrt{1-t}$                                        $1 - \frac{4}{(u+1)^2}$                      $3 - \frac{4}{u+1}$     
:::
::::

startsection section1@-2.0ex plus -0.5ex minus -.2ex1.5ex plus 0.3ex minus0.2ex

$f$-MICL[]{#sec:design label="sec:design"}

With the introduction of the more general $f$-MI we now proceed to the design of the objective and similarity function. Then we will analyze the property of the proposed framework and compare it with some existing InfoNCE-based benchmarks.

startsectionsubsection2@-1.8ex plus -0.5ex minus -.2ex0.8ex plus .2ex

$f$-MICL objective Contrastive learning is a popular *self-supervised* method for representation learning. In contrastive learning, we expect similar sample pairs to be close to each other in the embedding space, while dissimilar pairs to be far apart. Based on the $f$-MI introduced in § [\[sec:prem\]](#sec:prem){reference-type="ref" reference="sec:prem"}, we propose a general framework for contrastive learning, coined as $f$-MICL.

We denote $g: \mathcal{X}\to \mathbb{S}^{d-1}$ as the feature encoder (usually constructed by a neural network) from the input space $\mathcal{X}$ to the hypersphere, and we use the shorthands $x^g := g(x)$ and $y^g := g(y)$ to represent the feature embeddings. The notation $p_{d}$ stands for the data distribution, and $p_\times:= p_d\otimes p_d$ means its self product ([product of marginals, e.g., pairs of images]{style="color: black"}). We denote $p_+$ as the distribution of *positive pairs*, *i.e.*, two samples with similar feature embeddings ([joint distribution, e.g., the same image with different data augmentation]{style="color: black"}). Using the lower bound of the $f$-MI in [\[eq:dual_form_fMI\]](#eq:dual_form_fMI){reference-type="ref+label" reference="eq:dual_form_fMI"}, we have the general $f$-MICL objective as follows: $$\begin{align}
\label{eq:objective}
%\begin{split}
%\sup_{g\in \Gc, k\in \Kc} i_f(X;Y) &:= 
\textcolor{black}{\sup_{s\in \mathcal{F}}}~~
\mathbb{E}_{(x, y) \sim p_+} s(x^g,y^g) - \mathbb{E}_{(x, y)\sim p_\times} f^*\circ s(x^g,y^g),  
%\end{split}
\end{align}$$ where $s(x^g,y^g)$ can be understood as the similarity measurement between two feature embeddings in the context of contrastive learning. Essentially, we are studying the variational lower bound [\[eq:dual_form_fMI\]](#eq:dual_form_fMI){reference-type="ref+label" reference="eq:dual_form_fMI"} in the *feature space*, with the feature embeddings learnable. We can treat the first term as the similarity score between *positive pairs* with similar feature embeddings, and the second term as the similarity score between two random samples, a.k.a. *negative pairs*. As $f^*$ is an increasing function, maximizing the $f$-MI is equivalent to simultaneously maximizing the similarity between positive pairs and minimizing the similarity between negative pairs.

With [\[eq:objective\]](#eq:objective){reference-type="ref+label" reference="eq:objective"} we have answered the first question: there are a spectrum of $f$-MICL objectives that can be applied in contrastive learning by using different $f$ functions. We will discuss how to choose the best $f$ empirically in §[\[sec:exp\]](#sec:exp){reference-type="ref" reference="sec:exp"}.

startsectionsubsection2@-1.8ex plus -0.5ex minus -.2ex0.8ex plus .2ex

$f$-Gaussian similarity[]{#subsec:sim label="subsec:sim"}

![Experiment for verifying Assumption [2](#assmp:joint_vMF){reference-type="ref" reference="assmp:joint_vMF"}. Here we draw the relation between the squared distances $\|x^g - y^g\|^2$ and the averaged log likelihood $\log p_g$, with $\log p_g$ estimated by the flow model RealNVP [@dinh2016density]. [(**left**) Gaussian prior; **(right)** Uniform prior.]{style="color: black"} The features are learned by SimCLR trained on CIFAR-10. See more details in Appendix [\[sec:add_exp\]](#sec:add_exp){reference-type="ref" reference="sec:add_exp"}. ](images/gaussian_new.png){#fig:gaussian width="80%"}

Previous works in constrastive learning usually adopt some heuristic similarity function such as the cosine similarity function $s(x^g,y^g) = x^g \cdot y^g$. Although it shows promising performance in practice, our second question is that, can we provide a better similarity function than the popular cosine similarity?

Note that [\[eq:condition_T\]](#eq:condition_T){reference-type="ref+label" reference="eq:condition_T"} is a natural choice of $s(\cdot)$ from the perspective of deriving the $f$-MI. In the context of contrastive learning, by denoting the density functions of the marginal feature distributions as $p_g(x^g)$ and $p_g(y^g)$, and the density of the joint feature distribution as $p_g(x^g, y^g)$, from [\[eq:condition_T\]](#eq:condition_T){reference-type="ref+label" reference="eq:condition_T"} we have an optimal similarity function as follows:

::: restatable
lemoptimalsim[]{#lem:optimal_sim label="lem:optimal_sim"} Suppose $f$ is differentiable, and the embedding function $g$ is fixed. The following similarity function $s_\star$ maximizes [\[eq:objective\]](#eq:objective){reference-type="ref+label" reference="eq:objective"}: $$\begin{align}
\label{eq:optimal_similarity1}
s_\star(x^g, y^g) = f'\left( \frac{p_g(x^g, y^g)}{p_g(x^g)p_g(y^g)} \right).
\end{align}$$
:::

Obviously, the optimal $s_\star$ in fact induces the $f$-MI on the feature space, which is a low bound of the original $f$-MI. Although [\[eq:optimal_similarity1\]](#eq:optimal_similarity1){reference-type="ref+label" reference="eq:optimal_similarity1"} provides an optimal similarity function, it nevertheless depends on the unknown density functions. How can we implement [\[eq:optimal_similarity1\]](#eq:optimal_similarity1){reference-type="ref+label" reference="eq:optimal_similarity1"} in practice? Among various known density functions, it is natural to choose a typical kernel function for structured data for validation [@balcan2008theory; @powell87; @murphy2012machine], e.g., the Gaussian kernel.

::: {#assmp:joint_vMF .assump}
**Assumption 2** (**Gaussian kernel**). *The joint feature density (wrt the uniform distribution over the hypersphere $\mathbb{S}^{d-1}$) is proportional to a Gaussian kernel, namely $$\begin{align}
%\label{eq:joint_vMF}
p_g(x^g, y^g) \propto G_\sigma(\|x^g - y^g\|^2) = \mu \, \exp\left(-\tfrac{\|x^g - y^g\|^2}{2\sigma^2}\right),\nonumber
\end{align}$$ where $\mu := \exp(\tfrac{1}{\sigma^2}) \tfrac{C}{c^2}$ is a constant that we determine below.*
:::

Since $x^g, y^g \in \mathbb{S}^{d-1}$ have unit (Euclidean) norm, we have $$\begin{align}
\label{eq:vM-F}
    p_g(x^g, y^g) \propto \exp( \tfrac{x^g \cdot y^g}{\sigma^2} ),
\end{align}$$ which belongs to the von Mises-Fisher bivariate distribution [@Mardia75 Eq. 2.11]. It is clear that the marginals of $p_g$ are uniform. Indeed, for any orthogonal matrix $Q$, with $\tfrac{1}{C} := \mathbb{E}_{(x^g,y^g) \sim \mathbb{S}^{d-1}\times \mathbb{S}^{d-1}} \exp(\tfrac{x^g \cdot y^g}{\sigma^2} )$ we have $$\begin{align}
    p_g(Qx^g) = \mathbb{E}_{ y^g \sim \mathbb{S}^{d-1}} C \exp(\tfrac{Qx^g \cdot y^g}{\sigma^2} )= \mathbb{E}_{ y^g \sim \mathbb{S}^{d-1}} C \exp(\tfrac{x^g \cdot Q^\top y^g}{\sigma^2} ) = p_g(x^g) =: c,
\end{align}$$ where we have used the fact that the only invariant distribution on $\mathbb{S}^{d-1}$ wrt the orthogonal group is the uniform distribution. Similarly, we have $p_g(y^g) \equiv c$ (where $c$ is the reciprocal of the surface area of the hypersphere $\mathbb{S}^{d-1}$). The distribution ([\[eq:vM-F\]](#eq:vM-F){reference-type="ref" reference="eq:vM-F"}) has a nice interpretation[^2] in terms of maximum entropy [@Mardia75], and admits the factorization $$\begin{align}
    p_g(x^g, y^g) = p_g(x^g) \cdot p_g(y^g | x^g) = p_g(y^g) \cdot p_g(x^g | y^g),
\end{align}$$ where the marginals $p_g(x^g)$ and $p_g(y^g)$ are uniform while the conditionals $p_g(y^g|x^g)$ and $p_g(x^g | y^g)$ again belong to the von Mises-Fisher distribution.

[Combining [\[eq:optimal_similarity1\]](#eq:optimal_similarity1){reference-type="ref+label" reference="eq:optimal_similarity1"} and Assumption [2](#assmp:joint_vMF){reference-type="ref" reference="assmp:joint_vMF"}]{style="color: black"}, we can write the similarity function with the Gaussian kernel as follows: $$\begin{align}
\label{eq:optimal_similarity}
\textstyle
s_f(x^g, y^g) = f' \circ G_\sigma(\|x^g - y^g\|^2).
\end{align}$$ [As noted above, the product of marginals $p_g(x^g)p_g(y^g)$ is a constant, which has been absorbed into our definition of $G_\sigma$, see $\mu$ in Assumption [2](#assmp:joint_vMF){reference-type="ref" reference="assmp:joint_vMF"}.]{style="color: black"} We observe that $s_f(\cdot)$ depends on the choice of $f$ as well, thus we call it **$f$-Gaussian similarity**. As a result, we have provided a new way to design the similarity function, again from the $f$-MI perspective.

startsectionparagraph4@1.5ex plus 0.5ex minus .2ex-1emVerifying Assumption [2](#assmp:joint_vMF){reference-type="ref" reference="assmp:joint_vMF"}: One may question that Assumption [2](#assmp:joint_vMF){reference-type="ref" reference="assmp:joint_vMF"} can be too strong for practical usage. For example, replacing the Gaussian kernel $G_\sigma$ with any other decreasing function would also provide a valid assumption. However, we found that among several popular choices only the Gaussian kernel works well in practice. Also, we can empirically verify that Assumption [2](#assmp:joint_vMF){reference-type="ref" reference="assmp:joint_vMF"} approximately holds. To this end, it is sufficient to check whether the log density, *i.e.*, $\log p_g(x^g, y^g)$, is linear with the distance between each positive pair, *i.e.*, $\|x^g - y^g\|^2$. In [1](#fig:gaussian){reference-type="ref+Label" reference="fig:gaussian"}, we use the flow-based model RealNVP [@dinh2016density][^3] to estimate the log density [with a Gaussian prior and a uniform prior]{style="color: black"}, and learn the feature encoder $g$ from SimCLR [@ChenKNH20]. We observe that the linear relationship approximately holds for CIFAR-10 [^4].

We will empirically compare our $f$-Gaussian similarity with the cosine similarity in §[\[sec:exp\]](#sec:exp){reference-type="ref" reference="sec:exp"}.

startsectionsubsection2@-1.8ex plus -0.5ex minus -.2ex0.8ex plus .2ex

Implementation []{#subsec:imp label="subsec:imp"} With our designed $f$-Gaussian similarity $s_f$ we now have an implementable $f$-MICL objective in [\[eq:objective\]](#eq:objective){reference-type="ref+label" reference="eq:objective"}. Bringing the $f$-Gaussian Similarity $s_f$ in [\[eq:optimal_similarity\]](#eq:optimal_similarity){reference-type="ref+label" reference="eq:optimal_similarity"} into our objective [\[eq:objective\]](#eq:objective){reference-type="ref+label" reference="eq:objective"} we have a specific $f$-MICL objective: $$\begin{align}
\label{eq:objective_gaussian}
\underset{(x, y) \sim p_+}{\mathbb{E}} s_f(x^g, y^g) - \underset{(x, y)\sim p_\times}{\mathbb{E}} f^* \circ s_f(x^g, y^g).
\end{align}$$ Given a batch of $N$ samples, its empirical estimation is as follows: $$\begin{align}
\label{eq:objective_sample}
%\widehat{i}_f(X;Y) = 
%\textstyle
\frac{1}{N}\overset{N}{\underset{i=1}{\sum}} s_f(x_i^g, y_i^g) - \frac{1}{N(N -1)}\underset{i\neq j}{\sum} f^* \circ s_f(x_i^g, x_j^g),
\end{align}$$ where $x_i$ and $y_i$ are two types of data augmentation of the $i$-th sample, and $x_i$ and $x_j$ are different samples with [independently sampled]{style="color: black"} data augmentations.

With the $f$-MICL objective in [\[eq:objective_sample\]](#eq:objective_sample){reference-type="ref+label" reference="eq:objective_sample"} we propose our algorithm for contrastive learning in Algorithm [\[alg:f-MICL\]](#alg:f-MICL){reference-type="ref" reference="alg:f-MICL"}. To balance the two terms in our objective, we additionally include a weighting parameter $\alpha$ in front of the second term (which also absorbs the parameter $\mu$ in $G_\sigma$). This change can still be incorporated within our $f$-MICL framework, as we show in [\[sec:weighting\]](#sec:weighting){reference-type="ref+Label" reference="sec:weighting"}. Figure [2](#fig:f-MICL-intro){reference-type="ref" reference="fig:f-MICL-intro"} gives a high-level summary of our $f$-MICL framework. Given a batch of samples (*e.g.*, images), we generate *positive pairs* via data augmentation and *negative pairs* using other augmented samples in the same batch. This sampling method follows SimCLR [@ChenKNH20].

::: algorithm
:::

![Network architecture of $f$-MICL. $\mathtt{image}_i$: the $i^{\rm th}$ image in the current batch; $f$: the function used in the $f$-mutual information (§[\[sec:prem\]](#sec:prem){reference-type="ref" reference="sec:prem"}); $g$: feature embedding; $t$, $t_1$, $t_2$: augmentation functions drawn from the same family $\mathcal{T}$ of augmentations; $f'$: the derivative; $f^*$: the Fenchel conjugate. The symbol $\circ$ denotes the function composition. The sum of the two terms gives the variational lower bound of $f$-mutual information. [$x_i$ and $y_i$ are two types of data augmentation of the $i$-th sample, and $x_i$ and $x_j$ are different samples with [independently sampled]{style="color: black"} data augmentations.]{style="color: black"} [`max` stands for maximization.]{style="color: black"} See [\[eq:objective_sample\]](#eq:objective_sample){reference-type="ref+label" reference="eq:objective_sample"} for more details.](images/f-MICL.png){#fig:f-MICL-intro width="90%"}

startsectionsubsection2@-1.8ex plus -0.5ex minus -.2ex0.8ex plus .2ex

$f$-MICL family []{#sec:infonce label="sec:infonce"} In this section, we will deepen the understanding of $f$-MICL by drawing connections with some popular constrastive learning methods.

**Connection with InfoNCE:** Firstly, we show that InfoNCE is an upper bound of $f$-MICL. Recall our $f$-MICL objective in [\[eq:objective\]](#eq:objective){reference-type="ref+label" reference="eq:objective"}, and the popular InfoNCE objective $\mathcal{L}_\text{InfoNCE}$ as follows (here we take the maximization) [@oord2018representation]: $$\begin{align}
    \mathbb{E}_{(x, y) \sim p_+} s(x^g,y^g) - \mathbb{E}_{ x\sim p_d} \log \mathbb{E}_{ y\sim p_d} \exp (s(x^g,y^g)).
\end{align}$$ Consider that we perform a Donsker-Varadhan (DV) shift transformation $v$ [@dv; @tsai2021self] from [\[eq:objective\]](#eq:objective){reference-type="ref+label" reference="eq:objective"} such that by taking the maximum over the transformation we have: $$\begin{align}
    &\sup_{v\in \mathbb{R}} \Big( \mathbb{E}_{(x, y) \sim p_+} s(x^g,y^g) - v   - \mathbb{E}_{(x, y)\sim p_\times} f^*\circ (s(x^g,y^g)-v)   \Big).
    \label{eq:dv}
\end{align}$$ In practice, such a shift transformation can be approximated by a scaling factor ($\alpha$ in Algorithm [\[alg:f-MICL\]](#alg:f-MICL){reference-type="ref" reference="alg:f-MICL"}) such that [\[eq:objective\]](#eq:objective){reference-type="ref+label" reference="eq:objective"} and [\[eq:dv\]](#eq:dv){reference-type="ref+label" reference="eq:dv"} are equivalent. Given that $f$ is the KL divergence, thus $f(u) = u \log u$ and $f^*(t) = \exp(t-1)$ from Table [\[tbl:choices_f_div\]](#tbl:choices_f_div){reference-type="ref" reference="tbl:choices_f_div"}, the maximizer of $v$ in [\[eq:dv\]](#eq:dv){reference-type="ref+label" reference="eq:dv"} occurs at $v_\star = \log (\mathbb{E}_{(x, y)\sim p_\times} s(x^g,y^g))-1$). With $v_\star$, [\[eq:dv\]](#eq:dv){reference-type="ref+label" reference="eq:dv"} can be written as follows: $$\begin{align}
    \mathbb{E}_{(x, y) \sim p_+} s(x^g,y^g) - \log \mathbb{E}_{(x, y)\sim p_\times} \exp (s(x^g,y^g)).
    \label{eq:infonce}
\end{align}$$ According to Jensen's inequality we have $$\begin{align}
    &\mathbb{E}_{ x\sim p(\cdot)} \log \mathbb{E}_{ y\sim p(\cdot)} \exp (s(x^g,y^g)) \leq \log \mathbb{E}_{(x, y)\sim p_\times} \exp (s(x^g,y^g)).
\end{align}$$ Therefore, $$\begin{align}
    &\mathbb{E}_{(x, y) \sim p_+} s(x^g,y^g) - \log \mathbb{E}_{(x, y)\sim p_\times} \exp (s(x^g,y^g))
    \leq \mathcal{L}_{\text{InfoNCE}}.
\end{align}$$

<figure id="fig:relation">
<div class="center">
<img src="images/relation.png" style="width:40.0%" />
</div>
<figcaption><span class="math inline"><em>f</em></span>-MICL generalizes InfoNCE-based objectives.</figcaption>
</figure>

The above transformation shows that the InfoNCE loss is an upper bound of the $f$-MICL objective. In other words, maximizing $f$-MICL can potentially increase the InfoNCE objective.

**Connection with Alignment and Uniformity (AU) [@wang2020understanding]:** We further show that the Alignment and Uniformity (AU) loss is a special cases of $f$-MICL.

[@wang2020understanding] shows that InfoNCE approximately aligns positive feature embeddings while encouraging uniformly distributed negative ones. [@wang2020understanding] further proposes a new objective which quantifies such properties. Here we show that this new objective is essentially a subclass of the InfoNCE loss under the $f$-MICL framework. Concretely, applying the $f$-Gaussian similarity function for the KL divergence, we have $f'(u) = \log u +1$ from Table [\[tbl:choices_f_div\]](#tbl:choices_f_div){reference-type="ref" reference="tbl:choices_f_div"} and thus $s_f (x^g, y^g) = -\|x^g - y^g\|^2$. Using $s_f (x^g, y^g)$ in [\[eq:infonce\]](#eq:infonce){reference-type="ref+label" reference="eq:infonce"} we can recover the AU objective: $$\begin{align}
    -\mathbb{E}_{(x, y) \sim p_+} \|x^g - y^g\|^2 - \log \mathbb{E}_{(x, y)\sim p_\times} \left[ \exp (-\|x^g - y^g\|^2) \right].
\end{align}$$ Note that for KL, this is equivalent to the cosine similarity with a scaling factor: $-\|x^g - y^g\|^2=2x^g \cdot y^g-2$.

**Connection with the Spectral Contrastive Loss:** Here we show that $f$-MICL objective is closely related to the Spectral Contrastive Loss [@haochen2021provable]. Recall our objective: $$\begin{align}
\label{eq:objective1}
\mathbb{E}_{(x, y) \sim p_+} s(x^g,y^g) - \mathbb{E}_{(x, y)\sim p_\times} f^*\circ s(x^g,y^g),
\end{align}$$ where $s_f(x^g, y^g) = f' \circ G_\sigma(\|x^g - y^g\|^2)$. If we choose the Pearson $\chi^2$ divergence, where $f(u) = (u-1)^2, f'(u)=2(u-1), f^*\circ f'(u) = u^2-1$, we have our $\chi^2$-MICL objective: $$\begin{align}
\label{eq:chi}
&2\mathbb{E}_{(x, y) \sim p_+} G_\sigma(\|x^g - y^g\|^2) -\mathbb{E}_{(x, y)\sim p_\times} G_\sigma(\|x^g - y^g\|^2)^2 - 3.
\end{align}$$ This recovers the spectral contrastive loss exactly if we choose the proper hyperparameter and apply the cosine similarity instead. Thus we generalize the spectral contrastive loss as a special case of $\chi^2$-MICL.

**More on AU:** Finally, based on our objective in [\[eq:objective_gaussian\]](#eq:objective_gaussian){reference-type="ref+label" reference="eq:objective_gaussian"} we will show that the alignment and uniformity (AU) property of InfoNCE also extends to the general $f$-MICL family:

\(1\) Alignment: In the ideal case, maximizing the first term of [\[eq:objective_gaussian\]](#eq:objective_gaussian){reference-type="ref+label" reference="eq:objective_gaussian"} would yield $x^g = y^g$ for all $(x, y) \sim p_+$, *i.e.*, similar sample pairs should have aligned representations. Note that the derivative $f'$ is increasing since $f$ is convex. (2) Uniformity: We demonstrate the uniformity property by minimizing the second term of [\[eq:objective_gaussian\]](#eq:objective_gaussian){reference-type="ref+label" reference="eq:objective_gaussian"}, or more rigorously and realistically, its empirical version in [\[eq:objective_sample\]](#eq:objective_sample){reference-type="ref+label" reference="eq:objective_sample"}.

::: restatable
thmuniformDis[]{#thm:uniformity label="thm:uniformity"} Suppose that the batch size $N$ satisfies $2\leq N \leq d + 1$, with $d$ the dimension of the feature space. If the real function $$\begin{align}
\label{eq:f-div_property}
h(t) = f^* \circ f'\circ G_\sigma(t) \mbox{ is strictly convex on }[0, 4],
\end{align}$$ then all minimizers of the second term of [\[eq:objective_sample\]](#eq:objective_sample){reference-type="ref+label" reference="eq:objective_sample"}, *i.e.*, $\sum_{i\neq j}f^* \circ s_f(x_i^g, x_j^g)$, satisfy the following condition: the feature representations of all samples are distributed uniformly on the unit hypersphere $\mathbb{S}^{d-1}$.
:::

In Theorem [\[thm:uniformity\]](#thm:uniformity){reference-type="ref" reference="thm:uniformity"}, the assumption $N\leq d + 1$ is always satisfied in our experiments in §[\[sec:exp\]](#sec:exp){reference-type="ref" reference="sec:exp"}. For instance, on CIFAR-10 we chose $N = d = 512$. Also, we claim that the samples are "distributed uniformly" if the feature vectors form a regular simplex, and thus the distances between all sample pairs are the same. Although minimizing the negative term gives uniformity, the positive term is also needed for aligning similar pairs, as we observe in §[\[sec:exp\]](#sec:exp){reference-type="ref" reference="sec:exp"}. This implies the *tradeoff* between alignment and uniformity.

In fact, [\[eq:f-div_property\]](#eq:f-div_property){reference-type="ref+label" reference="eq:f-div_property"} provides us guidance to select proper $f$-divergences for uniformity. In Table [\[tbl:choices_f_div\]](#tbl:choices_f_div){reference-type="ref" reference="tbl:choices_f_div"}, we list some common choices of $f$-divergences. By inspecting the last column and using the definition of $G_\sigma$, we can easily verify that they all satisfy [\[eq:f-div_property\]](#eq:f-div_property){reference-type="ref+label" reference="eq:f-div_property"}. However, this is not true for all $f$-divergences. In Appendix [\[app:f-div\]](#app:f-div){reference-type="ref" reference="app:f-div"} we also provide some counterexamples that violate [\[eq:f-div_property\]](#eq:f-div_property){reference-type="ref+label" reference="eq:f-div_property"} and thus Theorem [\[thm:uniformity\]](#thm:uniformity){reference-type="ref" reference="thm:uniformity"}, such as the Reversed Kullback--Leibler (RKL) and the Neynman $\chi^2$ divergences. Experimentally, we found that these divergences generally result in feature collapse (*i.e.*, all feature vectors are the same) and thus poor performance in downstream applications.

startsection section1@-2.0ex plus -0.5ex minus -.2ex1.5ex plus 0.3ex minus0.2ex

Experiments[]{#sec:exp label="sec:exp"} In this section, we empirically evaluate the analysis on our provided answers: (1) Can we go beyond the KL-based objective (§[\[sec:exp_obj\]](#sec:exp_obj){reference-type="ref" reference="sec:exp_obj"}): we apply various $f$-MICL objectives to popular vision and language datasets. In particular, we show that under the same network architecture design, $f$-MICL can always provide a better choice of objective. We observe that the best-performing $f$-divergence is largely dataset dependent. (2) Can we design a better similarity function (§[\[sec:similarity\]](#sec:similarity){reference-type="ref" reference="sec:similarity"}): we show that the proposed $f$-Gaussian similarity is more powerful than the heuristic cosine similarity, regardless of the choice of $f$. Moreover, we confirm empirically that $f$-MICL extends the nice property of alignment and uniformity in §[\[sec:au\]](#sec:au){reference-type="ref" reference="sec:au"}.

startsectionsubsection2@-1.8ex plus -0.5ex minus -.2ex0.8ex plus .2ex

Experimental settings Our detailed settings can be found in Appendix [\[app:add_exp\]](#app:add_exp){reference-type="ref" reference="app:add_exp"}. In all our experiments, we change only the objective of different methods for fair comparison. We use the $f$-Gaussian similarity in $f$-MICL by default.

**Vision task.**   Our vision datasets include CIFAR-10 [@krizhevsky2009learning], STL-10 [@coates2011analysis], TinyImageNet [@chrabaszcz2017downsampled], and ImageNet [@deng2009imagenet] for image classification. After learning the feature embeddings, we evaluate the quality of representations using the test accuracy via a linear classifier. [Note that we use $\alpha=40$ across all vision experiments]{style="color: black"}.\
(1) Smaller datasets: For feature encoders, we use ResNet-18 [@he2016deep] for CIFAR-10; ResNet-50 [@he2016deep] for the rest. Our implementation is based on SimCLR [@ChenKNH20], [where we used the same 3-layer projection head during training]{style="color: black"}. All models are trained for [800]{style="color: black"} epochs.\
(2) ImageNet: We choose Vision Transformer (ViT-S) [@vit] as our feature encoder. We choose the smaller ViT-S model with 6 blocks because larger ViT models are extremely expensive to train on GPUs. Our implementations are based on MoCo V3 [@mocov3], [where models are trained for 1000 epochs]{style="color: black"}.

**Language task.** To show the wide applicability of our $f$-MICL framework, we also conduct experiments on a natural language dataset, English Wikipedia [@gao2021simcse]. We follow the experimental setting in [@gao2021simcse], which applies BERT-based models to SimCLR [@devlin2019bert; @liu2019roberta]. Specifically, we choose the BERT$_{\tt base}$ model due to limited computing resources. [For $f$-MICL objectives, we choose $\alpha=409600$.]{style="color: black"} The application task is called semantic textual similarity (STS [@agirre2013sem]) and we report the averaged Spearman's correlation in Table [\[tab:accs\]](#tab:accs){reference-type="ref" reference="tab:accs"} for comparison.

startsectionsubsection2@-1.8ex plus -0.5ex minus -.2ex0.8ex plus .2ex

$f$-MICL objectives []{#sec:exp_obj label="sec:exp_obj"} **Smaller datasets and language task.** We first compare $f$-MICL with several InfoNCE-based contrastive learning algorithms (i.e., SimCLR [@ChenKNH20], MoCo [@he2020momentum], and AU [@wang2020understanding]) on smaller datasets and the language task in Table [\[tab:accs\]](#tab:accs){reference-type="ref" reference="tab:accs"}. Here we choose four $f$-divergences with the best overall performance. See Appendix [\[app:add_exp\]](#app:add_exp){reference-type="ref" reference="app:add_exp"} for results on other $f$-divergences.

From Table [\[tab:accs\]](#tab:accs){reference-type="ref" reference="tab:accs"} we observe that: (1) As we have shown in §[\[sec:infonce\]](#sec:infonce){reference-type="ref" reference="sec:infonce"} that $f$-MICL generalizes InfoNCE-based objectives, empirically KL-MICL achieves similar performance to the baselines. In practice, we can tune the hyperparameter $\alpha$ such that KL-MICL outperforms the InfoNCE-based objectives. (2) KL-MICL is not always the optimal choice. We can see that the best-performing $f$-MICL objectives refer to four different $f$-divergences on four datasets.

::: table*
+-------------------------------+--------------------------------------------------------------+------------------------------------------------------------------------------------------------------------+---+
| **Dataset**                   | **Baselines**                                                | **$f$-MICL**                                                                                               |   |
+:=============================:+:==================:+:==================:+:==================:+:==========================:+:==========================:+:===================:+:==========================:+:=:+
| (l0ptr10pt)2-4(l-1ptr-1pt)5-8 | MoCo               | SimCLR             | AU                 | KL                         | JS                         | Pearson             | VLC                        |   |
+-------------------------------+--------------------+--------------------+--------------------+----------------------------+----------------------------+---------------------+----------------------------+---+
| CIFAR-10                      | 90.30$_{\pm 0.19}$ | 89.71$_{\pm 0.37}$ | 90.41$_{\pm 0.26}$ | **90.61**$_{\bf\pm 0.47}$  | 89.66$_{\pm 0.28}$         | 89.35 $_{\pm 0.52}$ | 89.13 $_{\pm 0.33}$        |   |
+-------------------------------+--------------------+--------------------+--------------------+----------------------------+----------------------------+---------------------+----------------------------+---+
| STL-10                        | 83.69$_{\pm 0.22}$ | 82.97$_{\pm 0.32}$ | 84.44$_{\pm 0.19}$ | 85.33$_{\pm 0.39}$         | **85.94**$_{\bf \pm 0.17}$ | 82.64$_{\pm 0.37}$  | **85.94**$_{\bf \pm 0.72}$ |   |
+-------------------------------+--------------------+--------------------+--------------------+----------------------------+----------------------------+---------------------+----------------------------+---+
| TinyImageNet                  | 35.72$_{\pm 0.17}$ | 30.56$_{\pm 0.28}$ | 41.20$_{\pm 0.19}$ | 39.46$_{\pm 0.20}$         | 42.98$_{ \pm 0.18}$        | **43.45$_{          | 38.65$_{\pm 0.45}$         |   |
|                               |                    |                    |                    |                            |                            | \bf \pm 0.54}$**    |                            |   |
+-------------------------------+--------------------+--------------------+--------------------+----------------------------+----------------------------+---------------------+----------------------------+---+
| Wikipedia                     | 77.88$_{\pm 0.15}$ | 77.40$_{\pm 0.12}$ | 77.95$_{\pm 0.08}$ | **78.02**$_{\bf \pm 0.13}$ | 76.76$_{\pm 0.09}$         | 77.59$_{\pm 0.12}$  | 55.07$_{\pm 0.13}$         |   |
+-------------------------------+--------------------+--------------------+--------------------+----------------------------+----------------------------+---------------------+----------------------------+---+
:::

::: table*
+--------------------------------+----------------------------------------------------------+---------------------------+
| **Dataset**                    | **Baselines**                                            | **$f$-MICL**              |
+:==============================:+:====:+:====:+:============:+:======:+:========:+:=======:+:====:+:========:+:=======:+
| (l0ptr10pt)2-7(l-1ptr-1pt)8-10 | SwAV | BYOL | Barlow Twins | VICReg | RényiCL  | MoCo v3 | KL   | JS       | Pearson |
+--------------------------------+------+------+--------------+--------+----------+---------+------+----------+---------+
| ImageNet                       | 75.3 | 74.3 | 73.2         | 73.2   | **76.2** | 73.2    | 73.9 | **76.5** | 74.6    |
+--------------------------------+------+------+--------------+--------+----------+---------+------+----------+---------+
:::

The above results indicate that $f$-MICL can provide a wide range of objective choices for the downstream tasks. Although how to derive an optimal $f$-divergence deserves more study in theory, in practice we can select the best $f$ among several common $f$-divergences on a validation set.

Besides the $f$-divergences in Table [\[tbl:choices_f_div\]](#tbl:choices_f_div){reference-type="ref" reference="tbl:choices_f_div"}, in Theorem [\[thm:uniformity\]](#thm:uniformity){reference-type="ref" reference="thm:uniformity"} we have identified non-satisfying $f$-divergences. In our experiments, we found that applying these $f$-divergences such as the RKL and Neyman $\chi^2$ divergences would result in feature collapse. For example, in Figure [4](#fig:uniformity_batch){reference-type="ref" reference="fig:uniformity_batch"} we show that with RKL the features all collapse to a constant.

<figure id="fig:uniformity_batch">
<figure>
<img src="images/uniform.png" style="width:100.0%" />
</figure>
<figure>
<img src="images/batch_size.png" style="width:100.0%" />
</figure>
<figcaption> <span><strong>(left and middle)</strong></span> Distances between pairs of normalized features within a batch. <span><strong>Green region:</strong></span> similar pairs. <span><strong>Orange region:</strong></span> dissimilar pairs. <span class="math inline"><em>f</em></span>-MICL gives nearly uniform distances for dissimilar pairs for the <span class="math inline"><em>f</em></span>-divergences in Table <a href="#tbl:choices_f_div" data-reference-type="ref" data-reference="tbl:choices_f_div">[tbl:choices_f_div]</a>. For non-satisfying <span class="math inline"><em>f</em></span>-divergences such as the RKL, the features collapse to a constant and thus the distances are zero. <span><strong>(right)</strong></span> The test accuracy v.s. the batch size after training <span class="math inline">200</span> epochs for all algorithms.</figcaption>
</figure>

**ImageNet results:** To further demonstrate the efficacy of $f$-MICL we then compare with several popular self-supervised learning methods, including both contrastive-based and non contrastive-based ones. These methods can be categorized into the ResNet-based (SwAV [@swav], BYOL [@huynh2020boosting], Barlow Twins [@barlow], VICReg [@vicreg]) and RényiCL [@lee2022renyicl], and the ViT-based [@vit]. For the ResNet-based methods, we directly retrieve results from [@vicreg], which are obtained by training ResNet-50 models for 1000 epochs. Chen et al. [@mocov3] show that these two types of methods are directly comparable in terms of the model size and supervised learning performance. For the ViT-based method and our $f$-MICL, we apply ViT-S for 1000 epochs. Specifically, our $f$-MICL follows MoCo v3 with only the objective changed for different choices of the $f$-divergence.

Results in Table [\[tab:imnet\]](#tab:imnet){reference-type="ref" reference="tab:imnet"} show that: (1) By only changing the objective function, our method improves MoCo v3 by 2.1% using JS-MICL. (2) $f$-MICL objectives are comparable with the ResNet-based methods (e.g., SwAV and RényiCL).

Overall, our experiments confirm that $f$-MICL can provide a better choice of objective than InfoNCE on a variety of datasets, tasks, and encoder architectures.

startsectionsubsection2@-1.8ex plus -0.5ex minus -.2ex0.8ex plus .2ex

$f$-Gaussian similarity []{#sec:similarity label="sec:similarity"} Next, we want to examine the effect of our similarity function [while fixing the $f$-divergences]{style="color: black"}. In Table [\[tab:cos_vs_gaussian\]](#tab:cos_vs_gaussian){reference-type="ref" reference="tab:cos_vs_gaussian"} we compare the cosine and $f$-Gaussian similarities for different $f$-divergences on CIFAR-10. It can be seen that under our $f$-MICL framework the $f$-Gaussian similarity consistently outperforms the cosine similarity for various $f$-divergences[^5]. This agrees with our Theorem [\[lem:optimal_sim\]](#lem:optimal_sim){reference-type="ref" reference="lem:optimal_sim"} and [\[eq:optimal_similarity\]](#eq:optimal_similarity){reference-type="ref+label" reference="eq:optimal_similarity"}, and also implies the validity of Assumption [2](#assmp:joint_vMF){reference-type="ref" reference="assmp:joint_vMF"}. In particular, we identify that without the theoretical guarantee, the heuristic cosine similarity would fail for certain $f$-MICL objectives (*e.g.*, VLC).

startsectionsubsection2@-1.8ex plus -0.5ex minus -.2ex0.8ex plus .2ex

Alignment and uniformity test []{#sec:au label="sec:au"} We empirically check the properties of alignment and uniformity for $f$-MICL by plotting the pairwise distance $\|x_i^g - x_j^g\|$ of the feature representations within the same batch on CIFAR-10. We compute the distances between the normalized features of every pair from a random batch, and then sort the pairs in increasing order. From Figure [4](#fig:uniformity_batch){reference-type="ref" reference="fig:uniformity_batch"} we can see that $f$-MICL gives nearly uniform distances for dissimilar pairs (orange regions) on both datasets with various proper $f$-divergences. In contrast, a random initialized model gives a less uniform distribution for dissimilar pairs. Besides, we observe small pairwise distances for similar pairs (green regions).

startsectionsubsection2@-1.8ex plus -0.5ex minus -.2ex0.8ex plus .2ex

Sensitivity to batch size Finally, we study the sensitivity to the batch size of our $f$-MICL framework on CIFAR-10. On the right panel of Figure [4](#fig:uniformity_batch){reference-type="ref" reference="fig:uniformity_batch"}, we evaluate the classification accuracy by varying the batch size for different $f$-divergences and SimCLR. We can see that for all different batch sizes and with the proper choice of $f$-divergences, our performance is always better than SimCLR. In other words, we require fewer negative samples to achieve the same performance.

startsection section1@-2.0ex plus -0.5ex minus -.2ex1.5ex plus 0.3ex minus0.2ex

Related Work []{#app:related label="app:related"}

**Contrastive learning.** Self-supervised contrastive learning learns representations by contrasting sample pairs. Recently it has been shown analytically that improving the contrastiveness can benefit the downstream applications [@SaunshiPAKK19; @tosh2021contrastive]. For popular contrastive learning methods such as Contrastive Predictive Coding (CPC) [@oord2018representation], SimCLR [@ChenKNH20], and MoCo [@he2020momentum], their loss functions can be interpreted as a lower bound of mutual information, which is essentially the KL divergence between the joint distribution and the product of margin distributions. Besides the KL divergence, other statistical divergences or distances have been individually studied under the context of contrastive learning, *e.g.*, the Wasserstein distance [@ozair2019wasserstein], Pearson $\chi^2$ divergence [@tsai2021self], and Jensen--Shannon divergence [@hjelm2018learning].

**$f$-divergences** have been widely used in generative models [@nowozin2016f] and domain adaptation [@acuna2021fdomainadversarial], for measuring the discrepancy of two distributions, where the variational lower bound is often employed for estimation. Compared to $f$-GAN [@nowozin2016f] and $f$-DAL [@acuna2021fdomainadversarial] which minimize the $f$-divergence between two different distributions, our $f$-MICL objective is to maximize the $f$-divergence between the joint distribution and the product of marginal distributions. This agrees with our purpose of contrasting sample pairs. Moreover, we provide a theoretical criterion for choosing proper $f$-divergences.

**Mutual Information** also plays an important role in the context of deep representation learning [@tian2020contrastive; @bachman2019learning; @hjelm2018learning; @TianSPKSI20; @poole2019variational; @belghazi2018mine]. *Loss function wise*, our losses partially cover the losses in the literature and generalizes them: e.g., [@poole2019variational] considers several variational lower bounds of mutual information, where we generalize the DV objective; (b) *application-wise*, none of them considers contrastive learning: e.g., [@poole2019variational] considers mutual information estimation, [@belghazi2018mine] improves adversarial generative models, [@hjelm2018learning] considers representation learning that maximizes local and global information.

**Metric learning.** Our work is closely related to metric learning [@kaya2019deep; @suarez2018tutorial], which aims to learn a distance metric bringing similar objects closer and distancing dissimilar objects further. In contrastive learning, a pre-defined similarity metric, *e.g.*, the cosine similarity [@ChenKNH20; @he2020momentum] or a bilinear function [@oord2018representation; @tian2020contrastive; @henaff2019data] is commonly used to measure the sample similarity. These pre-designed metrics may not necessarily lead to satisfactory performance in practice. Comparably, the design of our similarity function is empirically tailored for contrastive learning.

[Finally, we summarize existing representation learning methods that utilize $f$-divergences and compare with $f$-MICL in [\[tab:fdivergence\]](#tab:fdivergence){reference-type="ref+Label" reference="tab:fdivergence"} for a clear view of the literature. ]{style="color: black"}

startsection section1@-2.0ex plus -0.5ex minus -.2ex1.5ex plus 0.3ex minus0.2ex

Conclusion

We developed $f$-MICL for contrastive learning, which generalizes the KL-based mutual information to the $f$-mutual information. With $f$-MICL we provided a broad spectrum of objective choices with better downstream performance. We also proposed a novel $f$-Gaussian similarity function, which shows superior performance to the commonly used cosine similarity. In addition, we confirmed the generalization of $f$-MICL by comparing with popular InfoNCE-based objectives. Empirically, we exhibited the efficacy of $f$-MICL across a wide range of datasets from both vision and natural language.

**Limitations and future work.** While $f$-MICL provides a variety of objective functions, it is yet unclear how to choose an optimal $f$ based on a task and a dataset in theory, such that we usually rely on a validation set in practice for selection. An interesting future work is to learn an optimal $f$-divergence using a parametrized neural network. Moreover, [@lee2022renyicl] applied Skew Rényi divergence for contrastive learning. However, we observe that applying Rényi-MICL naively leads to a large variance (similar to Section 4.1 in @lee2022renyicl), and we leave the discussion on skew divergences for future works. Additionally, [@McAllesterS20] showed that there exist some inherent statistical limitations on accurately estimating the mutual information with various lower bounds. In future work it would be interesting to examine if such limitations extend to $f$-MI, and if a limited estimation of $f$-MI necessarily affects $f$-MICL whose goal is to compare and learn representations through (lower bounds of) $f$-MI.

startsection section1@-2.0ex plus -0.5ex minus -.2ex1.5ex plus 0.3ex minus0.2ex

\*Acknowledgement We thank the reviewers and the action editor for their constructive comments. Part of this work was performed during YL's internship at NRC. YY thanks NSERC and CIFAR for funding support. Resources used in preparing this research were provided, in part, by the Province of Ontario, the Government of Canada through CIFAR, and companies sponsoring the Vector Institute.

startsection section1@-2.0ex plus -0.5ex minus -.2ex1.5ex plus 0.3ex minus0.2ex

Additional theoretical results[]{#app:theory label="app:theory"}

In this appendix, we provide additional theoretical results, including additional $f$-divergences and the theory for weighting parameters.

**Notations.** We assume that a dominating measure $\lambda$ (e.g. Lebesgue) is given and all other probability measures are represented as some density w.r.t. $\lambda$. Given the joint density $p(x,y)$, we denote $p(x)  = \int p(x,y) \mathrm{d}\lambda(y)$ and $p(y) = \int p(x,y) \mathrm{d}\lambda(x)$ as the marginals. We use $\textrm{supp}(\cdot)$ to denote the support of a distribution, and $f^*$ to denote the conjugate of function $f$. Every norm presented is Euclidean. We use $x^g := g(x)$ as the shorthand notation for the feature embedding, with $x$ a raw sample. The notation $p_{d}$ stands for the data distribution, and $p_\times:= p_d\otimes p_d$ means its self product. We denote $p_+$ as the distribution of *positive pairs*, *i.e.*, two samples with similar feature embeddings. The symbol $\circ$ denotes function composition.

startsectionsubsection2@-1.8ex plus -0.5ex minus -.2ex0.8ex plus .2ex

Additional $f$-divergences []{#app:f-div label="app:f-div"}

We expand Table [\[tbl:choices_f_div\]](#tbl:choices_f_div){reference-type="ref" reference="tbl:choices_f_div"} and give more examples of $f$-divergences in Table [\[tbl:choices_f_div_app\]](#tbl:choices_f_div_app){reference-type="ref" reference="tbl:choices_f_div_app"}. As we will see in the proof of Theorem [\[thm:uniformity\]](#thm:uniformity){reference-type="ref" reference="thm:uniformity"}, Table [\[tbl:choices_f_div\]](#tbl:choices_f_div){reference-type="ref" reference="tbl:choices_f_div"} gives a special class of $f$-divergences that guarantees uniformity. A detailed description of $f$-divergences can be found in e.g. [@sason2016f].

:::: table*
::: center
  **Divergence**     $f(u)$                           $f^*(t)$                                                     $f'(u)$                                      $f^*\circ f'(u)$
  ------------------ -------------------------------- ------------------------------------------------------------ -------------------------------------------- ---------------------------------------------------
  KL                 $u \log u$                       $\exp(t-1)$                                                  $\log u + 1$                                 $u$
  Reverse KL         $- \log u$                       $-1-\log (-t)$                                               $-1/u$                                       $\log u - 1$
  JS                 $\varphi(u)$                     $- \log(2-e^t)$                                              $\log 2 + \log \frac{u}{1+u}$                $-\log 2 + \log( 1 + u)$
  Pearson $\chi^2$   $(u-1)^2$                        $t^2/4 + t$                                                  $2(u-1)$                                     $u^2 - 1$
  SH                 $(\sqrt{u} - 1)^2$               $\frac{t}{1 - t}$                                            $1 - u^{-1/2}$                               $u^{1/2} - 1$
  Neyman $\chi^2$    $\frac{(1 - u)^2}{u}$            $2 - 2\sqrt{1-t}$                                            $1 - u^{-2}$                                 $2 - 2 u^{-1}$
  Jeffrey            $(u - 1) \log u$                 $\widehat{W}(e^{1-t}) + t - 2$                               $1 - u^{-1} + \log u$                        $\widehat{W}(e^{1/u}/u) + \log u - \frac{1+u}{u}$
  Tsallis-$\alpha$   $\tfrac{u^\alpha}{\alpha - 1}$   $(\tfrac{\alpha - 1}{\alpha}t)^{\tfrac{\alpha}{\alpha-1}}$   $\frac{\alpha}{\alpha - 1} u^{\alpha - 1}$   $u^\alpha$
  Vincze--Le Cam     $\frac{(u-1)^2}{u + 1}$          $4 - t - 4\sqrt{1-t}$                                        $\frac{(u-1)(u+3)}{(u+1)^2}$                 $3 - \frac{4}{u+1}$
:::
::::

startsectionsubsection2@-1.8ex plus -0.5ex minus -.2ex0.8ex plus .2ex

Weighting parameters[]{#sec:weighting label="sec:weighting"}

In Algorithm [\[alg:f-MICL\]](#alg:f-MICL){reference-type="ref" reference="alg:f-MICL"} we added a weighting parameter $\alpha$ to balance the alignment and uniformity. We prove that even after adding this parameter we are still maximizing the $f$-mutual information, although with respect to a different $f$.

::: restatable
propweight[]{#prop:scaling label="prop:scaling"} Given $\alpha > 0$ and a closed convex function $f: \mathbb{R}_+ \to \mathbb{R}$ such that $f(1) = 0$, define $f_\alpha : \alpha \,\textrm{dom }f \to \mathbb{R}$ with $$f_\alpha(x) = \alpha f\left(\frac{x}{\alpha}\right) - \alpha f\left(\frac{1}{\alpha}\right)$$ for any $x\in \textrm{dom }f$. Then $I_{f_\alpha}$ is still a valid $f$-mutual information (see Definition [1](#def:fMI){reference-type="ref" reference="def:fMI"}). Besides, by replacing $f$ with $f_\alpha$ in [\[eq:objective_gaussian\]](#eq:objective_gaussian){reference-type="ref+label" reference="eq:objective_gaussian"} we have the following optimization problem: $$\sup_{g\in \mathcal{G}} \mathbb{E}_{(x, y) \sim p_+}\left[f'\left(\frac{G_\sigma(\|x^g - y^g\|^2)}{\alpha}\right)\right] - \alpha \mathbb{E}_{(x, y)\sim p_\times}\left[f^* \circ f'\left(\frac{G_\sigma(\|x^g - y^g\|^2)}{\alpha}\right)\right],$$ where $G_\sigma(\|x^g - y^g\|^2) = \mu \exp\left(-\frac{\|x^g - y^g\|^2}{2\sigma^2}\right)$ is the Gaussian kernel.
:::

Note that $\alpha \,\textrm{dom }f$ means the scalar multiplication of a set which is applied element-wisely. According to Definition [1](#def:fMI){reference-type="ref" reference="def:fMI"}, $f_\alpha$ is also a valid $f$-divergence. This proposition tells us that rescaling the second term with factor $\alpha$ is equivalent to changing the function $f$ to another convex function $f_\alpha$. The transformation from $f$ to $\alpha f\left(\frac{x}{\alpha}\right)$ is also known as right scalar multiplication [@urruty1993convex]. Let us now move on to our proof:

::: proof
*Proof.* By definition, we know that $f_\alpha$ is convex and closed with $f_\alpha(1) = 0$, and thus $I_{f_\alpha}$ is a valid $f$-mutual information according to Definition [1](#def:fMI){reference-type="ref" reference="def:fMI"}. Moreover, we have $f_\alpha'(x) = f'(\frac{x}{\alpha})$ for any $x\in \alpha \, \textrm{dom }f$ and $$\begin{align}
f_\alpha^*(t) &= \sup_{x \in \textrm{dom }f_\alpha} x t - f_\alpha(x) \nonumber \\
&= \sup_{x \in \alpha \textrm{dom }f} x t - \alpha f\left(\frac{x}{\alpha}\right) + \alpha f\left(\frac{1}{\alpha}\right) \nonumber \\
&= \sup_{\frac{x}{\alpha} \in \textrm{dom }f} \frac{x}{\alpha}\cdot (\alpha t) - \alpha f\left(\frac{x}{\alpha}\right) + \alpha f\left(\frac{1}{\alpha}\right) \nonumber \\
&= \alpha \sup_{\frac{x}{\alpha} \in \textrm{dom }f} \left(\frac{x}{\alpha}\cdot t -  f\left(\frac{x}{\alpha}\right)\right) + \alpha f\left(\frac{1}{\alpha}\right) \nonumber \\
& = \alpha f^*(t) + \alpha f\left(\frac{1}{\alpha}\right),
\end{align}$$ where in the last line we used the definition of $f^*(t)$. Plugging $f_\alpha'$ and $f_\alpha^*$ into [\[eq:objective_gaussian\]](#eq:objective_gaussian){reference-type="ref+label" reference="eq:objective_gaussian"} yields the desired result. ◻
:::

startsection section1@-2.0ex plus -0.5ex minus -.2ex1.5ex plus 0.3ex minus0.2ex

Proofs[]{#app:proofs label="app:proofs"}

::: proof
*Proof.* From Definition [1](#def:fMI){reference-type="ref" reference="def:fMI"}, we are computing the following supremum: $$\begin{align}
\sup_{g, s} \int \left( \frac{p_g(x^g, y^g)}{p_g(x^g)p_g(y^g)}s(x^g, y^g) - f^* \circ s(x^g, y^g)\right) dp_d^g \otimes p_d^g.
\end{align}$$ Suppose $s$ is unconstrained and we fix $g$. The optimal solution should satisfy: $$\begin{align}
\frac{p_g(x^g, y^g)}{p_g(x^g)p_g(y^g)} \in (\partial f^*)(s_\star(x^g, y^g)),
\end{align}$$ almost surely for $(x, y) \sim p_d\otimes p_d$. From (3.11) of [@rockafellar1966characterization] this is equivalent to: $$\begin{align}
s_\star(x^g, y^g) \in \partial f \left( \frac{p_g(x^g, y^g)}{p_g(x^g)p_g(y^g)}\right).
\end{align}$$If $f$ is differentiable, then for any $u\in \textrm{dom }f$, $\partial f(u) = \{f'(u)\}$ is a singleton. ◻
:::

Note that we say the samples are "distributed uniformly" if the feature vectors form a regular simplex (see Figure [5](#fig:tetrahedron){reference-type="ref" reference="fig:tetrahedron"}), and thus the distances between all sample pairs are the same.

::: proof
*Proof.* From the definition of $h$ it is clear that $h$ is decreasing since $f^*$ and $f'$ are both monotonically increasing white $G_\sigma$ is decreasing. Using $h$ we rewrite the second term of [\[eq:objective_sample\]](#eq:objective_sample){reference-type="ref+label" reference="eq:objective_sample"} as $$\begin{align}
\min_{x_1^g, \ldots, x_N^g \in \mathbb{S}^{d-1}} ~ \sum_{i, j} h(\|x_i^g - x_j^g \|^2).
\end{align}$$ When $N \in [2, d+1]$, there exists a neat characterization of the minimizers, see e.g. [@borodachov2019discrete]. We include the proof below for completeness.

Apply Jensen's inequality, we have: $$\begin{align}
\frac{1}{N^2}\sum_{i, j} h(\|x_i - x_j\|^2) &\geq h\left( \frac{1}{N^2}\sum_{i, j} \|x_i - x_j\|^2\right) \nonumber \\
& = h\left(\frac{1}{N^2}\sum_{i, j} \|x_i - x_j\|^2\right)\nonumber \\
&= h\left(\frac{1}{N^2}\sum_{i,j} (2 - 2x_i \cdot x_j)\right) \nonumber \\
& = h\left(2\left(1 -\left\|\frac1N\sum_{i=1}^N x_i\right\|^2\right)\right) \nonumber \\
& \geq h\left(2\right),
\end{align}$$ where in the first line we used Jensen's inequality; in the third line we used $\|x_i\| = \|x_j\| = 1$ for any $i, j\in [N]$; in the last line we note that $\|\sum_{i=1}^N x_i\| \geq 0$ and $h$ is a decreasing function. When $h$ is strictly convex and decreasing, it is in fact strictly decreasing, and hence the two inequalities above can be attained iff $$\begin{align}
\bar x := \frac1N\sum_i x_i = {\bf 0}, ~~\mbox{ and } \|x_i - x_j\|^2 \equiv c \mbox{ for all } i \ne j,
\end{align}$$ namely that $\{x_1, \ldots, x_N\}$ form a regular simplex with its center at the origin. We remark that when $h$ is merely convex, points forming a centered regular simplex may form a strict subset of the minimizers.

To see the necessity of $N \leq d+1$, let us note that $$\begin{align}
x_i^\top x_j = \begin{cases} 
1, & i = j \\
-\frac{1}{N-1} , & i\ne j
\end{cases}
,
\end{align}$$ since $$\begin{align}
\begin{split}
    &\sum_{ij} \|x_i-x_j\|^2 = 2N^2 = N(N-1) c \implies \\
    &c = \frac{2N}{N-1} = 2 + \frac{2}{N-1}.
\end{split}
\end{align}$$ Performing simple Gaussian elimination we note that the matrix $X^\top X$ has rank $N-1$ where $X = [x_1, \ldots, x_N] \in \mathbb{R}^{d\times N}$. Therefore, we must have $N-1 \leq d$.

Lastly, we need to show when $h$ is a (strictly) convex function, which may not always be true depending on the $f$-divergences. We give the following characterization (we ignore the constants $\mu$ and $2\sigma^2$ in Assumption [2](#assmp:joint_vMF){reference-type="ref" reference="assmp:joint_vMF"} as they do not affect convexity):

- $h$ strictly convex: $h_{\rm KL}(t) = e^{-t}$, $h_{\rm JS}(t) = \log(1 + e^{-t}) - \log 2$, $h_{\rm Pearson}(t) = e^{-2t} - 1$, $h_{\rm SH}(t) = e^{-t/2} - 1$, $h_{\rm Tsallis}(t) = e^{-\alpha t}$, $h_{\rm VLC} = 3 - \frac{4}{1 + e^{-t}}$;

- $h$ convex but not strictly convex: $h_{\rm RKL}(t) = -t - 1$ (RKL stands for Reversed Kullback--Leibler, see Appendix [\[app:f-div\]](#app:f-div){reference-type="ref" reference="app:f-div"});

- $h$ concave: $h_{\rm Neyman}(t) = 2 - 2 e^t$ (Neyman stands for Neyman $\chi^2$, see Appendix [\[app:f-div\]](#app:f-div){reference-type="ref" reference="app:f-div"}).

Only for the last case we do not have the guarantee that the minimizing configurations could form a regular simplex. For RKL, in fact, any configuration that centers at the origin suffices since $h$ is a linear function. ◻
:::

startsection section1@-2.0ex plus -0.5ex minus -.2ex1.5ex plus 0.3ex minus0.2ex

Estimation of $f$-MICL Objective

<figure id="fig:tetrahedron" data-latex-placement="t">
<div class="center">
<embed src="images/tetrahedron.pdf" style="width:23.0%" />
</div>
<figcaption>A regular simplex on a hypersphere.</figcaption>
</figure>

In § [\[subsec:imp\]](#subsec:imp){reference-type="ref" reference="subsec:imp"} we have provided the empirical estimation of our objective in [\[eq:objective_sample\]](#eq:objective_sample){reference-type="ref+label" reference="eq:objective_sample"}. However, it remains a question whether our estimation of $f$-mutual information is consistent. In this part we derive an upper bound for the estimation error from statistical learning theory.

We denote our $f$-MICL objective as $i_f(X;Y)$ ([\[eq:objective_gaussian\]](#eq:objective_gaussian){reference-type="ref+label" reference="eq:objective_gaussian"}) and its empirical estimation as $\widehat{i}_f(X;Y)$ ([\[eq:objective_sample\]](#eq:objective_sample){reference-type="ref+label" reference="eq:objective_sample"}). After fixing the similarity function , we have $T(x, y) = s_f(x^g, y^g) = f'\circ G_\sigma(\|x^g - y^g\|^2).$

::: restatable
thmEstimation[]{#thm:simclr_moco_negative label="thm:simclr_moco_negative"} Suppose that the function $T$ is taken from a function class $\mathcal{T}$ and define $\mathcal{T}_x$ as the function class of $T(x, \cdot)$ given some $x\in \textrm{supp}(p_d)$. Denote $\mathfrak{R}_N^P$ to be the Rademacher complexity w.r.t. the distribution $P$ with $N$ i.i.d. drawn samples. Then for any $T\in \mathcal{T}$, the estimation error $|i_f(X; Y) - \widehat{i}_f(X; Y)|$ is upper bounded with probability at least $1 - \delta$: $$\begin{align}
\label{eq:esterror}
\textstyle
&2 \mathfrak{R}_N^{p_+}( \mathcal{T}) + 2 \mu \Bigg( \underset{x\sim p_d}{\mathbb{E}} \mathfrak{R}_N^{p_d} ( \mathcal{T}_x) + \frac{1}{N} \sum_{i=1}^N \mathfrak{R}_{N - 1}^{p_d}( \mathcal{T}_{x_j}) \Bigg) %\nonumber
%\\
%&\quad \quad \quad \quad 
+ (r_T + 2 r_f)  \sqrt{\frac{\log 6/\delta}{2(N - 1)}},
\end{align}$$ with the constants $r_T  = f'(\mu) -  f'(\mu e^{-2/\sigma^2})$ and $$r_f = f^*  \circ  f'(\mu)  -  f^*  \circ  f'(\mu e^{-2/\sigma^2}).$$
:::

Here the constant $\mu$ is from our Gaussian kernel in Assumption [2](#assmp:joint_vMF){reference-type="ref" reference="assmp:joint_vMF"}. Rademacher complexity evaluates the richness of a class of real-valued functions regarding a probability distribution, and its formal definition can be found in [@koltchinskii2001rademacher].

**Non-i.i.d. proof.** Our conclusion is theoretically non-trivial since our sample pairs are *non-i.i.d.*: although the individual samples are assumed to be i.i.d., the negative pairs are not independently drawn (*e.g.*, $(x_1, x_2)$ and $(x_1, x_3)$), which makes the derivation challenging.

Note that the function class $\mathcal{T}$ depends on the class of the feature encoder $g$ and the $f$-divergence. Our estimation error eq. ([\[eq:esterror\]](#eq:esterror){reference-type="ref" reference="eq:esterror"}) is composed of three parts:

- the Rademacher complexity of the function class $\mathcal{T}$. In general, if $\mathcal{T}$ is richer then its Rademacher complexity is also larger.

- the expected Rademacher complexity of the one-side function class $\mathcal{T}_x$ and its empirical estimation;

- an error term that decreases with more samples.

Since the encoders are usually built with neural networks, we can use the existing theory [@bartlett2019nearly] to give more detailed bounds for the Rademacher complexities of $\mathcal{T}$. Specifically, if the Vapnik--Chervonenkis (VC) dimension of $\mathcal{T}$ is finite, then our estimation error in eq. ([\[eq:esterror\]](#eq:esterror){reference-type="ref" reference="eq:esterror"}) goes to zero as $N\to \infty$ [@mohri2018foundations].

**Approximation and estimation tradeoff.** In order to minimize the estimation error in [\[eq:esterror\]](#eq:esterror){reference-type="ref+label" reference="eq:esterror"}, we should choose a simpler function class $\mathcal{T}$ to reduce the Rademacher complexities. However, $\mathcal{T}$ should also be rich enough so that [\[eq:condition_T\]](#eq:condition_T){reference-type="ref+label" reference="eq:condition_T"} can be satisfied, since our objective $i_f(X;Y)$ should approximate the $f$-mutual information $I_f(X;Y)$ if we choose the optimal $T$. Therefore, there is a natural tradeoff between approximation and estimation errors when we change the complexity of $\mathcal{T}$.

startsection section1@-2.0ex plus -0.5ex minus -.2ex1.5ex plus 0.3ex minus0.2ex

Additional experimental results[]{#app:add_exp label="app:add_exp"}

We present additional experiment details in this appendix, to further support our experiments in the main paper.

startsectionsubsection2@-1.8ex plus -0.5ex minus -.2ex0.8ex plus .2ex

Implementation details []{#sec:imp label="sec:imp"} In this paper, we follow the implementations in SimCLR (<https://github.com/sthalles/SimCLR>) and MoCo v3 (<https://github.com/facebookresearch/moco-v3>). For vision tasks, we use ResNet [@he2016deep] and ViT-S [@vit] as the feature encoder, and we adopt the similar procedure of SimCLR/MoCo for sampling. For the language dataset, we follow the exact experimental setting of [@gao2021simcse] and only change the objective. Our experimental settings are detailed below:

- Hardware and package: We train on a GPU cluster with `NVIDIA T4` and `P100`. The platform we use is `pytorch`. Specifically, the pairwise summation can be easily implemented using $\mathtt{torch.nn.functional.pdist}$ from `pytorch`.

- Datasets: the datasets we consider include CIFAR-10, STL-10 [@coates2011analysis], TinyImageNet [@chrabaszcz2017downsampled], ImageNet [@deng2009imagenet] and English Wikipedia [@gao2021simcse].

- Augmentation method: For each sample in a dataset we create a sample pair, a.k.a. positive pair, using two different augmentation functions. For image samples, we choose the augmentation functions to be the standard ones in contrastive learning, e.g., in [@ChenKNH20] and [@he2020momentum]. The augmentation is a composition of random flipping, cropping, color jittering and gray scaling. For text samples, following the augmentation method of [@gao2021simcse] we use dropout masks.

- Neural architecture: For CIFAR-10 we use ResNet-18 [@he2016deep]; for STL-10, TinyImageNet we use ResNet-50 [@he2016deep]; for ImageNet we use ViT-S [@vit]; for the Wikipedia dataset we use BERT$_{\tt base}$ [@devlin2019bert].

- Batch size and embedding dimension: for experiments in CIFAR-10 we choose batch size 512; for STL-10 we choose batch size 64 to accommodate one GPU training; for TinyImageNet, we choose batch size 256; for ImageNet, we choose batch size 1024. For all the vision datasets, we choose the embedding dimension to be 512. Regarding the language dataset, the batch size is 64 with the feature dimension 768. In all of these cases, our assumption $N\leq d + 1$ in Theorem [\[thm:uniformity\]](#thm:uniformity){reference-type="ref" reference="thm:uniformity"} is satisfied.

- Hyperparameters: in all our experiments we fix the constant factor $\mu = 1$. We find that in practice the weight parameter $\alpha$ often needs to be large (*e.g.*, in the Wikipedia dataset), which requires moderate tuning.

- Optimizer and learning rate scheduler: For smaller vision tasks, we use SGD with momentum for optimization and the cosine learning rate scheduler [@loshchilov2016sgdr]. For the ImageNet task and natural language task, we use Adam with weight decay [@loshchilov2018decoupled] and the linear decay scheduler.

- Evaluation metric: for vision tasks, we use $k$-nearest-neighbor ($k$-NN) (only small datasets) and linear evaluation to evaluate the performance, based on the learned embeddings. For the NLP task, we use the Spearman's correlation to evaluate the averaged semantic textual similarity score [@gao2021simcse].

- Baseline methods: for the four baseline methods, we follow the implementations in:

  - MoCo: <https://github.com/facebookresearch/moco>;

  - SimCLR: <https://github.com/sthalles/SimCLR>;

  - Uniformity: <https://github.com/SsnL/align_uniform>;

  - MoCo v3: <https://github.com/facebookresearch/moco-v3>

  For fair comparison we use the experimental settings in Table [\[tab:setup\]](#tab:setup){reference-type="ref" reference="tab:setup"} for all the baseline methods, which might differ from the original settings.

Table [\[tab:setup\]](#tab:setup){reference-type="ref" reference="tab:setup"} gives common choices of hyperparameters for different datasets. Note that we may need to further finetune $\alpha$ and $\sigma$ for different $f$-divergences. See our supplementary code for more details.

::: table*
     Dataset           `arch`         $N$    $d$    `lr`    $\mu$   $(2\sigma^2)^{-1}$   $\alpha$   `epoch`    $k$
  -------------- ------------------- ------ ----- -------- ------- -------------------- ---------- --------- -------
     CIFAR-10         ResNet-18       512    512    0.1       1             1               40        800      200
      STL-10          ResNet-50        64    512    0.1       1             1               40        800      200
   TinyImageNet       ResNet-50       256    512    0.1       1             1               40        800      200
     ImageNet           ViT-S         1024   512    0.1       1             1               40       1000     `n/a`
    Wikipedia     BERT$_{\tt base}$    64    768   `3e-5`     1             20            409600       1      `n/a`
:::

::: {#tab:alpha1}
   $\alpha$    0.1      1      10      20      30        40        50
  ---------- ------- ------- ------- ------- ------- ----------- -------
      KL      13.16   77.60   83.53   83.77   81.39   **84.19**   82.77
      JS      8.84    73.31   81.39   83.21   83.49   **84.06**   82.61

  : Ablation study on weighting parameter $\alpha$ for KL and JS divergences on CIFAR-10. We compare test accuracies (%) for different choices of $\alpha$ using $k$-NN evaluation.
:::

::: {#tab:alpha2}
   $\alpha$     1      10     $10^2$   $10^3$   $10^4$   $10^5$    409600     $10^6$
  ---------- ------- ------- -------- -------- -------- -------- ----------- --------
      KL      67.52   70.47   72.43    75.12    76.90    77.78    **78.02**   77.78
   Pearson    64.58   67.78   71.58    74.03    74.95    74.40    **77.59**   76.47

  : Ablation study on weighting parameter $\alpha$ for KL and Pearson $\chi^2$ divergences on Wikipedia. We compare the semantic textual similarity (STS) via the Spearman's correlation for different choices of weighting parameter $\alpha$.
:::

startsectionsubsection2@-1.8ex plus -0.5ex minus -.2ex0.8ex plus .2ex

Additional ablation study on weighting parameter

We provide additional ablation study on the weighting parameter $\alpha$. We perform experiments using a vision dataset (CIFAR-10) and a language dataset (Wikipedia). For CIFAR-10, we vary $\alpha$ from 0.1 to 50 for KL and JS divergences and run for 200 epochs. [We perform the same experiments on KL when $\alpha=1$ for 800 epochs and observed an accuracy of 83.58% (lower than SimCLR). This observation further provides empirical evidence that KL-MICL is different from InfoNCE and needs special tuning on $\alpha$ to perform well.]{style="color: black"}

Table [1](#tab:alpha1){reference-type="ref" reference="tab:alpha1"} justifies our choice of $\alpha$ in Table [\[tab:setup\]](#tab:setup){reference-type="ref" reference="tab:setup"}, where the downstream test accuracy indicates the optimal performance when choosing $\alpha=40$. For the Wikipedia dataset, we observe that a much bigger $\alpha$ is desirable for maximum performance. We vary $\alpha$ from 1 to $10^6$ for KL and Pearson $\chi^2$ divergences and run for 1 epoch, as there is a large number of samples ($10^6$) in the language dataset. Table [2](#tab:alpha2){reference-type="ref" reference="tab:alpha2"} justifies our choice of $\alpha$ in Table [\[tab:setup\]](#tab:setup){reference-type="ref" reference="tab:setup"}, where the best performance is reached at $\alpha=409600$. Such an $\alpha$ is found by starting from $\alpha=100$ and doubling iteratively.

startsectionsubsection2@-1.8ex plus -0.5ex minus -.2ex0.8ex plus .2ex

Additional experiments[]{#sec:add_exp label="sec:add_exp"}

Our final experiments show that $f$-MICL is stable in terms of training and the variation of performance is well controlled.

startsectionparagraph4@1.5ex plus 0.5ex minus .2ex-1emTraining stability We depict the training loss curves of different divergences on CIFAR-10 in Figure [6](#fig:curve){reference-type="ref" reference="fig:curve"}. This figure shows that our methods exhibit stable training dynamics with fast convergence.

![The training loss curves of various $f$-divergences on CIFAR-10 with $200$ epochs.](images/curve.png){#fig:curve width="40%"}

startsectionparagraph4@1.5ex plus 0.5ex minus .2ex-1em$k$-NN evaluation and additional $f$-divergences We show more detailed results of Table [\[tab:accs\]](#tab:accs){reference-type="ref" reference="tab:accs"} in Table [\[tab:accs_full\]](#tab:accs_full){reference-type="ref" reference="tab:accs_full"}, including experiments using $k$-nearest neighbour ($k$-NN) evaluation. Additionally, we have added experiments on other $f$-divergences such as Squared Hellinger and Tsallis-$\alpha$ divergences.

![Experiment for verifying Assumption [2](#assmp:joint_vMF){reference-type="ref" reference="assmp:joint_vMF"}. We draw the relation between the squared distances $\|x^g - y^g\|^2$ and the averaged $\log p_g$ with RealNVP. The features are learned by different algorithms trained on CIFAR-10. (**left**) SimCLR; **(right)** $f$-MICL with the KL divergence. ](images/gaussian2.png){#fig:gaussian2 width="90%"}

startsectionparagraph4@1.5ex plus 0.5ex minus .2ex-1emVerification of Assumption [2](#assmp:joint_vMF){reference-type="ref" reference="assmp:joint_vMF"} Throughout our paper we made an assumption (Assumption [2](#assmp:joint_vMF){reference-type="ref" reference="assmp:joint_vMF"}) that the joint feature distribution is a Gaussian kernel. However, is it a valid assumption? In this experiment, we try to show some empirical evidence that this assumption approximately holds in practice. Recall that Assumption [2](#assmp:joint_vMF){reference-type="ref" reference="assmp:joint_vMF"} says that the joint feature distribution of positive pairs is: $$\begin{align}
\label{eq:gaussian_assumption_app}
p_g(x^g, y^g) \propto \exp\left(-\frac{\|x^g - y^g\|^2}{2\sigma^2}\right)
\end{align}$$ if the RBF kernel is Gaussian. In order to estimate the joint density of positive pairs, we use normalizing flows, which is a popular method for density estimation. Popular normalizing flow models include NICE [@dinh2014nice], RealNVP [@dinh2016density] and Glow [@kingma2018glow]. [\[eq:gaussian_assumption_app\]](#eq:gaussian_assumption_app){reference-type="ref+Label" reference="eq:gaussian_assumption_app"} is equivalent to the following: $$\begin{align}
\log p_g(x^g, y^g) = -\frac{\|x^g - y^g\|^2}{2\sigma^2} + {\rm const},
\end{align}$$ and thus it suffices to show that the log likelihood is linear w.r.t. the distances between each positive pair. In Figure [7](#fig:gaussian2){reference-type="ref" reference="fig:gaussian2"}, we plot the relation between $\log p_g$, estimated by RealNVP [^6] with a Gaussian prior, and the squared distances $\|x^g - y^g\|^2$. The representations are learned by SimCLR, and $f$-MICL with the KL divergence on the CIFAR-10 dataset. To alleviate the estimation error in the flow model, we divide the distances into small intervals and compute the average log-likelihood within each interval. We can see that the log-likelihood is roughly linear w.r.t. the squared distance, and thus verifying our Assumption [2](#assmp:joint_vMF){reference-type="ref" reference="assmp:joint_vMF"}.

::: table*
+--------------------------------+--------------+--------------------------------------------+----------------------------------------------------------------------------------------------------------------+
| **Evaluation**                 | **Dataset**  | **Baselines**                              | **$f$-MICL**                                                                                                   |
+:==============================:+:============:+:============:+:============:+:============:+:=====================:+:==================:+:===================:+:============:+:============:+:=============:+
| (l0ptr10pt)3-5(l-1ptr-1pt)6-11 |              | MoCo         | SimCLR       | Uniformity   | KL                    | JS                 | Pearson             | SH           | Tsallis      | VLC           |
+--------------------------------+--------------+--------------+--------------+--------------+-----------------------+--------------------+---------------------+--------------+--------------+---------------+
|                                | CIFAR-10     | 90.30        | 89.71        | 90.41        | **90.61**             | 89.66              | 89.35               | 89.52        | 89.15        | 89.13         |
+--------------------------------+              +--------------+--------------+--------------+-----------------------+--------------------+---------------------+--------------+--------------+---------------+
|                                |              | $\pm 0.19$   | $\pm0.37$    | $\pm0.26$    | $\mathbf{\pm0.47}$    | $\pm0.28$          | $\pm 0.52$          | $\pm 0.25$   | $\pm0.42$    | $\pm0.33$     |
+--------------------------------+--------------+--------------+--------------+--------------+-----------------------+--------------------+---------------------+--------------+--------------+---------------+
| (l0ptr0pt)3-11                 | STL-10       | 83.69        | 82.97        | 84.44        | 85.33                 | **85.94**          | 82.64               | 82.80        | 84.79        | **85.94**     |
+--------------------------------+              +--------------+--------------+--------------+-----------------------+--------------------+---------------------+--------------+--------------+---------------+
|                                |              | $\pm 0.22$   | $\pm 0.32$   | $\pm 0.19$   | $\pm 0.39$            | $\mathbf{\pm0.17}$ | $\pm 0.37$          | $\pm 0.27$   | $\pm 0.34$   | $\bf \pm0.72$ |
+--------------------------------+--------------+--------------+--------------+--------------+-----------------------+--------------------+---------------------+--------------+--------------+---------------+
| (l0ptr0pt)3-11                 | TinyImageNet | 35.72        | 30.56        | 41.20        | 34.95                 | 42.98              | **43.45**           | 40.83        | 32.99        | 38.65         |
+--------------------------------+              +--------------+--------------+--------------+-----------------------+--------------------+---------------------+--------------+--------------+---------------+
|                                |              | $\pm 0.17$   | $\pm 0.28$   | $\pm 0.19$   | $\pm 0.20$            | $\pm0.18$          | **$\pm 0.54$**      | $\pm 0.67$   | $\pm 0.49$   | $\pm 0.45$    |
+--------------------------------+--------------+--------------+--------------+--------------+-----------------------+--------------------+---------------------+--------------+--------------+---------------+
|                                | CIFAR-10     | 88.70        | 84.92        | 89.42        | 89.34                 | 89.12              | **89.44**           | 88.13        | 89.18        | 89.15         |
+--------------------------------+              +--------------+--------------+--------------+-----------------------+--------------------+---------------------+--------------+--------------+---------------+
|                                |              | $\pm 0.22$   | $\pm0.39$    | $\pm0.18$    | $\pm0.57$             | $\pm0.38$          | $\mathbf{\pm 0.60}$ | $\pm 0.18$   | $\pm0.62$    | $\pm0.23$     |
+--------------------------------+--------------+--------------+--------------+--------------+-----------------------+--------------------+---------------------+--------------+--------------+---------------+
| (l0ptr0pt)3-11                 | STL-10       | 78.77        | 74.34        | 79.57        | 79.99                 | **80.45**          | 76.64               | 78.31        | 76.11        | 79.34         |
+--------------------------------+              +--------------+--------------+--------------+-----------------------+--------------------+---------------------+--------------+--------------+---------------+
|                                |              | $\pm 0.25$   | $\pm 0.14$   | $\pm 0.52$   | $\pm0.47$             | $\mathbf{\pm0.19}$ | $\pm 0.26$          | $\pm 0.33$   | $\pm 0.24$   | $\pm 0.62$    |
+--------------------------------+--------------+--------------+--------------+--------------+-----------------------+--------------------+---------------------+--------------+--------------+---------------+
| (l0ptr0pt)3-11                 | TinyImageNet | 36.22        | 29.60        | 37.44        | 36.17                 | **38.20**          | 38.14               | 35.56        | 33.11        | 35.21         |
+--------------------------------+              +--------------+--------------+--------------+-----------------------+--------------------+---------------------+--------------+--------------+---------------+
|                                |              | $\pm 0.20$   | $\pm 0.39$   | $\pm 0.27$   | $\pm0.29$             | $\mathbf{\pm0.26}$ | $\pm 0.63$          | $\pm 0.77$   | $\pm 0.52$   | $\pm 0.33$    |
+--------------------------------+--------------+--------------+--------------+--------------+-----------------------+--------------------+---------------------+--------------+--------------+---------------+
| STS                            | Wikipedia    | 77.88        | 77.40        | 77.95        | **78.02**             | 76.76              | 77.59               | 73.60        | 72.68        | 55.07         |
|                                |              +--------------+--------------+--------------+-----------------------+--------------------+---------------------+--------------+--------------+---------------+
|                                |              | ${\pm 0.15}$ | ${\pm 0.12}$ | ${\pm 0.08}$ | $\mathbf{{\pm 0.13}}$ | ${\pm 0.09}$       | ${\pm 0.12}$        | ${\pm 0.10}$ | ${\pm 0.09}$ | ${\pm 0.13}$  |
+--------------------------------+--------------+--------------+--------------+--------------+-----------------------+--------------------+---------------------+--------------+--------------+---------------+
:::

[^1]: More precisely, this is the monotone convex conjugate since we restrict the domain of $f$ to $\mathbb{R}_+$.

[^2]: As suggested by the action editor, we may also interpret the distribution ([\[eq:vM-F\]](#eq:vM-F){reference-type="ref" reference="eq:vM-F"}) as a "copula," i.e., a joint density on $\mathbb{S}^{d-1} \times \mathbb{S}^{d-1}$ with uniform marginals. Note that the conventional notion of copula replaces the hypersphere $\mathbb{S}^{d-1}$ with the unit interval $[0,1]$. More generally, we could consider the "copula" $p_g(x^g, y^g) \propto h(x^g \cdot y^g)$ for an increasing function $h$, to capture other types of correlation between the two views $x^g$ and $y^g$.

[^3]: [RealNVP applies real-valued non-volume preserving transformation for log-likelihood computation.]{style="color: black"}

[^4]: The linear relationship in [1](#fig:gaussian){reference-type="ref+Label" reference="fig:gaussian"} might also depend on the data, i.e., the CIFAR-10 dataset here. In practice, other customized datasets might require additional verification.

[^5]: As we have shown the equivalence between cosine and Gaussian similarity on KL, the difference results on KL just show the choice of different scaling factors.

[^6]: Code available at <https://github.com/ikostrikov/pytorch-flows>.
