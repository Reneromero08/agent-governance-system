# Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey

Lauren Nicole DeLong, Ramon FernÃ¡ndez Mir, Jacques D. Fleuriot
This paper was produced by the Artificial Intelligence and its Applications Institute of the School of Informatics at the University of Edinburgh.

###### Abstract

Neurosymbolic AI is an increasingly active area of research that combines symbolic reasoning methods with deep learning to leverage their complementary benefits. As knowledge graphs are becoming a popular way to represent heterogeneous and multi-relational data, methods for reasoning on graph structures have attempted to follow this neurosymbolic paradigm. Traditionally, such approaches have utilized either rule-based inference or generated representative numerical embeddings from which patterns could be extracted. However, several recent studies have attempted to bridge this dichotomy to generate models that facilitate interpretability, maintain competitive performance, and integrate expert knowledge. Therefore, we survey methods that perform neurosymbolic reasoning tasks on knowledge graphs and propose a novel taxonomy by which we can classify them. Specifically, we propose three major categories: (1) logically-informed embedding approaches, (2) embedding approaches with logical constraints, and (3) rule learning approaches. Alongside the taxonomy, we provide a tabular overview of the approaches and links to their source code, if available, for more direct comparison. Finally, we discuss the unique characteristics and limitations of these methods, then propose several prospective directions toward which this field of research could evolve.

###### Index Terms:

Neurosymbolic AI, knowledge graphs, representation learning, hybrid AI, graph neural networks.

## I Introduction

Knowledge Graphs (KGs) are becoming an increasingly popular way to represent information because they are multi-relational, easily queried, and can store various types of data in a similar, consistent format [[156](#bib.bibx156)]. Furthermore, numerous methods have been developed to mine information and make novel predictions from KGs [[74](#bib.bibx74), [163](#bib.bibx163)], leading to advancements in drug discovery [[135](#bib.bibx135), [179](#bib.bibx179)], improved user recommendation systems [[169](#bib.bibx169)], and more efficient traffic forecasting [[59](#bib.bibx59), [164](#bib.bibx164), [172](#bib.bibx172)], amongst other applications. Traditionally, to make novel predictions about a knowledge base, approaches utilized either a set of rules [[51](#bib.bibx51), [114](#bib.bibx114)] or generated numerical representations from which patterns could be extracted [[115](#bib.bibx115), [150](#bib.bibx150)]. Recently, however, newer methods for knowledge discovery and reasoning on graph structures have attempted to blend these categories into novel neurosymbolic, or hybrid, methods.

Neurosymbolic artificial intelligence (AI), an increasingly active field of research, describes the combination of symbolic AI, which often includes logic and rule-based approaches, with neural networks and deep learning [[53](#bib.bibx53), [68](#bib.bibx68)]. Often, neurosymbolic AIâ€™s main advantages are described as achieving comparable performance to current deep learning methods while simultaneously fostering inherent interpretability [[53](#bib.bibx53), [151](#bib.bibx151), [68](#bib.bibx68)]. However, there are many different ways in which this new field has been approached. With this article, we aim to provide a comprehensive overview and structural classification of neurosymbolic methods designed for reasoning over KGs by describing their:

* â€¢

  overarching taxonomy,
* â€¢

  benefits and weaknesses,
* â€¢

  applications for which they are already used, and
* â€¢

  potential research directions.

Specifically, we classify approaches into three major categories: (1) logically-informed embedding approaches (Â§[IV-A](#S4.SS1 "IV-A Logically-Informed Embedding Approaches â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey")), which use symbolic inference and deep learning sequentially, (2) approaches which learn embeddings with logical constraints (Â§[IV-B](#S4.SS2 "IV-B Learning with Logical Constraints â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey")), and (3) those which learn logical rules for reasoning (Â§[IV-C](#S4.SS3 "IV-C Rule Learning for KGC â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey")). By addressing several key characteristics, including interpretability, guided training, the ability to encode underrepresented types and long range dependencies, as well as the efficient aggregation of heterogeneous information (Â§[IV](#S4 "IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey")), we explore how each category of approaches has unique capabilities for various facets of research.

### I-A Related Work and Novel Contributions

This survey is the first to provide a full, comprehensive overview of neurosymbolic methods for reasoning over KGs. Because neurosymbolic AI on KGs is a relatively young area, earlier articles which attempt to cover this topic tend to focus largely on background information. For example, a survey article by Zhang et al. [[177](#bib.bibx177)] comprises an extremely thorough background on general neurosymbolic AI, neural-network-based graph reasoning, and symbolic graph reasoning. However, it only covers a subset of neurosymbolic methods for reasoning on KGs. Similarly, a paper by Chen et al. [[26](#bib.bibx26)] also provides a thoroughly expansive overview of the information present in our Background (Â§[II](#S2 "II Background â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey")) and KG Completion (Â§[III](#S3 "III A History of Knowledge Graph Completion â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey")) sections, but it does not discuss neurosymbolic methods on KGs.

Previously, a relevant review by Lamb et al. [[87](#bib.bibx87)] described how several types of graph neural networks already possess tools, such as attention mechanisms, to be considered or used for neurosymbolic purposes. Additionally, work by Boschin et al. [[15](#bib.bibx15)] overlaps with our paper. However, we take a different approach in terms of grouping and describing neurosymbolic methods for KGs. In particular, while Boschin et al. focuses on grouping neurosymbolic methods by the various rules or domain-specific knowledge used, we outline a taxonomy in terms of the structure of the architectures, including the orders in which symbolic and neural modules are placed and integrated.

## II Background

### II-A A Brief Introduction to Knowledge Graphs

A *graph* or *network* structure GğºG is composed of a finite set of vertices or nodes, *V*, and a finite set of edges, E, connecting the vertices. A graph GğºG can be represented, then, as a tuple of these two sets, (V,E)ğ‘‰ğ¸(V,E). Each edge e=(u,v)âˆˆEğ‘’ğ‘¢ğ‘£ğ¸e=(u,v)\in E connects two adjacent or *neighbouring* vertices, uğ‘¢u and vâˆˆVğ‘£ğ‘‰v\in V. The term triple denotes a pair of vertices connected by an edge [[120](#bib.bibx120)].

A knowledge graph (KG) uses a graph structure to represent a knowledge base (KB), a collection of factual triples denoting relations between real-world entities [[74](#bib.bibx74)]. KGs have been employed in research domains from biochemistry and medical data [[41](#bib.bibx41), [54](#bib.bibx54), [80](#bib.bibx80)] to social media platforms [[72](#bib.bibx72)]. They may be as simple as nodes and edges [[27](#bib.bibx27)], or they may additionally contain semantic information, relation and entity types, and node and edge properties [[47](#bib.bibx47), [109](#bib.bibx109)]. KGs have become one of the most popular ways to represent information, as they capture multi-relational data well, are fast and easy to query when stored in a graph database, and allow the representation of many different types of data in a similar format (a â€œuniversal languageâ€). Some of the best known KGs, such as YAGO [[144](#bib.bibx144)], Googleâ€™s KG [[140](#bib.bibx140)], and DBpedia [[8](#bib.bibx8)] contain millions of entities and relations representing general information about people, places, and other general items such as movies and music and are widely used as benchmark datasets. Other smaller KGs exist to store information about niche areas, such as the COVID-19 pandemic [[41](#bib.bibx41), [135](#bib.bibx135), [162](#bib.bibx162)].

KGs are also becoming an increasingly popular way to represent growing or changing KBs due to their flexible storage schemas [[156](#bib.bibx156)]. However, because KGs house information that we, as people, know about some given domain, they are inherently incomplete in comparison to the real world. The practice of adding new information to a graph, known as KG completion, is, therefore, a common way to make predictions, a topic discussed further within Â§[III-A](#S3.SS1 "III-A Knowledge Graph Completion and Reasoning â€£ III A History of Knowledge Graph Completion â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey").

### II-B A Brief Introduction to Logic-Based Reasoning

While the â€œsymbolicâ€ components of neurosymbolic approaches vary, many of them utilize some form of logic-based reasoning. We can think of a logic as a formal system comprised of syntax, semantics and inference rules. Different logics may suit different applications either because of their ability to represent interesting properties or because of their level of automation. In this section, we will cover logic programming, probabilistic logic programming, fuzzy logic and description logic, highlighting how they are used for reasoning on graph structures.

*Logic programming* [[10](#bib.bibx10)] is one of the most prominent and widely used symbolic reasoning techniques. The fundamental building blocks of this paradigm are Horn clauses [[71](#bib.bibx71)], which are formulas of the form:

|  |  |  |
| --- | --- | --- |
|  | Hâ†B1âˆ§â‹¯âˆ§Bn.â†ğ»subscriptğµ1â‹¯subscriptğµğ‘›H\leftarrow B\_{1}\land\cdots\land B\_{n}. |  |

These are meant to be interpreted as rules where the body B1âˆ§â‹¯âˆ§Bnsubscriptğµ1â‹¯subscriptğµğ‘›B\_{1}\land\cdots\land B\_{n} implies the (positive) head Hğ»H, or as facts, if the body is empty. In its most basic form, every element is an atom and all the free variables are implicitly universally quantified. For example, consider the following background knowledge and rule:

|  |  |  |
| --- | --- | --- |
|  | likesâ€‹(aâ€‹lâ€‹iâ€‹câ€‹e,bâ€‹oâ€‹b),likesğ‘ğ‘™ğ‘–ğ‘ğ‘’ğ‘ğ‘œğ‘\displaystyle\text{likes}(alice,bob), |  |
|  |  |  |
| --- | --- | --- |
|  | likesâ€‹(bâ€‹oâ€‹b,aâ€‹lâ€‹iâ€‹câ€‹e),likesğ‘ğ‘œğ‘ğ‘ğ‘™ğ‘–ğ‘ğ‘’\displaystyle\text{likes}(bob,alice), |  |
|  |  |  |
| --- | --- | --- |
|  | friendsâ€‹(X,Y)â†likesâ€‹(X,Y)âˆ§likesâ€‹(Y,X).â†friendsğ‘‹ğ‘Œlikesğ‘‹ğ‘Œlikesğ‘Œğ‘‹\displaystyle\text{friends}(X,Y)\leftarrow\text{likes}(X,Y)\land\text{likes}(Y,X). |  |

Intuitively, we can deduce that friendsâ€‹(aâ€‹lâ€‹iâ€‹câ€‹e,bâ€‹oâ€‹b)friendsğ‘ğ‘™ğ‘–ğ‘ğ‘’ğ‘ğ‘œğ‘\text{friends}(alice,bob).

From this, an interesting question arises: given some background knowledge and a set of positive and negative examples, can we come up with consistent rules? This is the approach in *inductive logic programming* (ILP) [[106](#bib.bibx106), [182](#bib.bibx182)]. The First-Order Inductive Learner (FOIL) [[119](#bib.bibx119)] is a classical ILP implementation that iteratively creates rules using simple heuristics to pick candidates. The basic approach relies on clean data and a small knowledge base, although there are relevant tools such as Progol [[105](#bib.bibx105)] or Aleph [[142](#bib.bibx142)] that improve on this classical approach. There are also many variations and extensions of ILP using advanced statistical methods that refine the search procedure, for instance, SHERLOCK [[133](#bib.bibx133)] and AMIE [[51](#bib.bibx51)], which is mentioned again in Â§[III-B](#S3.SS2 "III-B Rule-based Methods for Knowledge Graph Completion â€£ III A History of Knowledge Graph Completion â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"). Cropper et al. [[32](#bib.bibx32)] provide a comprehensive survey of ILP, and Zhang et al. survey its use in explainable AI [[182](#bib.bibx182)]; we refer the interested reader to those surveys for further information.

A useful extension to Horn clauses to guide the search is to consider probabilistic logic [[110](#bib.bibx110)] with soft rules [[107](#bib.bibx107), [62](#bib.bibx62)], where a probability âˆˆ[0,1]absent01\in[0,1] is attached, e.g.

|  |  |  |
| --- | --- | --- |
|  | 0.75â€‹Â :: carâ€‹(X)â†wheelsâ€‹(X,4)âˆ§drivableâ€‹(X)â†0.75Â :: carğ‘‹wheelsğ‘‹4drivableğ‘‹{0.75\text{ :: car}(X)\leftarrow\text{wheels}(X,4)\land\text{drivable}(X)} |  |

says that we are 75% certain that four-wheeled drivable objects are cars. Many tools take this probabilistic approach as it allows noisy data and uncertainty, and takes advantage of statistical methods. A good example is ProbLog [[36](#bib.bibx36)] where all the rules are soft and independent, and the resolution algorithm is extended to keep track of probabilities.

Alternatively, we can attach a weight wisubscriptğ‘¤ğ‘–w\_{i} to every first-order formula Fisubscriptğ¹ğ‘–F\_{i} in a knowledge base, i.e. (Fi,wi)i=1nsuperscriptsubscriptsubscriptğ¹ğ‘–subscriptğ‘¤ğ‘–ğ‘–1ğ‘›(F\_{i},w\_{i})\_{i=1}^{n}. Assuming that our language has finitely many constants, we can now define a graph called a Markov Logic Network (MLN) [[122](#bib.bibx122)]. First, recall that an atom is a formula composed of constants, variables and functions but with no logical connectives, and that a formula is grounded if it has no variables. The nodes of a MLN are every grounding of every atom appearing in the Fisubscriptğ¹ğ‘–F\_{i}s, and there is an edge if the ground atoms appear together in at least one of the Fisubscriptğ¹ğ‘–F\_{i}s. We will illustrate the concept by adapting an example from Domingos et al. [[42](#bib.bibx42)]. Suppose we have constants Ağ´A and BğµB and the following formulas:

|  |  |  |
| --- | --- | --- |
|  | âˆ€x.Veganâ€‹(x)â‡’Healthyâ€‹(x)formulae-sequencefor-allğ‘¥â‡’Veganğ‘¥Healthyğ‘¥\displaystyle\forall x.\,\text{Vegan}(x)\Rightarrow\text{Healthy}(x) |  |
|  |  |  |
| --- | --- | --- |
|  | âˆ€x.âˆ€y.Friends(x,y)â‡’(Vegan(x)â‡”Vegan(y))\displaystyle\forall x.\,\forall y.\,\text{Friends}(x,y)\Rightarrow(\text{Vegan}(x)\Leftrightarrow\text{Vegan}(y)) |  |

with weights 1.31.31.3 and 0.70.70.7 respectively.

The ground Markov network looks as follows:

Friendsâ€‹(A,B)Friendsğ´ğµ\text{Friends}(A,B)Friendsâ€‹(A,A)Friendsğ´ğ´\text{Friends}(A,A)Veganâ€‹(A)Veganğ´\text{Vegan}(A)Veganâ€‹(B)Veganğµ\text{Vegan}(B)Friendsâ€‹(B,B)Friendsğµğµ\text{Friends}(B,B)Healthyâ€‹(A)Healthyğ´\text{Healthy}(A)Friendsâ€‹(B,A)Friendsğµğ´\text{Friends}(B,A)Healthyâ€‹(B)Healthyğµ\text{Healthy}(B)

A possible world, xğ‘¥x is a truth value assignment to each node. The probability distribution over the possible worlds, denoted by the random variable Xğ‘‹X, is given by

|  |  |  |
| --- | --- | --- |
|  | Pâ€‹(X=x)=expâ€‹(âˆ‘i=1nwiâ€‹niâ€‹(x))âˆ‘yexpâ€‹(âˆ‘i=1nwiâ€‹niâ€‹(y))ğ‘ƒğ‘‹ğ‘¥expsuperscriptsubscriptğ‘–1ğ‘›subscriptğ‘¤ğ‘–subscriptğ‘›ğ‘–ğ‘¥subscriptğ‘¦expsuperscriptsubscriptğ‘–1ğ‘›subscriptğ‘¤ğ‘–subscriptğ‘›ğ‘–ğ‘¦P(X=x)=\frac{\text{exp}\left(\sum\_{i=1}^{n}w\_{i}n\_{i}(x)\right)}{\sum\_{y}\text{exp}\left(\sum\_{i=1}^{n}w\_{i}n\_{i}(y)\right)} |  |

where nğ‘›n is the number of formulas and niâ€‹(x)subscriptğ‘›ğ‘–ğ‘¥n\_{i}(x) is the number of true groundings of Fisubscriptğ¹ğ‘–F\_{i} in xğ‘¥x. In our example, let xğ‘¥x be the world where Ağ´A and BğµB are vegan, unhealthy and have no friends. We can compute that Pâ€‹(X=x)â‰ˆ0.00055ğ‘ƒğ‘‹ğ‘¥0.00055P(X=x)\approx 0.00055. With this machinery in place, we can answer queries such as the probability that Ağ´A is healthy.

Instead of associating a weight (or probability) to each formula, representing the â€œdegree of beliefâ€, we can also attach a value to the â€œdegree of truthâ€ of the statement. This approach is known as fuzzy logic, in which the interpretation of the usual boolean logical connectives is adjusted [[174](#bib.bibx174)]. For instance, a fuzzy version of xâˆ§yğ‘¥ğ‘¦x\land y could be minâ€‹(x,y)minğ‘¥ğ‘¦\text{min}(x,y) where xğ‘¥x and yğ‘¦y are in the range [0,1]01[0,1]. In general, any binary operation on [0,1]01[0,1] that is commutative, monotone, associative, and respects the identity is called a t-norm and can represent conjunction in fuzzy logic. It is also possible to go back and forth while preserving some of the semantics. Doing so allows us to apply statistical inference to an originally discrete problem and, conversely, logical reasoning to uncertain data. As we will see in Â§[IV-B2](#S4.SS2.SSS2 "IV-B2 Logical Constraints on the Loss Function â€£ IV-B Learning with Logical Constraints â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"), fuzzy inference systems can be used to increase explainability in deep learning systems [[13](#bib.bibx13)].

Another angle is to consider specific fragments of first-order logic with different levels of expressive power. *Description logics* [[84](#bib.bibx84)] are knowledge representation languages that have a rich syntax to talk about concepts, hierarchies, and cardinalities. Therefore, they provide a convenient semantic framework to reason over KGs, particularly in an ontological sense. In fact, we can think of ontologies as data schemas for KGs. The W3C Web Ontology Language (OWL) [[100](#bib.bibx100)] is an important implementation of this idea designed originally to describe the Semantic Web. There are many variations and dialects such as OWL 2 EL, designed with biomedical ontologies in mind, which we will see in Â§[IV-A](#S4.SS1 "IV-A Logically-Informed Embedding Approaches â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"). Here, EL means â€œexistential languageâ€, so in OWL 2 EL we can have existential but not universal quantification. These restrictions enable polynomial time reasoning, which is crucial for large knowledge bases. In this context, reasoners are pieces of software that can infer logical consequences from a set of formulas.

### II-C A Brief Introduction to Neural Networks

As the name suggests, the â€œneuralâ€ components of neurosymbolic approaches comprise neural network-based structures. Recently, much state-of-the-art work in machine learning (ML) and AI involves artificial neural networks (NN) [[126](#bib.bibx126), [148](#bib.bibx148)] due to their impressive abilities to learn functions that capture underlying patterns in data [[83](#bib.bibx83), [132](#bib.bibx132), [38](#bib.bibx38)]. A NN uses a series of nodes, or neurons, with activation functions to act as some firing threshold. Similarly to biological neurons, if the input into each activation function exceeds that threshold, then there is an output. Specifically, deep NNs are those with two or more layers [[148](#bib.bibx148), [141](#bib.bibx141)].

In recent years, many deep NN variants have sprouted to handle various data structures and accomplish novel goals. Convolutional NNs (CNNs) [[83](#bib.bibx83)], for example, are generally used for image data, recurrent NNs (RNNs) with Long Short-Term Memory (LSTM) are ideal for dynamic temporal data [[70](#bib.bibx70), [139](#bib.bibx139)], and several forms of graph NNs (GNN) have been designed to handle multi-relational data in graph structures, such as factual triples in KGs, as described previously in Â§[II-A](#S2.SS1 "II-A A Brief Introduction to Knowledge Graphs â€£ II Background â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey") [[163](#bib.bibx163)]. However, many NNs are black-box models which lack interpretability [[21](#bib.bibx21)]. This means that a human cannot inherently understand the internal processes that lead to the model output. With multiple layers and sometimes up to millions or billions of parameters [[16](#bib.bibx16), [29](#bib.bibx29)], it becomes infeasible for a human to follow the individual actions taken by the model to derive predictions. In some cases, such as biomedical applications in which datasets may contain demographic bias, this may be ethically wrong or even dangerous [[113](#bib.bibx113), [112](#bib.bibx112)]. As a result, many current efforts centre upon adding some aspect of interpretability to such methods [[21](#bib.bibx21)].

### II-D Neurosymbolic AI: A Hybrid Approach

Neurosymbolic AI is the field of research that studies the combination of deep learning and symbolic reasoning [[152](#bib.bibx152), [53](#bib.bibx53), [147](#bib.bibx147), [68](#bib.bibx68)]. The argument for this hybrid approach is that neural and symbolic systems can complement each other and mitigate their respective weaknesses. For many applications, it is highly desirable to combine accountable and interpretable logic-based modules with effective deep learning ones. While interpretability is defined in various ways across the literature, it is generally viewed as the ability to be understood by a human [[96](#bib.bibx96)]. Essentially,
an end-user might easily comprehend the reasoning processes that led to model predictions.

Based on the classification given by Kautz at AAAI-20111<https://roc-hci.com/announcements/the-third-ai-summer/> (available as a written summary [[77](#bib.bibx77)]), neurosymbolic integration is often categorized as follows [[131](#bib.bibx131)]:

#### II-D1 Symbolic Neuro Symbolic

a NN takes a symbolic representation as input and reconstructs another symbolic representation as output.

#### II-D2 Symbolic[Neuro]

systems where a symbolic engine queries a NN during reasoning to, for example, estimate a utility function.

#### II-D3 Neuro;Symbolic

the NN and the symbolic module solve complementary tasks and communicate frequently to guide each other.

#### II-D4 Neuro:Symbolic â†’â†’\rightarrow Neuro

symbolic and neural components are tightly-coupled, but symbolic knowledge is â€œcompiledâ€ into the training set.

#### II-D5 NeuroSymbolic

symbolic logic rules provide the template for the structure of the NN by representing them as tensor embeddings.

#### II-D6 Neuro[Symbolic]

neural engines capable of logical reasoning at certain points in the execution.

To connect our survey back to this preliminary classification of neurosymbolic systems, we will refer back to which of these categories, if any, each section of approaches is most aligned. Generally, in Â§[IV-A](#S4.SS1 "IV-A Logically-Informed Embedding Approaches â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey") and Â§[IV-C](#S4.SS3 "IV-C Rule Learning for KGC â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"), the approaches most resemble the Neuro:Symbolic â†’â†’\rightarrow Neuro and Neuro;Symbolic categories. Systems that train with logical constraints are explained in Â§[IV-B](#S4.SS2 "IV-B Learning with Logical Constraints â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"), and fall mostly into the NeuroSymbolic and Neuro[Symbolic] classes.

## III A History of Knowledge Graph Completion

While there are various reasoning tasks which could be executed upon KGs, those which perform KG completion (see below) are especially common in the literature, and, more particularly, in the approaches surveyed in the next sections. Thus, we provide a brief background to understand the main body of this article.

### III-A Knowledge Graph Completion and Reasoning

Since KGs house human knowledge about some given domain, they are notoriously incomplete. KGs could contain false positives based on misinformation and incorrect assumptions. Furthermore, it is often unclear as to whether the absence of details in the KG should indicate missing information or the negation of such information [[27](#bib.bibx27)]. KG completion (KGC) denotes a series of methods which can be used to refine the graphs and uncover novel information. For example, such completion can reveal unknown molecular functions in a biological KG [[135](#bib.bibx135), [41](#bib.bibx41)] or predict user connections in a social one [[72](#bib.bibx72)].

In particular, link prediction is a popular type of KGC which determines whether an edge exists between a given pair of nodes (e.g., whether two users of a social network might be acquainted in real life). Another variant, known as relation prediction, aims to infer the existence of links of specific relation types (e.g., whether the two users are family, friends, or acquaintances) [[27](#bib.bibx27)]. Link and relation predictions have been used for tasks such as drug repurposing for COVID-19 [[179](#bib.bibx179)], polypharmacy side effect prediction [[19](#bib.bibx19), [186](#bib.bibx186)], and user recommendations in social networks [[43](#bib.bibx43), [35](#bib.bibx35), [95](#bib.bibx95)]. Notably, while other questions may be answered through reasoning over KGs, such as subgraph or graph similarity (e.g. representing and comparing chemical structures [[159](#bib.bibx159), [48](#bib.bibx48)]), a majority of surveyed approaches utilize KGC.

### III-B Rule-based Methods for Knowledge Graph Completion

Some of the simplest methods toward KGC utilize a set of rules that can be used for logical inference. The source of said rules varies between approaches. As previously stated in Â§[II-B](#S2.SS2 "II-B A Brief Introduction to Logic-Based Reasoning â€£ II Background â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"), ontologies are formalizations of knowledge base semantics and are often used to represent the unique patterns, relationships, and hierarchies within a specific knowledge domain [[44](#bib.bibx44)]. Notable examples in biomedicine, for example, include the Gene Ontology [[7](#bib.bibx7)] and the Disease Ontology [[134](#bib.bibx134)]. Therefore, ontologies based on description logics can serve as the source of such rules.

Since ontologies often comprise expert-curated knowledge [[44](#bib.bibx44)], they can be especially beneficial if the application requires domain-specific knowledge [[85](#bib.bibx85)]. However, ontologies do not exist for every specific domain, and existing ones may impose limitations on the type of predictions made. Alternatively, rules can be mined directly from the KG using ILP methods (discussed in Â§[II-B](#S2.SS2 "II-B A Brief Introduction to Logic-Based Reasoning â€£ II Background â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey")) such as AMIE and its variants [[51](#bib.bibx51)]. In these cases, rules are based on association patterns within the KG rather than the general application domain. Notably, GalÃ¡rraga et al. [[51](#bib.bibx51)] show that association rule mining can correspond to mining Horn clauses in sufficiently large KGs. Therefore, all approaches discussed here do logical rule mining.

The major benefit of rule-based approaches is that they are inherently interpretable. One could refer back to the rules which governed the algorithm to get a human-readable understanding of how and why certain predictions were made. For example, SAFRAN [[114](#bib.bibx114)], which was inspired by AnyBURL [[101](#bib.bibx101)], provides confidence-weighted, post-hoc explanations based on rules for every prediction. This allows model tuning, adjustment for incorrect predictions, explanations for end-users, and more [[114](#bib.bibx114)]. Unfortunately, compared to other methodologies, such as those discussed in the next section, rule-based methods do not always achieve the same performance [[55](#bib.bibx55), [101](#bib.bibx101)]. Although some types of logic, such as probabilistic and fuzzy logic (see Â§[II-B](#S2.SS2 "II-B A Brief Introduction to Logic-Based Reasoning â€£ II Background â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey")), allow the quantification of uncertainty, programs using them are still limited in their abilities to generalize beyond the patterns encoded in the rules [[55](#bib.bibx55)]. Additionally, rule-based methods often suffer from scalability issues when faced with large KGs [[154](#bib.bibx154), [78](#bib.bibx78)], an increasingly pressing issue with the rise of big data and digitalization [[74](#bib.bibx74)].

### III-C Graph Embedding-based Methods for Knowledge Graph Completion

In contrast to rule-based methods, those which generate representative KG embeddings (KGE) typically scale well to large datasets. Specifically, a KGE is a numerical vector representation of the KG constituents such that proximity in the embedding space approximates some sort of similarity in the original KG [[178](#bib.bibx178), [89](#bib.bibx89)]. From the embeddings, one can re-construct parts of the KG to answer some predictive task, such as those mentioned in Â§[III](#S3 "III A History of Knowledge Graph Completion â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"). Many popular KGE methods are based on NNs [[14](#bib.bibx14), [132](#bib.bibx132)] and show competitive predictive performances in various fields [[186](#bib.bibx186), [121](#bib.bibx121)]. In particular, GNNs are a popular category of methods to generate KGEs [[163](#bib.bibx163)]. GNNs account for connections between components; the input is typically a matrix representation of the graph and its constituents, such as node entities, edges and relation types, and node or edge features [[163](#bib.bibx163)].

The way in which â€œsimilarityâ€ is defined and encoded is a major discrepancy between KGE methods [[89](#bib.bibx89)]. For example, both DeepWalk [[115](#bib.bibx115)] and node2vec [[57](#bib.bibx57)] assess node similarity as the frequency in which nodes co-occur on random walks through the graph structure. In contrast, methods like TransE [[14](#bib.bibx14)] represent relationships as 1-1 translations in the embedding space. Notably, methodologies like RESCAL [[108](#bib.bibx108)], DistMult [[167](#bib.bibx167)], and ComplEx [[150](#bib.bibx150)] were groundbreaking for their stronger focus on relational information. Put succinctly, similar triples are encoded with similar tensor products, which are often converted into scores. Thereafter, models are trained to assign higher scores to positive triples in the KG and lower scores to negative ones that are not originally present in the data. Following training, the models are then used on some disjoint test set of triples. Negative triples which are scored highly are then regarded as novel positive predictions because they are considered to be similar to the positive samples in the KG [[167](#bib.bibx167), [150](#bib.bibx150), [81](#bib.bibx81)].

The Graph Convolutional Network (GCN) and its variants (e.g., Relational GCN (R-GCN) [[132](#bib.bibx132)] and Graph Attention Network (GAT) [[153](#bib.bibx153)]) have expanded upon this concept with a message-passing paradigm. These methods encode each node in the graph as a weighted combination of itself and its surrounding neighbors, thereby incorporating connectivity information. Thus, information is aggregated across edges at each hidden layer. However, this paradigm possesses several limitations as layers increase, including oversmoothing, the convergence of node representations to the same value, making them less distinguishable [[158](#bib.bibx158), [166](#bib.bibx166), [128](#bib.bibx128)], and oversquashing, the aggregation of too much information into a single vector, resulting in a less informative representation [[9](#bib.bibx9), [39](#bib.bibx39)]. Furthermore, GNNs that utilize the message-passing paradigm tend to aggregate information between dissimilar node types (graph heterophily) without accounting for the varied behaviors or relationships between them. This typically results in diminished or inconsistent performance [[166](#bib.bibx166), [20](#bib.bibx20)]. While a number of promising extensions that capture the semantics of heterogeneous graphs have been studied recently [[137](#bib.bibx137)], there are no strong guarantees that the encoding and aggregation mechanisms will be faithful to the heterogeneous relations in the graph.

Aside from these, KGE methods share other, general limitations. Unlike rule-based approaches, for example, the best performing KGE approaches tend to be black-box models, so no human-level explanations are generated along with predictions [[21](#bib.bibx21)]. Additionally, as many KGE methods utilize supervised learning, they require relatively large amounts of labeled data to capture relationships between entities [[183](#bib.bibx183), [90](#bib.bibx90)]. Several neurosymbolic approaches help to overcome such limitations through characteristics discussed in the next section.

## IV Neurosymbolic Approaches for Reasoning over KGs

As established, we observe a dichotomy between methods for reasoning on graph structures. Symbolic, rule-based methods (Â§[III-B](#S3.SS2 "III-B Rule-based Methods for Knowledge Graph Completion â€£ III A History of Knowledge Graph Completion â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey")) utilize domain knowledge and logic to infer new information in a naturally interpretable fashion. However, such approaches are limited in their performance and scalability to large KGs. In contrast, state-of-the-art approaches for learning KGEs (Â§[III-C](#S3.SS3 "III-C Graph Embedding-based Methods for Knowledge Graph Completion â€£ III A History of Knowledge Graph Completion â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey")) tend to be black-box methods which aggregate information in domain-agnostic ways. As discussed in Â§[II-D](#S2.SS4 "II-D Neurosymbolic AI: A Hybrid Approach â€£ II Background â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"), recent studies in *neurosymbolic AI* often combine aspects from deep learning and symbolic reasoning to mitigate their respective weaknesses and bridge such chasms [[152](#bib.bibx152), [53](#bib.bibx53), [147](#bib.bibx147)]. Within this section, we survey several such approaches. Since each approach hybridizes the two fields in various ways, we aim to formalize and simplify the language we use by referring to each methodâ€™s neural and symbolic modules. A neural module refers to the use of a NN, often to produce KGEs, and a symbolic module typically involves the use of logical rules. A list of the methods and representations each module could comprise, along with specific examples, is given in Table [I](#S4.T1 "TABLE I â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey").

|  |  |  |
| --- | --- | --- |
|  | Neural modules | Symbolic modules |
| Methods: | KGE methods (e.g., ComplEx [[150](#bib.bibx150)], R-GCN [[132](#bib.bibx132)]) | ILP / Logical rule mining (e.g., AMIE [[51](#bib.bibx51)], SAFRAN [[114](#bib.bibx114)]) |
|  | Other NN architectures (e.g., RNN [[139](#bib.bibx139)]) | Knowledge representation reasoners (e.g., OWL reasoner [[100](#bib.bibx100)]) |
| Representations: | KGEs | Ontology (e.g., GO [[7](#bib.bibx7)], DO [[134](#bib.bibx134)]) |
|  | Logic embeddings | Logic program (e.g., Horn clauses [[106](#bib.bibx106)], MLN [[122](#bib.bibx122)]) |

TABLE I: Methodologies that could serve as neural and symbolic modules in a neurosymbolic approach for KGs. Methods are the ways in which the Representations may be obtained. Novel predictions can be made using one or more Representations. Note that logic embeddings could be considered neural or symbolic.

Amongst the neurosymbolic approaches surveyed [[4](#bib.bibx4), [25](#bib.bibx25), [93](#bib.bibx93)], we note the following critical characteristics which are unique from symbolic or neural approaches alone:

#### IV-1 Interpretability

As mentioned, we often see a tradeoff between interpretability, which symbolic AI naturally possesses (in the form of verbal interpretabilityÂ [[149](#bib.bibx149)]), and performance, in which black-box, KGE methods seem to dominate. In particular, enforcing interpretability often comes with a drop in predictive performance [[46](#bib.bibx46), [103](#bib.bibx103)] and might lead to infeasible computational complexity [[11](#bib.bibx11), [46](#bib.bibx46)]. The surveyed neurosymbolic approaches foster interpretability and, in many cases, do not sacrifice performance heavily.

#### IV-2 Guided Training

Some neurosymbolic approaches can integrate ontological or expert-defined knowledge into an otherwise data-driven approach [[4](#bib.bibx4), [161](#bib.bibx161)], bypassing the need for the model to learn known patterns. Furthermore, this guides learning toward more domain-congruous patterns. In some contexts, this is ideal for working with limited or small datasets. As described in Â§[III-C](#S3.SS3 "III-C Graph Embedding-based Methods for Knowledge Graph Completion â€£ III A History of Knowledge Graph Completion â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"), KGE methods tend to require a lot of labeled training data. Often, when a dataset is small, researchers devise relevant pre-training tasks on similar, larger datasets to steer the modelâ€™s parameters toward a pertinent context [[91](#bib.bibx91), [170](#bib.bibx170)]. For example, one study pre-trained a GNN to predict atom-level perturbations in molecules to improve the modelâ€™s overall ability to predict other molecular properties [[79](#bib.bibx79)]. However, pre-training is an additional and often computationally expensive training step. Neurosymbolic approaches may be a practical alternative.

#### IV-3 Underrepresented Types

Even if a labeled dataset is sufficiently large, sparsity or imbalance amongst labels may result in fewer instances of a certain type or subclass within that data. Many KGE approaches struggle to capture such underrepresented patterns [[183](#bib.bibx183), [90](#bib.bibx90)]. Through KG augmentation, some neurosymbolic approaches pose novel ways to address this.

#### IV-4 Heterogeneous Aggregation

Heterogeneous KGs typically comprise nodes and edges of varying types, and these types may interact in different ways [[137](#bib.bibx137)]. As stated in Â§[III-C](#S3.SS3 "III-C Graph Embedding-based Methods for Knowledge Graph Completion â€£ III A History of Knowledge Graph Completion â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"), KGE methods using message-passing may perform inconsistently due to such heterophily. For example, biological KGs like Hetionet [[66](#bib.bibx66)] comprise drugs, proteins, and diseases as nodes. While drug-protein edges may describe direct, physical interactions (e.g., binds), drug-disease edges would constitute a more conceptual relationship (e.g., palliates) [[66](#bib.bibx66)]. Moreover, the features for such nodes would range from chemical structures to lettered sequences [[82](#bib.bibx82)]. The aggregation of features between dissimilar node types is a challenge which could be targeted via neurosymbolic methods: rather than adding complexity to KGE approaches to learn the relationships between node types, they can be represented through rules.

#### IV-5 Long-range Dependencies

Due to oversquashing and oversmoothing (see Â§[III-C](#S3.SS3 "III-C Graph Embedding-based Methods for Knowledge Graph Completion â€£ III A History of Knowledge Graph Completion â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey")), several GNN methods suffer from local receptive fields, experiencing peak performances at two layers [[111](#bib.bibx111)]. Consequently, they struggle to capture long-range dependencies (i.e., relationships between nodes that are several hops apart [[93](#bib.bibx93), [9](#bib.bibx9), [39](#bib.bibx39)]). Rule-based methods, though, can encode such relationships through a series of conjunctions in which each edge in the path is a binary predicate (e.g., friendsâ€‹(A,D)â†likesâ€‹(A,B)âˆ§likesâ€‹(B,C)âˆ§likesâ€‹(C,D)â†friendsğ´ğ·likesğ´ğµlikesğµğ¶likesğ¶ğ·\text{friends}(A,D)\leftarrow\text{likes}(A,B)\land\text{likes}(B,C)\land\text{likes}(C,D)), but inference scales poorly to large KGs. Specifically, Â§[IV-C3](#S4.SS3.SSS3 "IV-C3 Path-Based Rule Learning â€£ IV-C Rule Learning for KGC â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey") describes a series of hybrid approaches which account for long-range dependencies.

Within the remaining sections, we discuss various approaches along with their strengths and weaknesses, referring specifically to these enumerated characteristics as guides for discussion. Importantly, we group the approaches according to the taxonomy in Figure [1](#S4.F1 "Figure 1 â€£ IV-5 Long-range Dependencies â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey") to facilitate easy comparison. It is independent of Kautzâ€™s classification, introduced in Â§[II-D](#S2.SS4 "II-D Neurosymbolic AI: A Hybrid Approach â€£ II Background â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"), as we aim for a finer-grained division, specialized to KGs. However, there is naturally some overlap, so we indicate which types, according to Kautz, are present in each of our classes. Note that Kautzâ€™s types 1 and 2 do not appear in our taxonomy, as none of the surveyed approaches align with them. Additionally, we summarize the main points of this survey within TableÂ [II](#S4.T2 "TABLE II â€£ IV-C3 Path-Based Rule Learning â€£ IV-C Rule Learning for KGC â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey") and provide a curated a repository of available code for each work on GitHub222<https://github.com/NeSymGraphs>.

\forestset

mytree/.style=
baseline,
for tree=mynode,
l sep+=10pt,
edge=gray, thick,
parent anchor=south,
child anchor=north,
align=left,
edge path=
[draw, \forestoptionedge] (!u.parent anchor) â€“ +(0,-10pt) -| (.child anchor)\forestoptionedge label;
{forest}
mytree,
[,phantom
[A: Logically-Informed Embedding Approaches (Â§[IV-A](#S4.SS1 "IV-A Logically-Informed Embedding Approaches â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"))
[Modular, Two-Step Approaches(Â§[IV-A1](#S4.SS1.SSS1 "IV-A1 Modular, Two-Step Approaches â€£ IV-A Logically-Informed Embedding Approaches â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"))

(Neuro:Symbolic â†’â†’\rightarrow Neuro)

â€¢Â Walking RDF and OWL [[4](#bib.bibx4), [1](#bib.bibx1)]

â€¢Â RW-autodrive [[161](#bib.bibx161)]

â€¢Â SoLE [[176](#bib.bibx176)]
]
[Iterative Guidance From the
  
Logic Module(Â§[IV-A2](#S4.SS1.SSS2 "IV-A2 Iterative Guidance from the Logic Module â€£ IV-A Logically-Informed Embedding Approaches â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"))

(Neuro;Symbolic /

Neuro:Symbolic â†’â†’\rightarrow Neuro)

â€¢Â KGE\* [[75](#bib.bibx75)]

â€¢Â UniKER [[28](#bib.bibx28)]

â€¢Â RUGE [[62](#bib.bibx62)]

â€¢Â ReasonKGE [[73](#bib.bibx73)]
]
]
]

{forest}

mytree,
[,phantom
[B: Learning with Logical Constraints (Â§[IV-B](#S4.SS2 "IV-B Learning with Logical Constraints â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"))
[Logical Constraints on
  
the Embedding Space(Â§[IV-B1](#S4.SS2.SSS1 "IV-B1 Logical Constraints on the Embedding Space â€£ IV-B Learning with Logical Constraints â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"))

(Neuro[Symbolic])

â€¢Â R2N [[97](#bib.bibx97)]

â€¢Â SLRE [[61](#bib.bibx61)]

â€¢Â GCR [[23](#bib.bibx23)]

â€¢Â CQDA [[6](#bib.bibx6), [5](#bib.bibx5)]

â€¢Â KeGNN [[160](#bib.bibx160)]

â€¢Â KGER [[102](#bib.bibx102)]

â€¢Â SimplE+ [[49](#bib.bibx49)]
]
[Logical Constraints
  
on the Loss Function(Â§[IV-B2](#S4.SS2.SSS2 "IV-B2 Logical Constraints on the Loss Function â€£ IV-B Learning with Logical Constraints â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"))

(Neuro[Symbolic])

â€¢Â Low-rank Logic

Embeddings [[125](#bib.bibx125)]

â€¢Â KALE [[60](#bib.bibx60)]

â€¢Â ComplEx-NNE\_AER [[40](#bib.bibx40)]

â€¢Â Neuro-Symbolic Entropy

Regularisation [[2](#bib.bibx2)]

â€¢Â GeKCs [[94](#bib.bibx94)]
]
[Learning Representative
  
Logic Embeddings(Â§[IV-B3](#S4.SS2.SSS3 "IV-B3 Learning Representative Logic Embeddings â€£ IV-B Learning with Logical Constraints â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"))

(NeuroSymbolic)

â€¢Â QLogicE [[25](#bib.bibx25)]

â€¢Â JOIE [[64](#bib.bibx64)]

â€¢Â GQE [[63](#bib.bibx63)]

â€¢Â KALE [[60](#bib.bibx60)]
]
]
]

{forest}

mytree,
[,phantom
[C: Rule Learning for KGC (Â§[IV-C](#S4.SS3 "IV-C Rule Learning for KGC â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"))
[EM Algorithm Methods([IV-C1](#S4.SS3.SSS1 "IV-C1 EM Algorithm Methods â€£ IV-C Rule Learning for KGC â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"))

(Neuro;Symbolic) ,l sep+=20pt,
[Learning Rule Weights(Â§[IV-C1](#S4.SS3.SSS1.Px1 "EM Methods â€“ Learning Rule Weights â€£ IV-C1 EM Algorithm Methods â€£ IV-C Rule Learning for KGC â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"))

â€¢Â ExpressGNN [[181](#bib.bibx181)]

â€¢Â pLogicNet [[117](#bib.bibx117)]

â€¢Â pGAT [[65](#bib.bibx65)]

â€¢Â DiffLogic [[138](#bib.bibx138)]

â€¢Â BioGRER [[184](#bib.bibx184)]

â€¢Â IterE [[180](#bib.bibx180)]
,
edge path=
[draw, \forestoptionedge] (!u.parent anchor) â€“ +(0,-20pt) -| (.child anchor)\forestoptionedge label;
]
[Iterative Rule Mining(Â§[IV-C1](#S4.SS3.SSS1.Px2 "EM Methods â€“ Iterative Rule Mining â€£ IV-C1 EM Algorithm Methods â€£ IV-C Rule Learning for KGC â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"))

â€¢Â RNNLogic [[118](#bib.bibx118)]

â€¢Â SN-Hybrid [[146](#bib.bibx146)]

â€¢Â  Rule-IC [[92](#bib.bibx92)]
,
edge path=
[draw, \forestoptionedge] (!u.parent anchor) â€“ +(0,-20pt) -| (.child anchor)\forestoptionedge label;
]
]
[New Rules: Bridging the Discrete
  
and Continuous Spaces(Â§[IV-C2](#S4.SS3.SSS2 "IV-C2 New Rules: Bridging the Discrete and Continuous Spaces â€£ IV-C Rule Learning for KGC â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"))

(Neuro;Symbolic)

â€¢Â NeuralLP [[168](#bib.bibx168)]

â€¢Â DRUM [[129](#bib.bibx129)]

â€¢Â LPRules [[34](#bib.bibx34)]

â€¢Â RuLES [[69](#bib.bibx69)]

â€¢Â RNNLogic [[118](#bib.bibx118)]
]
[Path-Based Rule Learning(Â§[IV-C3](#S4.SS3.SSS3 "IV-C3 Path-Based Rule Learning â€£ IV-C Rule Learning for KGC â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"))

(Neuro;Symbolic /

Neuro:Symbolic â†’â†’\rightarrow Neuro)

â€¢Â PoLo [[93](#bib.bibx93)]

â€¢Â LPRules [[34](#bib.bibx34)]

â€¢Â LNN-MP [[136](#bib.bibx136)]

â€¢Â RNNLogic [[118](#bib.bibx118)]

â€¢Â Transductive Augmentation [[67](#bib.bibx67)]
]
]
]

Figure 1: Taxonomy of neurosymbolic approaches for graph reasoning.

### IV-A Logically-Informed Embedding Approaches

We begin by introducing some of the most intuitive neurosymbolic approaches for reasoning on graph structures. This section is depicted by Fig. [1](#S4.F1 "Figure 1 â€£ IV-5 Long-range Dependencies â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey")-A, and the recently discussed characteristics on [Guided Training](#S4.SS0.SSS2 "IV-2 Guided Training â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey") and [Underrepresented Types](#S4.SS0.SSS3 "IV-3 Underrepresented Types â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey") are most prominent. To combine the benefits of symbolic and neural modules, these approaches modularize the two and then feed the results from the former into the latter. Since symbolic approaches are often based on expert-defined rules, they can be viewed as methods to extend the ground truth. Therefore, inference by the symbolic module is used as a preliminary, KG augmentation step, which feeds into the neural module (typically a KGE method) for further processing and prediction. In some situations, this KG augmentation step could be exploited to expand small datasets or ameliorate dataset imbalance, thereby harnessing the [Underrepresented Types](#S4.SS0.SSS3 "IV-3 Underrepresented Types â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey") characteristic. Because the symbolic knowledge is integrated into the training set, these approaches are much like Kautzâ€™s Neuro:SymbolicÂ â†’â†’\rightarrowÂ Neuro category. We divide these approaches into two subcategories:

#### IV-A1 Modular, Two-Step Approaches

This is depicted by the leftmost subcategory of Fig. [1](#S4.F1 "Figure 1 â€£ IV-5 Long-range Dependencies â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey")-A and demonstrates the first example of the [Underrepresented Types](#S4.SS0.SSS3 "IV-3 Underrepresented Types â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey") characteristic. These approaches involve a simple, unidirectional flow of information from the symbolic module to the neural module. For example, Alshahrani et al. [[4](#bib.bibx4)] developed a biological KG from various sources and employed an ontological reasoner to augment and control the scope of the graph. With the reasoner, a fully deduced graph is thus obtained with newly inferred edges. As illustrated in Fig. [2](#S4.F2 "Figure 2 â€£ IV-A1 Modular, Two-Step Approaches â€£ IV-A Logically-Informed Embedding Approaches â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"), embeddings are subsequently generated from this augmented KG using the DeepWalk algorithm [[115](#bib.bibx115)] (see Â§[III-C](#S3.SS3 "III-C Graph Embedding-based Methods for Knowledge Graph Completion â€£ III A History of Knowledge Graph Completion â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey")). Finally, link prediction is performed using embeddings as input. They tested their pipeline, which they coined Walking RDF and OWL, on a drug repurposing task to determine whether two drugs share indications, represented by edges between drug and disease nodes. Not only did prediction performance improve with the augmented KG, but their model could also accurately predict other relevant links, like side effects. Agibetov and Samwald [[1](#bib.bibx1)] aimed to improve Walking RDF and OWL with an alternative, log-linear embedding method in the neural module. Because the relation types in their KG were all unique, the KG was â€˜flattenedâ€™, making the model faster and easier to train while improving indication prediction further. This follow-up study showed that different combinations of symbolic and neural modules may affect predictions. Consequently, one may wonder how consistently the KG augmentation step improves performance.

[e00,e01,â€¦,e0â€‹n][e10,e11,â€¦,e1â€‹n][â€¦]missing-subexpression

subscriptğ‘’00subscriptğ‘’01â€¦subscriptğ‘’0ğ‘›missing-subexpression

subscriptğ‘’10subscriptğ‘’11â€¦subscriptğ‘’1ğ‘›missing-subexpressiondelimited-[]â€¦\begin{aligned} &[e\_{00},e\_{01},\dots,e\_{0n}]\\
&[e\_{10},e\_{11},\dots,e\_{1n}]\\
&[\dots]\end{aligned}
Ã—\timesLogical inferenceKG embeddingPredictionIterate (?)

Figure 2: Logically-Informed Embedding Approaches (Â§[IV-A1](#S4.SS1.SSS1 "IV-A1 Modular, Two-Step Approaches â€£ IV-A Logically-Informed Embedding Approaches â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey")) These approaches augment the KG with logical inference, then use a KGE method on the augmented KG. As shown by the lowermost arrow, some approaches iterate between logical inference and KGE predictions, using the latter as ground truth for the next iterationâ€™s input (Â§[IV-A2](#S4.SS1.SSS2 "IV-A2 Iterative Guidance from the Logic Module â€£ IV-A Logically-Informed Embedding Approaches â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey")).

Another study by Wickramarachchi et al. [[161](#bib.bibx161)], which we will refer to as RW-autodrive, explored this, using varying neural modules in an autonomous driving application. For this studyâ€™s scene understanding task, data was derived from sensors, such as LIDAR and RADAR, as well as video data taken from the car position. They compared the performances of three different KGE algorithms between both raw KGs and KGs augmented by an ontological reasoner. They showed that, regardless of the KGE algorithm used, better performance was achieved on the augmented KGs, suggesting that incorporation of domain knowledge into KGE methods is especially useful within the autonomous driving domain [[161](#bib.bibx161)].

While incorporating domain-specific knowledge via logical inference often improves model performance, the obvious limitation is the requirement for hard rules in the form of an ontology or expert-defined rule set. Thus, there is no opportunity to represent uncertainty or consider new rules. In contrast, Soft Logical Rules Enhanced Embedding (SoLE) [[176](#bib.bibx176)] utilizes soft rules, as discussed in Â§[II-B](#S2.SS2 "II-B A Brief Introduction to Logic-Based Reasoning â€£ II Background â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"), alongside corresponding confidences learnt from KGs. Essentially, SoLEâ€™s symbolic module acts as a rule engine by mining soft rules from the KG. In the KG augmentation step, new groundings, or triples, are inferred by those soft rules and subsequently combined with existent KG triples. However, SoLE does not scale well to larger KGs due to the rule-mining process; an additional rule-pruning step would be useful, a tactic discussed in Â§[III-B](#S3.SS2 "III-B Rule-based Methods for Knowledge Graph Completion â€£ III A History of Knowledge Graph Completion â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey").

Overall, two-step approaches are clear and straightforward modular structures to compute KGEs, with room for flexibility on both the symbolic and neural sides. However, their usefulness depends upon the availability of relevant rule sets and face scalability issues when rule-mining is involved. Furthermore, these approaches do not take full advantage of the capabilities of either the symbolic or neural modules. In particular, as noted in Â§[IV](#S4 "IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"), many neurosymbolic methods are specifically designed to promote interpretability. However, these two-step methods have the same black-box limitation in their neural modules as any other KGE or GNN approach might since the logic module is only used for KG augmentation.

#### IV-A2 Iterative Guidance from the Logic Module

A few approaches extend the two-step pattern to increase interaction between the modules. We classify these approaches into a second subcategory, depicted in the rightmost subsection of Fig. [1](#S4.F1 "Figure 1 â€£ IV-5 Long-range Dependencies â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey")-A and characterized by [Guided Training](#S4.SS0.SSS2 "IV-2 Guided Training â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"). In the first subcategory, the directionality of information is one-way: logical inference from the symbolic module informs the KGE method, but the reverse is not true; the output of the embedding method is not used to alter the logical inference step. The minimal interaction between the symbolic and neural modules could, therefore, limit the practicality of these methods. In practice, utilizing the output of logical inference as ground truth labels for the neural module could be fallacious as it assumes that logical inference is monotonic, i.e.Â it always yield true results. In reality, expert-defined rules often have exceptions [[53](#bib.bibx53), [50](#bib.bibx50)], so regulating the symbolic module could be useful in some cases.

In light of the above, Kaoudi et al. â€‹â€™s KGE\* [[75](#bib.bibx75)], Cheng et al. â€‹â€™s Unified Framework for Knowledge Graph Inference (UniKER) [[28](#bib.bibx28)], and Guo et al. â€‹â€™s Rule-Guided Embedding (RUGE) [[62](#bib.bibx62)] all follow similar patterns: the two modules inform one another in an iterative style, as shown in Fig. [2](#S4.F2 "Figure 2 â€£ IV-A1 Modular, Two-Step Approaches â€£ IV-A Logically-Informed Embedding Approaches â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"). Both KGE\* and UniKER use Horn rules and forward chaining, a form of logical inference, to augment the KG as in the previous section. Alternatively to Horn rules, KGE\* also supports using an ontology for the inference step, while RUGE uses soft rules. After KG augmentation in the respective symbolic module, a KGE method is trained, and the resulting predictions are used to refine the KG by eliminating the least probable edges and adding the most probable ones. Thereafter, the newly refined KG is passed back to the beginning of the pipeline for the next iteration. One shared shortcoming of the these approaches, however, is that they are designed mainly to generate positive predictions; this can lead to an increased number of false positive predictions. As a solution, another iterative approach, called ReasonKGE [[73](#bib.bibx73)], adds a step in which negative samples are also updated.

By using an iterative process, these methods introduce bidirectionality in which both symbolic and neural modules inform one another by taking turns to refine the KG. One might argue, then, that these approaches fit most closely with Kautzâ€™s third category of Neuro;Symbolic AI, rather than the fourth. However, there is a key caveat: the parameters of the symbolic module are not updated as in the neural module; the rules of the symbolic module are static. Therefore, we still view these approaches as primarily neural. Methods which implement dynamic symbolic modules are discussed later within Â§[IV-C1](#S4.SS3.SSS1 "IV-C1 EM Algorithm Methods â€£ IV-C Rule Learning for KGC â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"). Ultimately, we consider the static symbolic modules in these approaches as simply guiding model training rather than driving it. Nevertheless, guidance via the symbolic module allows these approaches to fulfill the [Guided Training](#S4.SS0.SSS2 "IV-2 Guided Training â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey") characteristic. For example, if a user chooses to implement domain-specific rules, such as that from an ontology, into the symbolic module, then groundings from such rules will be included as input into the neural module. Therefore, the neural module will learn to encode patterns consistent with such rules. These approaches move conceptually closer, then, to those which apply logical constraints onto neural modules, the topic of the next section.

### IV-B Learning with Logical Constraints

In contrast to using two separate modules, a different neurosymbolic pattern on KGs involves imposing the symbolic module, in the form of logical or rule-based constraints, onto the neural module. This pattern corresponds to Fig. [1](#S4.F1 "Figure 1 â€£ IV-5 Long-range Dependencies â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey")-B. In this case, the focus is on training the neural module, but rules are used both to incorporate domain-specific knowledge and to limit the scope of predictions possible. These approaches are, therefore, the epitome of the [Guided Training](#S4.SS0.SSS2 "IV-2 Guided Training â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey") characteristic, as their primary goals include guiding neural training via the symbolic module. For reasons discussed toward the end of this section, these approaches tend to be the weakest with regard to [Interpretability](#S4.SS0.SSS1 "IV-1 Interpretability â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"). However, some are particularly useful for integrating heterogeneous information, i.e. the
[Heterogeneous Aggregation](#S4.SS0.SSS4 "IV-4 Heterogeneous Aggregation â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey") characteristic. This section fits most closely with Kautzâ€™s sixth category of Neuro[Symbolic] AI because the constraints often act as logical checkpoints within the neural engines.

These approaches fundamentally differ based on where logical constraints are imposed in the learning pipeline. Some approaches, discussed in Â§[IV-B1](#S4.SS2.SSS1 "IV-B1 Logical Constraints on the Embedding Space â€£ IV-B Learning with Logical Constraints â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey") below, impose them directly onto the learned embeddings, while other approaches, discussed in Â§[IV-B2](#S4.SS2.SSS2 "IV-B2 Logical Constraints on the Loss Function â€£ IV-B Learning with Logical Constraints â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"), influence model training by restricting the predictions made from embeddings. Â§[IV-B3](#S4.SS2.SSS3 "IV-B3 Learning Representative Logic Embeddings â€£ IV-B Learning with Logical Constraints â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"), for its part, discusses methods that learn entirely separate embeddings for logical constraints.

#### IV-B1 Logical Constraints on the Embedding Space

[e00,e01,â€¦,e0â€‹n][e10,e11,â€¦,e1â€‹n][â€¦]missing-subexpression

subscriptğ‘’00subscriptğ‘’01â€¦subscriptğ‘’0ğ‘›missing-subexpression

subscriptğ‘’10subscriptğ‘’11â€¦subscriptğ‘’1ğ‘›missing-subexpressiondelimited-[]â€¦\begin{aligned} &[e\_{00},e\_{01},\dots,e\_{0n}]\\
&[e\_{10},e\_{11},\dots,e\_{1n}]\\
&[\dots]\end{aligned}
Â§[IV-B1](#S4.SS2.SSS1 "IV-B1 Logical Constraints on the Embedding Space â€£ IV-B Learning with Logical Constraints â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey") Constraints onthe embedding space.Â§[IV-B2](#S4.SS2.SSS2 "IV-B2 Logical Constraints on the Loss Function â€£ IV-B Learning with Logical Constraints â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey") Constraints onthe loss function.KG embeddingPrediction

Figure 3: Learning with Logical Constraints. (Â§[IV-B1](#S4.SS2.SSS1 "IV-B1 Logical Constraints on the Embedding Space â€£ IV-B Learning with Logical Constraints â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey")) Logical constraints on the embedding space (first filter). These methods drive training by imposing logical constraints onto the embedding space, such as through a transformation. (Â§[IV-B2](#S4.SS2.SSS2 "IV-B2 Logical Constraints on the Loss Function â€£ IV-B Learning with Logical Constraints â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey")) Alternatively, logical constraints on the loss function (second filter). These methods drive which predictions are made by encoding logical constraints into the loss function, such as with a penalty term.

One way to approach imposing logical constraints onto KGE methods is to alter the embedding space in some meaningful way, as depicted in Fig. [3](#S4.F3 "Figure 3 â€£ IV-B1 Logical Constraints on the Embedding Space â€£ IV-B Learning with Logical Constraints â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey") and the leftmost branch of the taxonomy in Fig. [1](#S4.F1 "Figure 1 â€£ IV-5 Long-range Dependencies â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey")-B. Such approaches are also particularly useful for the [Guided Training](#S4.SS0.SSS2 "IV-2 Guided Training â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey") and [Heterogeneous Aggregation](#S4.SS0.SSS4 "IV-4 Heterogeneous Aggregation â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey") characteristics. The Relational Reasoning Network (R2N) [[97](#bib.bibx97)], for example, uses structural relational information as constraints on GNN training, sharing quite similar goals to the previously mentioned R-GCN [[132](#bib.bibx132)] (Â§[III-C](#S3.SS3 "III-C Graph Embedding-based Methods for Knowledge Graph Completion â€£ III A History of Knowledge Graph Completion â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey")). Using the phrase â€œrelational reasoning in the latent spaceâ€ to describe their process, they transform the latent space embeddings in a way that is dependent upon each nodeâ€™s neighbors in the original space. They argue that the transformation produces KGEs encoding information about both the individual nodes and the surrounding graph topology. In a similar manner to R2N, Soft Logical Regularity in Embeddings (SLRE) [[61](#bib.bibx61)] uses soft logical rules as constraints on relation embeddings. These rules are fused into the KGE process by imposing rule-based regularization onto generated embeddings. Consequently, embeddings are potentially more useful for generating predictions aligned with domain-specific or logic-based paradigms.

The emphasis on using relational information seen in R2N and SLRE is also key to a study by H. Chen et al. [[23](#bib.bibx23)], who propose Graph Collaborative Reasoning (GCR). The GCR extends the vanilla GNN to encode whole triples, rather than nodes alone, focusing their analysis on adjacent edges. Conceptually, they transform the KG structure into a series of logical expressions which are used to predict the probability that a novel edge is implied by its adjacent edges. This re-frames the link prediction problem as a neural logic reasoning one. More specifically, they accomplish this by applying logical regularizers to MLPs to simulate logical operations; these MLPs are then applied to triple embeddings to generate predictions. Furthermore, to avoid the requirement of specifying rules by hand, they introduce a method to learn potential rules, formulated as Horn clauses, from the available training triples; this benefit foreshadows an approach presented in Â§[IV-C](#S4.SS3 "IV-C Rule Learning for KGC â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"). Additionally, the authors claim that their method is more scalable to large, complex KGs than other neurosymbolic methods. However, we note that the scope of applications on which their method might be applicable could be limited. The logical expressions by which they perform link prediction rely on the idea that closely connected neighbors likely share similar interaction patterns. While this is often true for social KGs, it may not suit other domains in which entities with similar patterns might avoid interacting closely.

Two additional studies, Knowledge Enhanced Graph Neural Networks (KeGNN) [[160](#bib.bibx160)] and Continuous Query Decomposition (CQD) [[6](#bib.bibx6)] also view KGs in terms of logical expressions. Both demonstrate that, after using KGE methods for simple link prediction, tğ‘¡t-norms (see Â§[II-B](#S2.SS2 "II-B A Brief Introduction to Logic-Based Reasoning â€£ II Background â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey")) and tğ‘¡t-conorms (for disjunctions) can be applied to embeddings to answer more complex queries comprising multiple links. In theory, this approach could be useful for [Long-range Dependencies](#S4.SS0.SSS5 "IV-5 Long-range Dependencies â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey") if the queries involve a series of conjunctions; this idea is elaborated upon in Â§[IV-C3](#S4.SS3.SSS3 "IV-C3 Path-Based Rule Learning â€£ IV-C Rule Learning for KGC â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"). In particular, CQD demonstrates its usefulness for queries of up to eight links. Its extension, CQDA [[5](#bib.bibx5)], imposes learnable adaptation functions to accommodate for the interactions between parts of a complex query. Such adaptation functions could be a way to cope with graph heterophily, mentioned in Â§[III-C](#S3.SS3 "III-C Graph Embedding-based Methods for Knowledge Graph Completion â€£ III A History of Knowledge Graph Completion â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey").

Minervini et al. also explore the use of logical constraints as regularizers with an approach we will refer to as KGER [[102](#bib.bibx102)]. Specifically, they apply constraints to encode information about relations that may mean the same thing, such as PartOf and ComponentOf, through a model-dependent transformation on the embeddings. They derive these transformations for three different KGE methods and discover that they all yield more accurate link predictions with regularization. As with SLRE, applying constraints directly to the embeddings was found to be effective in integrating domain knowledge without impairing scalability [[102](#bib.bibx102)]. However, unlike SLRE, KGER is not model agnostic, requiring the user to derive a useful transformation for each individual KGE method. We note that this also poses the risk that the regularization method might not generalize well to any given embedding method.

Instead of model-dependent regularization, both SimplE+ [[49](#bib.bibx49)] and SLRE enforce constraints on embeddings by requiring non-negativity. The authors of SimplE+ explain that their motivation in using non-negativity, specifically, is to enforce the subsumption axiom from OWLâ€™s semantics [[100](#bib.bibx100)]. Essentially, subsumption determines whether something is a subclass or subproperty of another. By enforcing that generated entity embeddings must be non-negative, they prove that subsumption is enforced. Furthermore, they also impose a constraint on relations which can be subsumed from one another. Their method outperformed simple logical inference as well as the basic embedding method without non-negativity constraints [[49](#bib.bibx49)].

Imposing logical constraints onto the embedding space is ideal when one wishes to encode specific information about the relational nature of the graph. In particular, incorporating relational or ontological relationships into KGEs could be considered as a solution to the challenges posed by heterogeneous KGs, as described in Â§[IV](#S4 "IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"). These neurosymbolic approaches take into account the unique and varied relationships between different node types when generating predictions, thus harnessing the [Heterogeneous Aggregation](#S4.SS0.SSS4 "IV-4 Heterogeneous Aggregation â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey") characteristic. However, depending upon the needs of the researcher, such constraints may be too restrictive. Instead of applying constraints onto the embedding space, thereby limiting the types of patterns encoded, another subcategory of approaches, which we discuss in the next section, apply constraints to the processes of decoding the embeddings and generating predictions.

#### IV-B2 Logical Constraints on the Loss Function

A drawback of popular KGE algorithms (namely RESCAL, TransE, HolE, and ComplEx, as discussed in Â§[IV-A](#S4.SS1 "IV-A Logically-Informed Embedding Approaches â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey")) is that they only implicitly learn higher-order relationships among triples in the KG, requiring more data to learn its latent logical structure. Moreover, this learned structure is confined to the ground triples available during training. These limitations can be overcome by the next subcategory of approaches, depicted by the centre branch of Fig. [1](#S4.F1 "Figure 1 â€£ IV-5 Long-range Dependencies â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey")-B. These use logical constraints to guide training toward predictions which align with some knowledge base or logical entailment (Fig. [3](#S4.F3 "Figure 3 â€£ IV-B1 Logical Constraints on the Embedding Space â€£ IV-B Learning with Logical Constraints â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey")), aligning with the [Guided Training](#S4.SS0.SSS2 "IV-2 Guided Training â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey") characteristic. Here, this is accomplished by altering the loss function.

For example, RocktÃ¤schel et al. [[125](#bib.bibx125)] integrate logical constraints as differentiable loss functions, calling their approach Low-rank Logic Embeddings (LRLE). Given logical rules of the form

|  |  |  |
| --- | --- | --- |
|  | relation\_aâ€‹(X,Y)â†’relation\_bâ€‹(X,Y),â†’relation\_ağ‘‹ğ‘Œrelation\_bğ‘‹ğ‘Œ\text{relation\\_a}(X,Y)\rightarrow\text{relation\\_b}(X,Y), |  |

two distinct processes are carried out. First, the rules are applied to the existing training triples to generate new ones, as described in Â§[IV-A](#S4.SS1 "IV-A Logically-Informed Embedding Approaches â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"). Additionally, this encourages the learned embeddings to encode inter-relation structures. Second, a unified set of differentiable loss functions corresponding to both the set of training triples and the logical rules is created using fuzzy tğ‘¡t-norms (see Â§[II-B](#S2.SS2 "II-B A Brief Introduction to Logic-Based Reasoning â€£ II Background â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey")). This creates a joint optimization problem over capturing factual information while obeying the given constraints. A summation of the individual losses is formulated as a log-likelihood loss so that embeddings which assign a high marginal probability to rule and triple satisfaction are preferred. The authors argue that the framing of constraint satisfaction as a probabilistic model allows the embeddings to be unaffected by noisy data.

However, LRLE is limited in that it only models existent relations and entity-pairs. Consequently, rules will not be discovered for entity-pairs that do not appear in the training data. This shortcoming is significant as most KGs are inherently incomplete. This drawback is tackled by Guo et al. â€‹â€™s method, Embeddings by jointly modeling Knowledge And Logic (KALE) [[60](#bib.bibx60)], which performs the same unification of triples and rules, but also explicitly models entities on their own. This allows for novel relations between entities to be discovered. KALE extends TransE and maintains the same competitive time and space complexity thereof. Moreover, while their approach uses a pairwise ranking loss, the authors note that the generality of their approach facilitates the use of different losses such as with LRLE.

[e00,e01,â€¦,e0â€‹n][e10,e11,â€¦,e1â€‹n][â€¦]missing-subexpression

subscriptğ‘’00subscriptğ‘’01â€¦subscriptğ‘’0ğ‘›missing-subexpression

subscriptğ‘’10subscriptğ‘’11â€¦subscriptğ‘’1ğ‘›missing-subexpressiondelimited-[]â€¦\begin{aligned} &[e\_{00},e\_{01},\dots,e\_{0n}]\\
&[e\_{10},e\_{11},\dots,e\_{1n}]\\
&[\dots]\end{aligned}
KG embeddingConstraintsConstraintsLogic embedding[c00,c01,â€¦,c0â€‹n][c10,c11,â€¦,c1â€‹n][â€¦]missing-subexpression

subscriptğ‘00subscriptğ‘01â€¦subscriptğ‘0ğ‘›missing-subexpression

subscriptğ‘10subscriptğ‘11â€¦subscriptğ‘1ğ‘›missing-subexpressiondelimited-[]â€¦\begin{aligned} &[c\_{00},c\_{01},\dots,c\_{0n}]\\
&[c\_{10},c\_{11},\dots,c\_{1n}]\\
&[\dots]\end{aligned}
Prediction

Figure 4: Learning Representative Logic Embeddings. Some methods encode logical constraints as embeddings, then combine them with KGEs to make more informed predictions.

Unfortunately, the last two approaches have the additional computational burden of grounding universally quantified rules before training. This requirement hinders their ability to scale to large KGs with many rules, a weakness reminiscent of the methods within Â§[IV-A](#S4.SS1 "IV-A Logically-Informed Embedding Approaches â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"). In contrast, Ding et al. [[40](#bib.bibx40)] build upon the ComplEx algorithm, creating ComplEx-NNE\_AER, to include non-negativity constraints on entity representations and approximate entailment constraints on relation representations. These constraints are encoded through a penalty term applied to the objective function. The authors argue that these constraints are sufficient to impose a compact, informative prior structure of the KG on the embedding space, without negatively affecting efficiency or scalability. Most importantly, their approach does not require rule grounding like the previously mentioned ones. Unfortunately, however, constraints are incorporated into the loss in a way that does not support general first-order logic rules like that of LRLE or KALE. We thus surmise that their approach might perform comparatively worse on more complex KG structures.

More generally, Ahmed et al. [[2](#bib.bibx2)] encode constraints as a loss function that can be applied to any NN model, including GNNs. In an approach termed Neuro-Symbolic Entropy Regularisation, the authors use semantic loss [[165](#bib.bibx165)] as a measure of how much a first-order logic formula is satisfied by the output of a model. In contrast with methods which use fuzzy logic [[125](#bib.bibx125)], the semantic loss is a probabilistic definition measuring the likelihood that the output will satisfy the constraint, given the induced probability distribution over the trained NNâ€™s outputs. The semantic loss is then combined with entropy regularization. This encourages NN predictions to conform to a structure which satisfies the specified constraints while also ensuring more distinct decision boundaries in link prediction. Similarly, Generative KGE Circuits (GeKCs) [[94](#bib.bibx94)] takes advantage of probabilistic circuits [[155](#bib.bibx155)] to ensure that hard constraints are met [[3](#bib.bibx3)]; in other words, unlike semantic loss, which encourages conformity to constraints, GeKCs guarantee that predictions will satisfy constraints.

Within this and the previous subsection, the pipeline in which KGEs are trained is linear (see Fig. [3](#S4.F3 "Figure 3 â€£ IV-B1 Logical Constraints on the Embedding Space â€£ IV-B Learning with Logical Constraints â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey")), with the logical constraints being injected into the pipeline at some point. Alternatively, however, logical constraints could be treated as an independent source of information to be encoded in parallel to the KG. Within the next subsection, we discuss studies that explore that possibility.

#### IV-B3 Learning Representative Logic Embeddings

Within the last subcategory of learning with logical constraints, represented by the rightmost branch of Fig. [1](#S4.F1 "Figure 1 â€£ IV-5 Long-range Dependencies â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey")-B, some approaches work by learning representative embeddings for logical constraints. Then, they systematically combine them with the KGEs before generating predictions. This idea is illustrated in Fig. [4](#S4.F4 "Figure 4 â€£ IV-B2 Logical Constraints on the Loss Function â€£ IV-B Learning with Logical Constraints â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"). Similar to the previous subcategories, it helps to drive training toward established and possibly domain-specific knowledge, harnessing the [Guided Training](#S4.SS0.SSS2 "IV-2 Guided Training â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey") characteristic. For example, the Quantum Logic Empowered Embedding (QLogicE) method [[25](#bib.bibx25)] uses quantum logic [[12](#bib.bibx12)] to create logic embeddings while simultaneously computing KGEs. The scoring function for each possible triple is a weighted sum of the scores for the two embedding methods, so the overall loss is also computed as a weighted composition of the two respective losses. The weights of the scoring and loss functions determine what proportion of each embedding method to take into account. Such a tactic is used within the method, Joint Embedding of Instances and Ontological Concepts (JOIE) [[64](#bib.bibx64)]. Here, two separate embedding spaces are generated for the instantiated triples and the underlying ontological structure, so a joint loss is calculated. In JOIE, however, they found that performance is improved even further by including a third set of embeddings on a combined graph structure comprising both instantiated triples and ontological relations. Despite improved performance, one might wonder whether JOIE moves away from achieving an interpretable model. In contrast, a study by Hamilton et al. [[63](#bib.bibx63)] combines logic and node embedding spaces in a way that the logic embeddings represent a query. Their method, Graph Query Embeddings (GQE), uses geometric operations which represent logical operators to compute query embeddings. Thereafter, the likelihood that a set of nodes satisfy a query is calculated as the cosine similarity between the query embeddings and the respective node embeddings. This method essentially performs complex logical queries on a graph, making it similar to Kautzâ€™s second category of Symbolic[Neuro] AI.

Similarly to the above approaches, KALE [[60](#bib.bibx60)], mentioned previously, learns embeddings based on both the triples and logical rules. Essentially, they train a KGE algorithm while computing truth values for logical rules based on the presence of instantiated triples. Training is based on loss encompassing both triples and logical formulae. While KALE fits most closely into this category of logical constraints, one could also use the truth values of the logical rules as confidences. In Â§[IV-C](#S4.SS3 "IV-C Rule Learning for KGC â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"), we discuss the possibility of assigning confidence values to rules in order to quantify relative importance. Understanding which rules are most important in generating predictions can be a valuable source of interpretability. However, the authors of KALE do not explore this possibility.

In general, since all of the approaches described within this category use the symbolic module to guide training of the neural module, they have the potential to replace pre-training in some circumstances, as suggested in the [Guided Training](#S4.SS0.SSS2 "IV-2 Guided Training â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey") characteristic. Specifically, some studies use pre-training to steer a neural moduleâ€™s parameters toward a relevant context; this is typically due to a lack of sufficient data pertaining to the task at hand [[91](#bib.bibx91), [170](#bib.bibx170)]. In this case, the symbolic module accomplishes a similar function, guiding the neural moduleâ€™s parameters during training, as opposed to before training.

However, these approaches are weakest with regard to [Interpretability](#S4.SS0.SSS1 "IV-1 Interpretability â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey") as none of them necessarily provide insight as to why certain predictions were made over others. In fact, one could argue that the approaches which learn logic embeddings remove the inherent interpretability that is typically so fundamental to using logic. To fully leverage this property, the symbolic and neural components of an architecture should be combined in a way that they not only inform one another but also in a way that user-level explanations are generated. In the next section, we review a variety of approaches which aim to modify or learn new logical rules to do just that.

### IV-C Rule Learning for KGC

E-stepM-step[e00,e01,â€¦,e0â€‹n][e10,e11,â€¦,e1â€‹n][â€¦]missing-subexpression

subscriptğ‘’00subscriptğ‘’01â€¦subscriptğ‘’0ğ‘›missing-subexpression

subscriptğ‘’10subscriptğ‘’11â€¦subscriptğ‘’1ğ‘›missing-subexpressiondelimited-[]â€¦\begin{aligned} &[e\_{00},e\_{01},\dots,e\_{0n}]\\
&[e\_{10},e\_{11},\dots,e\_{1n}]\\
&[\dots]\end{aligned}
(p1)Rule 1(p2)Rule 2â‹¯new rules?missing-subexpressionsubscriptğ‘1missing-subexpressionRule 1missing-subexpressionsubscriptğ‘2missing-subexpressionRule 2missing-subexpressionmissing-subexpressionmissing-subexpressionâ‹¯missing-subexpressionmissing-subexpressionmissing-subexpressionnew rules?\begin{aligned} &(p\_{1})&&\text{Rule 1}\\
&(p\_{2})&&\text{Rule 2}\\
&&&\cdots\\
&&&\text{new rules?}\end{aligned}
KG embeddingPredictionWeight updatePredictionKG Embedding

  

Figure 5: EM Algorithm Methods. These approaches adopt the idea of iterative optimization from the EM algorithm [[37](#bib.bibx37)] to describe systems which iterate between symbolic and neural modules. Typically, the E-step involves KGC through a KGE method (the neural module), and the M-step involves updating the parameters of the symbolic module. To make the symbolic module (typically in the form of a MLN or rule mining approach) dynamic, some methods update rule confidences (Â§[IV-C1](#S4.SS3.SSS1.Px1 "EM Methods â€“ Learning Rule Weights â€£ IV-C1 EM Algorithm Methods â€£ IV-C Rule Learning for KGC â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey")). Other methods update and alter a pool of candidate rules (Â§[IV-C1](#S4.SS3.SSS1.Px2 "EM Methods â€“ Iterative Rule Mining â€£ IV-C1 EM Algorithm Methods â€£ IV-C Rule Learning for KGC â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey")). Note that approaches in this category may deviate from this portrayal.

In the previous sections, all of the described methodologies used predefined rules for KGC. However, quite like the MLN, approaches in this final category attempt to learn such rules. Such approaches are represented in Fig. [1](#S4.F1 "Figure 1 â€£ IV-5 Long-range Dependencies â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey")-C, and, collectively, they possess all of the five characteristics listed in Â§[IV](#S4 "IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"). Overall, most of these methods train some neural module to learn confidences for rules or systematically adjust a rule mining scheme which constitutes the symbolic module. The presence of dynamic symbolic modules makes these approaches unique from all previously described categories. In particular, it makes this category of approaches strong with regard to the [Interpretability](#S4.SS0.SSS1 "IV-1 Interpretability â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey") characteristic. As we will see next, the majority of these approaches fall into one or more subcategories based on similarities in their general architectures as well as the ways in which rules are learned.

#### IV-C1 EM Algorithm Methods

Many rule-learning methods claim to take an Expectation-Maximization algorithm (EM) based approach, alternating between two modules whose outcomes inform each other. These approaches are listed in the leftmost branch of Fig. [1](#S4.F1 "Figure 1 â€£ IV-5 Long-range Dependencies â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey")-C and distinguished by the [Interpretability](#S4.SS0.SSS1 "IV-1 Interpretability â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"), [Guided Training](#S4.SS0.SSS2 "IV-2 Guided Training â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"), and [Underrepresented Types](#S4.SS0.SSS3 "IV-3 Underrepresented Types â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey") characteristics. Put succinctly, the EM algorithm is used to estimate the maximum likelihood of model parameters for situations in which there is incomplete data [[37](#bib.bibx37)]. KGC, then, is an ideal and obvious setting for such methods. With varying degrees of faithfulness to the original EM algorithm proposed by Dempster et al. [[37](#bib.bibx37)], the approaches in this section all, generally, claim to adopt the iterative nature of the EM algorithm. More specifically, each approach alternates between the prediction of missing components (the â€œE-stepâ€), typically in the form of KGC, and the optimization of model parameters to account for such predictions (the â€œM-stepâ€). Normally, the parameters to be optimized belong to the symbolic module, which can be understood in this section as a dynamic framework involving logical rules (e.g., a MLN or rule mining system). These methods vary, however, in their interpretations of how an EM-based algorithm might be executed for KGC.

##### EM Methods â€“ Learning Rule Weights

The most straightforward approach for creating a dynamic symbolic module involves assigning weights, scores, or confidences to logical rules, which denote relative importance, then imposing incremental updates. Similar to approaches described in Â§[IV-A2](#S4.SS1.SSS2 "IV-A2 Iterative Guidance from the Logic Module â€£ IV-A Logically-Informed Embedding Approaches â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"), several EM methods use two complementary modules to inform one another, encapsulating the [Guided Training](#S4.SS0.SSS2 "IV-2 Guided Training â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey") characteristic, but the symbolic module is now dynamic, adjusting the rule base at each iteration (Fig. [5](#S4.F5.fig1 "Figure 5 â€£ IV-C Rule Learning for KGC â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey")). For instance, ExpressGNN [[181](#bib.bibx181)], pLogicNet [[117](#bib.bibx117)], pGAT [[65](#bib.bibx65)], and DiffLogic [[138](#bib.bibx138)] all incorporate a MLN into the symbolic module of their algorithms to learn corresponding weights, or confidences, for logical rules. During the E-step, a KG augmented by logical inference is utilized to train a KGE method. Recall that the primary goal of the KGE method is to distill patterns from the input graph into representative embeddings, from which the KG can be reconstructed along with its missing, or latent, components. Thereafter, during the M-step, the newly predicted triples from the KGE method are used to inform and update the weights of the MLN.

The above methods have only minor differences: for example, while ExpressGNN uses a vanilla GNN as the embedding method [[181](#bib.bibx181)], pGAT works with a variant of the GAT [[153](#bib.bibx153)] (see Â§[III-C](#S3.SS3 "III-C Graph Embedding-based Methods for Knowledge Graph Completion â€£ III A History of Knowledge Graph Completion â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey")), utilizing the attention coefficients as an additional source of semantic information [[65](#bib.bibx65)]. Operating similarly to pLogicNet and pGAT, Zhao et al. â€‹â€™s method, Biomedical KG refinement with Embedding and Rules (BioGRER) [[184](#bib.bibx184)], is additionally specialized to operate on a biological KGs. To do so, it incorporates domain-specific knowledge into logical rules. However, unlike the aforementioned approaches, while the rule-learning module is highly similar to a MLN, it is not explicitly based on a MLN.

In contrast, Zhang et al.â€™s IterE [[180](#bib.bibx180)] does not use a MLN. IterE initializes its symbolic module as a pool of randomly selected OWL-based axioms, followed by grounding. Axioms with more than one instance are selected for the final rule set. As in the previous methods, new triples inferred by the symbolic module are used as ground truth for training the neural module. Likewise, scores in the symbolic module are updated based on each iterationâ€™s KGEs. Unlike previous approaches, however, the KGEs are used directly for score updates, skipping the prediction step in between.

##### EM Methods â€“ Iterative Rule Mining

While pLogicNet, pGAT, IterE, and BioGRER learn and update rule weights, the rule set itself never changes. In contrast, Suresh and Nevilleâ€™s Hybrid Method (SN-Hybrid) [[146](#bib.bibx146)], Lin et al. â€‹â€™s Rule-enhanced Iterative Complementation (Rule-IC) [[92](#bib.bibx92)], and Qu et al. â€‹â€™s RNNLogic [[118](#bib.bibx118)] mine fresh rules over iterations, rather than using the same rules and adjusting confidences. In this sense, these methods are better aligned with the goals of ILP (Â§[II-B](#S2.SS2 "II-B A Brief Introduction to Logic-Based Reasoning â€£ II Background â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey")). In other words, KGE-derived predictions from the neural modules are used to reduce the rule mining search space and alter the pool of candidate rules, which are subsequently used for logical inference in the symbolic modules. To illustrate this, we refer the reader once again to Fig. [5](#S4.F5.fig1 "Figure 5 â€£ IV-C Rule Learning for KGC â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey") with the notion that the rule set is iteratively altered (hence the â€œnew rules?â€ option within the rule set). Notably, Rule-ICâ€™s rule set is ontology-based, leading to richer KG semantics, while SN-Hybrid and RNNLogic mine general Horn rules.

Another key difference between the rule mining processes used within these approaches is the ways in which they determine the pool of candidate rules or axioms. Rule-IC, for example, constructs its rule set similarly to the previous sectionâ€™s IterE, but it uses a computed confidence threshold rather than a frequency threshold [[92](#bib.bibx92)]. In SN-Hybrid, however, a pool of candidate rules is generated using an algorithm akin to AMIE [[51](#bib.bibx51)], in which partial rules are queued and then extended until they become proper rules. The candidate rules are later pruned via a combination of standard confidence measures, a measure of the ability to explain the existing triples, and the confidences determined through the neural module [[146](#bib.bibx146)].

This categoryâ€™s essential quality of back-and-forth guidance between symbolic and neural modules makes it most similar to Kautzâ€™s third category of Neuro;Symbolic AI. Consequently, one major perk of EM-based algorithms is the way they account for underrepresented relation types. Often, if a relation type occurs less frequently, KGE methods neglect to account for it, as there are fewer examples upon which to train, and the dataset imbalance creates bias toward more frequent relation types. Various KGE methods attempt to account for this via adjusted sampling or regularization methods [[132](#bib.bibx132)]. Iterative methods could be used to increase the occurrence of underrepresented relation types during training. Specifically, one might consider the inference step in the symbolic modules as a way to increase the number of positive edges being fed into the embedding module. For example, if the method utilizes rule confidences, such as in Â§[IV-C1](#S4.SS3.SSS1.Px1 "EM Methods â€“ Learning Rule Weights â€£ IV-C1 EM Algorithm Methods â€£ IV-C Rule Learning for KGC â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"), one could initialize or fix rules regarding rare relation types with high confidences so that the symbolic module predicts more high-probability instances of that type. Therefore, this subcategory of approaches handles small or imbalanced datasets well, capturing the [Underrepresented Types](#S4.SS0.SSS3 "IV-3 Underrepresented Types â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey") characteristic. We explore this prospective direction further within Â§[V-B5](#S5.SS2.SSS5 "V-B5 Few Shot Learning â€£ V-B Prospective Directions â€£ V Limitations & Prospective Directions â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey").

#### IV-C2 New Rules: Bridging the Discrete and Continuous Spaces

In the previous subcategory, we introduced approaches with dynamic symbolic modules. Specifically, these approaches either made iterative confidence updates to predefined or mined rules (Â§[IV-C1](#S4.SS3.SSS1.Px1 "EM Methods â€“ Learning Rule Weights â€£ IV-C1 EM Algorithm Methods â€£ IV-C Rule Learning for KGC â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey")) or mined a fresh set of rules based on the output of the neural module (Â§[IV-C1](#S4.SS3.SSS1.Px2 "EM Methods â€“ Iterative Rule Mining â€£ IV-C1 EM Algorithm Methods â€£ IV-C Rule Learning for KGC â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey")). However, previous approaches mined and filtered rules based on predefined heuristics, including the number or proportion of groundings [[146](#bib.bibx146)] or alignment with certain OWL axioms [[92](#bib.bibx92)]. Such heuristics may not be ideal for various applications, thereby limiting the generalizability of such methods. Alternatively, the neural module can be used directly for the task of generating and selecting descriptive rules. This leads to approaches, depicted in the centre branch of the taxonomy (Fig. [1](#S4.F1 "Figure 1 â€£ IV-5 Long-range Dependencies â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey")-C), that aim to learn both structural information in a discrete space and respective weight parameters in a continuous space [[168](#bib.bibx168)]. The creation of a method that can do both of these and train in an end-to-end fashion is inherently difficult [[168](#bib.bibx168), [129](#bib.bibx129)]. However, the payoff is an enhanced [Interpretability](#S4.SS0.SSS1 "IV-1 Interpretability â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey") characteristic.

Methods have been developed in response to suggest new ways to perform differentiable logic-based reasoning. TensorLog [[30](#bib.bibx30)], for example, encodes graph entities into vectors and unique relations into respective adjacency matrices. Logical inference can then be imitated as the product of the adjacency matrices for the relations in the body of a rule with the vector of a given entity. This produces a vector in which nonzero entries represent entities for which the rule holds true. Neural LP [[168](#bib.bibx168)] integrates TensorLog into an approach which additionally learns rule confidences in an end-to-end fashion. Specifically, it learns confidence values not just for every rule but for every relation involved in every rule, even accounting for differences in the varying lengths of rule bodies. DRUM [[129](#bib.bibx129)] is a refined version of Neural LP in which the authors point out that Neural LP learns high confidences for incorrect rules. To overcome this problem and reduce parameters, DRUM incorporates a bidirectional RNN to capture forward and backward information about possible pairings of relations, as not all relations can actually coexist in the same body of a rule. As a consequence, DRUM outperforms Neural LP and even some black-box embedding-based methods at link prediction within various datasets [[129](#bib.bibx129)].

In contrast, the authors of LPRules [[34](#bib.bibx34)] take a slightly different approach to learning rules and confidences simultaneously. Their approach generates a weighted combination of logical rules for link prediction by iteratively augmenting a small starter pool of candidate rules. It differs from the aforementioned methods which learn confidences for the entire set of possible rules because it only works with a subset of rules at any given time, thereby reducing the search space and optimizing the time it takes to find a set of rules which are highly predictive [[34](#bib.bibx34)]. Another method, RuLES [[69](#bib.bibx69)], operates very similarly, augmenting its rule set iterativey by constructing and extending rules with additional atoms and logical refinement operators. RuLES is arguably more sophisticated, though, because the quality of candidate rules is checked against pre-computed KGEs and, optionally, text embeddings. Notably, LPRules and RuLES are comparable to the EM-based algorithms because the output of previously generated rules influences the generation of rules in the next iteration, much like that of RNNLogic [[118](#bib.bibx118)]. In fact, similarly to RNNLogic, LPRules initializes its small pool of starter candidate rules via path-based heuristics. Accordingly, RNNLogic could also be classified within this category as it uses a NN variant for rule generation.

Like the EM-based methods, this category is also most similar to Kautzâ€™s third classification of Neuro;Symbolic AI. However, the major benefit of Neural LP, DRUM, and LPRules is that they train end-to-end. Unlike IterE [[180](#bib.bibx180)] and SN-Hybrid [[146](#bib.bibx146)], which use the neural module to guide rule mining, these approaches are not limited to predefined heuristics. As a major limitation, however, Neural LP and DRUM only train on positive examples and have yet to be tried on a dataset with negative examples. In many applications, positive predictions are more interesting than negative predictions. However, a lack of negative examples in the training data increases the risk of false positive predictions, which, in some contexts, such as drug-target prediction, could lead to a waste of time, money, and resources or potentially even health hazards.

#### IV-C3 Path-Based Rule Learning

Path-based rules(p1)(A,r1,D)â†(A,r2,B)âˆ§(B,r4,C)âˆ§(C,r3,D)(p2)(A,r6,C)â†(A,r5,B)âˆ§(B,r4,C)â‹¯missing-subexpressionmissing-subexpressionmissing-subexpressionPath-based rulesmissing-subexpressionsubscriptğ‘1missing-subexpressionâ†ğ´subscriptğ‘Ÿ1ğ·limit-fromğ´subscriptğ‘Ÿ2ğµmissing-subexpressionmissing-subexpressionmissing-subexpressionğµsubscriptğ‘Ÿ4ğ¶ğ¶subscriptğ‘Ÿ3ğ·missing-subexpressionsubscriptğ‘2missing-subexpressionâ†ğ´subscriptğ‘Ÿ6ğ¶limit-fromğ´subscriptğ‘Ÿ5ğµmissing-subexpressionmissing-subexpressionmissing-subexpressionğµsubscriptğ‘Ÿ4ğ¶missing-subexpressionmissing-subexpressionmissing-subexpressionâ‹¯\begin{aligned} &&&\text{Path-based rules}\\[5.0pt]
&(p\_{1})&&(A,r\_{1},D)\leftarrow(A,r\_{2},B)\ \land\\
&&&(B,r\_{4},C)\ \land\ (C,r\_{3},D)\\[5.0pt]
&(p\_{2})&&(A,r\_{6},C)\leftarrow(A,r\_{5},B)\ \land\\
&&&(B,r\_{4},C)\\[5.0pt]
&&&\cdots\end{aligned}
Path explorationPrediction

Figure 6: Path-Based Rule Learning. These methods generate rules based on paths in the KG to encode long-range dependencies. Path exploration is sometimes guided by a NN or KGE method, and confidence scores are often computed for rules.

Most of the previous approaches which mine or generate rules do so in a way that treats the latter as statements involving individual relation types. However, there exists another branch of work, the rightmost part of Fig. [1](#S4.F1 "Figure 1 â€£ IV-5 Long-range Dependencies â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey")-C, which accomplishes rule-learning through path-based approaches. Path-based methods can make inferences from chains of edges or relations, as shown in Fig. [6](#S4.F6 "Figure 6 â€£ IV-C3 Path-Based Rule Learning â€£ IV-C Rule Learning for KGC â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"). Consequently, the generated rule sets are not only interpretable but often more expressive, as they facilitate understanding of relationships between nodes that are several hops away from each other, leveraging long-range dependencies. Therefore, in addition to leveraging all the other listed characteristics in Â§[IV](#S4 "IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"), this subcategory of approaches is uniquely distinguished by the [Long-range Dependencies](#S4.SS0.SSS5 "IV-5 Long-range Dependencies â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey") characteristic.

Long-range dependency is particularly important in application domains such as biology, in which a chain of interactions between macromolecules connects entities several steps apart [[22](#bib.bibx22), [31](#bib.bibx31)]. This is why Liu et al. [[93](#bib.bibx93)] design a path-based neurosymbolic method for predicting novel drug indications (i.e., treatment possibilities for pharmaceutical drugs [[41](#bib.bibx41)]). Specifically, they investigate paths which start and end with drug and disease nodes, respectively. By exploring indirect drug indications which might operate through other entities, such as genes, they expand the set of novel discoveries possible. However, using path-based exploration expands the search space. Therefore, to search the KG meaningfully, they employ Policy-guided Walks with Logical Rules (PoLo). Essentially, they start by encoding general path patterns, or metapaths, as logical rules with pre-computed confidence scores. While metapaths can be manually selected from expert knowledge, they can also be mined, such as in the PoLo follow-up study by DrancÃ© et al. [[45](#bib.bibx45)]. In both studies, they train an agent to walk through a biological KG, storing history using a LSTM. To use the rules as guidance, the agent is not only rewarded when it finds a positive drug indication but also when the path it follows corresponds to one of the logically encoded metapaths. Ultimately, the incorporation of path-based rules in the symbolic module guides the neural module toward exploration of long-range dependencies so that indirect relationships between distant nodes can be predicted. Therefore, this can be seen as another example of the [Guided Training](#S4.SS0.SSS2 "IV-2 Guided Training â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey") characteristic. Furthermore, expert-curated metapaths tend to consider the most probable combinations of relationships between various node types, so these approaches have the potential to handle heterogeneous graphs well, fitting the [Heterogeneous Aggregation](#S4.SS0.SSS4 "IV-4 Heterogeneous Aggregation â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey") characteristic. Alternatively, if one uses rule mining to generate the metapaths, as DrancÃ© et al. did, the generated rules and their corresponding confidences facilitate the [Interpretability](#S4.SS0.SSS1 "IV-1 Interpretability â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey") characteristic of the model.

LPRules, previously mentioned in Â§[IV-C2](#S4.SS3.SSS2 "IV-C2 New Rules: Bridging the Discrete and Continuous Spaces â€£ IV-C Rule Learning for KGC â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"), operates in a similar style to PoLo. Specifically, one of the heuristics which LPRules proposes to generate and update the pool of candidate rules utilizes long-range dependencies in the KG. For rule generation regarding a specific relation, such as drug indications in the PoLo study, it iterates through instances of that relation and finds the shortest path between each respective pair of nodes, excluding the current direct edge. From the shortest path, a rule is generated based on its general sequence of relations (which PoLo called a metapath) [[34](#bib.bibx34)]. From the aforementioned studies, we can see that KGC can depend heavily upon information beyond a nodeâ€™s local neighborhood. However, one major challenge with path-based approaches is that various sequences of relations are often imbalanced, as some relations and patterns thereof are significantly rarer than others.

Dependent upon the application, less frequent path sequences might be less interesting or relevant to the end-goal. Sen et al. [[136](#bib.bibx136)] aim to address this. In their study, they first utilize a Mixture of Paths (MP), in which the body of each generated logical rule contains one of many possible length-kğ‘˜k relation sequences in the KG. To generate such rules, they use the so-called Logical NN (LNN) [[123](#bib.bibx123)], coining their neurosymbolic approach LNN-MP. The previously mentioned RNNLogic (see Â§[IV-C2](#S4.SS3.SSS2 "IV-C2 New Rules: Bridging the Discrete and Continuous Spaces â€£ IV-C Rule Learning for KGC â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey")) also generates path-based rules in a similar manner to the MP method. However, Sen et al. argue that RNN-based approaches tend to be unnecessarily complex. Because LNNs are based on real-valued boolean logic, LNN-MP is computationally simpler than RNN-based approaches, and the rules learned are fully interpretable. Thereafter, to represent the most common path patterns in the KG, LNN-MP can be used on KGEs pre-trained with bias toward more prevalent relation sequences. By doing so, LNN-MP is capable of learning rules most relevant to the paths present in the KG, improving KGC performance.

Similarly, an approach by Hirose et al. [[67](#bib.bibx67)] weighs path-based rules on relation path frequencies in the KG, focusing KGC upon the most prevalent relation sequences. Their method, Transductive Augmentation (TA), draws inspiration from UniKER, described in Â§[IV-A2](#S4.SS1.SSS2 "IV-A2 Iterative Guidance from the Logic Module â€£ IV-A Logically-Informed Embedding Approaches â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"). Like the latter, it follows the same general process, illustrated within Fig. [2](#S4.F2 "Figure 2 â€£ IV-A1 Modular, Two-Step Approaches â€£ IV-A Logically-Informed Embedding Approaches â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"), of iteratively augmenting a KG on which a KGE method trained. TA is made more sophisticated, however, by two major changes. First, relational path rules are mined from the KG using random walks. Then, based on the frequency of relation paths in the KG, confidence scores are computed to permit filtration for the top thousand rules. TA improves KGC performance in comparison to KGE methods alone, and it generates weighted, path-based rules which, once again, highlight the path-like patterns that are most relevant to predictions made.

While the benefits of path-based methods have already been abundantly discussed here, we note that these approaches also rely on several assumptions. First, they all operate on heterogeneous KGs containing multiple relation types; single-relation KGs may not benefit from such methods. Second, one might wonder, especially in the biological context of PoLo, whether these methods operate equally well on both directed and undirected graphs. Finally, approaches such as LNN-MP and TA assume that the most relevant path patterns are those which reflect the most prevalent sequences in the KG, but this may not always be the case; such approaches would lack the ability to handle underrepresented classes, therefore falling short of the [Underrepresented Types](#S4.SS0.SSS3 "IV-3 Underrepresented Types â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey") characteristic. On the other hand, DrancÃ© et al. point out that more interesting rules might be obtained by considering the goals of the application domain in which one is interested [[45](#bib.bibx45)], and sometimes, underrepresented relation types are most important or interesting for KGC, a topic explored within Â§[V-B5](#S5.SS2.SSS5 "V-B5 Few Shot Learning â€£ V-B Prospective Directions â€£ V Limitations & Prospective Directions â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey").

Because the path-based approaches are varied in the way they operate, it is difficult to categorize them into one of Kautzâ€™s types. Since these approaches share the most similarity with those in the previous rule-learning sections as well as Â§[IV-A2](#S4.SS1.SSS2 "IV-A2 Iterative Guidance from the Logic Module â€£ IV-A Logically-Informed Embedding Approaches â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"), we believe that this section could be split between Neuro:Symbolic â†’â†’\rightarrow Neuro and Neuro;Symbolic types. Because we focus on a specific area of neurosymbolic AI, we are untroubled by this imperfect division. As this area is still developing, we expect that future approaches will continue to refine and update both our classification and Kautzâ€™s.

TABLE II: Summary of Surveyed Neurosymbolic Approaches. D = incorporates domain knowledge, L = augments KG with logical inference, C = applies logical constraints, W = learns rule weights or confidences, U = updates candidate rule pool.

|  |  |  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
|  | Reference | Year | KGs | D | L | C | W | U |
| Logically-Informed Embedding | Walking RDF and OWL [[4](#bib.bibx4), [1](#bib.bibx1)] | 2017 | paper-specific biological KG | âœ“ | âœ“ | âœ— | âœ— | âœ— |
| UniKER [[28](#bib.bibx28)] | 2020 | WN18RR, FB15k-237, Kinship | âœ— | âœ“ | âœ— | âœ— | âœ— |
| RUGE [[62](#bib.bibx62)] | 2020 | FB15K, YAGO37 | âœ— | âœ“ | âœ— | âœ— | âœ— |
| RW-autodrive [[161](#bib.bibx161)] | 2020 | NuScenes, Lyft-Level5 | âœ— | âœ“ | âœ— | âœ— | âœ— |
| SoLE [[176](#bib.bibx176)] | 2019 | FB15K, DB100K | âœ— | âœ“ | âœ— | âœ— | âœ— |
| KGE\* [[75](#bib.bibx75)] | 2022 | DBpedia20k, LUBM | âœ“ | âœ“ | âœ— | âœ— | âœ— |
| ReasonKGE [[73](#bib.bibx73)] | 2021 | DBpedia15k, LUBM3U, Yago3-10 | âœ“ | âœ“ | âœ— | âœ— | âœ— |
| Logical Constraints | Neuro-Symbolic Entropy Reg. [[2](#bib.bibx2)] | 2022 | ACE05, SciERC | âœ— | âœ— | âœ“ | âœ— | âœ— |
| GeKCs [[94](#bib.bibx94)] | 2023 | FB15k-237, WN18RR, ogbl-biokg | âœ“ | âœ— | âœ“ | âœ— | âœ— |
| R2N [[98](#bib.bibx98)] | 2021 | Countries dataset, Nations dataset, UMLS, Kinship, Cora | âœ— | âœ— | âœ“ | âœ— | âœ— |
| SLRE [[61](#bib.bibx61)] | 2020 | FB15K, DB100K | âœ— | âœ— | âœ“ | âœ— | âœ— |
| GCR [[23](#bib.bibx23)] | 2022 | Amazon e-commerce, FB15k-237 | âœ— | âœ— | âœ“ | âœ— | âœ— |
| QLogicE [[25](#bib.bibx25)] | 2022 | FB15k, FB15k-237, YAGO3-10, UMLS, Kinship, WN18RR | âœ— | âœ— | âœ“ | âœ— | âœ— |
| ComplEx-NNE\_AER [[40](#bib.bibx40)] | 2018 | FB15K, DBpedia, WN18 | âœ— | âœ— | âœ“ | âœ— | âœ— |
| KALE [[60](#bib.bibx60)] | 2016 | WN18, FB122 | âœ— | âœ— | âœ“ | âœ“ | âœ— |
| JOIE [[64](#bib.bibx64)] | 2019 | YAGO26K-906, DB111K-174 | âœ“ | âœ— | âœ“ | âœ— | âœ— |
| Graph Query Embeddings [[63](#bib.bibx63)] | 2018 | paper-specific biological KG, Reddit | âœ— | âœ— | âœ“ | âœ— | âœ— |
| CQDA [[6](#bib.bibx6), [5](#bib.bibx5)] | 2023 | FB15K, FB15K-237, NELL995 | âœ— | âœ— | âœ“ | âœ— | âœ— |
| KeGNN [[160](#bib.bibx160)] | 2023 | Cora, Citeseer, PubMed, Flickr | âœ— | âœ— | âœ“ | âœ— | âœ— |
| Low-rank Logic Embeddings [[125](#bib.bibx125)] | 2015 | Freebase | âœ— | âœ“ | âœ“ | âœ— | âœ— |
| SimplE+ [[49](#bib.bibx49)] | 2019 | FB15K, WN18 | âœ— | âœ— | âœ“ | âœ— | âœ— |
|  | KGER [[102](#bib.bibx102)] | 2017 | DBpedia, YAGO, WordNet | âœ— | âœ— | âœ“ | âœ— | âœ— |
| Rule Learning | LPRules [[34](#bib.bibx34)] | 2021 | FB15k-237, YAGO3-10, DB111K-174, WN18RR, UMLS, Kinship | âœ— | âœ— | âœ— | âœ“ | âœ“ |
| RuLES [[69](#bib.bibx69)] | 2018 | FB15K, Wiki44K, Family Dataset | âœ— | âœ— | âœ— | âœ“ | âœ“ |
| pLogicNet [[117](#bib.bibx117)] | 2019 | FB15k, FB15k-237, WN18, WN18RR | âœ— | âœ“ | âœ— | âœ“ | âœ— |
| PoLo [[93](#bib.bibx93), [45](#bib.bibx45)] | 2021 | Hetionet, OREGANO KG | âœ“ | âœ— | âœ— | âœ“ | âœ— |
| pGAT [[65](#bib.bibx65)] | 2020 | WN18RR, FB15K-237 | âœ— | âœ“ | âœ— | âœ“ | âœ— |
| DiffLogic [[138](#bib.bibx138)] | 2023 | CodeX, YAGO3-10, WN18, WN18RR, Kinship | âœ— | âœ“ | âœ— | âœ“ | âœ— |
| Neural LP [[168](#bib.bibx168)] | 2017 | WordNet18, Freebase15K, Freebase15KSelected, UMLS, Kinship, WikiMovies | âœ— | âœ— | âœ— | âœ“ | âœ“ |
| RNNLogic [[118](#bib.bibx118)] | 2020 | WN18RR, FB15k-237, UMLS, Kinship | âœ— | âœ— | âœ— | âœ“ | âœ“ |
| DRUM [[129](#bib.bibx129)] | 2019 | UMLS, Kinship, Family Dataset, WN18RR, FB15K-237 | âœ— | âœ— | âœ— | âœ“ | âœ“ |
| ExpressGNN [[181](#bib.bibx181)] | 2020 | Cora, UW-CSE, Kinship, FB15K-237 | âœ— | âœ“ | âœ— | âœ“ | âœ— |
| BioGRER [[184](#bib.bibx184)] | 2020 | kg-covid-19 | âœ“ | âœ— | âœ— | âœ“ | âœ— |
| SN-Hybrid [[146](#bib.bibx146)] | 2020 | YAGO3-10, FB15K-237, WN18RR | âœ— | âœ— | âœ— | âœ“ | âœ“ |
| Rule-IC [[92](#bib.bibx92)] | 2021 | FB15k, FB15k-237, WN18, WN18RR | âœ— | âœ— | âœ— | âœ“ | âœ“ |
| IterE [[180](#bib.bibx180)] | 2019 | FB15k, FB15k-237, WN18, WN18RR | âœ— | âœ— | âœ— | âœ“ | âœ— |
| LNN-MP [[136](#bib.bibx136)] | 2021 | UMLS, Kinship, WN18RR, FB15k-237 | âœ— | âœ— | âœ— | âœ“ | âœ“ |
| Transductive Augmentation [[67](#bib.bibx67)] | 2021 | WN18RR, FB15k-237 | âœ— | âœ“ | âœ— | âœ“ | âœ— |

Within the last decade and particularly within the last six years, one can see that neurosymbolic approaches for reasoning on KGs have gained significant interest. We found that the surveyed approaches fit best within three major categories: (1) logically-informed embedding approaches (Â§[IV-A](#S4.SS1 "IV-A Logically-Informed Embedding Approaches â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey")), which are most similar to Kautzâ€™s Neuro:SymbolicÂ â†’â†’\rightarrowÂ Neuro category, (2) embedding approaches with logical constraints (Â§[IV-B](#S4.SS2 "IV-B Learning with Logical Constraints â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey")), most like Neuro[Symbolic] AI, and (3) rule-learning approaches (Â§[IV-C](#S4.SS3 "IV-C Rule Learning for KGC â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey")), which fit most closely to Neuro;Symbolic AI. Furthermore, we anticipate that this area of work will expand significantly within the next few years to fill out the parts of Kautzâ€™s neurosymbolic classifications which we did not frequently mention, such as Symbolic[Neuro] AI. Additionally, we expect that these approaches, along with their unique capabilities, will fill in several research gaps. In an attempt to facilitate and inspire future studies, we discuss prospective uses in the next section.

## V Limitations & Prospective Directions

As neurosymbolic reasoning over KGs is still a young field of research, there are plenty of technical and practical areas yet to be fully explored. We next mention several common limitations of these approaches, then suggest a number of prospective directions, which we hope will cultivate a greater interest in this domain. In particular, we note that many of these directions could be useful for biomedical data and applications, so we use a number of examples in this domain to illustrate our points.

### V-A Limitations

While respective benefits and weaknesses were discussed for each technical category, there are also several general limitations of these studies as well as neurosymbolic approaches as a whole.

#### V-A1 Increased Complexity

While, in theory, neurosymbolic methods could decrease computational complexity by guiding neural training, they may also increase it, depending upon their implementation. This is because existing symbolic methods which rely upon grounding rules, like the traditional implementation of the MLN, do not tend to scale well to large datasets [[145](#bib.bibx145)]. Additionally, as explained in Â§[IV-C2](#S4.SS3.SSS2 "IV-C2 New Rules: Bridging the Discrete and Continuous Spaces â€£ IV-C Rule Learning for KGC â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"), it is difficult to integrate the symbolic module with the neural module in an end-to-end fashion. Therefore, if a neurosymbolic pipeline fails to integrate the symbolic module in a way that improves scalability, then combining it with a neural module increases model complexity.

#### V-A2 Stacking-Induced Performance Gains

Several surveyed approaches stack two models, one each for symbolic and neural processes, into one combined model. Examples of such approaches can be found in Â§[IV-A](#S4.SS1 "IV-A Logically-Informed Embedding Approaches â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey") and Â§[IV-C1](#S4.SS3.SSS1 "IV-C1 EM Algorithm Methods â€£ IV-C Rule Learning for KGC â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"). Sometimes, studies using these approaches claim to see improved performance in comparison to the use of the neural module alone [[115](#bib.bibx115), [161](#bib.bibx161)]. However, work such as that by Rivas-Barragan et al.  [[124](#bib.bibx124)] have found that ensemble methods for KGs tend to see improved performance than the use of a single model. This calls to question whether the observed performance increase is simply due to model stacking, as opposed to the neurosymbolic aspect.

#### V-A3 Domain Knowledge Availability

Several neurosymbolic approaches discussed, such as those that require an ontology [[4](#bib.bibx4), [161](#bib.bibx161)], use domain-specific knowledge. Unfortunately, expert-curated knowledge is not readily available or digitized for every field. In such cases, these approaches are not easily applicable.

#### V-A4 Ill-defined Interpretability

As discussed in Â§[IV](#S4 "IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"), a major characteristic of the surveyed approaches is their facilitation of model interpretability. However, the concept of interpretability is generally ill-defined [[104](#bib.bibx104)], and it is not captured consistently across the surveyed approaches. Therefore, while there is a general consensus that neurosymbolic AI can improve interpretability, there is no official metric for it, making this a subjective evaluation.

### V-B Prospective Directions

#### V-B1 Underexplored Application Areas

The selection of application domains chosen in the surveyed papers was largely driven by a preference for commonly used and openly available KGs. For example, because they are popular benchmark datasets [[33](#bib.bibx33)], several KGs which encompass general knowledge bases, including YAGO [[144](#bib.bibx144)] and DBpedia [[8](#bib.bibx8)], were frequent choices with which to test the novel methods. While this is ideal for comparing performance between methods, few studies have attempted to demonstrate the efficacy of their method for a specific task or within a certain domain. Now, with our proposed taxonomy at hand and a general understanding of each categoryâ€™s features (see Table [II](#S4.T2 "TABLE II â€£ IV-C3 Path-Based Rule Learning â€£ IV-C Rule Learning for KGC â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey")), prospective studies could aim to demonstrate the usefulness of these methods within certain contexts. For instance, surveyed methods which foster increased interpretability, such as those within Â§[IV-C](#S4.SS3 "IV-C Rule Learning for KGC â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"), are likely to be especially helpful for areas that closely affect peoplesâ€™ health or livelihoods, such as biomedicine, for reasons previously established, or autonomous driving, to ensure user and bystander safety [[103](#bib.bibx103)]. Additionally, models which affect financial decisions, such as loan applications or customer turnover, might also have significant impacts on business success and benefit from interpretability [[103](#bib.bibx103)]. Alternatively, if a field is already well investigated and ample expert knowledge is available, this expertise could be used in a KG augmentation step as in Â§[IV-A](#S4.SS1 "IV-A Logically-Informed Embedding Approaches â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey") or as training constraints as in Â§[IV-B](#S4.SS2 "IV-B Learning with Logical Constraints â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey"). Example applications of this could include tasks within the natural sciences in which there are many openly available databases of information (e.g., biology [[7](#bib.bibx7), [134](#bib.bibx134)] or chemistry [[80](#bib.bibx80), [54](#bib.bibx54)]), language-oriented tasks which rely on human-defined rules [[56](#bib.bibx56), [185](#bib.bibx185)], and multimedia computing, which is built on human-made features [[185](#bib.bibx185)]. Because this field is still developing, there are plenty of new territories that researchers might explore.

#### V-B2 Multimodal Data Integration

Multimodal data describes a dataset or a combination of datasets that contain various forms, such as images, videos, text or long sequences and numerical measurements. For example, a news article might contain both text and images to describe a story [[52](#bib.bibx52), [86](#bib.bibx86)]. Specifically, multimodal data tends to be particularly popular within the biomedical and autonomous driving domains, among others [[86](#bib.bibx86), [130](#bib.bibx130), [171](#bib.bibx171)]. While using a single type of data at a time is much simpler, various data formats can complement and complete one another. However, fusing the various data modalities into a unified input is challenging: one must find a way to represent all modalities similarly, such as numerically, while avoiding adding bias. Existing approaches tend to either (1) concatenate modalities into one, input vector, in which modalities are no longer distinguishable, (2) project modalities into representative embdeddings via an autoencoder, or (3) use entirely separate models for each modality [[52](#bib.bibx52), [143](#bib.bibx143)]. However, a neurosymbolic approach could exploit the [Heterogeneous Aggregation](#S4.SS0.SSS4 "IV-4 Heterogeneous Aggregation â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey") characteristic by incorporating logic or domain-specific knowledge about the relationships between the modalities. Since multimodal data is valuable for the way its various forms complement one another, it is important to consider what kind of information these modalities contribute to one another, and how we can define these relationships through expert knowledge and logic.

#### V-B3 Conditional or Interdependent Edge Types

In some cases, certain relations or edge types might be dependent upon one another in a way that one edge does not occur unless other edges exist. An example of this includes molecular signalling cascades, in which one protein will not interact with another unless contact is made with another protein or molecule upstream [[22](#bib.bibx22), [31](#bib.bibx31)]. Such dependence could also be frequency-dependent. In another molecular signalling example, if an upstream inhibitor interacts with some protein, the less that protein may take part in other interactions [[58](#bib.bibx58), [116](#bib.bibx116)]. A similar example can be observed in traffic forecasting, in which traffic on certain roads can increase or decrease that on other roads [[59](#bib.bibx59), [164](#bib.bibx164)]. While such dependencies are not unique to neurosymbolic reasoning, it might be ideal for addressing them. For instance, with the incorporation of rules, neurosymbolic methods could encode dependency information between relation types. Potentially, a method such as GCR [[23](#bib.bibx23)], which uses logic to assess the probability that an edge is implied from its adjacent edges, could be ideal for interdependent edge types. Furthermore, since many neurosymbolic methods learn confidences for rules, there also exists the potential to learn confidences for which rules might coexist or influence one another. Neurosymbolic methods could, therefore, provide unique ways to interpret such dynamic KGs and the relationships between heterogeneous edge types.

#### V-B4 Spatiotemporal Reasoning

KGs might also have spatial or temporal dependencies to consider. This is highly relevant, once again, in the traffic forecasting [[164](#bib.bibx164)] and biological domains [[76](#bib.bibx76)], but also in areas such as scene understanding [[18](#bib.bibx18)]. Amongst previous approaches for reasoning over spatiotemporal KGs, many of which are available through the Pytorch Geometric Temporal package [[127](#bib.bibx127)], there is still a division between those which take rule-based approaches and those which use deep learning approaches [[157](#bib.bibx157)]. Thus, spatiotemporal applications are a promising direction for neurosymbolic hybridization.

Similarly to the proposed approach for interdependent edge types, neurosymbolic approaches could learn rules which describe the relationships between the various edge types and time, for example. Alternatively, rules could be learned for each specific time range or spatial location. Gene expression, for instance, is the process by which our genetic code is transcribed and translated into functional products; the set of genetic codes which are expressed as well as the magnitude to which they are expressed varies completely between bodily tissue types [[17](#bib.bibx17)]. One could model the interactions between the functional products as a graph, but the levels to which they affect one another depend directly upon their existence and abundance, factors controlled by the tissue type [[17](#bib.bibx17)]. This is an example of a network, therefore, which is spatially dependent, and learning rules specific to tissue types could unveil how processes differ between tissues. Additionally, short-term traffic forecasting, the prediction of traffic flows within a small time frame, relies heavily on spatial context, such as the layout and style of the roads being considered, as well as temporal context, such as the time of day or week [[88](#bib.bibx88)]. Such rules could add another layer of interpretability to informs researchers on how underlying processes differ across tissues, locations, time periods, and more.

Alternatively, a neurosymbolic architecture could encode time- or space-dependent rules as constraints. For example, in a chemical or biological network, the half-lives of molecules such as messenger ribonucleic acid (mRNA) not only act as a time limit on potential interactions but also determine the levels to which various functional proteins are expressed, and therefore the extent to which those proteins can have effects on other players in the network [[99](#bib.bibx99), [17](#bib.bibx17)]. Such constraints can more realistically model spatiotemporal dependencies by adding logic or domain-specific rules.

#### V-B5 Few Shot Learning

As mentioned under the [Underrepresented Types](#S4.SS0.SSS3 "IV-3 Underrepresented Types â€£ IV Neurosymbolic Approaches for Reasoning over KGs â€£ Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey") characteristic, some methods increase the number of rare relation types through logical inference. Such approaches could, therefore, be used as a solution for few shot learning problems. In few shot learning, there exist a small number of instances of some class within the training data [[173](#bib.bibx173)]. In KGs, this could include node classes or relation types. Oftentimes, KGE methods neglect rare relation types [[175](#bib.bibx175), [24](#bib.bibx24)]. This is often accounted for through sampling or regularization methods [[132](#bib.bibx132)], meta-learning [[24](#bib.bibx24), [175](#bib.bibx175), [183](#bib.bibx183)], and learned attention coefficients or relation-specific parameters, [[175](#bib.bibx175)] but neurosymbolic methods that exploit rule-based deduction could pose an alternative solution. Methods such as pLogicNet [[117](#bib.bibx117)] and pGAT [[65](#bib.bibx65)] augment the KG with logically inferred triples and feed the it into the neural module. The KG augmentation step may also increase the instances of rare relation types, so the neural module has a higher sample size on which to train. This poses potential to address few shot learning on KGs.

## VI Conclusion

Methods for reasoning on KGs are popular and widely applicable across domains [[184](#bib.bibx184), [45](#bib.bibx45), [168](#bib.bibx168), [23](#bib.bibx23)]. Therefore, it is unsurprising that there are already such a varied range of neurosymbolic methods, despite neurosymbolic AI being a young area of research. In this article, we introduce a taxonomy by which to classify these novel approaches based on the ways they contribute toward balancing interpretability, knowledge-integration, and improved predictive performance within the context of KGC. Specifically, we found that the surveyed methods fit quite well into three major categories: (1) logically-informed embedding approaches, (2) embedding approaches with logical constraints, and (3) rule-learning approaches. Throughout the article, we not only compare the various approaches but also delve into deeper subcategories of classification. For easy reference, we summarized our findings in a tabular view and compiled the available code repositories on one GitHub page333<https://github.com/NeSymGraphs>. Finally, we propose prospective application-based and technical directions toward which this field might steer. Through this survey, we provide a comprehensive overview of existing methods for neurosymbolic reasoning on KGs with hopes to guide future research.

## Acknowledgments

LND is funded by the University of Edinburgh (UoE) Informatics Graduate School through the Global Informatics Scholarship. RFM is funded by the UoE Institute for Academic Development through the Principalâ€™s Career Development PhD Scholarship. We thank these institutions for their support. We also thank Paola Galdi, Emile van Krieken, Matthew Whyte, and Zonglin Ji for their thoughtful contributions.

## References

* [1]
  Asan Agibetov and Matthias Samwald
  â€œFast and scalable learning of neuro-symbolic representations of biomedical knowledgeâ€
  In *arXiv preprint arXiv:1804.11105*, 2018
* [2]
  Kareem Ahmed, Eric Wang, Kai-Wei Chang and Guy Van den Broeck
  â€œNeuro-symbolic entropy regularizationâ€
  In *Uncertainty in Artificial Intelligence*, 2022, pp. 43â€“53
  PMLR
* [3]
  Kareem Ahmed et al.
  â€œSemantic probabilistic layers for neuro-symbolic learningâ€
  In *Advances in Neural Information Processing Systems* 35, 2022, pp. 29944â€“29959
* [4]
  Mona Alshahrani et al.
  â€œNeuro-symbolic representation learning on biological knowledge graphsâ€
  In *Bioinformatics* 33.17
  Oxford University Press, 2017, pp. 2723â€“2730
* [5]
  Erik Arakelyan et al.
  â€œAdapting Neural Link Predictors for Data-Efficient Complex Query Answeringâ€
  In *Thirty-seventh Conference on Neural Information Processing Systems*, 2023
* [6]
  Erik Arakelyan, Daniel Daza, Pasquale Minervini and Michael Cochez
  â€œComplex Query Answering with Neural Link Predictorsâ€
  In *International Conference on Learning Representations*, 2021, pp. 1â€“14
* [7]
  Michael Ashburner et al.
  â€œGene ontology: tool for the unification of biologyâ€
  In *Nature Genetics* 25.1
  Nature Publishing Group, 2000, pp. 25â€“29
* [8]
  SÃ¶ren Auer et al.
  â€œDBpedia: A nucleus for a web of open dataâ€
  In *The Semantic Web*
  Springer, 2007, pp. 722â€“735
* [9]
  Pradeep Kr Banerjee et al.
  â€œOversquashing in GNNs through the lens of information contraction and graph expansionâ€
  In *2022 58th Annual Allerton Conference on Communication, Control, and Computing (Allerton)*, 2022, pp. 1â€“8
  IEEE
* [10]
  Chitta Baral and Michael Gelfond
  â€œLogic programming and knowledge representationâ€
  In *The Journal of Logic Programming* 19
  Elsevier, 1994, pp. 73â€“148
* [11]
  Dimitris Bertsimas, Arthur Delarue, Patrick Jaillet and Sebastien Martin
  â€œThe price of interpretabilityâ€
  In *arXiv preprint arXiv:1907.03419*, 2019
* [12]
  Garrett Birkhoff and John Von Neumann
  â€œThe logic of quantum mechanicsâ€
  In *Annals of mathematics*
  JSTOR, 1936, pp. 823â€“843
* [13]
  David Bonanno et al.
  â€œAn approach to explainable deep learning using fuzzy inferenceâ€
  In *Next-Generation Analyst V* 10207, 2017, pp. 132â€“136
  SPIE
* [14]
  Antoine Bordes et al.
  â€œTranslating embeddings for modeling multi-relational dataâ€
  In *Advances in Neural Information Processing Systems* 26, 2013
* [15]
  Armand Boschin, Nitisha Jain, Gurami Keretchashvili and Fabian Suchanek
  â€œCombining Embeddings and Rules for Fact Predictionâ€
  In *International Research School in Artificial Intelligence in Bergen (AIB 2022)*, 2022
  Schloss Dagstuhl-Leibniz-Zentrum fÃ¼r Informatik
* [16]
  Tom Brown et al.
  â€œLanguage models are few-shot learnersâ€
  In *Advances in Neural Information Processing Systems* 33, 2020, pp. 1877â€“1901
* [17]
  Christopher Buccitelli and Matthias Selbach
  â€œmRNAs, proteins and the emerging principles of gene expression controlâ€
  In *Nature Reviews Genetics* 21.10
  Nature Publishing Group, 2020, pp. 630â€“644
* [18]
  Chengzhi Cao, Chao Yang, Ruimao Zhang and Shuang Li
  â€œDiscovering Intrinsic Spatial-Temporal Logic Rules to Explain Human Actionsâ€
  In *Thirty-seventh Conference on Neural Information Processing Systems*, 2023
  URL: <https://openreview.net/forum?id=avuRopYsCg>
* [19]
  Vincenzo Carletti et al.
  â€œPredicting Polypharmacy Side Effects Through a Relation-Wise Graph Attention Networkâ€
  In *Joint IAPR International Workshops on Statistical Techniques in Pattern Recognition (SPR) and Structural and Syntactic Pattern Recognition (SSPR)*, 2021, pp. 119â€“128
  Springer
* [20]
  Andrea Cavallo et al.
  â€œGCNH: A Simple Method For Representation Learning On Heterophilous Graphsâ€
  In *arXiv preprint arXiv:2304.10896*, 2023
* [21]
  Supriyo Chakraborty et al.
  â€œInterpretability of deep learning models: A survey of resultsâ€
  In *2017 IEEE Smartworld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computed, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation*, 2017, pp. 1â€“6
  IEEE
* [22]
  Lufen Chang and Michael Karin
  â€œMammalian MAP kinase signalling cascadesâ€
  In *Nature* 410.6824
  Nature Publishing Group, 2001, pp. 37â€“40
* [23]
  Hanxiong Chen et al.
  â€œGraph Collaborative Reasoningâ€
  In *Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining*, WSDM â€™22
  Virtual Event, AZ, USA: Association for Computing Machinery, 2022, pp. 75â€“84
  DOI: [10.1145/3488560.3498410](https://dx.doi.org/10.1145/3488560.3498410)
* [24]
  Mingyang Chen et al.
  â€œMeta relational learning for few-shot link prediction in knowledge graphsâ€
  In *arXiv preprint arXiv:1909.01515*, 2019
* [25]
  Panfeng Chen, Yisong Wang, Xiaomin Yu and Renyan Feng
  â€œQLogicE: Quantum Logic Empowered Embedding for Knowledge Graph Completionâ€
  In *Knowledge-Based Systems* 239
  Elsevier, 2022, pp. 107963
* [26]
  Xiaojun Chen, Shengbin Jia and Yang Xiang
  â€œA review: Knowledge reasoning over knowledge graphâ€
  In *Expert Systems with Applications* 141
  Elsevier, 2020, pp. 112948
* [27]
  Zhe Chen et al.
  â€œKnowledge graph completion: A reviewâ€
  In *IEEE Access* 8
  IEEE, 2020, pp. 192435â€“192456
* [28]
  Kewei Cheng, Ziqing Yang, Ming Zhang and Yizhou Sun
  â€œUniKER: A unified framework for combining embedding and Horn rules for knowledge graph inferenceâ€
  In *ICML Workshop on Graph Representation Learning and Beyond*, 2020
* [29]
  Aakanksha Chowdhery et al.
  â€œPalm: Scaling language modeling with pathwaysâ€
  In *arXiv preprint arXiv:2204.02311*, 2022
* [30]
  William W Cohen
  â€œTensorlog: A differentiable deductive databaseâ€
  In *arXiv preprint arXiv:1605.06523*, 2016
* [31]
  Peter B Crino
  â€œThe mTOR signalling cascade: paving new roads to cure neurological diseaseâ€
  In *Nature Reviews Neurology* 12.7
  Nature Publishing Group, 2016, pp. 379â€“392
* [32]
  Andrew Cropper, Sebastijan DumanÄiÄ‡ and Stephen H Muggleton
  â€œTurning 30: New ideas in inductive logic programmingâ€
  In *arXiv preprint arXiv:2002.11002*, 2020
* [33]
  Yuanfei Dai, Shiping Wang, Neal N Xiong and Wenzhong Guo
  â€œA survey on knowledge graph embedding: Approaches, applications and benchmarksâ€
  In *Electronics* 9.5
  MDPI, 2020, pp. 750
* [34]
  Sanjeeb Dash and Joao Goncalves
  â€œLPRules: Rule Induction in Knowledge Graphs Using Linear Programmingâ€
  In *arXiv preprint arXiv:2110.08245*, 2021
* [35]
  Nur Nasuha Daud et al.
  â€œApplications of link prediction in social networks: A reviewâ€
  In *Journal of Network and Computer Applications* 166
  Elsevier, 2020, pp. 102716
* [36]
  Luc De Raedt, Angelika Kimmig and Hannu Toivonen
  â€œProbLog: A Probabilistic Prolog and Its Application in Link Discovery.â€
  In *IJCAI* 7, 2007, pp. 2462â€“2467
  Hyderabad
* [37]
  Arthur P Dempster, Nan M Laird and Donald B Rubin
  â€œMaximum likelihood from incomplete data via the EM algorithmâ€
  In *Journal of the royal statistical society: series B (methodological)* 39.1
  Wiley Online Library, 1977, pp. 1â€“22
* [38]
  Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova
  â€œBert: Pre-training of deep bidirectional transformers for language understandingâ€
  In *arXiv preprint arXiv:1810.04805*, 2018
* [39]
  Francesco Di Giovanni et al.
  â€œHow does over-squashing affect the power of GNNs?â€
  In *arXiv preprint arXiv:2306.03589*, 2023
* [40]
  Boyang Ding, Quan Wang, Bin Wang and Li Guo
  â€œImproving knowledge graph embedding using simple constraintsâ€
  In *arXiv preprint arXiv:1805.02408*, 2018
* [41]
  Daniel Domingo-FernÃ¡ndez et al.
  â€œCOVID-19 Knowledge Graph: a computable, multi-modal, cause-and-effect knowledge model of COVID-19 pathophysiologyâ€
  In *Bioinformatics* 37.9
  Oxford University Press, 2021, pp. 1332â€“1334
* [42]
  Pedro Domingos et al.
  â€œMarkov Logicâ€
  In *Probabilistic inductive logic programming*
  Springer, 2008, pp. 92â€“117
* [43]
  Yuxiao Dong et al.
  â€œLink prediction and recommendation across heterogeneous social networksâ€
  In *2012 IEEE 12th International conference on data mining*, 2012, pp. 181â€“190
  IEEE
* [44]
  Dejing Dou, Hao Wang and Haishan Liu
  â€œSemantic data mining: A survey of ontology-based approachesâ€
  In *Proceedings of the 2015 IEEE 9th international conference on semantic computing (IEEE ICSC 2015)*, 2015, pp. 244â€“251
  IEEE
* [45]
  Martin DrancÃ©, Marina Boudin, Fleur Mougin and Gayo Diallo
  â€œNeuro-symbolic XAI for Computational Drug Repurposing.â€
  In *KEOD*, 2021, pp. 220â€“225
* [46]
  Gintare Karolina Dziugaite, Shai Ben-David and Daniel M Roy
  â€œEnforcing Interpretability and its Statistical Impacts: Trade-offs between Accuracy and Interpretabilityâ€
  In *arXiv preprint arXiv:2010.13764*, 2020
* [47]
  Lisa Ehrlinger and Wolfram WÃ¶ÃŸ
  â€œTowards a definition of knowledge graphs.â€
  In *SEMANTiCS (Posters, Demos, SuCCESS)* 48.1-4
  Citeseer, 2016, pp. 2
* [48]
  Yin Fang et al.
  â€œKnowledge-aware Contrastive Molecular Graph Learningâ€
  In *arXiv preprint arXiv:2103.13047*, 2021
* [49]
  Bahare Fatemi, Siamak Ravanbakhsh and David Poole
  â€œImproved knowledge graph embedding using background taxonomic informationâ€
  In *Proceedings of the AAAI Conference on Artificial Intelligence* 33.01, 2019, pp. 3526â€“3533
* [50]
  Benjamin LS Furman et al.
  â€œSex chromosome evolution: so many exceptions to the rulesâ€
  In *Genome biology and evolution* 12.6
  Oxford University Press, 2020, pp. 750â€“763
* [51]
  Luis GalÃ¡rraga, Christina Teflioudi, Katja Hose and Fabian M Suchanek
  â€œFast rule mining in ontological knowledge bases with AMIE+â€
  In *The VLDB Journal* 24.6
  Springer, 2015, pp. 707â€“730
* [52]
  Jing Gao, Peng Li, Zhikui Chen and Jianing Zhang
  â€œA survey on deep learning for multimodal data fusionâ€
  In *Neural Computation* 32.5
  MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-infoÂ â€¦, 2020, pp. 829â€“864
* [53]
  Artur dâ€™Avila Garcez and Luis C Lamb
  â€œNeurosymbolic AI: The 3 rd waveâ€
  In *Artificial Intelligence Review*
  Springer, 2023, pp. 1â€“20
* [54]
  Anna Gaulton et al.
  â€œThe ChEMBL database in 2017â€
  In *Nucleic Acids Research* 45.D1
  Oxford University Press, 2017, pp. D945â€“D954
* [55]
  Aryo Pradipta Gema et al.
  â€œKnowledge Graph Embeddings in the Biomedical Domain: Are They Useful? A Look at Link Prediction, Rule Learning, and Downstream Polypharmacy Tasksâ€
  In *arXiv preprint arXiv:2305.19979*, 2023
* [56]
  Sujatha Das Gollapalli, Xiao-Li Li and Peng Yang
  â€œIncorporating expert knowledge into keyphrase extractionâ€
  In *Proceedings of the AAAI Conference on Artificial Intelligence* 31.1, 2017
* [57]
  Aditya Grover and Jure Leskovec
  â€œnode2vec: Scalable feature learning for networksâ€
  In *Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining*, 2016, pp. 855â€“864
* [58]
  David Gubb et al.
  â€œProtease inhibitors and proteolytic signalling cascades in insectsâ€
  In *Biochimie* 92.12
  Elsevier, 2010, pp. 1749â€“1759
* [59]
  Shengnan Guo et al.
  â€œAttention based spatial-temporal graph convolutional networks for traffic flow forecastingâ€
  In *Proceedings of the AAAI conference on artificial intelligence* 33.01, 2019, pp. 922â€“929
* [60]
  Shu Guo et al.
  â€œJointly Embedding Knowledge Graphs and Logical Rulesâ€
  In *EMNLP*, 2016
* [61]
  Shu Guo et al.
  â€œKnowledge Graph Embedding Preserving Soft Logical Regularityâ€
  In *Proceedings of the 29th ACM International Conference on Information & Knowledge Management*, 2020, pp. 425â€“434
* [62]
  Shu Guo et al.
  â€œKnowledge graph embedding with iterative guidance from soft rulesâ€
  In *Proceedings of the AAAI Conference on Artificial Intelligence* 32.1, 2018
* [63]
  Will Hamilton et al.
  â€œEmbedding logical queries on knowledge graphsâ€
  In *Advances in Neural Information Processing Systems* 31, 2018
* [64]
  Junheng Hao et al.
  â€œUniversal representation learning of knowledge bases by jointly embedding instances and ontological conceptsâ€
  In *Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining*, 2019, pp. 1709â€“1719
* [65]
  L Vivek Harsha Vardhan, Guo Jia and Stanley Kok
  â€œProbabilistic logic graph attention networks for reasoningâ€
  In *Companion Proceedings of the Web Conference 2020*, 2020, pp. 669â€“673
* [66]
  Daniel Scott Himmelstein et al.
  â€œSystematic integration of biomedical knowledge prioritizes drugs for repurposingâ€
  In *eLife* 6
  eLife Sciences Publications, Ltd, 2017
* [67]
  Yushi Hirose, Masashi Shimbo and Taro Watanabe
  â€œTransductive Data Augmentation with Relational Path Rule Mining for Knowledge Graph Embeddingâ€
  In *2021 IEEE International Conference on Big Knowledge (ICBK)*, 2021, pp. 377â€“384
  IEEE
* [68]
  Pascal Hitzler et al.
  â€œNeuro-symbolic approaches in Artificial Intelligenceâ€
  In *National Science Review* 9.6
  Oxford University Press, 2022, pp. nwac035
* [69]
  Vinh Thinh Ho et al.
  â€œRule learning from knowledge graphs guided by embedding modelsâ€
  In *The Semantic Webâ€“ISWC 2018: 17th International Semantic Web Conference, Monterey, CA, USA, October 8â€“12, 2018, Proceedings, Part I 17*, 2018, pp. 72â€“90
  Springer
* [70]
  Sepp Hochreiter and JÃ¼rgen Schmidhuber
  â€œLong short-term memoryâ€
  In *Neural computation* 9.8
  MIT Press, 1997, pp. 1735â€“1780
* [71]
  Alfred Horn
  â€œOn sentences which are true of direct unions of algebras1â€
  In *The Journal of Symbolic Logic* 16.1
  Cambridge University Press, 1951, pp. 14â€“21
* [72]
  Jui-Ting Huang et al.
  â€œEmbedding-based retrieval in facebook searchâ€
  In *Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining*, 2020, pp. 2553â€“2561
* [73]
  Nitisha Jain, Trung-Kien Tran, Mohamed H Gad-Elrab and Daria Stepanova
  â€œImproving knowledge graph embeddings with ontological reasoningâ€
  In *The Semantic Webâ€“ISWC 2021: 20th International Semantic Web Conference, ISWC 2021, Virtual Event, October 24â€“28, 2021, Proceedings*, 2021, pp. 410â€“426
  Springer
* [74]
  Shaoxiong Ji et al.
  â€œA survey on knowledge graphs: Representation, acquisition, and applicationsâ€
  In *IEEE Transactions on Neural Networks and Learning Systems* 33.2
  IEEE, 2021, pp. 494â€“514
* [75]
  Zoi Kaoudi, Abelardo Carlos Martinez Lorenzo and Volker Markl
  â€œTowards Loosely-Coupling Knowledge Graph Embeddings and Ontology-based Reasoningâ€
  In *arXiv preprint arXiv:2202.03173*, 2022
* [76]
  Amol Kapoor et al.
  â€œExamining covid-19 forecasting using spatio-temporal graph neural networksâ€
  In *arXiv preprint arXiv:2007.03113*, 2020
* [77]
  Henry A. Kautz
  â€œThe third AI summer: AAAI Robert S. Engelmore Memorial Lectureâ€
  In *AI Magazine* 43.1, 2022, pp. 105â€“125
  DOI: [https://doi.org/10.1002/aaai.12036](https://dx.doi.org/https://doi.org/10.1002/aaai.12036)
* [78]
  Mishal Kazmi, Peter SchÃ¼ller and YÃ¼cel SaygÄ±n
  â€œImproving scalability of inductive logic programming via pruning and best-effort optimisationâ€
  In *Expert Systems with Applications* 87
  Elsevier, 2017, pp. 291â€“303
* [79]
  Tal Kiani et al.
  â€œUtilizing Perturbation of Atomsâ€™ Positions for Equivariant Pre-Training in 3D Molecular Analysisâ€
  In *2023 IEEE 33rd International Workshop on Machine Learning for Signal Processing (MLSP)*, 2023, pp. 1â€“6
  IEEE
* [80]
  Sunghwan Kim et al.
  â€œPubChem substance and compound databasesâ€
  In *Nucleic Acids Research* 44.D1
  Oxford University Press, 2016, pp. D1202â€“D1213
* [81]
  Xiang Kong, Xianyang Chen and Eduard Hovy
  â€œDecompressing knowledge graph representations for link predictionâ€
  In *arXiv preprint arXiv:1911.04053*, 2019
* [82]
  Sophia Krix et al.
  â€œMultiGML: multimodal graph machine learning for prediction of adverse drug eventsâ€
  In *Heliyon* 9.9
  Elsevier, 2023
* [83]
  Alex Krizhevsky, Ilya Sutskever and Geoffrey E Hinton
  â€œImagenet classification with deep convolutional neural networksâ€
  In *Advances in Neural Information Processing Systems* 25, 2012
* [84]
  Markus KrÃ¶tzsch, Frantisek Simancik and Ian Horrocks
  â€œA description logic primerâ€
  In *arXiv preprint arXiv:1201.4089*, 2012
* [85]
  Maxat Kulmanov, Fatima Zohra Smaili, Xin Gao and Robert Hoehndorf
  â€œSemantic similarity and machine learning with ontologiesâ€
  In *Briefings in bioinformatics* 22.4
  Oxford University Press, 2021
* [86]
  Dana Lahat, TÃ¼lay Adali and Christian Jutten
  â€œMultimodal data fusion: an overview of methods, challenges, and prospectsâ€
  In *Proceedings of the IEEE* 103.9
  IEEE, 2015, pp. 1449â€“1477
* [87]
  Luis C Lamb et al.
  â€œGraph neural networks meet neural-symbolic computing: A survey and perspectiveâ€
  In *arXiv preprint arXiv:2003.00330*, 2020
* [88]
  Ibai Lana, Javier Del Ser, Manuel Velez and Eleni I Vlahogianni
  â€œRoad traffic forecasting: Recent advances and new challengesâ€
  In *IEEE Intelligent Transportation Systems Magazine* 10.2
  IEEE, 2018, pp. 93â€“109
* [89]
  Bentian Li and Dechang Pi
  â€œNetwork representation learning: a systematic literature reviewâ€
  In *Neural Computing and Applications* 32.21
  Springer, 2020, pp. 16647â€“16679
* [90]
  Jing Li, Shanshan Feng and Billy Chiu
  â€œFew-Shot Relation Extraction With Dual Graph Neural Network Interactionâ€
  In *IEEE Transactions on Neural Networks and Learning Systems*
  IEEE, 2023
* [91]
  Jun Li et al.
  â€œSparseness analysis in the pretraining of deep neural networksâ€
  In *IEEE Transactions on Neural Networks and Learning Systems* 28.6
  IEEE, 2016, pp. 1425â€“1438
* [92]
  Qika Lin et al.
  â€œRule-enhanced iterative complementation for knowledge graph reasoningâ€
  In *Information Sciences* 575
  Elsevier, 2021, pp. 66â€“79
* [93]
  Yushan Liu et al.
  â€œNeural multi-hop reasoning with logical rules on biomedical knowledge graphsâ€
  In *European Semantic Web Conference*, 2021, pp. 375â€“391
  Springer
* [94]
  Lorenzo Loconte, Nicola Di Mauro, Robert Peharz and Antonio Vergari
  â€œHow to Turn Your Knowledge Graph Embeddings into Generative Modelsâ€
  In *Thirty-seventh Conference on Neural Information Processing Systems*, 2023
* [95]
  Linyuan LÃ¼ and Tao Zhou
  â€œLink prediction in complex networks: A surveyâ€
  In *Physica A: Statistical Mechanics and its Applications* 390.6, 2011, pp. 1150â€“1170
  DOI: [https://doi.org/10.1016/j.physa.2010.11.027](https://dx.doi.org/https://doi.org/10.1016/j.physa.2010.11.027)
* [96]
  RiÄards MarcinkeviÄs and Julia E Vogt
  â€œInterpretability and explainability: A machine learning zoo mini-tourâ€
  In *arXiv preprint arXiv:2012.01805*, 2020
* [97]
  Giuseppe Marra, Michelangelo Diligenti and Francesco Giannini
  â€œLearning Representations for Sub-Symbolic Reasoningâ€
  In *arXiv preprint arXiv:2106.00393*, 2021
* [98]
  Giuseppe Marra and OndÅ™ej KuÅ¾elka
  â€œNeural markov logic networksâ€
  In *Uncertainty in Artificial Intelligence*, 2021, pp. 908â€“917
  PMLR
* [99]
  David M Mauger et al.
  â€œmRNA structure regulates protein expression through changes in functional half-lifeâ€
  In *Proceedings of the National Academy of Sciences* 116.48
  National Acad Sciences, 2019, pp. 24075â€“24083
* [100]
  Deborah L McGuinness and Frank Van Harmelen
  â€œOWL web ontology language overviewâ€
  In *W3C recommendation* 10.10, 2004, pp. 2004
* [101]
  Christian Meilicke, Melisachew Wudage Chekol, Daniel Ruffinelli and Heiner Stuckenschmidt
  â€œAnytime Bottom-Up Rule Learning for Knowledge Graph Completion.â€
  In *IJCAI*, 2019, pp. 3137â€“3143
* [102]
  Pasquale Minervini et al.
  â€œRegularizing knowledge graph embeddings via equivalence and inversion axiomsâ€
  In *Joint European Conference on Machine Learning and Knowledge Discovery in Databases*, 2017, pp. 668â€“683
  Springer
* [103]
  Christoph Molnar
  â€œInterpretable Machine Learningâ€, 2022
  URL: <https://christophm.github.io/interpretable-ml-book>
* [104]
  Christoph Molnar, Giuseppe Casalicchio and Bernd Bischl
  â€œInterpretable machine learningâ€“a brief history, state-of-the-art and challengesâ€
  In *Joint European conference on machine learning and knowledge discovery in databases*, 2020, pp. 417â€“431
  Springer
* [105]
  Stephen Muggleton
  â€œInverse entailment and Progolâ€
  In *New generation computing* 13.3
  Springer, 1995, pp. 245â€“286
* [106]
  Stephen Muggleton and Luc De Raedt
  â€œInductive logic programming: Theory and methodsâ€
  In *The Journal of Logic Programming* 19
  Elsevier, 1994, pp. 629â€“679
* [107]
  Ndapandula Nakashole, Mauro Sozio, Fabian M Suchanek and Martin Theobald
  â€œQuery-Time Reasoning in Uncertain RDF Knowledge Bases with Soft and Hard Rules.â€
  In *VLDS* 884, 2012, pp. 15â€“20
* [108]
  Maximilian Nickel, Volker Tresp and Hans-Peter Kriegel
  â€œA three-way model for collective learning on multi-relational dataâ€
  In *International Conference on Machine Learning*, 2011
* [109]
  Maximilian Nickel, Kevin Murphy, Volker Tresp and Evgeniy Gabrilovich
  â€œA review of relational machine learning for knowledge graphsâ€
  In *Proceedings of the IEEE* 104.1
  IEEE, 2015, pp. 11â€“33
* [110]
  Nils J Nilsson
  â€œProbabilistic logicâ€
  In *Artificial intelligence* 28.1
  Elsevier, 1986, pp. 71â€“87
* [111]
  Humaira Noor et al.
  â€œDetermining the Optimal Number of GAT and GCN Layers for Node Classification in Graph Neural Networksâ€
  In *2023 IEEE 8th International Conference On Software Engineering and Computer Systems (ICSECS)*, 2023, pp. 111â€“116
  IEEE
* [112]
  Natalia Norori et al.
  â€œAddressing bias in big data and AI for health care: A call for open scienceâ€
  In *Patterns* 2.10
  Elsevier, 2021, pp. 100347
* [113]
  Jacob P Olejarczyk and Michael Young
  â€œPatient Rights And Ethicsâ€
  StatPearls Publishing, Treasure Island (FL), 2022
  URL: <http://europepmc.org/books/NBK538279>
* [114]
  Simon Ott, Christian Meilicke and Matthias Samwald
  â€œSAFRAN: An interpretable, rule-based link prediction method outperforming embedding modelsâ€
  In *arXiv preprint arXiv:2109.08002*, 2021
* [115]
  Bryan Perozzi, Rami Al-Rfou and Steven Skiena
  â€œDeepWalk: Online Learning of Social Representationsâ€, KDD â€™14
  New York, New York, USA: Association for Computing Machinery, 2014, pp. 701â€“710
  DOI: [10.1145/2623330.2623732](https://dx.doi.org/10.1145/2623330.2623732)
* [116]
  Stephan Pleschka et al.
  â€œInfluenza virus propagation is impaired by inhibition of the Raf/MEK/ERK signalling cascadeâ€
  In *Nature cell biology* 3.3
  Nature Publishing Group, 2001, pp. 301â€“305
* [117]
  Meng Qu and Jian Tang
  â€œProbabilistic logic neural networks for reasoningâ€
  In *Advances in Neural Information Processing Systems* 32, 2019
* [118]
  Meng Qu et al.
  â€œRNNLogic: Learning Logic Rules for Reasoning on Knowledge Graphsâ€
  In *International Conference on Learning Representations*
* [119]
  J. Quinlan
  â€œLearning logical definitions from relationsâ€
  In *Machine learning* 5.3
  Springer, 1990, pp. 239â€“266
* [120]
  Md Saidur Rahman
  â€œBasic graph theoryâ€
  Springer, 2017
* [121]
  Neal Ravindra et al.
  â€œDisease state prediction from single-cell data using graph attention networksâ€
  In *Proceedings of the ACM conference on health, inference, and learning*, 2020, pp. 121â€“130
* [122]
  Matthew Richardson and Pedro Domingos
  â€œMarkov Logic Networksâ€
  In *Machine learning* 62.1
  Springer, 2006, pp. 107â€“136
* [123]
  Ryan Riegel et al.
  â€œLogical neural networksâ€
  In *arXiv preprint arXiv:2006.13155*, 2020
* [124]
  Daniel Rivas-Barragan, Daniel Domingo-FernÃ¡ndez, Yojana Gadiya and David Healey
  â€œEnsembles of knowledge graph embedding models improve predictions for drug discoveryâ€
  In *Briefings in Bioinformatics* 23.6
  Oxford University Press, 2022, pp. bbac481
* [125]
  Tim RocktÃ¤schel, Sameer Singh and Sebastian Riedel
  â€œInjecting logical background knowledge into embeddings for relation extractionâ€
  In *Proceedings of the 2015 conference of the north American Chapter of the Association for Computational Linguistics: Human Language Technologies*, 2015, pp. 1119â€“1129
* [126]
  Frank Rosenblatt
  â€œThe perceptron: a probabilistic model for information storage and organization in the brain.â€
  In *Psychological review* 65.6
  American Psychological Association, 1958, pp. 386
* [127]
  Benedek Rozemberczki et al.
  â€œPytorch geometric temporal: Spatiotemporal signal processing with neural machine learning modelsâ€
  In *Proceedings of the 30th ACM International Conference on Information & Knowledge Management*, 2021, pp. 4564â€“4573
* [128]
  T Konstantin Rusch, Michael M Bronstein and Siddhartha Mishra
  â€œA survey on oversmoothing in graph neural networksâ€
  In *arXiv preprint arXiv:2303.10993*, 2023
* [129]
  Ali Sadeghian, Mohammadreza Armandpour, Patrick Ding and Daisy Zhe Wang
  â€œDrum: End-to-end differentiable rule mining on knowledge graphsâ€
  In *Advances in Neural Information Processing Systems* 32, 2019
* [130]
  Yasamin Salimi et al.
  â€œADataViewer: exploring semantically harmonized Alzheimerâ€™s disease cohort datasetsâ€
  In *Alzheimerâ€™s Research & Therapy* 14.1
  BioMed Central, 2022, pp. 1â€“12
* [131]
  Md Kamruzzaman Sarker, Lu Zhou, Aaron Eberhart and Pascal Hitzler
  â€œNeuro-symbolic artificial intelligence: Current trendsâ€
  In *arXiv preprint arXiv:2105.05330*, 2021
* [132]
  Michael Schlichtkrull et al.
  â€œModeling relational data with graph convolutional networksâ€
  In *European semantic web conference*, 2018, pp. 593â€“607
  Springer
* [133]
  Stefan Schoenmackers, Jesse Davis, Oren Etzioni and Daniel S Weld
  â€œLearning first-order horn clauses from web textâ€
  In *Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing*, 2010, pp. 1088â€“1098
* [134]
  Lynn M Schriml et al.
  â€œThe Human Disease Ontology 2022 updateâ€
  In *Nucleic Acids Research* 50.D1
  Oxford University Press, 2022, pp. D1255â€“D1261
* [135]
  Bruce Schultz et al.
  â€œThe COVID-19 PHARMACOME: A method for the rational selection of drug repurposing candidates from multimodal knowledge harmonizationâ€
  In *bioRxiv*
  Cold Spring Harbor Laboratory, 2021, pp. 2020â€“09
* [136]
  Prithviraj Sen et al.
  â€œCombining Rules and Embeddings via Neuro-Symbolic AI for Knowledge Base Completionâ€
  In *arXiv preprint arXiv:2109.09566*, 2021
* [137]
  Zezhi Shao et al.
  â€œHeterogeneous Graph Neural Network With Multi-View Representation Learningâ€
  In *IEEE Transactions on Knowledge and Data Engineering* 35.11, 2023, pp. 11476â€“11488
  DOI: [10.1109/TKDE.2022.3224193](https://dx.doi.org/10.1109/TKDE.2022.3224193)
* [138]
  Chen Shengyuan et al.
  â€œDifferentiable Neuro-Symbolic Reasoning on Large-Scale Knowledge Graphsâ€
  In *Thirty-seventh Conference on Neural Information Processing Systems*, 2023
* [139]
  Alex Sherstinsky
  â€œFundamentals of recurrent neural network (RNN) and long short-term memory (LSTM) networkâ€
  In *Physica D: Nonlinear Phenomena* 404
  Elsevier, 2020, pp. 132306
* [140]
  Amit Singhal
  â€œIntroducing the knowledge graph: things, not stringsâ€
  In *Official google blog* 5, 2012, pp. 16
* [141]
  Sandro Skansi
  â€œIntroduction to Deep Learning: From Logical Calculus to Artificial Intelligenceâ€
  Springer Publishing Company, Incorporated, 2018
* [142]
  Ashwin Srinivasan
  â€œThe Aleph Manualâ€, 2001
  URL: <http://web.comlab.ox.ac.uk/oucl/research/areas/machlearn/Aleph/>
* [143]
  SÃ¶ren Richard Stahlschmidt, Benjamin Ulfenborg and Jane Synnergren
  â€œMultimodal deep learning for biomedical data fusion: a reviewâ€
  In *Briefings in Bioinformatics* 23.2
  Oxford University Press, 2022, pp. bbab569
* [144]
  Fabian M Suchanek, Gjergji Kasneci and Gerhard Weikum
  â€œYago: a core of semantic knowledgeâ€
  In *Proceedings of the 16th international conference on World Wide Web*, 2007, pp. 697â€“706
* [145]
  Zhengya Sun et al.
  â€œScalable learning and inference in Markov logic networksâ€
  In *International Journal of Approximate Reasoning* 82
  Elsevier, 2017, pp. 39â€“55
* [146]
  Susheel Suresh and Jennifer Neville
  â€œA hybrid model for learning embeddings and logical rules simultaneously from knowledge graphsâ€
  In *2020 IEEE International Conference on Data Mining (ICDM)*, 2020, pp. 1280â€“1285
  IEEE
* [147]
  Zachary Susskind et al.
  â€œNeuro-Symbolic AI: An Emerging Class of AI Workloads and their Characterizationâ€
  In *arXiv preprint arXiv:2109.06133*, 2021
* [148]
  Peter Tino, Lubica Benuskova and Alessandro Sperduti
  â€œArtificial neural network modelsâ€
  In *Springer Handbook of Computational Intelligence*
  Springer, 2015, pp. 455â€“471
* [149]
  Erico Tjoa and Cuntai Guan
  â€œA survey on explainable Artificial Intelligence (XAI): Toward medical XAIâ€
  In *IEEE Transactions on Neural Networks and Learning Systems* 32.11
  IEEE, 2020, pp. 4793â€“4813
* [150]
  ThÃ©o Trouillon et al.
  â€œComplex embeddings for simple link predictionâ€
  In *International conference on machine learning*, 2016, pp. 2071â€“2080
  PMLR
* [151]
  Efthymia Tsamoura, Timothy Hospedales and Loizos Michael
  â€œNeural-symbolic integration: A compositional perspectiveâ€
  In *Proceedings of the AAAI Conference on Artificial Intelligence* 35, 2021, pp. 5051â€“5060
* [152]
  Efthymia Tsamoura and Loizos Michael
  â€œNeural-symbolic integration: A compositional perspectiveâ€
  In *arXiv preprint arXiv:2010.11926*, 2020
* [153]
  Petar VeliÄkoviÄ‡ et al.
  â€œGraph attention networksâ€
  In *arXiv preprint arXiv:1710.10903*, 2017
* [154]
  Deepak Venugopal and Vibhav G Gogate
  â€œScaling-up importance sampling for Markov logic networksâ€
  In *Advances in Neural Information Processing Systems* 27, 2014
* [155]
  Antonio Vergari, Y Choi, Robert Peharz and Guy Van den Broeck
  â€œProbabilistic circuits: Representations, inference, learning and applicationsâ€
  In *AAAI Tutorial*, 2020
* [156]
  Chad Vicknair et al.
  â€œA comparison of a graph database and a relational database: a data provenance perspectiveâ€
  In *Proceedings of the 48th annual Southeast regional conference*, 2010, pp. 1â€“6
* [157]
  Jiapu Wang et al.
  â€œA Survey on Temporal Knowledge Graph Completion: Taxonomy, Progress, and Prospectsâ€, 2023
  arXiv:[2308.02457 [cs.AI]](https://arxiv.org/abs/2308.02457)
* [158]
  Jie Wang, Jianqing Liang, Jiye Liang and Kaixuan Yao
  â€œGUIDE: Training Deep Graph Neural Networks via Guided Dropout Over Edgesâ€
  In *IEEE Transactions on Neural Networks and Learning Systems*
  IEEE, 2022
* [159]
  Yuyang Wang, Jianren Wang, Zhonglin Cao and Amir Barati Farimani
  â€œMolCLR: molecular contrastive learning of representations via graph neural networksâ€
  In *arXiv preprint arXiv:2102.10056*, 2021
* [160]
  Luisa Werner, Nabil LayaÄ±Ìˆda, Pierre GenevÃ¨s and Sarah Chlyah
  â€œKnowledge Enhanced Graph Neural Networksâ€
  In *arXiv preprint arXiv:2303.15487*, 2023
* [161]
  Ruwan Wickramarachchi, Cory Henson and Amit Sheth
  â€œAn evaluation of knowledge graph embeddings for autonomous driving data: Experience and practiceâ€
  In *arXiv preprint arXiv:2003.00344*, 2020
* [162]
  Colby Wise et al.
  â€œCOVID-19 knowledge graph: accelerating information retrieval and discovery for scientific literatureâ€
  In *arXiv preprint arXiv:2007.12731*, 2020
* [163]
  Zonghan Wu et al.
  â€œA comprehensive survey on graph neural networksâ€
  In *IEEE Transactions on Neural Networks and Learning Systems* 32.1
  IEEE, 2020, pp. 4â€“24
* [164]
  Haoyi Xiong et al.
  â€œPredicting traffic congestion propagation patterns: A propagation graph approachâ€
  In *Proceedings of the 11th ACM SIGSPATIAL International Workshop on Computational Transportation Science*, 2018, pp. 60â€“69
* [165]
  Jingyi Xu et al.
  â€œA Semantic Loss Function for Deep Learning with Symbolic Knowledgeâ€
  In *Proceedings of the 35th International Conference on Machine Learning* 80, Proceedings of Machine Learning Research
  PMLR, 2018, pp. 5502â€“5511
* [166]
  Yujun Yan et al.
  â€œTwo sides of the same coin: Heterophily and oversmoothing in graph convolutional neural networksâ€
  In *2022 IEEE International Conference on Data Mining (ICDM)*, 2022, pp. 1287â€“1292
  IEEE
* [167]
  Bishan Yang et al.
  â€œEmbedding entities and relations for learning and inference in knowledge basesâ€
  In *arXiv preprint arXiv:1412.6575*, 2014
* [168]
  Fan Yang, Zhilin Yang and William W Cohen
  â€œDifferentiable learning of logical rules for knowledge base reasoningâ€
  In *Advances in Neural Information Processing Systems* 30, 2017
* [169]
  Liangwei Yang et al.
  â€œConsisRec: Enhancing gnn for social recommendation via consistent neighbor aggregationâ€
  In *Proceedings of the 44th international ACM SIGIR conference on Research and development in information retrieval*, 2021, pp. 2141â€“2145
* [170]
  Yu Yao, Baosheng Yu, Chen Gong and Tongliang Liu
  â€œUnderstanding how pretraining regularizes deep learning algorithmsâ€
  In *IEEE Transactions on Neural Networks and Learning Systems*
  IEEE, 2021
* [171]
  Jia-Li Yin, Bo-Hao Chen, Kuo-Hua Robert Lai and Ying Li
  â€œAutomatic dangerous driving intensity analysis for advanced driver assistance systems from multimodal driving signalsâ€
  In *IEEE Sensors Journal* 18.12
  IEEE, 2017, pp. 4785â€“4794
* [172]
  Bing Yu, Haoteng Yin and Zhanxing Zhu
  â€œSpatio-temporal graph convolutional networks: A deep learning framework for traffic forecastingâ€
  In *arXiv preprint arXiv:1709.04875*, 2017
* [173]
  Zhongqi Yue, Hanwang Zhang, Qianru Sun and Xian-Sheng Hua
  â€œInterventional few-shot learningâ€
  In *Advances in Neural Information Processing Systems* 33, 2020, pp. 2734â€“2746
* [174]
  Lotfi A Zadeh
  â€œFuzzy logicâ€
  In *Computer* 21.4
  IEEE, 1988, pp. 83â€“93
  DOI: [10.1109/2.53](https://dx.doi.org/10.1109/2.53)
* [175]
  Chuxu Zhang et al.
  â€œFew-shot knowledge graph completionâ€
  In *Proceedings of the AAAI Conference on Artificial Intelligence* 34.03, 2020, pp. 3041â€“3048
* [176]
  Jindou Zhang and Jing Li
  â€œEnhanced knowledge graph embedding by jointly learning soft rules and factsâ€
  In *Algorithms* 12.12
  MDPI, 2019, pp. 265
* [177]
  Jing Zhang et al.
  â€œNeural, symbolic and neural-symbolic reasoning on knowledge graphsâ€
  In *AI Open* 2
  Elsevier, 2021, pp. 14â€“35
* [178]
  Muhan Zhang and Yixin Chen
  â€œLink prediction based on graph neural networksâ€
  In *Advances in Neural Information Processing Systems*, 2018, pp. 5165â€“5175
* [179]
  Rui Zhang et al.
  â€œDrug repurposing for COVID-19 via knowledge graph completionâ€
  In *Journal of biomedical informatics* 115
  Elsevier, 2021, pp. 103696
* [180]
  Wen Zhang et al.
  â€œIteratively learning embeddings and rules for knowledge graph reasoningâ€
  In *The World Wide Web Conference*, 2019, pp. 2366â€“2377
* [181]
  Yuyu Zhang et al.
  â€œEfficient Probabilistic Logic Reasoning with Graph Neural Networksâ€
  In *International Conference on Learning Representations*
* [182]
  Zheng Zhang, Levent Yilmaz and Bo Liu
  â€œA Critical Review of Inductive Logic Programming Techniques for Explainable AIâ€
  In *IEEE Transactions on Neural Networks and Learning Systems*
  IEEE, 2023
* [183]
  Feng Zhao, Tiancheng Huang and Donglin Wang
  â€œGraph few-shot learning via restructuring task graphâ€
  In *IEEE Transactions on Neural Networks and Learning Systems*
  IEEE, 2022
* [184]
  Sendong Zhao, Bing Qin, Ting Liu and Fei Wang
  â€œBiomedical knowledge graph refinement with embedding and logic rulesâ€
  In *arXiv preprint arXiv:2012.01031*, 2020
* [185]
  Yue-ting Zhuang, Fei Wu, Chun Chen and Yun-he Pan
  â€œChallenges and opportunities: from big data to knowledge in AI 2.0â€
  In *Frontiers of Information Technology & Electronic Engineering* 18.1
  Springer, 2017, pp. 3â€“14
* [186]
  Marinka Zitnik, Monica Agrawal and Jure Leskovec
  â€œModeling polypharmacy side effects with graph convolutional networksâ€
  In *Bioinformatics* 34.13
  Oxford University Press, 2018, pp. i457â€“i466