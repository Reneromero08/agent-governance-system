# Latent Refinement Decoding: Enhancing Diffusion-Based Language Models by Refining Belief States

Qinglin Zhu1,âˆ— Yizhen Yao1,âˆ— Runcong Zhao1,â€  Yanzheng Xiang1 Amrutha Saseendran3
  
Chen Jin3 Philip Teare3â€ƒBin Liang4,5â€ƒYulan He1,2â€ƒLin Gui1â€ƒ
  
1 Kingâ€™s College London, UK â€ƒâ€ƒ2 The Alan Turing Institute, UK â€ƒ
  
3 Centre for AI, Data Science & Artificial Intelligence, BioPharmaceuticals R&D, AstraZeneca, UK
  
4 The Chinese University of Hong Kong â€ƒâ€ƒ5 MoE Lab, CUHK
  
{qinglin.1.zhu,yizhen.yao,runcong.zhao,yanzheng.xiang}@kcl.ac.uk
  
{philip.teare,amrutha.saseendran,chen.jin}@astrazeneca.com
  
{bin.liang}@cuhk.edu.hk â€ƒ{yulan.he,lin.1.gui}@kcl.ac.uk

###### Abstract

Autoregressive (AR) models remain the standard for natural language generation but still suffer from high latency due to strictly sequential decoding.
Recent diffusion-inspired approaches, such as LlaDA and Dream, mitigate this by generating in parallel, yet they suffer from two core limitations: information loss, as predictive distributions for non-finalized tokens are discarded at each step, and premature commitment, where local decisions are made without sufficient global coordination.
We introduce Latent Refinement Decoding (LRD), a two-stage framework with Latent Refinement and a Predictive Feedback Loop. The first stage maintains masked positions as distributional mixtures of predicted tokens and the mask embedding, allowing the model to establish more globally consistent beliefs. The second stage progressively finalizes confident tokens while retaining uncertain ones for iterative feedback. KL-divergence dynamics provide a principled and reliable criterion for convergence and early stopping.
Experiments across coding (HumanEval +6.3, MBPP +2.6) and reasoning (GSM8K +2.9, MATH500 +3.8) show that LRD improves accuracy while delivering speedups of up to 10.6Ã—, making it a strong and versatile alternative for parallel sequence generation.

\*\*footnotetext: Equal contribution.â€ â€ footnotetext: Corresponding Author.

## 1 Introduction

Autoregressive (AR) models have long defined the standard for natural language generationÂ (Brown etÂ al., [2020](#bib.bib5); Fei etÂ al., [2025](#bib.bib10); Achiam etÂ al., [2023](#bib.bib1); Yang etÂ al., [2025](#bib.bib38)), but their inherently sequential token-by-token decoding imposes a fundamental bottleneck on inference latencyÂ (Touvron etÂ al., [2023](#bib.bib35); Sun etÂ al., [2024](#bib.bib34)). This constraint has motivated the development of parallel decoding paradigms. Among them, diffusion-inspired approaches such as LLaDAÂ (Nie etÂ al., [2025](#bib.bib28)) and DreamÂ (Ye etÂ al., [2025](#bib.bib40)) offer a particularly promising direction. By formulating text generation as an iterative refinement process and updating all token positions in parallel at each step, these methods provide a compelling alternative to traditional AR decoding, achieving significant speedups while maintaining competitive qualityÂ (Labs etÂ al., [2025](#bib.bib17); Deepmind, [2025](#bib.bib9)).
Despite recent progress, diffusion language models employ hard assignment strategiesÂ (Gong etÂ al., [2025](#bib.bib11); Nie etÂ al., [2025](#bib.bib28); Ye etÂ al., [2025](#bib.bib40)): at each denoising step, they commit high-confidence positions to specific tokens while resetting remaining positions to uniform [MASK] tokens. The predictive distributions from earlier steps are discarded, limiting the modelâ€™s ability to build upon partial beliefs established in earlier iterations.

This design introduces two limitations: (i) Information loss from hard maskingÂ (Li etÂ al., [2024](#bib.bib21)): At each denoising step, positions below confidence thresholds are reset to uniform [MASK] embeddings, completely discarding their predictive distributions. This prevents uncertain positions from sharing probabilistic information through self-attention, forcing each masked position to be predicted in isolation. When mispredictions occur, the hard assignment yields infinite KL divergence from the true posterior, as it assigns zero probability mass to the correct token.
(ii) Inefficient convergence dynamicsÂ (Luxembourg etÂ al., [2025](#bib.bib25); Li & Cai, [2025](#bib.bib18)): The binary nature of hard assignment creates a dilemma:
aggressive selection commits early and can lock in incorrect predictions, propagating errors through later steps; conservative selection keeps many positions masked, which slows progress and requires many denoising iterations.
Moreover, using a fixed number of iterations ignores the varying complexity across different generation tasks, wasting computation on simple cases while potentially underserving complex ones.

To overcome these limitations, we move beyond purely discrete denoising and introduce Latent Refinement Decoding (LRD), a hybrid framework that operates in both embedding and token spaces. LRD restructures the denoising process into two coordinated stages. Phase 1: Latent Refinement performs distribution-preserving updates entirely in the embedding space: for each masked position, we form a mix embedding by mixing the [MASK] embedding with the entropy-normalised expectation over top-pp predicted token embeddings, allowing
the model to â€œthink latentlyâ€ in continuous embedding space, establishing globally coherent beliefs before committing to discrete decisions.
Once the predictive distributions stabilise, Phase 2: Predictive Feedback Loop progressively converts low-entropy positions into discrete tokens while keeping the remaining positions in soft form, feeding each stepâ€™s predictions back into the next soft mixture; KL-based monitors govern the soft-to-hard transition and enable adaptive early stopping.
Specifically, the main contributions of LRD are:

1. 1.

   Soft diffusion that enables continuous denoising in embedding space by mixing [MASK] with weighted token representations. This preserves distributional information across steps and enables cross-position refinement through self-attention.
2. 2.

   Adaptive two-phase sampling that combines soft refinement for global coherence with hard decoding for precise convergence. KL-based monitoring enables automatic phase transitions and early stopping based on actual convergence rather than fixed iteration counts.
3. 3.

   We validate LRD across diverse model families, generation lengths, and benchmarks spanning coding (HumanEval: +6.3, MBPP: +2.6) and reasoning (GSM8K: +2.9, MATH500: +3.8), consistently improving accuracy while achieving speedups of up to 10.6Ã—.

![Refer to caption](/html/2510.11052/assets/x1.png)

Figure 1: Comparison between the existing decoding strategy and the proposed method. Different colours represent distinct tokens, while gradient colours indicate predicted token representations. Top: In the existing strategy, all [MASK] tokens share the same embedding and are repeatedly remasked if not selected. Bottom: In LRD, Phase 1 refines each [MASK] embedding, and Phase 2 progressively commits confident tokens while keeping uncertain ones soft for context-aware decoding.

## 2 Preliminary

For dLLMs (Ou etÂ al., [2024](#bib.bib29); Zheng etÂ al., [2024](#bib.bib44); Shi etÂ al., [2024](#bib.bib31); Gong etÂ al., [2025](#bib.bib11)), the forward process corrupts data ğ±0âˆˆ{1,â€¦,V}L\mathbf{x}\_{0}\in\{1,...,V\}^{L} (a sequence of LL tokens from vocabulary size VV) into progressively noisier versions ğ±1,â€¦,ğ±T\mathbf{x}\_{1},...,\mathbf{x}\_{T}. At each timestep, the forward process is defined as a categorical distribution:

|  |  |  |  |
| --- | --- | --- | --- |
|  | qâ€‹(ğ±t|ğ±tâˆ’1)=Catâ€‹(ğ±t;ğtâŠ¤â€‹ğ±tâˆ’1)q(\mathbf{x}\_{t}|\mathbf{x}\_{t-1})=\text{Cat}(\mathbf{x}\_{t};\mathbf{Q}\_{t}^{\top}\mathbf{x}\_{t-1}) |  | (1) |

where ğ±tâˆˆ{0,1}VÃ—L\mathbf{x}\_{t}\in\{0,1\}^{V\times L} is the one-hot representation of tokens at time tt, and ğtâˆˆ[0,1]VÃ—V\mathbf{Q}\_{t}\in[0,1]^{V\times V} is the transition matrix. Each token either remains unchanged with probability 1âˆ’Î²t1-\beta\_{t} or transitions to the special [MASK] token with probability Î²tâˆˆ(0,1)\beta\_{t}\in(0,1): ğt=(1âˆ’Î²t)â€‹ğˆ+Î²tâ€‹ğŸâ€‹ğ¦âŠ¤\mathbf{Q}\_{t}=(1-\beta\_{t})\mathbf{I}+\beta\_{t}\mathbf{1}\mathbf{m}^{\top}, where ğˆâˆˆâ„VÃ—V\mathbf{I}\in\mathbb{R}^{V\times V} is the identity matrix, ğŸâˆˆâ„V\mathbf{1}\in\mathbb{R}^{V} is an all-ones vector, and ğ¦âˆˆ{0,1}V\mathbf{m}\in\{0,1\}^{V} is the one-hot encoding of the [MASK] token. Under continuous-time formulation with tâˆˆ[0,1]t\in[0,1], the cumulative transition matrix from ğ±0\mathbf{x}\_{0} to ğ±t\mathbf{x}\_{t} becomes: ğÂ¯t=Î±tâˆ—â€‹ğˆ+(1âˆ’Î±tâˆ—)â€‹ğŸâ€‹ğ¦âŠ¤\overline{\mathbf{Q}}\_{t}=\alpha\_{t}^{\*}\mathbf{I}+(1-\alpha\_{t}^{\*})\mathbf{1}\mathbf{m}^{\top}, where Î±tâˆ—=âˆs=1t(1âˆ’Î²s)\alpha\_{t}^{\*}=\prod\_{s=1}^{t}(1-\beta\_{s}) represents the probability of a token remaining unmasked from time 0 to time tt.

The reverse process pÎ¸â€‹(ğ±tâˆ’1|ğ±t)p\_{\theta}(\mathbf{x}\_{t-1}|\mathbf{x}\_{t}) aims to reconstruct the original data by iteratively denoising from ğ±T\mathbf{x}\_{T} (fully masked) to ğ±0\mathbf{x}\_{0} (clean text). At each denoising step tt, the model predicts a distribution over tokens for each position: p^Î¸(i)â€‹(ğ±0|ğ±t)\hat{p}\_{\theta}^{(i)}(\mathbf{x}\_{0}|\mathbf{x}\_{t}) for position ii.

In transformer-based diffusion models, each token is represented by a learnable embedding vector. Let ğvâˆˆâ„d\mathbf{e}\_{v}\in\mathbb{R}^{d} denote the embedding for token vâˆˆVv\in V, and ğ[MASK]âˆˆâ„d\mathbf{e}\_{[\text{MASK}]}\in\mathbb{R}^{d} the embedding for the [MASK] token. During the reverse process, traditional sampling strategies employ hard assignment, selecting tokens based on prediction confidence:

|  |  |  |  |
| --- | --- | --- | --- |
|  | vt(i)={argâ€‹maxvâˆˆVâ¡p^Î¸(i)â€‹(v|ğ±t),ifÂ â€‹iâˆˆtop-1â€‹({Ht(j)}j=1L)[MASK],otherwisev\_{t}^{(i)}=\begin{cases}\operatorname\*{arg\,max}\_{v\in V}\;\hat{p}\_{\theta}^{(i)}(v|\mathbf{x}\_{t}),&\text{if }i\in\text{top-1}(\{H\_{t}^{(j)}\}\_{j=1}^{L})\\ \texttt{[MASK]},&\text{otherwise}\end{cases} |  | (2) |

where Ht(j)=âˆ’âˆ‘vp^Î¸(j)â€‹(v|ğ±t)â€‹logâ¡p^Î¸(j)â€‹(v|ğ±t)H\_{t}^{(j)}=-\sum\_{v}\hat{p}\_{\theta}^{(j)}(v|\mathbf{x}\_{t})\log\hat{p}\_{\theta}^{(j)}(v|\mathbf{x}\_{t}) is the entropy at position jj, and top-11 selects the position with lowest entropy (highest confidence). This creates a binary embedding assignment: each position uses either ğ[MASK]\mathbf{e}\_{[\text{MASK}]} or a specific token embedding ğvt(i)\mathbf{e}\_{v\_{t}^{(i)}}, resulting in complete information loss for positions not selected. This binary decision mechanism creates a discontinuous mapping from probability distributions to discrete embeddings: positions below the confidence threshold are reset to pure [MASK] embeddings, completely discarding their distributional information p^Î¸(â‹…|ğ±t)\hat{p}\_{\theta}(\cdot|\mathbf{x}\_{t}), resulting in abrupt information loss and suboptimal exploration of the posterior distribution.

## 3 Methodology

Effective discrete diffusion sampling requires maintaining sufficient uncertainty for exploration while gradually incorporating token-specific information for convergence. To achieve this balance, we propose LRD: instead of binary decisions that abruptly switch between pure noise ([MASK]) and deterministic tokens, we create intermediate representations through continuous embedding interpolation. Specifically, we construct mixed embeddings that blend [MASK] and token embeddings weighted by prediction uncertainty, where high-entropy positions retain more mask-like characteristics (preserving exploration) while low-entropy positions incorporate more token information (enabling commitment). This enables a gradual denoising trajectory where the noise-signal ratio smoothly decreases, yielding better-calibrated probability distributions for subsequent sampling steps.

### 3.1 Soft Diffusion

Our method operates in the embedding space rather than discrete token space. At each timestep tt, we maintain a set of soft embeddings â„°t={ğ~t(1),â€¦,ğ~t(N)}\mathcal{E}\_{t}=\{\tilde{\mathbf{e}}\_{t}^{(1)},\ldots,\tilde{\mathbf{e}}\_{t}^{(N)}\} where

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ~t(i)=(1âˆ’Î±t(i))â‹…ğ[MASK]+Î±t(i)â‹…âˆ‘vâˆˆğ’¯t+1(i)pÂ¯t+1(i)â€‹(v)â‹…ğv\tilde{\mathbf{e}}\_{t}^{(i)}=(1-\alpha\_{t}^{(i)})\cdot\mathbf{e}\_{\texttt{[MASK]}}+\alpha\_{t}^{(i)}\cdot\sum\_{v\in\mathcal{T}\_{t+1}^{(i)}}\bar{p}\_{t+1}^{(i)}(v)\cdot\mathbf{e}\_{v} |  | (3) |

Here, ğvâˆˆâ„d\mathbf{e}\_{v}\in\mathbb{R}^{d} denotes the embedding of the token vâˆˆğ’¯t+1(i)v\in\mathcal{T}\_{t+1}^{(i)}, where ğ’¯t+1(i)\mathcal{T}\_{t+1}^{(i)} is the top-pp nucleus set. ğ[MASK]\mathbf{e}\_{\texttt{[MASK]}} is the [MASK] embedding,
pÂ¯t+1(i)â€‹(v)\bar{p}\_{t+1}^{(i)}(v) denotes the probability mass of token vv at position ii, renormalised to the nucleus set ğ’¯t+1(i)\mathcal{T}\_{t+1}^{(i)}.
The coefficient Î±t(i)âˆˆ[0,1]\alpha\_{t}^{(i)}\in[0,1] controls the interpolation strength.
The mixing weight Î±t\alpha\_{t} is controlled by entropy:

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î±t(i)=rfâ‹…(1âˆ’H^t+1(i))=rfâ‹…(1+âˆ‘k=1|V|pt+1(i)â€‹(k)â€‹logâ¡pt+1(i)â€‹(k)logâ¡|V|)\alpha\_{t}^{(i)}=r\_{f}\cdot(1-\hat{H}\_{t+1}^{(i)})=r\_{f}\cdot(1+\frac{\sum\_{k=1}^{|V|}p\_{t+1}^{(i)}(k)\log p\_{t+1}^{(i)}(k)}{\log|V|}) |  | (4) |

where pt+1(i)â€‹(k)p\_{t+1}^{(i)}(k) refers to the probability distribution over the full vocabulary, H^t+1(i)\hat{H}\_{t+1}^{(i)} is the normalised entropy of this distribution, and rfâˆˆ(0,1]r\_{f}\in(0,1] sets the maximum interpolation strength. Since the entropy of a categorical distribution over a vocabulary of size VV lies in [0,logâ¡|V|][0,\log|V|], we divide by logâ¡|V|\log|V| to normalise it into [0,1][0,1].
This design ensures that uncertain positions stay mask-like while confident ones commit to tokens. Formal justification and stability analysis are deferred to AppendixÂ [D](#A4 "Appendix D Stability Analysis of Mixed Embedding Updates â€£ Latent Refinement Decoding: Enhancing Diffusion-Based Language Models by Refining Belief States").

Consider the absorbing discrete diffusion process where the true posterior distribution qâˆ—â€‹(xtâˆ’1|xt,x0)q^{\*}(x\_{t-1}|x\_{t},x\_{0}) represents optimal denoising. For masked positions where xt(i)=[MASK]x\_{t}^{(i)}=\texttt{[MASK]}, Bayesâ€™ rule yields:

|  |  |  |  |
| --- | --- | --- | --- |
|  | qâˆ—â€‹(xtâˆ’1(i)|xt(i)=[MASK],x0(i))=Î±tâˆ’1âˆ—âˆ’Î±tâˆ—1âˆ’Î±tâˆ—â€‹Î´x0(i)+1âˆ’Î±tâˆ’1âˆ—1âˆ’Î±tâˆ—â€‹Î´[MASK]q^{\*}(x\_{t-1}^{(i)}|x\_{t}^{(i)}=\texttt{[MASK]},x\_{0}^{(i)})=\frac{\alpha\_{t-1}^{\*}-\alpha\_{t}^{\*}}{1-\alpha\_{t}^{\*}}\delta\_{x\_{0}^{(i)}}+\frac{1-\alpha\_{t-1}^{\*}}{1-\alpha\_{t}^{\*}}\delta\_{\texttt{[MASK]}} |  | (5) |

where the detailed derivation is provided in AppendixÂ [B](#A2 "Appendix B Derivation of the True Posterior in the Masking Process â€£ Latent Refinement Decoding: Enhancing Diffusion-Based Language Models by Refining Belief States"). Hard assignment approximates this by a degenerate distribution
q^hardâˆˆ{Î´x^0(i),Î´[MASK]}\hat{q}\_{\mathrm{hard}}\in\{\delta\_{\hat{x}\_{0}^{(i)}},\delta\_{\texttt{[MASK]}}\}.
If x^0(i)â‰ x0(i)\hat{x}\_{0}^{(i)}\neq x\_{0}^{(i)}, then q^hard\hat{q}\_{\mathrm{hard}} assigns zero probability where qâˆ—q^{\*} is positive, leading to KLâ€‹(qâˆ—âˆ¥q^hard)=âˆ\mathrm{KL}(q^{\*}\|\hat{q}\_{\mathrm{hard}})=\infty. Moreover, positions that remain masked are represented by a fixed embedding ğ[MASK]\mathbf{e}\_{\texttt{[MASK]}}, which conveys no distributional information to neighbouring positions.

Latent Refinement Decoding mitigates both issues. First, q^soft\hat{q}\_{\mathrm{soft}} assigns non-zero probability to all tokens, ensuring the true token retains a positive mass even under misprediction. Second, the weighted mixture âˆ‘vpÂ¯t(i)â€‹(v)â€‹ğv\sum\_{v}\bar{p}\_{t}^{(i)}(v)\mathbf{e}\_{v} can be viewed as the expected embedding under the modelâ€™s belief at position ii. Since self-attention is linear in the embeddings, this representation propagates uncertainty information across positions, enabling different tokens to condition on each otherâ€™s belief states.

### 3.2 Adaptive Sampling with Soft-to-Hard Scheduling

The optimal denoising strategy must balance two objectives: preserving sufficient uncertainty for exploration while progressively reducing entropy for convergence. Latent Refinement Decoding provides a smooth relaxation in the embedding space, where gradient-based updates are well behaved and guarantee contraction toward fixed points. This geometry enables rapid early progress, as the gradients carry informative signals across the entire vocabulary. However, Latent Refinement Decoding cannot fully collapse distributions to one-hot states, since embeddings always encode mixtures rather than discrete commitments. As a result, convergence slows in later stages when sharper updates are required for final token generation.

To overcome this limitation, we adopt a two-phase schedule. Phase 1 exploits the favourable geometry of soft embeddings to quickly reach a stable neighborhood of the optimum. Once the modelâ€™s predictive distributions stabilise (ğ’ŸKL(t)<Ï„refine\mathcal{D}\_{\text{KL}}^{(t)}<\tau\_{\text{refine}}), Phase 2 transitions to hard assignment, which enables decisive discrete optimisation within the well-conditioned basin. This design follows the principle of graduated optimisation: begin with a smooth relaxation to encourage global exploration, then progressively sharpen the objective to encourage convergence.

#### Phase 1: Latent Refinement via Soft Embeddings.

During the initial refinement phase, the model iteratively refines predictive distributions through soft embedding propagation without committing to any discrete tokens. Starting from t=Tt=T (fully masked), we compute soft embeddings using EquationÂ [3](#S3.E3 "In 3.1 Soft Diffusion â€£ 3 Methodology â€£ Latent Refinement Decoding: Enhancing Diffusion-Based Language Models by Refining Belief States"), where predictions pt(i)â€‹(v)=dâ€‹Lâ€‹Lâ€‹MÎ¸â€‹(â„°~t)(i)p\_{t}^{(i)}(v)=dLLM\_{\theta}(\tilde{\mathcal{E}}\_{t})^{(i)} are conditioned on the previous soft embeddings rather than discrete tokens. This allows distributional information to propagate across timesteps.

As refinement progresses, the soft embeddings approach a fixed point where the modelâ€™s predictions become self-consistent, that is, the output distribution given the current soft embeddings closely matches the distribution encoded in those embeddings. At this convergence point, the model has extracted all available information from the global distributional structure and further soft refinement yields diminishing returns. We detect this saturation by monitoring the KL divergence between consecutive predictions:

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ’ŸKL(t)=1Lâ€‹âˆ‘i=1LDKLâ€‹(pt(i)âˆ¥pt+1(i))\mathcal{D}\_{\text{KL}}^{(t)}=\frac{1}{L}\sum\_{i=1}^{L}D\_{\text{KL}}(p\_{t}^{(i)}\|p\_{t+1}^{(i)}) |  | (6) |

When ğ’ŸKL(t)<Ï„refine\mathcal{D}\_{\text{KL}}^{(t)}<\tau\_{\text{refine}}, the belief state has stabilised, indicating that the model can no longer benefit from the soft embeddingâ€™s global information and requires discrete commitments to make further progress. This triggers the transition to Phase 2, where discrete token generation can exploit the well-initialised distributions from Phase 1.
Alternatively, if convergence is not achieved within TrefineT\_{\text{refine}} steps, we still transition to Phase 2 for computational efficiency, as extended refinement shows diminishing returns while incurring additional computational cost.

#### Phase 2: Predictive Feedback Loop.

Once convergence is detected at timestep tâˆ—t^{\*}, we switch to Predictive Feedback decoding for the remaining timesteps tâˆˆ[tâˆ—,0]t\in[t^{\*},0]. We modify the standard hard assignment (EquationÂ [2](#S2.E2 "In 2 Preliminary â€£ Latent Refinement Decoding: Enhancing Diffusion-Based Language Models by Refining Belief States")) by replacing [MASK] embeddings with soft embeddings for unselected positions:

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğt(i)={ğargâ¡maxvâ¡pt(i)â€‹(v),ifÂ â€‹iâˆˆtop-â€‹1â€‹({Ht(j)}j=1L)ğ~t(i),otherwise\mathbf{e}\_{t}^{(i)}=\begin{cases}\mathbf{e}\_{\arg\max\_{v}p\_{t}^{(i)}(v)},&\text{if }i\in\text{top-}1(\{H\_{t}^{(j)}\}\_{j=1}^{L})\\ \tilde{\mathbf{e}}\_{t}^{(i)},&\text{otherwise}\end{cases} |  | (7) |

This preserves the distributional information from Phase 1â€™s refinement in uncommitted positions, providing richer context for subsequent decoding steps while still allowing confident positions to make discrete commitments.

During decoding, we continue monitoring ğ’ŸKL(t)\mathcal{D}\_{\text{KL}}^{(t)} (EquationÂ [6](#S3.E6 "In Phase 1: Latent Refinement via Soft Embeddings. â€£ 3.2 Adaptive Sampling with Soft-to-Hard Scheduling â€£ 3 Methodology â€£ Latent Refinement Decoding: Enhancing Diffusion-Based Language Models by Refining Belief States")). If ğ’ŸKL(t)<Ï„decode\mathcal{D}\_{\text{KL}}^{(t)}<\tau\_{\text{decode}}, the predictive distributions over the whole sentence have converged to a stable configuration and further iterations would be redundant. This early stopping mechanism terminates the generation and outputs the final sequence, ensuring computational efficiency without sacrificing output quality. In practice, this allows the model to adaptively adjust its generation length based on the problem complexity rather than using a fixed number of steps.

## 4 Experiments

### 4.1 Implementation Details

We evaluate our method on two representative diffusion-based language models: LLaDA 8BÂ (Nie etÂ al., [2025](#bib.bib28); Zhu etÂ al., [2025](#bib.bib45)) and Dream 7BÂ (Ye etÂ al., [2025](#bib.bib40)), each with both Base and Instruct variants. To ensure robustness, we fix the temperature to 0 and always select the token with the minimum entropy at each decoding step, detailed configuration in AppendixÂ [A](#A1 "Appendix A Experiment Details â€£ Latent Refinement Decoding: Enhancing Diffusion-Based Language Models by Refining Belief States"). All experiments are conducted on a server equipped with 8 NVIDIA A100 80GB GPUs.

### 4.2 Benchmarks and Metrics

To comprehensively assess the effectiveness of our approach, we conduct experiments on four benchmarks spanning mathematical reasoning and code generation. For mathematical reasoning, we use GSM8KÂ (Cobbe etÂ al., [2021](#bib.bib7)), which consists of grade-school math word problems, and the more challenging MATH500Â (Lightman etÂ al., [2024](#bib.bib22)), a benchmark of competition-level mathematics problems. For code generation, we evaluate on MBPPÂ (Austin etÂ al., [2021b](#bib.bib3)), which features entry-level Python programming tasks, and HumanEvalÂ (Chen etÂ al., [2021](#bib.bib6)), a set of handwritten coding problems for program synthesis.
Following prior work, all Instruct models are evaluated under the zero-shot setting. For Base models, we follow standard few-shot settings for each benchmark: zero-shot for HumanEval, 3-shot for MBPP, 4-shot for MATH500, and 8-shot for GSM8K. For all benchmarks, we report accuracy for mathematical reasoning and pass@1 for code generation.

Table 1: Performance of different models and methods across benchmarks.
Speed denotes relative runtime (baseline = 1.0Ã—), where larger values indicate faster and more efficient inference.
Baseline results are shown in grey, and ours LRD improvements in green.

|  |  |  |  |  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Model | Len | Method | HumanEval | | MBPP | | GSM8K | | MATH500 | |
| Acc | Speed | Acc | Speed | Acc | Speed | Acc | Speed |
| Dream-Base-7B | 256 | baseline | 50.6 | 1.0Ã— | 55.8 | 1.0Ã— | 75.3 | 1.0Ã— | 36.9 | 1.0Ã— |
| Ours | 56.9+6.3 | 1.2Ã— | 57.6+1.8 | 2.3Ã— | 78.2+2.9 | 1.8Ã— | 39.8+2.9 | 1.4Ã— |
| 512 | baseline | 54.4 | 1.0Ã— | 55.8 | 1.0Ã— | 76.2 | 1.0Ã— | 37.5 | 1.0Ã— |
| Ours | 58.8+4.4 | 2.6Ã— | 58.4+2.6 | 4.5Ã— | 77.4+1.2 | 3.4Ã— | 40.8+3.3 | 1.8Ã— |
| 1024 | baseline | 54.8 | 1.0Ã— | 58.0 | 1.0Ã— | 76.8 | 1.0Ã— | 39.1 | 1.0Ã— |
| Ours | 59.1+4.3 | 4.4Ã— | 58.8+0.8 | 7.6Ã— | 77.8+1.0 | 4.2Ã— | 42.4+3.3 | 2.2Ã— |
| Dream-Ins-7B | 256 | baseline | 55.4 | 1.0Ã— | 57.4 | 1.0Ã— | 80.8 | 1.0Ã— | 37.9 | 1.0Ã— |
| Ours | 61.6+6.2 | 1.4Ã— | 59.4+2.0 | 2.4Ã— | 83.0+2.2 | 1.4Ã— | 40.6+2.7 | 1.1Ã— |
| 512 | baseline | 56.1 | 1.0Ã— | 56.7 | 1.0Ã— | 80.2 | 1.0Ã— | 38.6 | 1.0Ã— |
| Ours | 60.9+4.8 | 2.9Ã— | 58.8+2.1 | 4.6Ã— | 82.7+2.5 | 3.6Ã— | 41.8+3.2 | 1.2Ã— |
| 1024 | baseline | 56.0 | 1.0Ã— | 57.3 | 1.0Ã— | 81.3 | 1.0Ã— | 40.1 | 1.0Ã— |
| Ours | 61.0+5.0 | 9.3Ã— | 59.0+1.7 | 10.6Ã— | 83.5+2.2 | 5.5Ã— | 43.9+3.8 | 1.7Ã— |
| LLaDA-Base-8B | 256 | baseline | 32.9 | 1.0Ã— | 39.7 | 1.0Ã— | 69.1 | 1.0Ã— | 30.2 | 1.0Ã— |
| Ours | 36.0+3.1 | 1.3Ã— | 41.4+1.7 | 1.5Ã— | 71.2+2.1 | 1.6Ã— | 32.4+2.2 | 1.4Ã— |
| 512 | baseline | 32.8 | 1.0Ã— | 39.8 | 1.0Ã— | 70.8 | 1.0Ã— | 30.8 | 1.0Ã— |
| Ours | 36.0+3.2 | 1.7Ã— | 41.4+1.6 | 1.9Ã— | 72.5+1.7 | 2.2Ã— | 32.4+1.6 | 1.6Ã— |
| 1024 | baseline | 31.7 | 1.0Ã— | 39.8 | 1.0Ã— | 71.4 | 1.0Ã— | 30.1 | 1.0Ã— |
| Ours | 34.8+3.1 | 2.2Ã— | 40.8+1.0 | 3.6Ã— | 72.1+0.7 | 3.3Ã— | 32.2+2.1 | 2.1Ã— |
| LLaDA-Ins-8B | 256 | baseline | 38.7 | 1.0Ã— | 36.9 | 1.0Ã— | 77.4 | 1.0Ã— | 33.8 | 1.0Ã— |
| Ours | 43.3+4.6 | 1.2Ã— | 40.0+3.1 | 1.3Ã— | 78.8+1.4 | 1.5Ã— | 35.8+2.0 | 1.4Ã— |
| 512 | baseline | 43.9 | 1.0Ã— | 38.2 | 1.0Ã— | 81.3 | 1.0Ã— | 37.7 | 1.0Ã— |
| Ours | 48.4+4.5 | 1.3Ã— | 40.6+2.4 | 1.5Ã— | 84.5+3.2 | 2.0Ã— | 39.8+2.1 | 1.4Ã— |
| 1024 | baseline | 44.6 | 1.0Ã— | 37.4 | 1.0Ã— | 82.3 | 1.0Ã— | 39.4 | 1.0Ã— |
| Ours | 49.5+4.9 | 1.7Ã— | 39.6+2.2 | 3.7Ã— | 83.7+1.4 | 4.3Ã— | 42.2+2.8 | 2.0Ã— |
| LLaDA-1.5-8B | 256 | baseline | 38.4 | 1.0Ã— | 38.6 | 1.0Ã— | 79.2 | 1.0Ã— | 33.4 | 1.0Ã— |
| Ours | 44.5+6.1 | 1.2Ã— | 39.8+1.2 | 1.3Ã— | 80.4+1.2 | 1.5Ã— | 36.6+3.2 | 1.3Ã— |
| 512 | baseline | 45.1 | 1.0Ã— | 37.6 | 1.0Ã— | 82.9 | 1.0Ã— | 38.6 | 1.0Ã— |
| Ours | 49.6+4.5 | 1.2Ã— | 40.2+2.6 | 1.5Ã— | 84.5+1.6 | 1.9Ã— | 41.0+2.4 | 1.4Ã— |
| 1024 | baseline | 45.7 | 1.0Ã— | 37.4 | 1.0Ã— | 82.5 | 1.0Ã— | 39.6 | 1.0Ã— |
| Ours | 50.6+4.9 | 1.7Ã— | 39.6+2.2 | 3.5Ã— | 83.9+1.4 | 4.0Ã— | 41.8+2.2 | 1.9Ã— |

### 4.3 Main Results

Performance on Benchmarks. TableÂ [1](#S4.T1 "Table 1 â€£ 4.2 Benchmarks and Metrics â€£ 4 Experiments â€£ Latent Refinement Decoding: Enhancing Diffusion-Based Language Models by Refining Belief States") reports the performance of different models and decoding methods across four representative benchmarks. Our Latent Refinement Decoding framework consistently improves accuracy across all settings. For instance, on HumanEval, LRD boosts pass@1 by up to +6.3 points (Dream-Base-7B, 256 tokens) and +6.2 points (Dream-Ins-7B, 256 tokens) compared to the baseline. Similar trends are observed for MBPP, GSM8K, and MATH500, where our method outperforms the baseline by margins of +1.0 to +4.8 points in most cases. These results are consistent across different sequence lengths (256, 512, 1024), confirming that the benefits of LRD are robust to context window size and apply uniformly to both Base and Instruct model families.

Efficiency and Decoding Speed.
Beyond accuracy, LRD substantially accelerates inference. As shown in TableÂ [1](#S4.T1 "Table 1 â€£ 4.2 Benchmarks and Metrics â€£ 4 Experiments â€£ Latent Refinement Decoding: Enhancing Diffusion-Based Language Models by Refining Belief States"), our method delivers at least 1.2Ã—1.2\times speedup in all cases, with the largest gains observed for longer contexts. For example, Dream-Ins-7B achieves up to 9.3Ã—9.3\times faster decoding at length 1024, while LLaDA models reach up to 4.3Ã—4.3\times speedup under the same condition. The improvement comes from two factors: (i) the mix operation in the latent refinement phase accelerates convergence by reducing the number of tokens that need to be generated (see SectionÂ [4.5](#S4.SS5 "4.5 Ablation Study â€£ 4 Experiments â€£ Latent Refinement Decoding: Enhancing Diffusion-Based Language Models by Refining Belief States")), and (ii) the entropy-based early stopping criterion prevents unnecessary refinement steps, especially in long sequences. These results indicate that LRD is particularly advantageous in large-context scenarios, where traditional parallel decoding incurs significant overhead.

![Refer to caption](/html/2510.11052/assets/images/kl.png)

Figure 2: KL divergence between step-wise predictive distributions and final decoded results for LLaDA-1.5 and Dream-Ins across benchmarks. The red vertical line marks where decoding begins after a fixed 20-step latent refinement.

![Refer to caption](/html/2510.11052/assets/images/hotstart.png)

Figure 3: Convergence ratios across latent refinement steps for LLaDA-1.5 and Dream-Ins on four benchmarks. Since computing the difference in KL divergence requires at least three consecutive steps, the curves are plotted starting from step 2.

### 4.4 Convergence Analysis

KL divergence decreases steadily during refinement and decoding.
FigureÂ [3](#S4.F3 "Figure 3 â€£ 4.3 Main Results â€£ 4 Experiments â€£ Latent Refinement Decoding: Enhancing Diffusion-Based Language Models by Refining Belief States") shows the KL divergence between step-wise predictive distributions and the final decoded outputs for LLaDA-1.5 and Dream-Ins across four benchmarks. For ease of observation, we fix the latent refinement phase to 20 steps. The divergence exhibits a clear downward trend: during the latent refinement phase, the KL values drop rapidly and stabilise, indicating that the latent belief state quickly converges before decoding begins. Once decoding starts, the KL divergence continues to decrease with mild fluctuations, reflecting the modelâ€™s progressive confidence sharpening. For most benchmarks, the divergence approaches zero within about 300 steps, whereas Dream-Ins converges even faster, reaching near-zero divergence around 140 steps. The MATH500 benchmark proves more challenging, with non-negligible divergence persisting until the full 512-step horizon. Overall, these patterns are consistent with our expectations: the refinement phase provides a stable initialisation, and the subsequent decoding stage steadily drives the system toward convergence.

Most examples converge within the first few latent refinement steps.
FigureÂ [3](#S4.F3 "Figure 3 â€£ 4.3 Main Results â€£ 4 Experiments â€£ Latent Refinement Decoding: Enhancing Diffusion-Based Language Models by Refining Belief States") reports the proportion of cases converging at each latent refinement step. Across benchmarks, the majority of runs converge within the first few refinement steps. For example, on HumanEval with Dream-Ins, 68.9% of samples converge by step 2, and more than 85% by step 3. Similar trends hold for GSM8K, MBPP, and MATH500, where over 70% of cases converge within the first three to four steps. These results confirm that the latent refinement is highly efficient in practice: most examples stabilize very early, reducing the need for excessive refinement iterations and validating the design of our latent refinement mechanism.

### 4.5 Ablation Study

Table 2: Ablation study on decoding variants at length 512, where Auto uses adaptive latent refinement, and LFÃ—k enforces kk latent refinement steps prior to each token commitment.

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
| Method | HumanEval | MBPP | GSM8K | MATH500 |
| Baseline | 56.1 | 56.7 | 80.2 | 38.6 |
| Ours | 60.9+4.8 | 58.8+2.1 | 82.7+2.5 | 41.8+3.2 |
| Auto | 59.6+3.5 | 57.7+1.0 | 81.5+1.3 | 41.4+2.8 |
| LFÃ—1 | 60.4+4.3 | 57.8+1.1 | 81.6+1.4 | 40.2+1.6 |
| LFÃ—2 | 58.3+2.2 | 57.2+0.5 | 81.2+1.0 | 40.2+1.6 |
| LFÃ—3 | 57.9+1.8 | 57.8+1.1 | 81.8+1.6 | 39.0+0.4 |
| LFÃ—4 | 60.8+4.7 | 57.2+0.5 | 80.9+0.7 | 39.6+1.0 |
| LFÃ—5 | 58.8+2.7 | 57.6+0.9 | 80.7+0.5 | 39.2+0.6 |

Excessive latent refinement brings no benefit but slows decoding.
TableÂ [2](#S4.T2 "Table 2 â€£ 4.5 Ablation Study â€£ 4 Experiments â€£ Latent Refinement Decoding: Enhancing Diffusion-Based Language Models by Refining Belief States") compares our two-stage strategy (one initial latent refinement followed by standard decoding) with variants that enforce latent refinement at every step, either a fixed number of times (LFÃ—kk) or adaptively (Auto). Results show that while all variants outperform the baseline, none surpass our method: enforcing repeated latent refinements (LFÃ—2â€“5) generally degrades accuracy, and even adaptive scheduling (Auto) underperforms compared to ours. The reason is that excessive latent refinement adds redundant computation without providing additional guidance once the model has stabilised. In contrast, our two-stage design strikes a better balance by leveraging latent refinement only at the beginning, yielding both higher accuracy and substantially faster decoding.

Table 3: Ablation study on decoding variants, where red and green numbers show the change compared to our full method.

|  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- |
| Len | Method | HumanEval | MBPP | GSM8K | MATH500 |
| 256 | baseline | 55.4 | 57.4 | 80.8 | 37.9 |
| Ours | 61.6+6.2 | 59.4+2.0 | 83.0+2.2 | 40.6+2.7 |
| w/o latent refinement | 60.1-1.5 | 58.6-0.8 | 82.3-0.7 | 39.4-1.2 |
| w/o mix embed | 59.5-2.1 | 58.8-0.6 | 82.7-0.3 | 38.9-1.7 |
| w/o early stop | 61.8+0.2 | 59.4+0.0 | 83.2+0.2 | 40.6+0.0 |
| 512 | baseline | 56.1 | 56.7 | 80.2 | 38.6 |
| Ours | 60.9+4.8 | 58.8+2.1 | 82.7+2.5 | 41.8+3.2 |
| w/o latent refinement | 59.9-1.0 | 57.8-1.0 | 82.2-0.5 | 41.0-0.8 |
| w/o mix embed | 58.0-2.9 | 57.8-1.0 | 80.7-2.0 | 40.8-1.0 |
| w/o early stop | 61.2+0.3 | 58.8+0.0 | 82.9+0.2 | 41.9+0.1 |
| 1024 | baseline | 56.0 | 57.3 | 81.3 | 40.1 |
| Ours | 61.0+5.0 | 59.0+1.7 | 83.5+2.2 | 43.9+3.8 |
| w/o latent refinement | 60.7-0.3 | 58.8-0.2 | 83.2-0.3 | 42.4-1.5 |
| w/o mix embed | 59.1-1.9 | 58.7-0.3 | 82.9-0.6 | 41.4-2.5 |
| w/o early stop | 61.4+0.4 | 59.0+0.0 | 83.7+0.2 | 44.2+0.3 |

Both components contribute, with mixing more critical.
TableÂ [3](#S4.T3 "Table 3 â€£ 4.5 Ablation Study â€£ 4 Experiments â€£ Latent Refinement Decoding: Enhancing Diffusion-Based Language Models by Refining Belief States") reports ablation results in accuracy. Removing latent refinement or mixed embeddings consistently reduces performance, confirming the importance of both. The absence of mixed embeddings causes larger drops (up to âˆ’2.9-2.9 on HumanEval and âˆ’2.5-2.5 on MATH500), showing that the predictive feedback loop is the key driver of improvements. In contrast, early stopping incurs almost no accuracy loss while providing substantial efficiency gains. Overall, latent refinement and mixed embeddings are essential for accuracy, whereas early stopping boosts efficiency at virtually no cost.

Table 4: Ablation study on decoding variants, reporting Speed and effective token number EtokenE\_{\text{token}}, where red and green numbers show the change compared to our full method.

|  |  |  |  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Length | Method | Speed | | | | EtokenE\_{\text{token}} | | | |
| HumanEval | MBPP | GSM8K | MATH500 | HumanEval | MBPP | GSM8K | MATH500 |
| 256 | baseline | 1.0Ã— | 1.0Ã— | 1.0Ã— | 1.0Ã— | 117.2 | 53.5 | 132.4 | 228.4 |
| Ours | 1.4Ã—+0.4Ã— | 2.4Ã—+1.4Ã— | 1.4Ã—+0.4Ã— | 1.1Ã—+0.1Ã— | 108.4-8.8 | 49.2-4.3 | 128.6-5.8 | 226.0-2.4 |
| w/o latent refinement | 1.5Ã—+0.1Ã— | 2.5Ã—+0.1Ã— | 1.5Ã—+0.1Ã— | 1.1Ã—+0.0Ã— | 108.7+0.3 | 50.4+1.2 | 129.9+1.3 | 226.8+0.8 |
| w/o mix embed | 1.3Ã—-0.1Ã— | 2.2Ã—-0.2Ã— | 1.5Ã—+0.1Ã— | 1.0Ã—-0.1Ã— | 117.2+8.8 | 49.5+0.3 | 129.4+0.8 | 228.4+2.4 |
| w/o early stop | 0.8Ã—-0.6Ã— | 0.7Ã—-1.7Ã— | 0.8Ã—-0.6Ã— | 0.9Ã—-0.2Ã— | 109.7+1.3 | 51.4+2.2 | 129.9+1.3 | 228.0+2.0 |
| 512 | baseline | 1.0Ã— | 1.0Ã— | 1.0Ã— | 1.0Ã— | 116.2 | 55.7 | 135.2 | 378.9 |
| Ours | 2.9Ã—+1.9Ã— | 4.6Ã—+3.6Ã— | 3.6Ã—+2.6Ã— | 1.2Ã—+0.2Ã— | 103.9-12.3 | 51.8-4.1 | 125.9-9.3 | 363.5-15.4 |
| w/o latent refinement | 3.1Ã—+0.2Ã— | 4.9Ã—+0.3Ã— | 3.8Ã—+0.2Ã— | 1.2Ã—+0.0Ã— | 106.3+2.4 | 52.6+0.8 | 127.9+2.0 | 363.0-0.5 |
| w/o mix embed | 2.7Ã—-0.2Ã— | 4.3Ã—-0.3Ã— | 3.0Ã—-0.6Ã— | 1.0Ã—-0.2Ã— | 116.2+12.3 | 51.8+0.0 | 126.2+0.3 | 368.9+5.4 |
| w/o early stop | 0.8Ã—-2.1Ã— | 0.7Ã—-3.9Ã— | 0.8Ã—-2.8Ã— | 0.8Ã—-0.4Ã— | 106.2+2.3 | 53.6+1.8 | 127.2+1.3 | 366.0+2.5 |
| 1024 | baseline | 1.0Ã— | 1.0Ã— | 1.0Ã— | 1.0Ã— | 90.4 | 60.5 | 135.5 | 482.3 |
| Ours | 9.3Ã—+8.3Ã— | 10.6Ã—+9.6Ã— | 5.5Ã—+4.5Ã— | 1.7Ã—+0.7Ã— | 84.6-5.8 | 57.2-3.3 | 123.7-11.8 | 437.3-45.0 |
| w/o latent refinement | 9.3Ã—+0.0Ã— | 10.7Ã—+0.1Ã— | 5.6Ã—+0.1Ã— | 1.7Ã—+0.0Ã— | 83.9-0.7 | 58.2+1.0 | 125.0+1.3 | 455.4+18.1 |
| w/o mix embed | 9.1Ã—-0.2Ã— | 10.4Ã—-0.2Ã— | 5.1Ã—-0.4Ã— | 1.3Ã—-0.4Ã— | 90.4+5.8 | 61.7+4.5 | 130.5+6.8 | 483.5+46.2 |
| w/o early stop | 0.8Ã—-8.5Ã— | 0.7Ã—-9.9Ã— | 0.8Ã—-4.7Ã— | 0.8Ã—-0.9Ã— | 86.2+1.6 | 59.2+2.0 | 126.1+2.4 | 438.9+1.6 |

latent refinement slows generation, mixed embeddings aid convergence, and early stopping is the main accelerator.
TableÂ [4](#S4.T4 "Table 4 â€£ 4.5 Ablation Study â€£ 4 Experiments â€£ Latent Refinement Decoding: Enhancing Diffusion-Based Language Models by Refining Belief States") reveals several key insights. First, removing the latent refinement phase (w/o latent refinement) yields faster decoding, showing that latent refinement introduces extra refinement steps and slightly slows down speed, though it improves stability. Second, removing mixed embeddings (w/o mix embed) makes decoding slower and increases effective token counts, indicating that mixing embeddings is critical for helping the model converge earlier. Third, early stopping (w/o early stop) leads to dramatic slowdowns, with speed dropping from multi-fold acceleration to even below baseline, despite only negligible changes in EtokenE\_{\text{token}}. This confirms that early stopping is the primary driver of speedup. Finally, both latent refinement and mixed embeddings reduce effective token usage under the full model, demonstrating that they improve convergence efficiency even though their speed impact differs.

![Refer to caption](/html/2510.11052/assets/images/base_rate.png)

Figure 4: Accuracy of Dream-Ins on four benchmarks under different Maximum token proportion, where rfr\_{f}=0 corresponds to the no mixing.

![Refer to caption](/html/2510.11052/assets/images/topp.png)

Figure 5: Effect of top-pp mixing on Dream-Ins across four benchmarks. The purple curve shows the log fraction of tokens included in the mixture.

Full mixing collapses the model, while best at intermediate rfr\_{f}.
We further investigate the effect of the maximum mix ratio rfr\_{f}, which scales the interpolation between predicted token embeddings and the [MASK] embedding during refinement (Eq.Â [4](#S3.E4 "In 3.1 Soft Diffusion â€£ 3 Methodology â€£ Latent Refinement Decoding: Enhancing Diffusion-Based Language Models by Refining Belief States")). When rfr\_{f}=0, the model falls back to always using the [MASK] token for unfinalised positions, equivalent to the baseline. At the other extreme, setting rfr\_{f}=1 allows the mixing weight to fully follow the entropy schedule, meaning that in high-entropy cases the [MASK] embedding may vanish. As shown in FigureÂ [5](#S4.F5 "Figure 5 â€£ 4.5 Ablation Study â€£ 4 Experiments â€£ Latent Refinement Decoding: Enhancing Diffusion-Based Language Models by Refining Belief States"), both extremes are suboptimal: the baseline propagates information slowly, while overly aggressive mixing destabilises refinement and leads to collapse. Intermediate values of rfr\_{f} achieve the best trade-off, providing sufficient mask guidance while still leveraging predictive feedback.

Mix matters more than how many tokens are mixed.
As shown in FigureÂ [5](#S4.F5 "Figure 5 â€£ 4.5 Ablation Study â€£ 4 Experiments â€£ Latent Refinement Decoding: Enhancing Diffusion-Based Language Models by Refining Belief States"), when p=0p=0 no mixing occurs and the method degenerates to the baseline, giving the lowest accuracy across all benchmarks. Increasing pp quickly improves performance, even though the token ratio curve indicates that only a very small fraction of tokens are mixed at pâ‰¤0.2p\leq 0.2. This suggests that the key factor is enabling mixing rather than the absolute number of tokens included. Beyond pâ‰ˆ0.2p\approx 0.2, accuracy stabilises and fluctuates slightly, showing that adding more low-probability tokens offers little benefit while introducing potential noise. These results confirm that top-pp mixing provides a good balance: minimal mixing is already highly effective, and larger pp values do not bring further gains.

## 5 Related Work

Diffusion LLMs (dLLMs).
Diffusion models, as generative models, initially achieved significant success in continuous data domains such as imageÂ (Song etÂ al., [2020](#bib.bib32); Ho etÂ al., [2020](#bib.bib13); Nichol etÂ al., [2021](#bib.bib27); Rombach etÂ al., [2022](#bib.bib30)) and speech generationÂ (Huang etÂ al., [2023](#bib.bib15); Yang etÂ al., [2023](#bib.bib39)). Their application in the language domain has been limited due to the discrete nature of text. One promising approach is the use of Masked Diffusion Models (MDMs)Â (Austin etÂ al., [2021a](#bib.bib2); Ou etÂ al., [2024](#bib.bib29); Shi etÂ al., [2024](#bib.bib31); Lou etÂ al., [2023](#bib.bib24)), which represent a particular type of discrete diffusion that works with sequences through the iterative prediction of masked tokens using contextual information. Current research has concentrated on substantially expanding these MDMs.
DiffuLLaMAÂ (Gong etÂ al., [2025](#bib.bib11)), developed through continual pre-training based on LLaMA parameters, has produced diffusion Large Language Models (dLLMs) and demonstrated that dLLMs can achieve performance comparable to autoregressive models. Subsequently, higher-performance commercial dLLMs such as MercuryÂ (Labs etÂ al., [2025](#bib.bib17)) and Gemini DiffusionÂ (Deepmind, [2025](#bib.bib9)) have been announced, along with the introduction of high-quality open-source models such as LLaDAÂ (Nie etÂ al., [2025](#bib.bib28); Zhu etÂ al., [2025](#bib.bib45)) and DreamÂ (Ye etÂ al., [2025](#bib.bib40)). However, the limitations of dLLMs cannot be overlooked. Due to the lack of components analogous to KV cache and the requirement to compute results for all positions in each step, the deployment of dLLMs has consistently been constrained by inference efficiency. While reducing the number of inference steps can improve inference efficiency, this severely compromises model performance. Whether it is possible to enhance dLLMsâ€™ performance while accelerating inference remains a critical research topic for dLLMs at the current stage.

Efficient dLLMs. To improve dLLM inference speed while maintaining generation quality, recent works have proposed efficient dLLMs in two main directions: integrating KV cache and optimising computational load. For KV cache integration, dLLM-CacheÂ (Liu etÂ al., [2025](#bib.bib23)) proposes a training-free adaptive caching framework addressing dual computational redundancy, specifically quasi-static prompt and dynamic response redundancy, while integrating long-interval prompt caching and V-verify mechanisms. Fast-dLLMÂ (Wu etÂ al., [2025](#bib.bib37)) designs block-wise KV cache reuse mechanisms exploiting activation similarity in bidirectional attention, combined with confidence-aware dynamic parallel decoding. Sparse-dLLMÂ (Song etÂ al., [2025](#bib.bib33)) combines dynamic cache eviction with sparse attention, leveraging temporal consistency of token saliency for plug-and-play inference acceleration. For computational optimisation, ProphetÂ (Li etÂ al., [2025b](#bib.bib20)) exploits the finding that 99% of samples converge early, proposing confidence-gap-based early commitment decoding to effectively reduce decoding steps. DAEDALÂ (Li etÂ al., [2025a](#bib.bib19)) implements two-stage dynamic length expansion through EOS confidence prediction and low-confidence region identification, thereby enabling adaptive generation length allocation. However, all of the current worksÂ (Ben-Hamu etÂ al., [2025](#bib.bib4); Yu etÂ al., [2025](#bib.bib41); Ma etÂ al., [2025](#bib.bib26); Israel etÂ al., [2025](#bib.bib16)) primarily prioritize efficiency over generation quality, largely ignoring that existing dLLMs cannot significantly outperform AR models in overall generation quality. Inspired by mixed token improvements in AR modelsÂ (Zhang etÂ al., [2025](#bib.bib43); Wang etÂ al., [2024](#bib.bib36); Hao etÂ al., [2024](#bib.bib12)), our work emphasizes enhancing dLLMsâ€™ performance while simultaneously leveraging computed KL divergence for reliable early stopping to improve efficiency.

## 6 Conclusion

We introduced Latent Refinement Decoding, a unified two-stage decoding framework for diffusion language models that addresses the twin bottlenecks of information loss from hard masking and suboptimal convergence speed. By first enabling the model to iteratively refine global beliefs in the continuous embedding space, and then entering a predictive feedback loop that progressively finalizes confident tokens while adaptively monitoring convergence through KL dynamics, LRD preserves more information throughout the generation process and supports principled early stopping for greater stability. Extensive experiments on both code generation and mathematical reasoning benchmarks demonstrate that LRD achieves consistent and significant gains in output quality and inference efficiency over standard diffusion decoding baselines, particularly as sequence length increases and complexity grows. Looking forward, LRD can serve as a flexible drop-in decoding module for future diffusion-based LMs, and its efficiency can be further enhanced by integrating with systems-level optimizations such as KV caching, speculative decoding, and potentially other hardware-aware acceleration techniques. This opens exciting new opportunities to combine architectural and algorithmic advances for even faster, more robust, and highly scalable parallel generation.

## Reproducibility statement

To ensure the reproducibility of our work, we provide complete source code as supplementary materials, including implementations for all five models (LLaDA-base, LLaDA-instruct, LLaDA-1.5, Dream-base, and Dream-instruct) evaluated on four datasets (MBPP, GSM8K, HumanEval, and MATH500), accompanied by detailed execution instructions. The model architectures are comprehensively described in SectionÂ [3](#S3 "3 Methodology â€£ Latent Refinement Decoding: Enhancing Diffusion-Based Language Models by Refining Belief States"), while hyperparameters for models are specified in AppendixÂ [A](#A1 "Appendix A Experiment Details â€£ Latent Refinement Decoding: Enhancing Diffusion-Based Language Models by Refining Belief States").

## References

* Achiam etÂ al. (2023)

  Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, FlorenciaÂ Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, etÂ al.
  Gpt-4 technical report.
  *arXiv preprint arXiv:2303.08774*, 2023.
* Austin etÂ al. (2021a)

  Jacob Austin, DanielÂ D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van DenÂ Berg.
  Structured denoising diffusion models in discrete state-spaces.
  *Advances in neural information processing systems*, 34:17981â€“17993, 2021a.
* Austin etÂ al. (2021b)

  Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton.
  Program synthesis with large language models, 2021b.
  URL <https://arxiv.org/abs/2108.07732>.
* Ben-Hamu etÂ al. (2025)

  Heli Ben-Hamu, Itai Gat, Daniel Severo, Niklas Nolte, and Brian Karrer.
  Accelerated sampling from masked diffusion models via entropy bounded unmasking.
  *arXiv preprint arXiv:2505.24857*, 2025.
* Brown etÂ al. (2020)

  Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, JaredÂ D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, etÂ al.
  Language models are few-shot learners.
  *Advances in neural information processing systems*, 33:1877â€“1901, 2020.
* Chen etÂ al. (2021)

  Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, HenriqueÂ Ponde deÂ OliveiraÂ Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, and et al.
  Evaluating large language models trained on code, 2021.
  URL <https://arxiv.org/abs/2107.03374>.
* Cobbe etÂ al. (2021)

  Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, etÂ al.
  Training verifiers to solve math word problems.
  *arXiv preprint arXiv:2110.14168*, 2021.
* Dasoulas etÂ al. (2021)

  George Dasoulas, Kevin Scaman, and Aladin Virmaux.
  Lipschitz normalization for self-attention layers with application to graph neural networks, 2021.
  URL <https://arxiv.org/abs/2103.04886>.
* Deepmind (2025)

  Deepmind.
  Gemini diffusion, 2025.
  URL <https://deepmind.google/models/gemini-diffusion/>.
* Fei etÂ al. (2025)

  Xiang Fei, Jinghui Lu, QiÂ Sun, Hao Feng, Yanjie Wang, Wei Shi, An-Lan Wang, Jingqun Tang, and Can Huang.
  Advancing sequential numerical prediction in autoregressive models.
  *arXiv preprint arXiv:2505.13077*, 2025.
* Gong etÂ al. (2025)

  Shansan Gong, Shivam Agarwal, Yizhe Zhang, Jiacheng Ye, Lin Zheng, Mukai Li, Chenxin An, Peilin Zhao, Wei Bi, Jiawei Han, Hao Peng, and Lingpeng Kong.
  Scaling diffusion language models via adaptation from autoregressive models.
  In *The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025*. OpenReview.net, 2025.
  URL <https://openreview.net/forum?id=j1tSLYKwg8>.
* Hao etÂ al. (2024)

  Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian.
  Training large language models to reason in a continuous latent space.
  *arXiv preprint arXiv:2412.06769*, 2024.
* Ho etÂ al. (2020)

  Jonathan Ho, Ajay Jain, and Pieter Abbeel.
  Denoising diffusion probabilistic models.
  *Advances in neural information processing systems*, 33:6840â€“6851, 2020.
* Hu etÂ al. (2024)

  Xixu Hu, Runkai Zheng, Jindong Wang, CheukÂ Hang Leung, QiÂ Wu, and Xing Xie.
  Specformer: Guarding vision transformer robustness via maximum singular value penalization, 2024.
  URL <https://arxiv.org/abs/2402.03317>.
* Huang etÂ al. (2023)

  Rongjie Huang, Jiawei Huang, Dongchao Yang, YiÂ Ren, Luping Liu, Mingze Li, Zhenhui Ye, Jinglin Liu, Xiang Yin, and Zhou Zhao.
  Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models.
  In *International Conference on Machine Learning*, pp. 13916â€“13932. PMLR, 2023.
* Israel etÂ al. (2025)

  Daniel Israel, Guy VanÂ den Broeck, and Aditya Grover.
  Accelerating diffusion llms via adaptive parallel decoding.
  *arXiv preprint arXiv:2506.00413*, 2025.
* Labs etÂ al. (2025)

  Inception Labs, Samar Khanna, Siddhant Kharbanda, Shufan Li, Harshit Varma, Eric Wang, Sawyer Birnbaum, Ziyang Luo, Yanis Miraoui, Akash Palrecha, Stefano Ermon, Aditya Grover, and Volodymyr Kuleshov.
  Mercury: Ultra-fast language models based on diffusion, 2025.
  URL <https://arxiv.org/abs/2506.17298>.
* Li & Cai (2025)

  Gen Li and Changxiao Cai.
  A convergence theory for diffusion language models: An information-theoretic perspective, 2025.
  URL <https://arxiv.org/abs/2505.21400>.
* Li etÂ al. (2025a)

  Jinsong Li, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Jiaqi Wang, and Dahua Lin.
  Beyond fixed: Variable-length denoising for diffusion large language models.
  *arXiv preprint arXiv:2508.00819*, 2025a.
* Li etÂ al. (2025b)

  Pengxiang Li, Yefan Zhou, Dilxat Muhtar, LuÂ Yin, Shilin Yan, LiÂ Shen, YiÂ Liang, Soroush Vosoughi, and Shiwei Liu.
  Diffusion language models know the answer before decoding.
  *arXiv preprint arXiv:2508.19982*, 2025b.
* Li etÂ al. (2024)

  Yuchen Li, Alexandre Kirchmeyer, Aashay Mehta, Yilong Qin, Boris Dadachev, Kishore Papineni, Sanjiv Kumar, and Andrej Risteski.
  Promises and pitfalls of generative masked language modeling: Theoretical framework and practical guidelines, 2024.
  URL <https://arxiv.org/abs/2407.21046>.
* Lightman etÂ al. (2024)

  Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe.
  Letâ€™s verify step by step.
  In *The Twelfth International Conference on Learning Representations*, 2024.
  URL <https://openreview.net/forum?id=v8L0pN6EOi>.
* Liu etÂ al. (2025)

  Zhiyuan Liu, Yicun Yang, Yaojie Zhang, Junjie Chen, Chang Zou, Qingyuan Wei, Shaobo Wang, and Linfeng Zhang.
  dllm-cache: Accelerating diffusion large language models with adaptive caching, 2025.
  URL <https://arxiv.org/abs/2506.06295>.
* Lou etÂ al. (2023)

  Aaron Lou, Chenlin Meng, and Stefano Ermon.
  Discrete diffusion language modeling by estimating the ratios of the data distribution.
  2023.
* Luxembourg etÂ al. (2025)

  Omer Luxembourg, Haim Permuter, and Eliya Nachmani.
  Plan for speed: Dilated scheduling for masked diffusion language models, 2025.
  URL <https://arxiv.org/abs/2506.19037>.
* Ma etÂ al. (2025)

  Xinyin Ma, Runpeng Yu, Gongfan Fang, and Xinchao Wang.
  dkv-cache: The cache for diffusion language models.
  *arXiv preprint arXiv:2505.15781*, 2025.
* Nichol etÂ al. (2021)

  Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen.
  Glide: Towards photorealistic image generation and editing with text-guided diffusion models.
  *arXiv preprint arXiv:2112.10741*, 2021.
* Nie etÂ al. (2025)

  Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, and Chongxuan Li.
  Large language diffusion models, 2025.
  URL <https://arxiv.org/abs/2502.09992>.
* Ou etÂ al. (2024)

  Jingyang Ou, Shen Nie, Kaiwen Xue, Fengqi Zhu, Jiacheng Sun, Zhenguo Li, and Chongxuan Li.
  Your absorbing discrete diffusion secretly models the conditional distributions of clean data.
  *arXiv preprint arXiv:2406.03736*, 2024.
* Rombach etÂ al. (2022)

  Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer.
  High-resolution image synthesis with latent diffusion models.
  In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pp. 10684â€“10695, 2022.
* Shi etÂ al. (2024)

  Jiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, and Michalis Titsias.
  Simplified and generalized masked diffusion for discrete data.
  *Advances in neural information processing systems*, 37:103131â€“103167, 2024.
* Song etÂ al. (2020)

  Yang Song, Jascha Sohl-Dickstein, DiederikÂ P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.
  Score-based generative modeling through stochastic differential equations.
  *arXiv preprint arXiv:2011.13456*, 2020.
* Song etÂ al. (2025)

  Yuerong Song, Xiaoran Liu, Ruixiao Li, Zhigeng Liu, Zengfeng Huang, Qipeng Guo, Ziwei He, and Xipeng Qiu.
  Sparse-dllm: Accelerating diffusion llms with dynamic cache eviction.
  *arXiv preprint arXiv:2508.02558*, 2025.
* Sun etÂ al. (2024)

  Tianxiang Sun, Xiaotian Zhang, Zhengfu He, Peng Li, Qinyuan Cheng, Xiangyang Liu, Hang Yan, Yunfan Shao, Qiong Tang, Shiduo Zhang, etÂ al.
  Moss: An open conversational large language model.
  *Machine Intelligence Research*, 21(5):888â€“905, 2024.
* Touvron etÂ al. (2023)

  Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, etÂ al.
  Llama: Open and efficient foundation language models.
  *arXiv preprint arXiv:2302.13971*, 2023.
* Wang etÂ al. (2024)

  Xinyi Wang, Lucas Caccia, Oleksiy Ostapenko, Xingdi Yuan, WilliamÂ Yang Wang, and Alessandro Sordoni.
  Guiding language model reasoning with planning tokens, 2024.
  URL <https://arxiv.org/abs/2310.05707>.
* Wu etÂ al. (2025)

  Chengyue Wu, Hao Zhang, Shuchen Xue, Zhijian Liu, Shizhe Diao, Ligeng Zhu, Ping Luo, Song Han, and Enze Xie.
  Fast-dllm: Training-free acceleration of diffusion llm by enabling kv cache and parallel decoding, 2025.
  URL <https://arxiv.org/abs/2505.22618>.
* Yang etÂ al. (2025)

  AnÂ Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, BoÂ Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, etÂ al.
  Qwen3 technical report.
  *arXiv preprint arXiv:2505.09388*, 2025.
* Yang etÂ al. (2023)

  Dongchao Yang, Jianwei Yu, Helin Wang, Wen Wang, Chao Weng, Yuexian Zou, and Dong Yu.
  Diffsound: Discrete diffusion model for text-to-sound generation.
  *IEEE/ACM Transactions on Audio, Speech, and Language Processing*, 31:1720â€“1733, 2023.
* Ye etÂ al. (2025)

  Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong.
  Dream 7b: Diffusion large language models.
  *arXiv preprint arXiv:2508.15487*, 2025.
* Yu etÂ al. (2025)

  Runpeng Yu, Xinyin Ma, and Xinchao Wang.
  Dimple: Discrete diffusion multimodal large language model with parallel decoding, 2025.
  URL <https://arxiv.org/abs/2505.16990>.
* Yudin etÂ al. (2025)

  Nikolay Yudin, Alexander Gaponov, Sergei Kudriashov, and Maxim Rakhuba.
  Pay attention to attention distribution: A new local lipschitz bound for transformers, 2025.
  URL <https://arxiv.org/abs/2507.07814>.
* Zhang etÂ al. (2025)

  Zhen Zhang, Xuehai He, Weixiang Yan, AoÂ Shen, Chenyang Zhao, Shuohang Wang, Yelong Shen, and XinÂ Eric Wang.
  Soft thinking: Unlocking the reasoning potential of llms in continuous concept space, 2025.
  URL <https://arxiv.org/abs/2505.15778>.
* Zheng etÂ al. (2024)

  Lin Zheng, Jianbo Yuan, Lei Yu, and Lingpeng Kong.
  A reparameterized discrete diffusion model for text generation.
  In *Conference on Language Modeling (COLM)*, Philadelphia, PA, USA, October 7â€“9 2024.
  2024a.
* Zhu etÂ al. (2025)

  Fengqi Zhu, Rongzhen Wang, Shen Nie, Xiaolu Zhang, Chunwei Wu, Jun Hu, Jun Zhou, Jianfei Chen, Yankai Lin, Ji-Rong Wen, and Chongxuan Li.
  Llada 1.5: Variance-reduced preference optimization for large language diffusion models, 2025.
  URL <https://arxiv.org/abs/2505.19223>.

## The Use of LLMs

In the preparation of this manuscript, we used Large Language Models (LLMs) in a limited capacity for two specific purposes: preliminary literature survey to help identify relevant research directions and keywords during the early stages of our work, and limited language polishing to improve the clarity and grammatical correctness of certain sections in the paper. All core research ideas, theoretical contributions, experimental design, implementation, and analysis were independently conceived and conducted by the authors without LLM assistance. The LLM-generated suggestions were carefully reviewed, verified, and substantially modified by the authors before incorporation. We take full responsibility for all content presented in this paper, including any text that may have been refined with LLM assistance.

## Appendix A Experiment Details

For Base models, we follow standard few-shot settings for each benchmark: zero-shot for HumanEval, 3-shot for MBPP, 4-shot for MATH500, and 8-shot for GSM8K. For all benchmarks, we report accuracy for mathematical reasoning and pass@1 for code generation. We set the nucleus threshold to top-p=0.9p=0.9. The hyperparameter rfr\_{f} is varied between 0.10.1 and 0.20.2. The thresholds for stopping latent refinement and early decoding are Ï„refine=0.1\tau\_{\text{refine}}=0.1 and Ï„decode=0.1\tau\_{\text{decode}}=0.1, respectively. We cap the latent refinement stage at a maximum of Trefine=20T\_{\text{refine}}=20 steps.
For LLaDA-Instruct and LLaDA-1.5 models, generation is conducted under the official semi-AR frameworkÂ (Nie etÂ al., [2025](#bib.bib28)), where the sequence is divided into blocks and decoded autoregressively at the block level. Within each block, instead of the standard hard masking used in the original work, we integrate our Latent Refinement and Predictive Feedback Loop, enabling refinement of token distributions before discrete commitment. Detailed integration steps are provided in AppendixÂ [C](#A3 "Appendix C Integration with Semi-AR Framework â€£ Latent Refinement Decoding: Enhancing Diffusion-Based Language Models by Refining Belief States").

## Appendix B Derivation of the True Posterior in the Masking Process

We derive Eq.Â [5](#S3.E5 "In 3.1 Soft Diffusion â€£ 3 Methodology â€£ Latent Refinement Decoding: Enhancing Diffusion-Based Language Models by Refining Belief States") for the true posterior distribution in the absorbing masking forward process.
For each position ii, the forward process is defined as

|  |  |  |
| --- | --- | --- |
|  | Prâ¡(xt(i)=x0(i)âˆ£x0(i))=Î±tâˆ—,Prâ¡(xt(i)=[MASK]âˆ£x0(i))=1âˆ’Î±tâˆ—,\Pr(x\_{t}^{(i)}=x\_{0}^{(i)}\mid x\_{0}^{(i)})=\alpha\_{t}^{\*},\qquad\Pr(x\_{t}^{(i)}=\texttt{[MASK]}\mid x\_{0}^{(i)})=1-\alpha\_{t}^{\*}, |  |

with (Î±tâˆ—)t=0T(\alpha\_{t}^{\*})\_{t=0}^{T} monotonically decreasing. Thus each token can only either remain as its original value x0(i)x\_{0}^{(i)} or transition to the
special token [MASK]. By Bayesâ€™ rule,

|  |  |  |  |
| --- | --- | --- | --- |
|  | qâˆ—â€‹(xtâˆ’1(i)âˆ£xt(i)=[MASK],x0(i))=Prâ¡(xt(i)=[MASK]âˆ£xtâˆ’1(i),x0(i))â€‹Prâ¡(xtâˆ’1(i)âˆ£x0(i))Prâ¡(xt(i)=[MASK]âˆ£x0(i)).q^{\*}(x\_{t-1}^{(i)}\mid x\_{t}^{(i)}=\texttt{[MASK]},x\_{0}^{(i)})=\frac{\Pr(x\_{t}^{(i)}=\texttt{[MASK]}\mid x\_{t-1}^{(i)},x\_{0}^{(i)})\Pr(x\_{t-1}^{(i)}\mid x\_{0}^{(i)})}{\Pr(x\_{t}^{(i)}=\texttt{[MASK]}\mid x\_{0}^{(i)})}. |  | (8) |

There are two possible values for xtâˆ’1(i)x\_{t-1}^{(i)}:

* â€¢

  The probability of xtâˆ’1(i)=x0(i)x\_{t-1}^{(i)}=x\_{0}^{(i)} is Î±tâˆ’1âˆ—\alpha\_{t-1}^{\*}, and transitioning to mask at step tt occurs with probability 1âˆ’Î±tâˆ—Î±tâˆ’1âˆ—1-\tfrac{\alpha\_{t}^{\*}}{\alpha\_{t-1}^{\*}}.
  Hence the joint probability is Î±tâˆ’1âˆ—âˆ’Î±tâˆ—\alpha\_{t-1}^{\*}-\alpha\_{t}^{\*}.
* â€¢

  The probability of xtâˆ’1(i)=[MASK]x\_{t-1}^{(i)}=\texttt{[MASK]} is 1âˆ’Î±tâˆ’1âˆ—1-\alpha\_{t-1}^{\*}, and once masked, the token remains masked with probability 11.
  Hence the joint probability is 1âˆ’Î±tâˆ’1âˆ—1-\alpha\_{t-1}^{\*}.

The marginal probability of being masked at step tt is Pâ€‹râ€‹(xt(i)=[MASK]âˆ£x0(i))=1âˆ’Î±tâˆ—Pr(x\_{t}^{(i)}=\texttt{[MASK]}\mid x\_{0}^{(i)})=1-\alpha\_{t}^{\*}. So we obtain

|  |  |  |
| --- | --- | --- |
|  | qâˆ—â€‹(xtâˆ’1(i)âˆ£xt(i)=[MASK],x0(i))=Î±tâˆ’1âˆ—âˆ’Î±tâˆ—1âˆ’Î±tâˆ—â€‹Î´x0(i)+1âˆ’Î±tâˆ’1âˆ—1âˆ’Î±tâˆ—â€‹Î´[MASK].q^{\*}(x\_{t-1}^{(i)}\mid x\_{t}^{(i)}=\texttt{[MASK]},x\_{0}^{(i)})=\frac{\alpha\_{t-1}^{\*}-\alpha\_{t}^{\*}}{1-\alpha\_{t}^{\*}}\,\delta\_{x\_{0}^{(i)}}+\frac{1-\alpha\_{t-1}^{\*}}{1-\alpha\_{t}^{\*}}\,\delta\_{\texttt{[MASK]}}. |  |

## Appendix C Integration with Semi-AR Framework

In the semi-AR setting in LLaDAÂ (Nie etÂ al., [2025](#bib.bib28)), a sequence of length LL is partitioned into BB blocks {b1,b2,â€¦,bB}\{b\_{1},b\_{2},...,b\_{B}\}. While their original work uses standard hard masking within each block, we apply soft embeddings as follows:

For each block bib\_{i} conditioned on previously generated blocks {b1,â€¦,biâˆ’1}\{b\_{1},...,b\_{i-1}\}:

1. 1.

   Soft Refinement: Initialise positions in bib\_{i} with [MASK] embeddings, then apply soft embedding refinement (EquationÂ [3](#S3.E3 "In 3.1 Soft Diffusion â€£ 3 Methodology â€£ Latent Refinement Decoding: Enhancing Diffusion-Based Language Models by Refining Belief States")) until convergence.
2. 2.

   Progressive Decoding: Use the converged soft embeddings to guide token selection within the block.

## Appendix D Stability Analysis of Mixed Embedding Updates

Our method operates in the embedding space rather than the discrete token space.
At each timestep tt, we maintain a set of soft embeddings
â„°t={ğ~t(1),â€¦,ğ~t(L)}\mathcal{E}\_{t}=\{\tilde{\mathbf{e}}\_{t}^{(1)},\ldots,\tilde{\mathbf{e}}\_{t}^{(L)}\} defined as

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ~t(i)=(1âˆ’Î±t(i))â‹…ğ[MASK]+Î±t(i)â‹…âˆ‘vâˆˆğ’¯t(i)pÂ¯t+1(i)â€‹(v)â‹…ğv,\tilde{\mathbf{e}}\_{t}^{(i)}=(1-\alpha\_{t}^{(i)})\cdot\mathbf{e}\_{\texttt{[MASK]}}+\alpha\_{t}^{(i)}\cdot\sum\_{v\in\mathcal{T}\_{t}^{(i)}}\bar{p}\_{t+1}^{(i)}(v)\cdot\mathbf{e}\_{v}, |  | (9) |

where ğ[MASK]\mathbf{e}\_{\texttt{[MASK]}} denotes the [MASK] embedding, ğv\mathbf{e}\_{v} denotes the embedding of token vv, ğ’¯t(i)\mathcal{T}\_{t}^{(i)} is the top-pp nucleus set at position ii, and pÂ¯t+1(i)â€‹(v)\bar{p}\_{t+1}^{(i)}(v) is the renormalised predicted distribution over the nucleus set at position ii.

To analyse stability, an ideal approach would be to examine the Jacobian of the update operator through its spectral radius. However, in practice this is intractable: transformer structures involve many linear and nonlinear components (layer normalisation, residual connections, multi-head attention), making it nearly impossible to provide a formal global analysis. The effective Jacobian inherits the complexity of the underlying transformer, and its spectral radius (or even its spectral norm) may be large and not easily bounded. As a result, although the iteration often stabilises empirically, a rigorous global convergence guarantee cannot be obtained.

Therefore, in this section, we follow the discussion from existing work (Yudin etÂ al., [2025](#bib.bib42); Hu etÂ al., [2024](#bib.bib14); Dasoulas etÂ al., [2021](#bib.bib8))
and focus on *local* Lipschitz continuity. This analysis considers only a single self-attention layer without any other operators and provides intuition to support our method and explain empirical results.

Specifically, the local Lipschitz bound suggests that for all soft embedding ete\_{t} within an Ïµ\epsilon-ball at original point (i.e. â€–etâ€–â‰¤Ïµ\|e\_{t}\|\leq\epsilon), where Ïµ\epsilon in fact bounds the maximum norm of embeddings, the following inequality holds after one-layer self-attention mapping:

|  |  |  |  |
| --- | --- | --- | --- |
|  | â€–ğt+1sâˆ’ğtsâ€–2â‰¤Kâ€‹â€–ğt+1âˆ’ğtâ€–2,\|\mathbf{e}\_{t+1}^{s}-\mathbf{e}\_{t}^{s}\|\_{2}\leq K\|\mathbf{e}\_{t+1}-\mathbf{e}\_{t}\|\_{2}, |  | (10) |

where ğts\mathbf{e}\_{t}^{s} is the output of ğt\mathbf{e}\_{t} after one-layer self-attention mapping, KK is the local Lipschitz constant. Following Hu etÂ al. ([2024](#bib.bib14)), we approximate KK in the form

|  |  |  |  |
| --- | --- | --- | --- |
|  | Kâ€‹(Ïµ)âˆcâ€‹â€–ğ–hVâ€–2â€‹â€–ğ–hQâ€‹(ğ–hK)âŠ¤â€–2â€‹Ïµ2,K(\epsilon)\;\propto\;c\,\|\mathbf{W}^{V}\_{h}\|\_{2}\,\|\mathbf{W}^{Q}\_{h}(\mathbf{W}^{K}\_{h})^{\top}\|\_{2}\,\epsilon^{2}, |  | (11) |

depends on the local norm Ïµ\epsilon, with query, key, and value matrices ğ–hQ,ğ–hK,ğ–hV\mathbf{W}^{Q}\_{h},\mathbf{W}^{K}\_{h},\mathbf{W}^{V}\_{h}, and a scaling constant cc.

The ideal outcome of such a mapping would be a contraction, i.e. Kâ‰¤1K\leq 1, which ensures that differences shrink across layers. However, in transformer blocks the large parameter norms often make this condition difficult to satisfy. Since ğ–hQ\mathbf{W}^{Q}\_{h}, ğ–hK\mathbf{W}^{K}\_{h}, and ğ–hV\mathbf{W}^{V}\_{h} are fixed for a pretrained model, stability in practice relies on keeping Ïµ\epsilon sufficiently small, which is under our control. This motivates us to restrict the update within a small Ïµ\epsilon-ball neighbourhood of the [MASK] embedding, which can be taken as a reference point near the origin. For comparison, in DreamÂ (Ye etÂ al., [2025](#bib.bib40)), while the [MASK] embedding has a very small â„“2\ell\_{2} norm of 0.33400.3340 in 3,584 dimensions (corresponding to a per-dimension RMS of about 0.00550.0055), regular token embeddings are much larger. For instance,
a typical token embedding has an â„“2\ell\_{2} norm of about 0.87210.8721, which corresponds to an average per-dimension RMS magnitude of approximately 0.01420.0142.

To connect this bound back to the embedding updates, we require ğ~t(i)\tilde{\mathbf{e}}\_{t}^{(i)} and ğ~t+1(i)\tilde{\mathbf{e}}\_{t+1}^{(i)} to lie within an Ïµ\epsilon-ball at origin, which requires a very small Ïµ\epsilon. Since both are formed as weighted sums of the [MASK] embedding and candidate token embeddings (Equation. [9](#A4.E9 "In Appendix D Stability Analysis of Mixed Embedding Updates â€£ Latent Refinement Decoding: Enhancing Diffusion-Based Language Models by Refining Belief States")), a straightforward way to reduce this distance is to bound the mixing coefficient Î±t(i)\alpha\_{t}^{(i)}. Intuitively, this means the search for efficient mixed embeddings remains close to the [MASK] token, with exploration constrained to a small neighbourhood. In this way, the iterative updates remain within a contraction-like region, which empirically yields stable predictive distributions.

To simplify, we introduce a base rate rfr\_{f} and set Î±t(i)=rfâ‹…H^t+1(i)\alpha\_{t}^{(i)}=r\_{f}\cdot\hat{H}\_{t+1}^{(i)}, where H^t+1(i)âˆˆ[0,1]\hat{H}\_{t+1}^{(i)}\in[0,1] is the normalised entropy. Since maxiâ¡Î±t(i)â‰¤rf\max\_{i}\alpha\_{t}^{(i)}\leq r\_{f}, ensuring the difference is within Ïµ\epsilon reduces to choosing a sufficiently small rfr\_{f}. Empirically, we find that the method is stable and effective when rfr\_{f} is small, but fails to converge for large rfr\_{f} (see FigureÂ [5](#S4.F5 "Figure 5 â€£ 4.5 Ablation Study â€£ 4 Experiments â€£ Latent Refinement Decoding: Enhancing Diffusion-Based Language Models by Refining Belief States")).

We further evaluate the stability of output embeddings before the logit prediction step across adjacent timesteps. Since the token space is sparse and high-dimensional, we use the KL divergence as the metric. This reveals clear convergence during the latent refinement phase when rfr\_{f} is small, even after deep iteration with multi-layer self-attention in a transformer (see FigureÂ [3](#S4.F3 "Figure 3 â€£ 4.3 Main Results â€£ 4 Experiments â€£ Latent Refinement Decoding: Enhancing Diffusion-Based Language Models by Refining Belief States")).

Another observation that implicitly supports our claim is the case of top-pp selection. If pp is set very small, only a few candidate tokens contribute to the weighted sum âˆ‘vâˆˆğ’¯t(i)pÂ¯t+1(i)â€‹(v)â€‹ğv\sum\_{v\in\mathcal{T}\_{t}^{(i)}}\bar{p}\_{t+1}^{(i)}(v)\mathbf{e}\_{v}. Even without an explicit scaling factor such as Î±t(i)\alpha\_{t}^{(i)}, restricting the support of the soft embedding effectively yields a small Ïµ\epsilon, which can help stabilise the updates. This explains why our method maintains reasonable performance even under extreme top-pp settings (see FigureÂ [5](#S4.F5 "Figure 5 â€£ 4.5 Ablation Study â€£ 4 Experiments â€£ Latent Refinement Decoding: Enhancing Diffusion-Based Language Models by Refining Belief States")).

In summary, although a rigorous global convergence guarantee for mixed embedding iterations is intractable due to the nonlinear, high-capacity nature of transformers, our local Lipschitz analysis provides useful theoretical insight. Together with empirical validation, this suggests that while strict guarantees remain challenging, the proposed method is practically stable and effective for reasoning with diffusion LLMs.