# Introduction {#intro}

Tokenization is a relatively understudied area, but it can greatly impact model performance and efficiency [@rust-etal-2021-good; @hofmann2022embarrassingly; @ali2023tokenizer; @toraman2023impact; @petrov2024language; @singh2024tokenization; @rajaraman2024toward; @shao2024flexibly; @wang2024tokenization]. Vocabularies should be efficient, as every additional token in the vocabulary increases embedding parameters, and thus model size. Each vocabulary item should contribute enough to model performance to justify the use of parameters.

In this paper, we focus on Byte-Pair Encoding (BPE; @gage1994new [@sennrich-etal-2016-neural]) tokenizers. BPE tokenization works by breaking down a text into each of its characters or bytes and then building tokens in the vocabulary through a series of merges. The result of each merge must be stored as a token in the vocabulary. Tokens which are used only to execute merges are sometimes referred to as intermediate "junk" tokens [@bostrom-durrett-2020-byte]. An example is illustrated in Figure [1](#fig:kentucky){reference-type="ref" reference="fig:kentucky"}. Intermediate tokens clutter the vocabulary and are hardly ever used during tokenization.

In addition to efficiency, we consider other model behaviors that may be driven by tokenization. @land2024fishing recently showed that very low-frequency tokens in the vocabulary may be under-trained by a model. This leads to worse downstream performance and unwanted outputs, such as hallucinations. Under-trained tokens can also potentially be exploited to avoid safety measures through the use of these out-of-distribution items. These tokens are also called "glitch tokens" [@rumbelow2023solidgoldmagikarp; @geiping2024coercing; @li2024glitch].

<figure id="fig:kentucky" data-latex-placement="t!">
<img src="figures/kentucky_noT.png" />
<figcaption>An example of a series of merges to produce a token <code>Kentucky</code>. The pre-merge token frequencies are demonstrated in corresponding circles. In the vanilla BPE algorithm, <code>entucky</code> should also be stored in the vocabulary, whereas it is redundant after the merge. In this example, the IoS metric effectively captures the intermediate token, as <span class="math inline">IoS(<code>entucky</code>)‚ÄÑ‚â•‚ÄÑùíØ‚ÄÑ=‚ÄÑ0.9</span>.</figcaption>
</figure>

Vocabulary trimming, which entails removing items from a tokenizer's vocabulary, has been proposed as a method to remove unnecessary tokens, *e.g.*, language- or domain-specific tokens. Trimming has been shown to reduce embedding parameters without degrading downstream performance [@ushio-etal-2023-efficient; @pang2024specialising]. Under-trained token indicators were shown to be correlated with token frequency in the training corpus, where less frequent tokens are more likely to be under-trained¬†[@land2024fishing]. Vocabulary trimming, thus, is well suited to address the issue of under-trained tokens.

Vocabulary trimming has mostly been implemented after tokenizer training [@yang-etal-2022-textpruner; @cognetta2024analysis]. This means that it is difficult to determine the vocabulary size in advance because it is not known in advance how many tokens will be removed by the trimming procedure. Setting a fixed vocabulary size might be important, for example, in increasing training throughput [@groeneveld2024olmo].

In this paper, we introduce **Picky BPE**[^2] --- a modified BPE tokenizer that implements vocabulary refinement during tokenizer training. Unlike other trimming procedures, Picky BPE effectively removes intermediate tokens once they become useless and seamlessly collects the vocabulary of the desired size without data-specific heuristics. Our method leads to more efficient usage of the limited vocabulary, and thus the embedding parameters. We show that our method leads to equal or better performance on a downstream translation task (¬ß[4](#sec:translation){reference-type="ref" reference="sec:translation"}). Furthermore, we reduce the number of tokens that are likely to be under-trained (¬ß[5](#sec:under_trained){reference-type="ref" reference="sec:under_trained"}) and free space for higher-quality word-initial tokens. Due to the improved quality of the desired-size vocabulary, Picky BPE does not compromise text compression (¬ß[6](#sec:features){reference-type="ref" reference="sec:features"}) unlike other trimming methods, which makes it suitable for practical use.

# Related Work

Several common alternatives to BPE tokenization implicitly address the issue of intermediate low-frequency tokens. For instance, WordPiece tokenization¬†[@wu2016googles] is based on a series of merges akin to BPE, but along with the pair frequency, it also takes individual token frequencies into account. Thus, the tokenizer is less likely to add merges that would leave redundant tokens. However, this does not guarantee that the tokenizer adds merges in an optimal order, nor does it facilitate the retrospective removal of intermediate tokens that might eventually appear.

Another popular algorithm is Unigram tokenization [@kudo-2018-subword] used in SentencePiece¬†[@kudo-richardson-2018-sentencepiece]. The core of this algorithm is different from BPE-like solutions. Unigram works by creating a large vocabulary and iteratively pruning it until it reaches the desired size. The pruning is performed according to how much the token removal affects the likelihood of the subword sequence, and takes into account individual token frequencies. Intermediate tokens are also less likely to appear in such a scenario, which might suggest that Unigram tokenization implicitly performs a form of vocabulary trimming. We compare our method with Unigram tokenization in¬†¬ß[6](#sec:features){reference-type="ref" reference="sec:features"}.

There are also several proposed modifications to BPE, which address the issues raised in ¬ß[1](#intro){reference-type="ref" reference="intro"}. BPE-Dropout was proposed to mitigate the issue of rare subwords by dropping merges randomly during tokenizer training [@provilkov-etal-2020-bpe]. This method regularizes the BPE training to expose a model to alternate tokenizations of the same strings, making it more robust to noisy input, such as misspellings. BPE-dropout also helps in reducing the under-training of low-frequency tokens. However, this method does not change the tokenizer vocabulary that is used during inference, and ultimately does not bear on the issue of vocabulary efficiency.

@sennrich-etal-2017-university use an absolute frequency cut-off to prevent very low-frequency tokens from being added to the vocabulary. Similarly, @vilar-federico-2021-statistical propose a stopping criterion in order to select the optimal vocabulary for BPE. The authors propose a maximum likelihood constraint, where BPE stops adding merges during training if a merge decreases the overall likelihood of the token sequence.

@cognetta2024analysis implemented a vocabulary trimming method for BPE. They found that they were able to reduce the vocabulary size without significantly reducing downstream translation performance. Their method, however, did worsen compression. In some cases, when they showed the greatest task improvement, they found an increase of over 13% in sequence length, *i.e.*, text length in number of tokens. Better compression has been shown to correlate with better model performance [@galle-2019-investigating; @liang-etal-2023-xlm; @goldman2024unpacking] and lead to faster inference time [@song-etal-2021-fast; @petrov2024language; @yamaguchi2024empirical].

Our Picky BPE differs from this method as we do not reduce the final vocabulary size. In addition, our trimming is performed during training, which preserves the overall distribution of token frequencies and does not require heuristic data-related post-processing, *i.e.*, choosing an absolute frequency threshold that is different for every dataset¬†[@cognetta2024analysis].

In a concurrent work, @lian2024scaffold also identify the issue of intermediate ("scaffold\") tokens and introduce Scaffold-BPE. The authors propose to identify intermediate tokens when they are below the current range of frequencies during the tokenizer training. Compared to our method that uses a thresholding hyperparameter (see ¬ß[3](#sec:alg){reference-type="ref" reference="sec:alg"}), there is no way to regulate the strength of Scaffold-BPE. In addition, the authors propose to run inference by first tokenizing the input text using both vocabulary and scaffold tokens and then splitting the scaffold tokens into the shortest valid token sequences. This approach does not strictly adhere to the training process and leads to inaccuracies in tokenization and worse compression (see the example and the comparison in Appendix¬†[10](#app:inference){reference-type="ref" reference="app:inference"}).

Another contemporaneous work by @bauwens-delobelle-2024-bpe also proposes a method of pruning merges that lead to undesired segmentation and bloated vocabularies. This approach differs in at least two key ways from Picky BPE: 1) it allows merges of more than two tokens and 2) it uses a semi-supervised method to determine which merges to remove, based on manually annotated language-specific morphological segmentations.

# Picky BPE {#sec:alg}

:::: algorithm
::: algorithmic
**Input:** Vocabulary $\mathcal{V}$; Tokenized corpus $\mathcal{C}$; Events order $\mathcal{E}$; IoS threshold $\mathcal{T}$ **Output:** Updated $\mathcal{V}, \mathcal{C}, \mathcal{E}$

$x_1, x_2 \leftarrow$ the most frequent pair in $\mathcal{C}$ $x_3 = x_1 
 + x_2$ $\mathcal{V} \leftarrow \mathcal{V} + \{x_3\}$ $\mathcal{E} \leftarrow \mathcal{E} + \{\mathrm{Merge}, (x_1, x_2)\}$ $\mathcal{V} \leftarrow \mathcal{V} \setminus \{x_1\}$ $\mathcal{E} \leftarrow \mathcal{E} + \{\mathrm{Remove}, x_1\}$ $\mathcal{V} \leftarrow \mathcal{V} \setminus \{x_2\}$ $\mathcal{E} \leftarrow \mathcal{E} + \{\mathrm{Remove}, x_2\}$ Update $\mathcal{C}$ based on events from this iteration $\mathcal{V}, \mathcal{C}, \mathcal{E}$
:::
::::

Our method is a modification of the original BPE algorithm¬†[@gage1994new; @sennrich-etal-2016-neural]. The intuition behind our modification is that we can identify intermediate tokens based on their individual frequency and frequency within a larger token. Intermediate tokens should have low frequency outside of the context of the token that contains them. For example, in Figure¬†[1](#fig:kentucky){reference-type="ref" reference="fig:kentucky"}, an intermediate token `entucky` is almost always a part of `Kentucky`, which is easy to capture by comparing the frequencies of `Kentucky` and `entucky`. To formalize this approach, we introduce a measure called *Intersection over Self (IoS)*, which is computed as follows:

$$\begin{equation}
    \mathrm{IoS}(x_1\ |\ x_1, x_2) = \frac{f_p(x_1, x_2)}{f_t(x_1)};
\end{equation}$$

$$\begin{equation}
    \mathrm{IoS}(x_2\ |\ x_1, x_2) = \frac{f_p(x_1, x_2)}{f_t(x_2)}.
\end{equation}$$

Here $x_1$ and $x_2$ are the tokens being merged, $f_t$ is token frequency, and $f_p$ is pair frequency. $\mathrm{IoS}(x_1\ |\ x_1, x_2)$ shows how often token $x_1$ occurs as part of a pair $\{x_1, x_2\}$ compared to all occurrences of $x_1$. If this value is too high, *i. e.*, close to 1, $x_1$ is highly likely an intermediate token, an integral part of a longer, more meaningful token $x_1 + x_2$. Adding $x_1 + x_2$ to the vocabulary makes $x_1$ redundant and we can consider removing it.

:::: algorithm
::: algorithmic
**Input:** Word $w$; Vocabulary $\mathcal{V}$; Events order $\mathcal{E}$ **Output:** Tokenized word $\mathcal{W}$

$\mathcal{W} \leftarrow$ split $w$ into symbols $\in \mathcal{V}$ $\mathcal{M} \leftarrow$ possible merges in $\mathcal{W}$ $\mathcal{R} \leftarrow$ possible removals in $\mathcal{W}$ $\varepsilon \leftarrow$ earliest event in $\mathcal{E}$, $\varepsilon \in \mathcal{M} \cup \mathcal{R}$ perform $\varepsilon$ update $\mathcal{M}, \mathcal{R}$ exclude events from $\mathcal{E}$ earlier than $\varepsilon$ $\mathcal{W}$
:::
::::

<figure id="fig:should_would_could" data-latex-placement="!htbp">
<img src="figures/PBPE_example.png" />
<figcaption>Picky BPE tokenization example. Token frequencies are demonstrated in the corresponding circles and are updated on merges. Token <code>‚Äò‚Äòould‚Äô‚Äô</code> is removed only after merging into three common tokens containing it. The corresponding IoS values are visualized on every merge. Once IoS becomes greater or equal to the threshold <span class="math inline">ùíØ</span>, 0.9 in this example, the token <code>‚Äò‚Äòould‚Äô‚Äô</code> is removed.</figcaption>
</figure>

## Algorithm description {#sec:algorithm_description}

The training of Picky BPE follows the main steps of the vanilla BPE training. The text is first split into a sequence of characters/bytes, initializing the vocabulary with unique symbols. Optionally, the coverage parameter (we use 0.9999 in our experiments) is used to replace the rarest symbols with `<unk>`. After that, the algorithm iteratively chooses the most frequent pair of tokens to merge and adds it to the vocabulary. The difference comes after each merge when we check whether we can remove any of the merged tokens judging by the IoS value. The pseudocode for a training step is demonstrated in Algorithm¬†[\[alg:merging\]](#alg:merging){reference-type="ref" reference="alg:merging"}. We integrate the IoS metric into the merging stage. When a pair of tokens is merged, we check whether we can safely remove either of the two tokens from the vocabulary. For this, we introduce a hyperparameter $\mathcal{T}$, the IoS threshold. If $\mathrm{IoS}(x_1\ |\ x_1,x_2) \geq \mathcal{T}$, we remove $x_1$. Thus, $\mathcal{T}$ leverages the strength of the removal policy: $\mathcal{T}$ is a positive value $\leq1$, and the closer it is to 1, the less strict becomes the removing criterion. For instance, $\mathcal{T}=0.9$ means that only the tokens present not more than 10% of the time outside of the current merge will be removed. In an extreme case, $\mathcal{T}=1$ means that no removals are possible, thus the algorithm becomes the vanilla BPE. Another unique feature of our algorithm is that the merges and removals are stored in the events order array $\mathcal{E}$ in the order of happening. The events order is crucial for the tokenization step.

The tokenization (inference) step is described in Algorithm¬†[\[alg:tokenization\]](#alg:tokenization){reference-type="ref" reference="alg:tokenization"}. We first split the input word into a series of in-vocabulary symbols. Then we collect the sets of possible merges and removals in the current tokenization and iteratively greedily choose the earliest possible event using event order $\mathcal{E}$. The action associated with the chosen event is performed and the sets of possible merges and removal are updated. This process strictly follows the tokenizer training and avoids compression issues happening in the approximation methods (see Appendix¬†[10](#app:inference){reference-type="ref" reference="app:inference"}).

## Algorithm analysis and justification

The training of Picky BPE is longer than that of the original vanilla BPE. However, the difference is not drastic. When a token is removed, recalculating the distances requires a constant number of operations, which makes the training time depend linearly on the number of events (merges and removals). With threshold $\mathcal{T}$ values of 0.6 and higher, the proportion of removed tokens generally does not surpass 10% (for details refer to Appendix¬†[13](#app:removed){reference-type="ref" reference="app:removed"}), which makes the number of removals inferior to the number of merges. On the tokenization stage, the time depends on the number of evens, just as the tokenization time of the vanilla BPE depends on the number of merges. As we show in Appendix¬†[13](#app:removed){reference-type="ref" reference="app:removed"}, merges comprise the largest partition of overall events, therefore removal events do not significantly slow down the inference. Depending on the programming language and the implementation, the astronomical time of both stages of the algorithm can differ significantly.

Here we also enumerate several algorithmic advantages of the proposed method.

#### Universal threshold.

The threshold $\mathcal{T}$ is relative and does not depend on the size of the training corpus or the desired vocabulary. This is one of the advantages of our method compared to the main counterparts, such as¬†@cognetta2024analysis. Furthermore, the removals happen during training resulting in the desired vocabulary size that does not require any post-processing.

#### Variety of intermediate tokens.

An intermediate token may be part of more than one token, as shown in Figure¬†[2](#fig:should_would_could){reference-type="ref" reference="fig:should_would_could"}. Our algorithm handles these cases, removing the token only after there are few to no words it can be merged into.

#### Second chances.

Any removed token may be merged again if its frequency is higher at a later point in the order of merges. This is usually the case for tokens removed in the very beginning when the frequencies of new tokens are very high. For example, `(‚Äò‚Äòt‚Äô‚Äô, ‚Äò‚Äòhe‚Äô‚Äô)` is likely to be merged early in tokenizer training because `‚Äò‚Äòthe‚Äô‚Äô` is a frequent word. Because the relative frequency of `‚Äò‚Äòhe‚Äô‚Äô` is lower, `‚Äò‚Äòhe‚Äô‚Äô` may be split into `(‚Äò‚Äòh‚Äô‚Äô, ‚Äò‚Äòe‚Äô‚Äô)`. But because `‚Äò‚Äòhe‚Äô‚Äô` is still a high-frequency word, it is likely to be merged again. If a previously removed token is restored, it is re-activated to keep its original place in the list of merges. This is essential to the merge order during tokenization.

# Machine Translation Experiments {#sec:translation}

To evaluate the downstream performance of our algorithm, we conduct several machine translation (MT) experiments. We experiment with three translation directions: English--German (EN--DE), German--Estonian (DE--ET), and Ukrainian--Estonian (UK--ET). With this choice of language pairs, we aim to cover diverse MT tasks of varying difficulty. German and English are related languages and share the same script. This language pair represents an easier translation task. German and Estonian use the same script, but are much less closely related, belonging to different language families. Translation for this pair should be more difficult. Finally, Ukrainian and Estonian represent the most difficult translation pair in our experiments. These languages are not only distant but also use different scripts.

To train the EN--DE models, we use the training corpus from the WMT16 news translation task¬†[@Bojar2016FindingsOT], with `newstest2016` corpus for evaluation. For DE--ET and UK--ET, we use the mixtures of parallel corpora assembled by¬†@estonian_centric_mt. For the evaluations of outputs in Estonian, we use the development set of the [Flores]{.smallcaps} benchmark¬†[@10.1162/tacl_a_00474].

::: {#tab:mt}
+------------+---------------+-------------------+--------------------+
| Experiment | $\mathcal{T}$ | BLEU $(\uparrow)$ | COMET $(\uparrow)$ |
+:===========+:=============:+:=================:+:==================:+
| EN--DE     | 1.0$^{\ast}$  | 30.1 $\pm$ 0.7    | 0.431              |
|            +---------------+-------------------+--------------------+
|            | 0.9           | 30.3 $\pm$ 0.7    | 0.431              |
|            +---------------+-------------------+--------------------+
|            | 0.8           | 30.0 $\pm$ 0.7    | 0.431              |
|            +---------------+-------------------+--------------------+
|            | 0.7           | 30.6 $\pm$ 0.7    | **0.434**          |
|            +---------------+-------------------+--------------------+
|            | 0.6           | 30.3 $\pm$ 0.7    | 0.431              |
+------------+---------------+-------------------+--------------------+
| DE--ET     | 1.0$^{\ast}$  | 19.4 $\pm$ 1.0    | 0.516              |
|            +---------------+-------------------+--------------------+
|            | 0.9           | 19.9 $\pm$ 1.0    | **0.520**          |
|            +---------------+-------------------+--------------------+
|            | 0.8           | 19.8 $\pm$ 1.0    | **0.520**          |
|            +---------------+-------------------+--------------------+
|            | 0.7           | 19.9 $\pm$ 1.0    | **0.520**          |
|            +---------------+-------------------+--------------------+
|            | 0.6           | 19.9 $\pm$ 1.1    | **0.520**          |
+------------+---------------+-------------------+--------------------+
| UK--ET     | 1.0$^{\ast}$  | 16.9 $\pm$ 1.0    | 0.506              |
|            +---------------+-------------------+--------------------+
|            | 0.9           | 15.8 $\pm$ 1.5    | 0.508              |
|            +---------------+-------------------+--------------------+
|            | 0.8           | 16.7 $\pm$ 1.3    | **0.511**          |
|            +---------------+-------------------+--------------------+
|            | 0.7           | 17.2 $\pm$ 1.0    | 0.509              |
|            +---------------+-------------------+--------------------+
|            | 0.6           | 16.9 $\pm$ 0.9    | **0.511**          |
+------------+---------------+-------------------+--------------------+

: Machine translation results with vocabulary size 8192 on `newstest2016` set¬†[@Bojar2016FindingsOT] for EN--DE, and on [Flores]{.smallcaps}-dev¬†[@10.1162/tacl_a_00474] for DE--ET and UK--ET. For every threshold¬†$\mathcal{T}$, we report BLEU¬†[@papineni-etal-2002-bleu] and COMET¬†[@rei-etal-2020-comet] scores on the translation task. The best scores are highlighted in **bold**. Other scores that are not statistically significantly different from the best are also highlighted in **bold**. If neither of the scores is significantly better, nothing is highlighted. $^{\ast}\mathcal{T} = 1.0$ represents the baseline vanilla BPE without intermediate token removal.
:::

We test our method with several thresholds: 0.6, 0.7, 0.8, 0.9. We did not consider lower thresholds as they would remove too many useful tokens. For the baseline, we chose vanilla BPE, which we obtained by training our Picky BPE with $\mathcal{T} = 1$ to ensure effects are not driven by implementation differences. We use the `transformer-iwslt` model from `fairseq`¬†[@ott-etal-2019-fairseq] for all translation tasks. The architecture and training details can be found in Appendix¬†[11](#app:arch){reference-type="ref" reference="app:arch"}.

For generation, we use beam search with beam size 5 in all our experiments. We use BLEU¬†[@papineni-etal-2002-bleu] from `sacreBLEU`¬†[@post-2018-call] and COMET¬†[@rei-etal-2020-comet] scores for automatic evaluation. We compute paired t-Test with bootstrapping[^3] to compare the obtained translation systems with statistical significance¬†[@koehn-2004-statistical].

#### Smaller vocabularies.

First, we conduct experiments on all three language pairs with a small vocabulary size of 8192. We chose such a restrictive setting to make sure all the tokens are sufficiently trained, as the relatively small training datasets we used ($\sim$`<!-- -->`{=html}1--4M sentence pairs) do not necessitate large vocabularies [@sennrich-zhang-2019-revisiting] and the effect of using our method might be less pronounced. The results are presented in Table¬†[1](#tab:mt){reference-type="ref" reference="tab:mt"}. Overall, the models trained with Picky BPE vocabulary performed comparably to the vanilla BPE, with at least one Picky BPE threshold significantly outperforming it for all three translation directions according to the COMET metric. COMET scores for the DE--ET experiment show that all Picky BPE models were better than the vanilla baseline.

::: tabular
cccc Vocabulary & $\mathcal{T}$ & BLEU $(\uparrow)$ & COMET $(\uparrow)$\
& 1.0$^{\ast}$ & 30.7 $\pm$ 0.7 & 0.431\
& 0.9 & 30.4 $\pm$ 0.7 & 0.431\
& 0.8 & 30.3 $\pm$ 0.7 & 0.430\
& 0.7 & 30.3 $\pm$ 0.7 & 0.430\
& 0.6 & 30.8 $\pm$ 0.7 & **0.432**\
& 1.0$^{\ast}$ & 31.1 $\pm$ 0.7 & 0.433\
& 0.9 & 31.1 $\pm$ 0.7 & 0.433\
& 0.8 & 31.0 $\pm$ 0.7 & **0.435**\
& 0.7 & 31.4 $\pm$ 0.7 & **0.435**\
& 0.6 & 31.1 $\pm$ 0.7 & **0.435**\
& 1.0$^{\ast}$ & 29.8 $\pm$ 0.7 & 0.418\
& 0.9 & 29.6 $\pm$ 0.8 & **0.428**\
& 0.8 & 30.5 $\pm$ 0.7 & **0.430**\
& 0.7 & 30.4 $\pm$ 0.7 & **0.430**\
& 0.6 & 28.3 $\pm$ 0.8 & 0.416\
& 1.0$^{\ast}$ & 31.1 $\pm$ 0.7 & 0.436\
& 0.9 & 31.2 $\pm$ 0.7 & 0.436\
& 0.8 & 30.9 $\pm$ 0.6 & 0.434\
& 0.7 & 31.1 $\pm$ 0.7 & 0.436\
& 0.6 & 31.3 $\pm$ 0.7 & **0.438**\
& 1.0$^{\ast}$ & 30.9 $\pm$ 0.7 & **0.435**\
& 0.9 & 31.1 $\pm$ 0.7 & 0.434\
& 0.8 & 31.1 $\pm$ 0.7 & **0.437**\
& 0.7 & 30.9 $\pm$ 0.7 & **0.436**\
& 0.6 & 30.9 $\pm$ 0.7 & 0.431\
& 1.0$^{\ast}$ & 28.5 $\pm$ 0.7 & 0.421\
& 0.9 & 28.4 $\pm$ 0.7 & **0.427**\
& 0.8 & 28.6 $\pm$ 0.7 & **0.425**\
& 0.7 & 28.0 $\pm$ 0.7 & 0.416\
& 0.6 & 28.8 $\pm$ 0.7 & 0.420\
:::

#### Larger vocabularies.

We also tested Picky BPE with larger vocabularies for the EN--DE task. We used two settings: separate vocabularies for input and output, and joint vocabularies. In both cases, we used total vocabulary sizes 16384, 32768, and 65536. The results for all these experiments are presented in Table¬†[\[tab:en-de-large\]](#tab:en-de-large){reference-type="ref" reference="tab:en-de-large"}. As with the smaller vocabulary setting, we see models based on Picky BPE tokenization performing on par with the ones based on the vanilla BPE. In most experiments, our method brings downstream improvements judging by the values of the COMET metric. We also observe by the BLEU scores that for the largest vocabularies of sizes 32768 + 32768 and 65536 the performance is generally worse than with the smaller vocabularies, regardless of the tokenization method. This is likely due to the volume of training data being insufficient for such a large vocabulary. However, in this setting Picky BPE still outperforms vanilla BPE by COMET.

<figure id="fig:scatter" data-latex-placement="t!">
<figure id="fig:removed-0-9">
<p><img src="figures/removed_0_9.png" style="width:93.0%" alt="image" /> <span id="fig:removed-0-9" data-label="fig:removed-0-9"></span></p>
</figure>
<figure id="fig:added-0-9">
<p><img src="figures/added_0_9.png" style="width:93.0%" alt="image" /> <span id="fig:added-0-9" data-label="fig:added-0-9"></span></p>
</figure>
<figcaption>Input embedding vectors for Picky BPE tokens with <strong>(a)</strong> <span class="math inline">ùíØ‚ÄÑ=‚ÄÑ1</span> and <strong>(b)</strong> <span class="math inline">ùíØ‚ÄÑ=‚ÄÑ0.9</span> for English vocabularies of size 16384 in EN‚ÄìDE experiments with separate vocabularies. For each token we compute its probability in the training corpus (y-axis), and the L2 norm of its embedding vector in the trained model (x-axis).</figcaption>
</figure>

<figure id="fig:pdf" data-latex-placement="t!">
<img src="figures/pdf_cutout.png" style="width:95.0%" />
<figcaption>Token frequency distributions for English vocabularies of size 16384 in EN‚ÄìDE experiments with separate vocabularies for input and output. The left tail becomes less heavy as we decrease the threshold.</figcaption>
</figure>

# Under-Trained Tokens {#sec:under_trained}

We also test whether Picky BPE decreases the number tokens likely to be under-trained. These tokens can be identified by looking for very low L2 norm of the token embeddings [@land2024fishing]. We plot L2 norms for $\mathcal{T} = 0.9$ in Figure¬†[5](#fig:scatter){reference-type="ref" reference="fig:scatter"} and those for the remaining thresholds in Appendix¬†[12](#app:under-trained){reference-type="ref" reference="app:under-trained"}. There are two groups of low-L2 norm tokens: the first is the low-frequency tokens, which can be seen in the lower left of Figure [3](#fig:removed-0-9){reference-type="ref" reference="fig:removed-0-9"}. According to @land2024fishing, these are the candidates for under-training. There is also a group of the highest-frequency tokens with low L2 norms (top left, Figure [3](#fig:removed-0-9){reference-type="ref" reference="fig:removed-0-9"}). We posit that these are general-purpose tokens that occur in a wide variety of contexts, and thus their representations are less specific. It has long been observed that high-frequency words are more likely to have more senses, *i.e.*, meanings [@zipf1945meaning], and thus be more general-purpose.

A large portion of the tokens removed by Picky BPE (Figure¬†[3](#fig:removed-0-9){reference-type="ref" reference="fig:removed-0-9"}) are likely to become under-trained. By contrast, the added tokens (Figure [4](#fig:added-0-9){reference-type="ref" reference="fig:added-0-9"}) have higher L2 norms and higher probability. The high-frequency general tokens are not removed by Picky BPE. We argue that Picky BPE reduces the likelihood of under-trained tokens and the risks that come with them, such as increased hallucinations.

We also find that as we lower the threshold for Picky BPE, there is a decrease in the left tail of the token frequency distribution, which represents the low-frequency tokens (Figure¬†[6](#fig:pdf){reference-type="ref" reference="fig:pdf"}). Trimming methods that involve an absolute frequency cut-off, such as the one used by @cognetta2024analysis and originally proposed in @sennrich-etal-2017-university, would completely eliminate the left tail and leave an abrupt fall-off on the distribution. We observe that Picky BPE preserves the overall distribution and does not eliminate the left tail. This shows that Picky BPE is not another implementation of the post-training trimming of low-frequency tokens.

::: {#tab:ablation}
+:-------------:+:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:+:----------------:+
|               | ::: {#tab:ablation}                                                                                                                                                                                                                                                                                                       | \# unique tokens |
|               |   ------------------                                                                                                                                                                                                                                                                                                      |                  |
|               |    \# unique tokens                                                                                                                                                                                                                                                                                                       |                  |
|               |     vs vanilla BPE                                                                                                                                                                                                                                                                                                        |                  |
|               |   ------------------                                                                                                                                                                                                                                                                                                      |                  |
|               |                                                                                                                                                                                                                                                                                                                           |                  |
|               |   : Comparison of tokens from picky BPE and vanilla BPE for joint EN--DE vocabularies of size 8192. For each threshold $\mathcal{T}$, we report the number of unique tokens in the Picky BPE vocabulary compared to the vanilla BPE $(\mathcal{T} = 1)$ with and without low-frequency token trimming on post-processing. |                  |
|               | :::                                                                                                                                                                                                                                                                                                                       |                  |
+---------------+                                                                                                                                                                                                                                                                                                                           +------------------+
| $\mathcal{T}$ |                                                                                                                                                                                                                                                                                                                           | vs vanilla BPE   |
+---------------+                                                                                                                                                                                                                                                                                                                           +------------------+
|               |                                                                                                                                                                                                                                                                                                                           | \+ post-trimming |
+---------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------+
| 0.9           | 168 (2.1%)                                                                                                                                                                                                                                                                                                                | 115 (1.4%)       |
+---------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------+
| 0.8           | 391 (4.8%)                                                                                                                                                                                                                                                                                                                | 248 (3.0%)       |
+---------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------+
| 0.7           | 625 (7.6%)                                                                                                                                                                                                                                                                                                                | 393 (4.8%)       |
+---------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------+
| 0.6           | 869 (10.6%)                                                                                                                                                                                                                                                                                                               | 588 (7.2%)       |
+---------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------+

: Comparison of tokens from picky BPE and vanilla BPE for joint EN--DE vocabularies of size 8192. For each threshold $\mathcal{T}$, we report the number of unique tokens in the Picky BPE vocabulary compared to the vanilla BPE $(\mathcal{T} = 1)$ with and without low-frequency token trimming on post-processing.
:::

:::: table*
::: tabular
\@cccccccc@ & & & &\
(lr)3-4 (lr)5-7 && German & English & Dropped $(\downarrow)$ & Added $(\uparrow)$ & Overall $(\uparrow)$ &\
$^{\ast}$ & 0 & 1.000 & 1.000 & --- & --- & 61.5 & 5.38\
& 160 & 0.997 & 0.996 & 43.8 & 65.5 & 61.9 & 5.40\
0.8 & 358 & 0.995 & 0.993 & **41.1** & **67.5** & 62.7 & 5.44\
0.7 & 588 & 0.994 & 0.991 & 42.0 & 66.9 & 63.3 & 5.47\
0.6 & 805 & **0.992** & **0.989** & 42.1 & 64.2 & **63.6** & **5.50**\
:::
::::

Table [3](#tab:ablation){reference-type="ref" reference="tab:ablation"} shows the difference between Picky BPE and vanilla BPE with and without post-processing trimming. By post-trimming we mean training the vanilla BPE to have a larger vocabulary with further trimming low-frequency tokens to achieve the desired vocabulary size. We train the initial tokenizer so the number of additional tokens is the number of replaced tokens from the corresponding Picky BPE tokenizer. Through both differences in the number of replaced tokens in the two different strategies, we show that Picky BPE is not simply a different implementation of the post-trimming akin to¬†@cognetta2024analysis, but it leads to a fundamentally different resulting vocabulary.

# Features of Picky BPE {#sec:features}

#### Text Compression.

Text compression is generally considered to be an important aspect of tokenizer evaluation [@galle-2019-investigating; @goldman2024unpacking], and language models that compress more have been shown to have better performance [@liang-etal-2023-xlm; @goldman2024unpacking]. We use *corpus token count* (CTC; @schmidt2024tokenization) to measure compression. CTC, also called sequence length, is the number of tokens needed to represent a given text. The fewer tokens are needed, the better the compression.

Table [\[tab:qualities\]](#tab:qualities){reference-type="ref" reference="tab:qualities"} shows the changes in compression as a percentage relative to the tokenizer of the same vocabulary size with a threshold of 1, all for EN--DE vocabularies of size 8192. We report additional compression rates in Appendix¬†[14](#app:compression){reference-type="ref" reference="app:compression"}. We find that Picky BPE shows no loss in compression. This is an improvement over the method in @cognetta2024analysis, which shows worse compression after vocabulary trimming.

#### Token Qualities.

In addition to the above metrics, we compare the tokens themselves. One quality of interest is the proportion of word-initial tokens, which are stored in the tokenizer with an underscore at the beginning to represent a space character. @yehezkel-pinter-2023-incorporating also notice that their trimming procedure leads to an increased number of word-initial tokens.

In Table [\[tab:qualities\]](#tab:qualities){reference-type="ref" reference="tab:qualities"}, we also report the percentage of word-initial tokens from the added and removed tokens as well as overall proportions for the EN--DE vocabulary of size 8192. We report results for the other experiments in Appendix [15](#app:word-initial){reference-type="ref" reference="app:word-initial"}. We find that dropped tokens are far less likely to be word-initial than added tokens. Therefore, Picky BPE is adding more word-initial tokens than it is removing. As the threshold is lowered, we see slightly fewer word-initial tokens added to the vocabulary. This might be due to the intensive removals happening with lower thresholds. In the overall rates of word-initial tokens, we see a slight increase as $\mathcal{T}$ goes down.

Upon inspection of the added tokens, we see that many of the word-initial tokens are also complete, meaningful words, for example `renovated`, `overcoat`, `cognition`, and `unconventional`. Increased rates of word-initial tokens may be indicative of improved token quality.

We also found that many of the tokens removed by Picky BPE were intermediate, much like `entucky` (Figure [1](#fig:kentucky){reference-type="ref" reference="fig:kentucky"}). These tokens are relatively long and only occur in the context of a longer token that is also present in the vocabulary. Often, these tokens are missing only one or two characters relative to the full word. We find word-initial and word-medial intermediate tokens, *e.g.*, `Chicag`, `algorith`, `roprietary`, `omenclature` (cf. 'Chicago', 'algorithm', 'proprietary', 'nomenclature').

Following @bostrom-durrett-2020-byte, we also measure mean token length. They argue that longer mean token length is associated with gold-standard morphologically-aligned tokenization, and thus with better token quality. Additionally, longer tokens on average will lead to increased compression, as a text of a fixed length can be represented with fewer, longer tokens. We find that the mean token length slightly but consistently increases as we lower the threshold (see Table¬†[\[tab:qualities\]](#tab:qualities){reference-type="ref" reference="tab:qualities"}). We report additional mean token length results in Appendix¬†[16](#app:token_len){reference-type="ref" reference="app:token_len"}.

We additionally compare Picky BPE with Unigram tokenization in Table¬†[\[tab:sp\]](#tab:sp){reference-type="ref" reference="tab:sp"}. Unigram tokenization seems to have longer tokens with a higher proportion of word-initial tokens. However, it drastically worsens the compression. We hypothesize that Unigram adds many meaningful full-word tokens which are not optimal for the text compression under the restriction of the vocabulary size.

::: tabular
\@lcccc@ & & &\
(lr)2-3 & EN & DE & &\
Unigram & 1.143 & 1.124 & **75.6** & **7.73**\
$\mathcal{T} = 1.0$ & 1.000 & 1.000 & 72.2 & 6.85\
$\mathcal{T} = 0.9$ & 0.997 & 0.998 & 72.8 & 6.88\
$\mathcal{T} = 0.8$ & 0.996 & 0.998 & 73.2 & 6.91\
$\mathcal{T} = 0.7$ & 0.994 & 0.997 & 73.6 & 6.94\
$\mathcal{T} = 0.6$ & **0.992** & **0.996** & 73.9 & 6.95\
:::

# Discussion {#sec:discussion}

We believe Picky BPE would be beneficial for Large Language Models (LLMs), however, the lack of computational resources does not allow us to carry out a side-by-side comparison. Instead, we provide a series of experiments that we believe illustrate key properties of the proposed method. To put these results into perspective, we want to reiterate two core aspects of the provided experiments: first, there is no universal methodology that could assess tokenizer quality; second, the inefficiencies associated with undertrained tokens discussed by @land2024fishing depend on the size of vocabulary relative to the size of training data.

#### Evaluating tokenizers.

It is not always clear how to best compare different tokenizers [@zouhar-etal-2023-tokenization]. One approach is training models for each tokenizer and evaluating downstream performance, *e.g.*, @goldman2024unpacking. However, these results may be driven by confounding factors, such as differences in compression leading to the model effectively being trained on less text [@petrov2024language], and downstream task results may also be task-specific. The second general approach to evaluating tokenizers is to evaluate some quality of the tokenizer's output such as fertility (average number of tokens per word; @rust-etal-2021-good), similarity of tokenizer boundaries to morphological boundaries [@hofmann-etal-2021-superbizarre], and cognitive plausibility of tokens [@beinborn-pinter-2023-analyzing]. There is no consensus about which metric(s) provide the best overall estimation of tokenizer quality.

#### Role of undertrained tokens.

We achieved better or equal performance on machine translation with small vocabularies compared to the vanilla BPE. However, we did not improve the performance with a large vocabulary. The restriction on vocabulary size was set intentionally to reduce redundancy and ensure all tokens receive enough training. We expect to see the same effect with LLMs as their large vocabulary size corresponds to the massive scale of training data and model size. This is well-justified by our analysis of under-trained tokens in response to the exploration of LLMs by¬†@land2024fishing. We also witness improved token quality that comes with our method, which does not affect text compression, see comparison to the Unigram tokenization in ¬ß[6](#sec:features){reference-type="ref" reference="sec:features"}.

# Conclusion

In this paper, we propose a novel tokenization algorithm, Picky BPE, which refines vocabulary during tokenizer training targeting intermediate tokens. Our results show that our algorithm may improve downstream performance in a setting of limited vocabulary, which we can extrapolate on larger vocabularies given enough training. Our method also mitigates the issue of under-trained tokens, efficiently removing them during training, and improves token quality and text compression, filling the freed vocabulary space with meaningful tokens with higher frequency. These factors suggest that Picky BPE can be considered for larger models to improve downstream performance and safety and avoid undesired behavior, *e.g.*, hallucinations.

# Limitations

Picky BPE behavior depends on the choice of threshold $\mathcal{T}$. Even though the threshold is relative and mostly intuitive in use, one must consider that with lower thresholds the probability of eliminating useful tokens grows and the behavior becomes less stable. Therefore, it is important to start with safer larger thresholds, analyzing the tokenization using vocabulary-related measures.

In this paper, the only downstream task we evaluate our models on is translation. Training a larger language model and evaluating it on other downstream tasks may show different patterns. This may allow us to better understand the contribution of Picky BPE as well as its potential drawbacks.

@rust-etal-2021-good show that different tasks have variable correlation with tokenizer evaluations like fertility. To the best of our knowledge, there is not enough empirical work to determine which tasks would be most informative for evaluating tokenizer quality. This is an important area for future work.

Our experiments are also limited to a relatively small set of languages. We selected pairs of languages that were typologically varied and used different writing systems, however, all the languages are spoken in Europe. Future work should evaluate whether a larger and more diverse sample of languages exhibit the same trends as in this paper.

# Acknowledgments {#acknowledgments .unnumbered}

The authors acknowledge the HPC resource allocation by Erlangen National High-Performance Computing Center (NHR@FAU) of the Friedrich-Alexander-Universit√§t Erlangen-N√ºrnberg (FAU) (joint project with the Center for Artificial Intelligence (CAIRO), THWS) and Jean Zay (compute grant #GC011015451). The authors would also like to thank the other members of the PleIAs Research team for helpful discussion and feedback.

# Inference options {#app:inference}

Picky BPE inference strictly follows the training order of events and executes merges and removals in the same chronological order (Algorithm¬†[\[alg:tokenization\]](#alg:tokenization){reference-type="ref" reference="alg:tokenization"}). Concurrent works use a different approach to inference: the input text is first tokenized with a vanilla BPE tokenizer using both active and removed tokens and then the low-frequency¬†[@cognetta2024analysis] or scaffold¬†[@lian2024scaffold] tokens are split into the shortest available sequences of valid tokens. The latter approach is suboptimal, as the training events order is likely to be broken.

For example, imagine the token sequence `[t, h, e, r, e]` on a certain training step. Tokens `(h, e)` are merged into `he` (event $e_{i_1}$). The sequence becomes `[t, he, r, e]`. Later, token `he` becomes useless and is removed (event $e_{i_2}, i_2 > i_1$). Thus, the sequence returns to `[t, h, e, r, e]`. It can happen now that tokens `(e, r)` are merged into a new token `er` (event $e_{i_3}, i_3 > i_2$). The resulting tokenization is `[t, h, er, e]`. Picky BPE tokenization will follow event order $e_{i_1},..., e_{i_2},..., e_{i_3}$ and result in `[t, h, er, e]`. The tokenization when the tokens are removed after the vanilla BPE process will first achieve `[t, he, r, e]`, as it will execute all the available merges. In a simplified example, there are no merges to perform after this step, and the algorithm will move to the removals phase: `he` will be split, and the resulting tokenization will become `[t, h, e, r, e]`. Therefore, `er` will not be merged, as it happened after the removal and contains a part of the removed token.

When repeated several times, the described issue may lead to undesired tokenization results and compromise compression. In Table¬†[6](#tab:inference){reference-type="ref" reference="tab:inference"}, we compare the compression rates of the two methods. The compression issues become more pronounced with lower thresholds as more tokens are removed.

Apart from the described inference methods, Picky BPE can use any inference method requiring a fixed vocabulary: for example, greedy left-to-right decoding¬†[@wu2016googles] or recently introduced PathPiece¬†[@schmidt2024tokenization].

::: {#tab:inference}
+:-------------:+:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:+:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:+
| $\mathcal{T}$ | ::: {#tab:inference}                                                                                                                                                                                                                                | ::: {#tab:inference}                                                                                                                                                                                                                                |
|               |   -------------------                                                                                                                                                                                                                               |   -----------                                                                                                                                                                                                                                       |
|               |      BPE inference                                                                                                                                                                                                                                  |    Picky BPE                                                                                                                                                                                                                                        |
|               |    with post-removal                                                                                                                                                                                                                                |    inference                                                                                                                                                                                                                                        |
|               |   -------------------                                                                                                                                                                                                                               |   -----------                                                                                                                                                                                                                                       |
|               |                                                                                                                                                                                                                                                     |                                                                                                                                                                                                                                                     |
|               |   : Comparison of compression rates $(\downarrow)$ for the vanilla BPE inference followed by splitting undesired tokens and Picky BPE inference by events order for EN--DE vocabularies of size 32768. The compression rates are shown for English. |   : Comparison of compression rates $(\downarrow)$ for the vanilla BPE inference followed by splitting undesired tokens and Picky BPE inference by events order for EN--DE vocabularies of size 32768. The compression rates are shown for English. |
|               | :::                                                                                                                                                                                                                                                 | :::                                                                                                                                                                                                                                                 |
|               |                                                                                                                                                                                                                                                     |                                                                                                                                                                                                                                                     |
|               |                                                                                                                                                                                                                                                     |                                                                                                                                                                                                                                                     |
+---------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 1.0           | 1.000                                                                                                                                                                                                                                               | 1.000                                                                                                                                                                                                                                               |
+---------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 0.9           | 0.998                                                                                                                                                                                                                                               | **0.997**                                                                                                                                                                                                                                           |
+---------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 0.8           | 0.998                                                                                                                                                                                                                                               | **0.996**                                                                                                                                                                                                                                           |
+---------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 0.7           | 1.000                                                                                                                                                                                                                                               | **0.994**                                                                                                                                                                                                                                           |
+---------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 0.6           | 1.005                                                                                                                                                                                                                                               | **0.992**                                                                                                                                                                                                                                           |
+---------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

: Comparison of compression rates $(\downarrow)$ for the vanilla BPE inference followed by splitting undesired tokens and Picky BPE inference by events order for EN--DE vocabularies of size 32768. The compression rates are shown for English.
:::

# Training details {#app:arch}

Table¬†[7](#tab:hyperparameters){reference-type="ref" reference="tab:hyperparameters"} shows the main model and training hyperparameters we used in every machine translation experiment. We trained every model for 20 epochs, except for a larger vocabulary of 32768 tokens where we trained for 25 epochs, on a single NVIDIA A40 GPU (driver version 555.42.02, CUDA version 12.5).

::: {#tab:hyperparameters}
  Parameter                   Value
  ----------------------- --------------
  Encoder layers                6
  Decoder layers                6
  Embedding dim                512
  Hidden dim                   1024
  Attention heads               4
  Max tokens in a batch        4096
  Optimizer                    Adam
  Weight decay                 1e-4
  Learning rate (LR)           5e-4
  LR Scheduler             inverse sqrt
  Warmup steps                 4000
  Precision                   `fp16`

  : `transformer-iwslt` architecture and training details configuration from `fairseq`¬†[@ott-etal-2019-fairseq].
:::

<figure id="fig:oov-appendix" data-latex-placement="ht!">
<figure id="fig:removed-0-8">
<p><img src="figures/removed_0_8.png" style="width:93.0%" alt="image" /> <span id="fig:removed-0-8" data-label="fig:removed-0-8"></span></p>
</figure>
<figure id="fig:added-0-8">
<p><img src="figures/added_0_8.png" style="width:93.0%" alt="image" /> <span id="fig:added-0-8" data-label="fig:added-0-8"></span></p>
</figure>
<figure id="fig:removed-0-7">
<p><img src="figures/removed_0_7.png" style="width:93.0%" alt="image" /> <span id="fig:removed-0-7" data-label="fig:removed-0-7"></span></p>
</figure>
<figure id="fig:added-0-7">
<p><img src="figures/added_0_7.png" style="width:93.0%" alt="image" /> <span id="fig:added-0-7" data-label="fig:added-0-7"></span></p>
</figure>
<figure id="fig:removed-0-6">
<p><img src="figures/removed_0_6.png" style="width:93.0%" alt="image" /> <span id="fig:removed-0-6" data-label="fig:removed-0-6"></span></p>
</figure>
<figure id="fig:added-0-6">
<p><img src="figures/added_0_6.png" style="width:93.0%" alt="image" /> <span id="fig:added-0-6" data-label="fig:added-0-6"></span></p>
</figure>
<figcaption>Input embedding vectors for Picky BPE tokens with <strong>(a, c, e)</strong> <span class="math inline">ùíØ‚ÄÑ=‚ÄÑ1.0</span>, <strong>(b)</strong> <span class="math inline">ùíØ‚ÄÑ=‚ÄÑ0.8</span>, <strong>(d)</strong> <span class="math inline">ùíØ‚ÄÑ=‚ÄÑ0.7</span>, and <strong>(f)</strong> <span class="math inline">ùíØ‚ÄÑ=‚ÄÑ0.6</span> for English vocabularies of size 16384 in EN‚ÄìDE experiments with separate vocabularies. For each token we compute its probability in the training corpus (y-axis), and the L2 norm of its embedding vector in the trained model (x-axis).</figcaption>
</figure>

# Under-trained tokens inspection {#app:under-trained}

Figure¬†[13](#fig:oov-appendix){reference-type="ref" reference="fig:oov-appendix"} shows examples of token embedding norm distributions for thresholds 0.6, 0.7, and 0.8. As we lower the threshold, the proportion of unique tokens gets larger. However, there is no change in their nature: we remove mostly infrequent tokens and add more frequent tokens with higher norms that are close to the overall distribution.

# Number of Added/Removed Tokens {#app:removed}

Tables [8](#tab:num_rem_deen){reference-type="ref" reference="tab:num_rem_deen"}, [9](#tab:num_rem_deet){reference-type="ref" reference="tab:num_rem_deet"}, and [10](#tab:num_rem_uket){reference-type="ref" reference="tab:num_rem_uket"}, report the number of added/removed tokens for each tokenizer. This is equivalent to the size of $V_i$, discussed in ¬ß[3.1](#sec:algorithm_description){reference-type="ref" reference="sec:algorithm_description"}.

::: {#tab:num_rem_deen}
  ------------ ----- -----------------
   Vocabulary         Added / Removed
      Size                Tokens
                0.9         160
                0.8         358
                0.7         588
      8192      0.6         805
                0.9         342
                0.8         707
                0.7        1092
     16384      0.6        1468
                0.9         677
                0.8        1280
                0.7        1970
     32768      0.6        2563
                0.9        1149
                0.8        2165
                0.7        3312
     65536      0.6        4431
  ------------ ----- -----------------

  : Numbers of added (removed) tokens at different thresholds for the EN--DE tokenizers used for the translation experiments.
:::

::: {#tab:num_rem_deet}
  ------------ ----- -----------------
   Vocabulary         Added / Removed
      Size                Tokens
                0.9         133
                0.8         313
                0.7         506
      8192      0.6         718
  ------------ ----- -----------------

  : Numbers of added (removed) tokens at different thresholds for the DE--ET tokenizers used for the translation experiments.
:::

::: {#tab:num_rem_uket}
  ------------ ----- -----------------
   Vocabulary         Added / Removed
      Size                Tokens
                0.9         107
                0.8         255
                0.7         446
      8192      0.6         605
  ------------ ----- -----------------

  : Numbers of added (removed) tokens at different thresholds for the UK--ET tokenizers used for the translation experiments.
:::

# Compression {#app:compression}

In Tables¬†[\[tab:compression-ende\]](#tab:compression-ende){reference-type="ref" reference="tab:compression-ende"}, [\[tab:compression-deet\]](#tab:compression-deet){reference-type="ref" reference="tab:compression-deet"}, and [\[tab:compression-uket\]](#tab:compression-uket){reference-type="ref" reference="tab:compression-uket"}, we show compression metrics for Picky BPE tokenizers relative to the vanilla BPE. We notice that compression is most pronounced in smaller vocabularies, as for the sizes of the datasets that we used larger vocabularies have large redundancy and a larger partition of tokens is allowed to be unused.

::: tabularx
0.9cccc & &\
&& English & German\
& 1.0 & 1.000 & 1.000\
& 0.9 & 0.997 & 0.996\
& 0.8 & 0.995 & 0.993\
& 0.7 & 0.994 & 0.991\
& 0.6 & 0.992 & 0.989\
& 1.0 & 1.000 & 1.000\
& 0.9 & 0.996 & 0.998\
& 0.8 & 0.994 & 0.996\
& 0.7 & 0.993 & 0.995\
& 0.6 & 0.991 & 0.993\
& 1.0 & 1.000 & 1.000\
& 0.9 & 0.997 & 0.998\
& 0.8 & 0.996 & 0.998\
& 0.7 & 0.994 & 0.997\
& 0.6 & 0.992 & 0.996\
& 1.0 & 1.000 & 1.000\
& 0.9 & 0.998 & 0.998\
& 0.8 & 0.997 & 0.998\
& 0.7 & 0.997 & 0.998\
& 0.6 & 0.996 & 0.997\
:::

::: tabularx
0.9cccc & &\
&& German & Estonian\
& 1.0 & 1.000 & 1.000\
& 0.9 & 0.998 & 0.998\
& 0.8 & 0.994 & 0.996\
& 0.7 & 0.991 & 0.993\
& 0.6 & 0.989 & 0.991\
:::

::: tabularx
0.9cccc & &\
&& Ukrainian & Estonian\
& 1.0 & 1.000 & 1.000\
& 0.9 & 0.998 & 0.998\
& 0.8 & 0.996 & 0.996\
& 0.7 & 0.993 & 0.994\
& 0.6 & 0.992 & 0.993\
:::

# Word-Initial Tokens {#app:word-initial}

In Tables¬†[\[tab:word-initial-ende\]](#tab:word-initial-ende){reference-type="ref" reference="tab:word-initial-ende"}, [\[tab:word-initial-deet\]](#tab:word-initial-deet){reference-type="ref" reference="tab:word-initial-deet"}, and [\[tab:word-initial-uket\]](#tab:word-initial-uket){reference-type="ref" reference="tab:word-initial-uket"}, we show the proportions of added and removed word-initial tokens for different vocabulary sizes and language pairs. In Tables¬†[11](#tab:word-initial-ende2){reference-type="ref" reference="tab:word-initial-ende2"}, [12](#tab:word-initial-deet2){reference-type="ref" reference="tab:word-initial-deet2"}, and [13](#tab:word-initial-uket2){reference-type="ref" reference="tab:word-initial-uket2"}, we show overall proportions of word-initial tokens.

::: tabularx
0.95cccc & &\
&& Dropped & Added\
& 0.9 & 43.8 & 65.5\
& 0.8 & 41.1 & 67.5\
& 0.7 & 42.0 & 66.9\
& 0.6 & 42.1 & 64.2\
& 0.9 & 43.9 & 69.6\
& 0.8 & 43.7 & 67.1\
& 0.7 & 45.3 & 68.1\
& 0.6 & 45.3 & 65.8\
& 0.9 & 46.7 & 73.3\
& 0.8 & 44.8 & 68.3\
& 0.7 & 47.5 & 68.5\
& 0.6 & 48.7 & 67.9\
& 0.9 & 50.6 & 74.6\
& 0.8 & 49.2 & 71.0\
& 0.7 & 51.5 & 69.9\
& 0.6 & 52.0 & 69.0\
:::

::: tabularx
0.95cccc & &\
&& Dropped & Added\
& 0.9 & 33.1 & 60.9\
& 0.8 & 32.3 & 63.3\
& 0.7 & 37.0 & 60.3\
& 0.6 & 40.4 & 58.4\
:::

::: tabularx
0.95cccc & &\
&& Dropped & Added\
& 0.9 & 31.8 & 73.6\
& 0.8 & 33.3 & 66.3\
& 0.7 & 37.4 & 61.8\
& 0.6 & 39.0 & 61.3\
:::

::: {#tab:word-initial-ende2}
  ------------ ----- ----------------
   Vocabulary            % Word-
      Size            Initial Tokens
                1.0        61.5
                0.9        61.9
                0.8        62.7
                0.7        63.3
      8192      0.6        63.6
                1.0        68.0
                0.9        68.6
                0.8        69.2
                0.7        69.7
     16384      0.6        70.0
                1.0        72.2
                0.9        72.8
                0.8        73.2
                0.7        73.6
     32768      0.6        73.9
                1.0        75.2
                0.9        75.7
                0.8        76.1
                0.7        76.3
     65536      0.6        76.6
  ------------ ----- ----------------

  : Overall proportion of word-initial tokens at different thresholds for the EN--DE tokenizers used for the translation experiments.
:::

::: {#tab:word-initial-deet2}
  ------------ ----- ----------------
   Vocabulary            % Word-
      Size            Initial Tokens
                1.0        58.1
                0.9        58.6
                0.8        59.4
                0.7        59.8
      8192      0.6        60.0
  ------------ ----- ----------------

  : Proportion of word-initial tokens at different thresholds for the DE--ET tokenizers used for the translation experiments.
:::

::: {#tab:word-initial-uket2}
  ------------ ----- ----------------
   Vocabulary            % Word-
      Size            Initial Tokens
                1.0        59.8
                0.9        60.4
                0.8        60.9
                0.7        61.1
      8192      0.6        61.5
  ------------ ----- ----------------

  : Proportion of word-initial tokens at different thresholds for the UK--ET tokenizers used for the translation experiments.
:::

# Token Length {#app:token_len}

In Tables¬†[14](#tab:token_len){reference-type="ref" reference="tab:token_len"}, [15](#tab:token_len_deet){reference-type="ref" reference="tab:token_len_deet"}, and [16](#tab:token_len_uket){reference-type="ref" reference="tab:token_len_uket"}, we show mean token lengths over different vocabulary sizes that we used in the translation experiments.

::: {#tab:token_len}
  ------------ ----- -----------------------------
   Vocabulary                 Mean Token
      Size            Length (Chars) ($\uparrow$)
                1.0              5.38
                0.9              5.40
                0.8              5.44
                0.7              5.47
      8192      0.6              5.50
                1.0              6.19
                0.9              6.21
                0.8              6.24
                0.7              6.26
     16384      0.6              6.28
                1.0              6.85
                0.9              6.88
                0.8              6.91
                0.7              6.94
     32768      0.6              6.95
                1.0              7.44
                0.9              7.46
                0.8              7.49
                0.7              7.51
     65536      0.6              7.53
  ------------ ----- -----------------------------

  : Mean token length at different thresholds for the EN--DE tokenizers used for the translation experiments.
:::

::: {#tab:token_len_deet}
  ------------ ----- -----------------------------
   Vocabulary                 Mean Token
      Size            Length (Chars) ($\uparrow$)
                1.0              5.35
                0.9              5.38
                0.8              5.40
                0.7              5.41
      8192      0.6              5.42
  ------------ ----- -----------------------------

  : Mean token length at different thresholds for the DE--ET tokenizers used for the translation experiments.
:::

::: {#tab:token_len_uket}
  ------------ ----- -----------------------------
   Vocabulary                 Mean Token
      Size            Length (Chars) ($\uparrow$)
                1.0              4.84
                0.9              4.85
                0.8              4.86
                0.7              4.88
      8192      0.6              4.90
  ------------ ----- -----------------------------

  : Mean token length at different thresholds for the UK--ET tokenizers used for the translation experiments.
:::

[^1]: \*equal contribution

[^2]: <https://github.com/pchizhov/picky_bpe>

[^3]: We evaluate 1000 bootstrap resamples and use t-Test with confidence level 0.95.
