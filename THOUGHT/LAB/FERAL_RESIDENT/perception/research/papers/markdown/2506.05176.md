maketitle thanks aketitle

::: center
  -- ---------------------------------------------
     <https://huggingface.co/Qwen>
     <https://modelscope.cn/organization/qwen>
     <https://github.com/QwenLM/Qwen3-Embedding>
  -- ---------------------------------------------
:::

[^1]

# Introduction {#sec:intro}

Text embedding and reranking are fundamental components in numerous natural language processing and information retrieval applications, including web search, question answering, recommendation systems, and beyond [@karpukhin2020dense; @huang2020embedding; @zhao2023embedding; @zhao2024dense]. High-quality embeddings enable models to capture semantic relationships between texts, while effective reranking mechanisms ensure that the most relevant results are prioritized. Recently, emerging application paradigms such as Retrieval-Augmented Generation (RAG) and agent systems, driven by the advancement of large language models (e.g., Qwen3 [@yang2025qwen3], GPT-4o [@hurst2024gpt]), have introduced new requirements and challenges for text embedding and reranking, both in terms of model training paradigms and application scenarios. Despite significant advancements, training embedding and reranking models that perform well in scalability, contextual understanding, and alignment with specific downstream tasks remains challenging.

The emergence of large language models (LLMs) has significantly advanced the development of text embedding and reranking models. Prior to the introduction of LLMs, the predominant approach involved using encoder-only pretrained language models like BERT as the foundational model for training [@reimers-gurevych-2019-sentence]. The richer world knowledge, text understanding, and reasoning abilities inherent in LLMs have led to further enhancements in models trained on these architectures. Additionally, there has been considerable research facilitating the integration of LLMs into processes such as training data synthesis and quality data filtering [@wang-etal-2024-improving-text; @lee2024nv; @lee2025gemini]. The fundamental characteristics of LLMs have also inspired the introduction of new training paradigms. For instance, during the embedding model training process, incorporating differentiated tasks across aspects such as instruction type, domain, and language has yielded improved performance in downstream tasks [@su2023one]. Similarly, for reranking model training, advancements have been realized through both zero-shot methods based on user prompts and approaches combining supervised fine-tuning [@ma2023zero; @pradeep2023rankvicuna; @zhang2024two; @zhuang2024setwise].

In this work, we introduce the Qwen3 Embedding series models, which are constructed on top of the Qwen3 foundation models. The Qwen3 foundation has simultaneously released base and instruct model versions, and we exploit the robust multilingual text understanding and generation capabilities of these models to fully realize their potential in training embedding and reranking models. To train the embedding models, we implement a multi-stage training pipeline that involves large-scale unsupervised pre-training followed by supervised fine tuning on high-quality datasets. We also employ model merging with various model checkpoints to enhance robustness and generalization. The Qwen3 instruct model allows for efficient synthesis of a vast, high-quality, multilingual, and multi-task text relevance dataset. This synthetic data is utilized in the initial unsupervised training stage, while a subset of high-quality, small-scale data is selected for the second stage of supervised training. For the reranking models, we adopt a two-stage training scheme in a similar manner, consisting of high-quality supervised fine tuning and a model merging stage. Based on different sizes of the Qwen3 backbone models (including 0.6B, 4B, and 8B), we ultimately trained three text embedding models and three text reranking models. To facilitate their application in downstream tasks, the Qwen3 Embedding series supports several practical features, such as flexible dimension representation for embedding models and customizable instructions for both embedding and reranking models.

We evaluate the Qwen3 Embedding series across a comprehensive set of benchmarks spanning multiple tasks and domains. Experimental results demonstrate that our embedding and reranking models achieve state-of-the-art performance, performing competitively against leading proprietary models in several retrieval tasks. For example, the flagship model Qwen3-8B-Embedding attains a score of 70.58 on the MTEB Multilingual benchmark [@enevoldsen2025mmteb] and 80.68 on the MTEB Code benchmark [@enevoldsen2025mmteb], surpassing the previous state-of-the-art proprietary embedding model, Gemini-Embedding [@lee2025gemini]. Moreover, our reranking model delivers competitive results across a range of retrieval tasks. The Qwen3-Reranker-0.6B model exceeds previously top-performing models in numerous retrieval tasks, while the larger Qwen3-Reranker-8B model demonstrates even superior performance, improving ranking results by 3.0 points over the 0.6B model across multiple tasks. Furthermore, we include a constructive ablation study to elucidate the key factors contributing to the superior performance of the Qwen3 Embedding series, providing insights into its effectiveness.

In the following sections, we describe the design of the model architecture, detail the training procedures, present the experimental results for both the embedding and reranking models of the Qwen3 Embedding Series, and conclude this technical report by summarizing the key findings and outlining potential directions for future research.

![Model architecture of Qwen3-Embedding (left) and Qwen3-Reranker (right).](figures/q3e-model-arc.png){#fig:enter-label width="90%"}

# Model Architecture {#sec:model_arc}

The core idea behind embedding and reranking models is to evaluate relevance in a task-aware manner. Given a query $q$ and a document $d$, embedding and reranking models assess their relevance based on a similarity criterion defined by instruction $I$. To enable the models for task-aware relevance estimation, training data is often organized as $\{I_i, q_i, d_i^+, d_{i,1}^-, \cdots, d_{i, n}^-\}$, where $d_i^+$ represents a positive (relevant) document for query $q_i$, and $d_{i,j}^-$ are negative (irrelevant) documents. Training the model on diverse text pairs broadens its applicability to a range of downstream tasks, including retrieval, semantic textual similarity, classification, and clustering.

#### Architecture

The Qwen3 embedding and reranking models are built on the dense version of Qwen3 foundation models and are available in three sizes: 0.6B, 4B, and 8B parameters. We initialize these models using the Qwen3 foundation models to leverage their capabilities in text modeling and instruction following. The model layers, hidden size, and context length for each model configuration are detailed in Table [\[tab:table-model-arc\]](#tab:table-model-arc){reference-type="ref" reference="tab:table-model-arc"}.

#### Embedding Models

For text embeddings, we utilize LLMs with causal attention, appending an `[EOS]` token at the end of the input sequence. The final embedding is derived from the hidden state of the last layer corresponding to this `[EOS]` token.

To ensure embeddings follow instructions during downstream tasks, we concatenate the instruction and the query into a single input context, while leaving the document unchanged before processing with LLMs. The input format for queries is as follows:

::: promptblock
    {Instruction} {Query}<|endoftext|>
:::

#### Reranking Models

To more accurately evaluate text similarity, we employ LLMs for point-wise reranking within a single context. Similar to the embedding model, to enable instruction-following capability, we include the instruction in the input context. We use the LLM chat template and frame the similarity assessment task as a binary classification problem. The input to LLMs adheres to the template shown below:

::: promptblock
    <|im_start|>system
    Judge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be "yes" or "no".<|im_end|>
    <|im_start|>user
    <Instruct>: {Instruction}
    <Query>: {Query}
    <Document>: {Document}<|im_end|>
    <|im_start|>assistant
    <think>\n\n</think>\n\n
:::

To calculate the relevance score based on the given input, we assess the likelihood of the next token being \"yes\" or \"no.\" This is expressed mathematically as follows: $$\begin{equation*}
    \textrm{score}(q,d) = \frac{e^{P(\text{yes}|I,q,d)}}{e^{P(\text{yes}|I,q,d)}+e^{P(\text{no}|I,q,d)}}
\end{equation*}$$

![Training pipeline of Qwen3 Embedding and Reranking models.](figures/q3e-train-pipeline.png){#fig:pipeline width="\\linewidth"}

# Models Training

In this section, we describe the multi-stage training pipeline adopted and present the key elements of this training recipe, including training objective, training data synthesis, and filtering of high-quality training data.

## Training Objective

Before introducing our training pipeline, we first outline the optimized loss functions used for the embedding and reranking models during the training process. For the embedding model, we utilize an improved contrastive loss based on the InfoNCE framework [@oord2018representation]. Given a batch of $N$ training instances, the loss is defined as: $$\begin{equation}
\label{loss:embedding}
    L_\textrm{embedding} = - \frac{1}{N} \sum_i^N \log\frac{e^{(s(q_i, d_i^+)/\tau)}}{Z_i},
\end{equation}$$ where $s(\cdot, \cdot)$ is a similarity function (we use cosine similarity), $\tau$ is a temperature parameter, and $Z_i$ is the normalization factor that aggregates the similarity scores of the positive pair against various negative pairs: $$\begin{equation*}
    Z_i = e^{(s(q_i, d_i^+) / \tau)}  + \sum_k^K m_{ik}e^{(s(q_i, d_{i,k}^-)/\tau)} + \sum_{j\neq i} m_{ij}e^{(s(q_i, q_j) / \tau)} + \sum_{j\neq i} m_{ij}e^{(s(d_i^+, d_j) / \tau)} + \sum_{j\neq i} m_{ij}e^{(s(q_i, d_j) / \tau)}
\end{equation*}$$ where these terms represent similarities with: (1) the positive document $d_i^+$, (2) $K$ hard negatives $d_{i,k}^-$, (3) other in-batch queries $q_j$, (4) other in-batch documents $d_j$ compared against the positive document $d_i^+$. (5) other in-batch documents $d_j$ compared against the query $q_i$. The mask factor $m_{ij}$ is designed to mitigate the impact of false negatives and is defined as: $$\begin{equation*}
 m_{ij} = 
\begin{cases} 
0 & \text{if } s_{ij} > s(q_i, d_i^+) + 0.1 \text{ or } d_j == d_i^+, \\
1 & \text{otherwise},
\end{cases}
\end{equation*}$$ among which $s_{ij}$ is the corresponding score of $q_i, d_j$ or $q_i, q_j$.

For the reranking model, we optimize the Supervised Fine-Tuning (SFT) loss defined as: $$\begin{equation}
\label{loss:reranking}
        L_\textrm{reranking} = -\log p(l|\mathcal{P}(q,d)),
\end{equation}$$ where $p(\cdot|*)$ denotes the probability assigned by LLM. The label $l$ is "yes" for positive documents and "no" for negatives. This loss function encourages the model to assign higher probabilities to correct labels, thereby improving the ranking performance.

## Multi-stage Training

The multi-stage training approach is a common practice for training text embedding models [@li2023generaltextembeddingsmultistage; @wang2024textembeddingsweaklysupervisedcontrastive; @chen-etal-2024-m3]. This strategy typically begins with initial training on large-scale, semi-supervised data that includes noise, followed by fine-tuning using smaller, high-quality supervised datasets. This two-step process enhances the performance and generalization capabilities of embedding models. Large-scale weakly supervised training data contribute significantly to the model's generalization, while fine-tuning with high-quality data in subsequent stages further improves model performance. Both stages of training for embedding models utilize the optimization objective defined in Equation [\[loss:embedding\]](#loss:embedding){reference-type="ref" reference="loss:embedding"}, whereas the reranking model training employs the loss function defined in Equation [\[loss:reranking\]](#loss:reranking){reference-type="ref" reference="loss:reranking"} as the optimization target.

Building upon the existing multi-stage training framework, the Qwen3 Embedding series introduces the following key innovations:

- Large-Scale Synthetic Data-Driven Weak Supervision Training: Unlike previous works (e.g., GTE, E5, BGE models), where weakly supervised training data are primarily collected from open-source communities such as Q$\&$A forums or academic papers, we propose leveraging the text understanding and generation capabilities of foundation models to synthesize pair data directly. This approach allows for arbitrary definition of various dimensions of the desired pair data, such as task, language, length, and difficulty within the synthesis prompts. Compared to data collection from open-domain sources, foundation model-driven data synthesis offers greater controllability, enabling precise management of the quality and diversity of the generated data, particularly in low-resource scenarios and languages.

- High-Quality Synthetic Data Utilization in Supervised Fine Tuning: Due to the exceptional performance of the Qwen3 Foundation model, the synthesized data is of notably high quality. Therefore, in the second stage of supervised training, selective incorporation of this high-quality synthetic data further enhances the overall model performance and generalization capabilities.

- Model Merging: Inspired by previous work [@li2024improving], after completing the supervised fine-tuning, we applied a model merging technique based on spherical linear interpolation (slerp). This technique involves merging multiple model checkpoints saved during the fine-tuning process. This step aims to boost the model's robustness and generalization performance across various data distributions.

It is important to note that the reranking model's training process does not include a first-stage weakly supervised training phase.

## Synthetic Dataset

To create a robust synthetic dataset for training models on various similarity tasks, we generate diverse text pairs spanning categories such as retrieval, bitext mining, classification, and semantic textual similarity (STS). The quality of these synthetic data pairs is ensured by utilizing the Qwen3-32B model as the foundational model for data synthesis. We have designed a diverse prompting strategy to improve the variety and authenticity of the generated data. For instance, in the text retrieval task, we synthesize data using the multilingual pre-training corpus from Qwen3. During the data synthesis process, specific roles are assigned to each document to simulate potential users querying that document. This injection of user perspectives enhances the diversity and realism of the synthetic queries. Specifically, we utilize a retrieval model to identify the top five role candidates for each document from a role library and present these documents along with their role candidates to the prompt. This guides the model in outputting the most suitable role configuration for query generation. Moreover, the prompt incorporates various dimensions such as query type (e.g., keyword, factual, summary, judgment), query length, difficulty, and language. This multidimensional approach ensures the quality and diversity of the synthetic data.

Finally, we create a total of approximately 150 million pairs of multi-task weak supervision training data. Our experiments reveal that the embedding model trained with these synthetic data performs exceptionally well in downstream evaluations, particularly surpassing many previously supervised models in the MTEB Multilingual benchmarks. This motivates us to filter the synthetic data to identify high-quality pairs for inclusion in a second stage of supervised training. We employ a simple cosine similarity calculation to select data pairs, retaining those with a cosine similarity greater than 0.7 from randomly sampled data. Ultimately, approximately 12 million high-quality supervised training data pairs are selected for further training.

# Evaluation

We conduct comprehensive and fair evaluations across multiple benchmarks to assess the capabilities of Qwen3 Embedding models.

## Settings {#sec:evaluation_settings}

For the text embedding models, we utilize the Massive Multilingual Text Embedding Benchmark (MMTEB) [@enevoldsen2025mmteb] for evaluation. MMTEB is a large-scale, community-driven expansion of MTEB [@muennighoff-etal-2023-mteb], covering over 500 quality-controlled evaluation tasks across more than 250 languages. In addition to classic text tasks such as as a variety of retrieval, classification, and semantic textual similarity, MMTEB includes a diverse set of challenging and novel tasks, such as instruction following, long-document retrieval, and code retrieval, representing the largest multilingual collection of evaluation tasks for embedding models to date. Our MMTEB evaluations encompass 216 individual evaluation tasks, consisting of 131 tasks for MTEB (Multilingual) [@enevoldsen2025mmteb], 41 tasks for MTEB (English, v2) [@muennighoff-etal-2023-mteb], 32 tasks for CMTEB [@xiao2024cpack], and 12 code retrieval tasks for MTEB (Code) [@enevoldsen2025mmteb].

Moreover, we select a series of text retrieval tasks to assess the text reranking capabilities of our models. We explore three types of retrieval tasks: (1) Basic Relevance Retrieval, categorized into English, Chinese, and Multilingual, evaluated on MTEB [@muennighoff-etal-2023-mteb], CMTEB [@xiao2024cpack], MMTEB [@enevoldsen2025mmteb], and MLDR [@chen-etal-2024-m3], respectively; (2) Code Retrieval, evaluated on MTEB-Code [@enevoldsen2025mmteb], which comprises only code-related retrieval data.; and (3) Complex Instruction Retrieval, evaluated on FollowIR [@weller2024followir].

#### Compared Methods

We compare our models with the most prominent open-source text embedding models and commercial API services. The open-source models include the GTE [@li2023generaltextembeddingsmultistage; @zhang-etal-2024-mgte], E5 [@wang2024textembeddingsweaklysupervisedcontrastive], and BGE [@xiao2024cpack] series, as well as NV-Embed-v2 [@lee2025nvembed], GritLM-7B [@muennighoff2025generative]. The commercial APIs evaluated are text-embedding-3-large from OpenAI, Gemini-embedding from Google, and Cohere-embed-multilingual-v3.0. For reranking, we compare with the rerankers of jina[^2], mGTE [@zhang-etal-2024-mgte] and BGE-m3 [@chen-etal-2024-m3].

## Main Results

#### Embedding

In Table [\[tab:model_comparison\]](#tab:model_comparison){reference-type="ref" reference="tab:model_comparison"}, we present the evaluation results on MMTEB [@enevoldsen2025mmteb], which comprehensively covers a wide range of embedding tasks across multiple languages. Our Qwen3-Embedding-4B/8B models achieve the best performance, and our smallest model, Qwen3-Embedding-0.6B, only lags behind the best-performing baseline method (Gemini-Embedding), despite having only 0.6B parameters. In Table [\[tab:mteb-ezc\]](#tab:mteb-ezc){reference-type="ref" reference="tab:mteb-ezc"}, we present the evaluation results on MTEB (English, v2) [@muennighoff-etal-2023-mteb], CMTEB [@xiao2024cpack], and MTEB (Code) [@enevoldsen2025mmteb]. The scores reflect similar trends as MMTEB, with our Qwen3-Embedding-4B/8B models consistently outperforming others. Notably, the Qwen3-Embedding-0.6B model ranks just behind the Gemini-Embedding, while being competitive with the gte-Qwen2-7B-instruct.

#### Reranking

In Table [\[tab:main-rerank\]](#tab:main-rerank){reference-type="ref" reference="tab:main-rerank"}, we present the evaluation results on various reranking tasks (§[4.1](#sec:evaluation_settings){reference-type="ref" reference="sec:evaluation_settings"}). We utilize the Qwen3-Embedding-0.6B model to retrieve the top-100 candidates and then apply different reranking models for further refinement. This approach ensures a fair evaluation of the reranking models. Our results indicate that all three Qwen3-Reranker models enhance performance compared to the embedding model and surpass all baseline reranking methods, with Qwen3-Reranker-8B achieving the highest performance across most tasks.

## Analysis

To further analyze and explore the key elements of the Qwen3 Embedding model training framework, we conduct an analysis from the following dimensions:

**Effectiveness of Large-Scale Weakly Supervised Pre-Training** We first analyze the effectiveness of the large-scale weak supervised training stage for the embedding models. As shown in Table [\[tab:mteb-analysis\]](#tab:mteb-analysis){reference-type="ref" reference="tab:mteb-analysis"}, the Qwen3-Embedding-0.6B model trained solely on synthetic data (without subsequent training stages, as indicated in the first row) achieves reasonable and strong performance compared to the final Qwen3-Embedding-0.6B model (as shown in the last row). If we further remove the weak supervised training stage (i.e., without synthetic data training, as seen in the second row), the final performance shows a clear decline. This indicates that the large-scale weak supervised training stage is crucial for achieving superior performance.

**Effectiveness of Model Merging** Next, we compare the performance differences arising from the model merging stage. As shown in Table [\[tab:mteb-analysis\]](#tab:mteb-analysis){reference-type="ref" reference="tab:mteb-analysis"}, the model trained without model merging techniques (the third row, which uses data sampling to balance various tasks) performs considerably worse than the final Qwen3-Embedding-0.6B model (which employs model merging, as shown in the last row). This indicates that the model merging stage is also critical for developing strong models.

# Conclusion

In this technical report, we present the Qwen3-Embedding series, a comprehensive suite of text embedding and reranking models based on the Qwen3 foundation models. These models are designed to excel in a wide range of text embedding and reranking tasks, including multilingual retrieval, code retrieval, and complex instruction following. The Qwen3-Embedding models are built upon a robust multi-stage training pipeline that combines large-scale weakly supervised pre-training on synthetic data with supervised fine-tuning and model merging on high-quality datasets. The Qwen3 LLMs play a crucial role in synthesizing diverse training data across multiple languages and tasks, thereby enhancing the models' capabilities. Our comprehensive evaluations demonstrate that the Qwen3-Embedding models achieve state-of-the-art performance across various benchmarks, including MTEB, CMTEB, MMTEB, and several retrieval benchmarks. We are pleased to open-source the Qwen3-Embedding and Qwen3-Reranker models (0.6B, 4B, and 8B), making them available for the community to use and build upon.

# Appendix

## Synthetic Data {#synthetic_data}

We construct four types of synthetic data---retrieval, bitext mining, semantic textual similarity, and classification to enable the model to adapt to various similarity tasks during pre-training. To ensure both multilingual and cross-lingual diversity, the data is generated using Qwen3 32B. Below is an example of a synthetic retrieval text pair. The retrieval data is synthesized using a document-to-query approach. We collect a multilingual corpus from the pre-training corpus of the Qwen3 base model to serve as the document source. A two-stage generation pipeline is then applied, consisting of: (1) configuration and (2) query generation. In the configuration stage, we use large language models (LLMs) to determine the "Question Type", "Difficulty", and "Character" for the synthetic query. The candidate characters are retrieved from Persona Hub [@ge2024scaling], selecting the top five most relevant to the given document. This step aims to enhance the diversity of the generated queries. The template used is as follows:

::: promptblock
    Given a **Passage** and **Character**, select the appropriate option from three fields: Character, Question_Type, Difficulty, and return the output in JSON format.
    First, select the Character who are likely to be interested in the Passage from the candidates. Then select the Question_Type that the Character might ask about the Passage; Finally, choose the Difficulty of the possible question based on the Passage, the Character, and the Question_Type.
    Character: Given by input **Character**

    Question_Type:
    - keywords: ...
    - acquire_knowledge: ...
    - summary: ...
    - yes_or_no: ...
    - background: ...

    Difficulty:
    - high_school: ...
    - university: ...
    - phd: ...

    Here are some examples
    <Example1> <Example2> <Example3>

    Now, generate the **output** based on the **Passage** and **Character** from user, the **Passage** will be in {language} language and the **Character** will be in English.
    Ensure to generate only the JSON output with content in English.

    **Passage**:
    {passage}
    **Character**:
    {character}
:::

In the query generation stage, we use the configuration selected in the first stage to guide the generation of queries. Additionally, we explicitly specify the desired length and language of the generated query. The template used is as follows:

::: promptblock
    Given a **Character**, **Passage**, and **Requirement**, generate a query from the **Character**'s perspective that satisfies the **Requirement** and can be used to retrieve the **Passage**. Please return the result in JSON format.

    Here is an example:
    <example>

    Now, generate the **output** based on the **Character**, **Passage** and **Requirement** from user, the **Passage** will be in {corpus_language} language, the **Character** and **Requirement** will be in English.
    Ensure to generate only the JSON output, with the key in English and the value in {queries_language} language.

    **Character**
    {character}
    **Passage**
    {passage}
    **Requirment**
    - Type: {type};
    - Difficulty: {difficulty};
    - Length: the length of the generated sentences should be {length} words;
    - Languange: the language in which the results are generated should be {language} language;
:::

::: table*
:::

## Detail Results

::: table*
:::

::: table*
:::

::: table*
:::

[^1]: $^{*}$ Equal contribution

[^2]: <https://hf.co/jinaai/jina-reranker-v2-base-multilingual>
