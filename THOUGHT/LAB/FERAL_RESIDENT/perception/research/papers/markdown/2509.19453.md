# Astronomy and the Platonic Representation Hypothesis

Three historical waves of increasingly automated connectionism have lapped the shores of astronomy. The late 1980s brought with them MLPs tuned for astronomical applications on manually selected inputs [e.g. @ref_adorf1988; @ref_angel1990; @ref_odewahn1992]. With the advent of CNNs, RNNs, and deep learning, these MLP models gave way to raw data ingestion [e.g. @ref_dieleman2015; @ref_charnock2018; @ref_wu2020]. And the third wave of unsupervised and self-supervised learning entirely removed the need for human supervision, with connectionist methods inferring astronomical knowledge directly from the raw data [e.g. @ref_sarmiento2021; @ref_smith2022]. The swell of a fourth wave has now broken upon astronomy's shores---the application of foundation models to astronomical observations, publications, and survey data [@ref_smith2023]. The fourth wave has brought with it diverse approaches in the search for a viable path towards a single, canonical, astro-foundation model. As a rough overview, research groups have explored contrastive methods [e.g. @ref_slijepcevic2024; @ref_parker2024; @ref_mishra2024; @ref_zhao2025], generative architectures [e.g. @ref_leung2023; @ref_koblischke2024; @ref_ore2024], autoregressive modelling [e.g. @ref_smith2024; @ref_pan2024; @ref_euclid2025; @ref_zuo2025; @ref_heneka2025; @ref_moriwaki2025], and approaches that directly finetune large language models on astronomical text [e.g. @ref_nguyen2023; @ref_perkowski2024; @ref_dehaan2024; @ref_zaman2025].

In this paper we explore the hypothesis that the neural architecture and training regime of our eventual canonical astronomical foundation model *does not matter*: that any sufficient neural network will converge to an equivalent embedding space when pre-trained on enough data with enough compute. This conjecture has already gained traction in the deep learning community as the 'Platonic Representation Hypothesis' (PRH), with perhaps the best known example being put forward as a position paper at ICML 2024[^2] [@ref_huh2024].

The PRH as defined by @ref_huh2024 proposes that neural networks trained with different objectives on different data modalities are converging toward a shared statistical model of reality in their representation spaces. The authors draw inspiration from Plato's 'Allegory of the Cave', where the cave-dwellers mistake shadows on a wall for reality itself [@ref_plato-375]. In this analogy, our training data are the shadowy projections of an underlying reality, and our models are learning to recover representations (or 'Forms') of the reality that generates these data. As our models become larger and are trained on more diverse tasks, they converge toward a Platonic ideal representation: a perfect lossless Form of our underlying reality. This convergence is driven by three key mechanisms: *task generality* (models trained on more diverse tasks require representations that capture more information about underlying reality), *model capacity* (larger models are more likely to find optimal representations), and *simplicity bias* (neural networks naturally favour simpler solutions that generalise better).

Under the PRH, we expect progressively larger models to exhibit more similar representations, even if models are trained across different data modalities. We now quantitatively test whether this is true.

# Astronomical Data as Imperfect Phenomena of Forms {#sec_data}

Astronomical observations provide an important testbed for the PRH due to their fundamental nature as different projections of the same underlying cosmic reality. These observations are inherently linked through shared physical processes; a galaxy's morphology (captured in images), chemical composition (revealed through spectroscopy), and integrated properties (measured via photometry) all emerge from the same stellar populations, gas and dust dynamics, and underlying matter distributions. This shared physical origin suggests that foundation models viewing different astronomical modalities should converge toward representations that capture the underlying fundamental physics governing these phenomena. All the pieces are in place to test the PRH in astronomy: the scale and diversity of modern surveys provide the data volume necessary to test convergence across multiple model architectures and training objectives, and recent work has eased the crossmodal[^3] use of such data [@ref_mmu2024].

We therefore test the PRH on a selection of vision and spectra foundation models using datasets compiled by the Multimodal Universe. Below, we briefly describe and motivate our chosen data and model architectures below. Further information about the crossmatched datasets and model specifications can be found in Appendix [6](#sec_furtherinfo){reference-type="ref" reference="sec_furtherinfo"}.

**Data.** We test across four crossmatched astronomical datasets that capture fundamentally different projections of galaxy Forms: Hyper Suprime-Cam [HSC; @ref_miyazaki2018], DESI Legacy Imaging Survey [@ref_dey2019], and James Webb Space Telescope [JWST; @ref_jwst2023] images from public surveys [@ref_valentino2023]; and DESI spectra [@ref_desi2024].[^4] We use ground-based HSC imaging as our reference baseline. We include DESI spectroscopy to enable cross-modal representation alignment testing between images and spectra. The DESI Legacy Survey's inclusion allows us to test representational alignment across different ground-based imaging survey strategies. JWST NIRCam imaging represents the most extreme imaging test: the telescope produces space-based infrared observations that reveal dust emission and dust-obscured stellar populations invisible to our HSC and Legacy optical surveys. We use the Multimodal Universe (MMU) to crossmatch between data modalities [@ref_mmu2024].

We rescale our images by setting the min and max channel-wise pixel values to their respective 5th and 99th percentile values, computed using batches of up to 10000 images per dataset. For HSC and Legacy Survey we take the $z$, $r$, and $g$ bands as our RGB image channels, and for JWST we take the F444W, F277W, and F090W bands, ensuring maximum wavelength coverage while remaining suitable for our foundation models trained on RGB natural images. We perform any further data pre-processing steps described in the original model authors.

#### Model architectures.

We test across six fundamentally different neural architectures and training paradigms: ViT, ConvNeXtv2, DINOv2, IJEPA, AstroPT, and Specformer. ConvNeXTv2 and ViT represent two approaches (convolutional and self-attentional) that are trained under a 'traditional' supervised paradigm, having been pre-trained or fine-tuned under classification objectives [@ref_dosovitskiy2020; @ref_woo2023]. DINOv2 employs self-supervised learning through knowledge distillation [@ref_caron2021; @ref_oquab2023], and IJEPA employs a non-generative self-supervised approach that predicts abstract representations of image regions rather than reconstructing pixel-level details [@ref_lecun2022; @ref_assran2023]. DINO and IJEPA's inclusion allows us to test representation convergence across a range of self-supervised approaches. AstroPTv2 is an autoregressive decoder transformer designed for astronomical applications [@ref_smith2024]. As a model pre-trained exclusively on astronomical observations, AstroPT's inclusion tests whether models pre-trained on specialised datasets converge toward the same representations as models pre-trained on more general data. Our other astronomy-specific model, Specformer, processes one-dimensional astronomical spectra via a transformer, and represents an entirely distinct data modality compared to image-based models [@ref_parker2024]. Specformer's inclusion tests the most extreme case: whether models pre-trained on fundamentally different input types adhere to the PRH.

**Measuring representational alignment.** Following the methodology established in the original PRH work, we measure representational alignment using the mutual $k$-nearest neighbour (MKNN) metric [@ref_chechik2010]. Given two embeddings $(\mathbf{z}_1, \mathbf{z}_2)$ corresponding to the same object as viewed by two different instruments or models, the MKNN score is computed as the cardinality of intersections for each object's $k$-nearest neighbours in the embedding space: $\text{MKNN}(\mathbf{z}_1, \mathbf{z}_2) = k^{-1} | N_k(\mathbf{z}_{1}) \cap N_k(\mathbf{z}_{2})|$ where $N_k$ is the $k$-nearest neighbours operation, and $|\cdot|$ denotes set cardinality.

We test representational alignment via the MKNN score *across* astronomical modes (*crossmodal*), and *within* a mode (*intramodal*). For the intramodal case, we calculate the MKNN on embeddings produced by two different sizes of the same model architecture, given the same modality. For the crossmodal case we take a model of a specified architecture type and compute the embeddings for two crossmatched astronomical modalities for a range of model sizes. If the PRH holds, then the intramodal and crossmodal MKNN scores should consistently increase with increasing model size.

# Convergence Toward Shared Representations

::: SCtable*
  Model Pairs                 JWST    Legacy    HSC
  -------------------------- ------- -------- -------
  AstroPTv2 Small vs Base     49.7%    8.1%    10.3%
  AstroPTv2 Base vs Large     56.2%   10.0%    13.5%
  ConvNeXtv2 Nano vs Tiny     33.3%    4.5%    5.3%
  ConvNeXtv2 Tiny vs Base     29.5%    3.8%    4.4%
  ConvNeXtv2 Base vs Large    35.8%    6.3%    7.8%
  DINOv2 Small vs Base        32.8%    4.2%    4.6%
  DINOv2 Base vs Large        32.1%    5.6%    5.7%
  DINOv2 Large vs Giant       40.2%   10.2%    10.9%
  ViT Base vs Large           28.7%    3.1%    4.3%
  ViT Large vs Huge           32.6%    4.4%    5.0%
:::

**Results.** We show results from the *intramodal* trials in Tab. [\[tab_intramodal\]](#tab_intramodal){reference-type="ref" reference="tab_intramodal"} and the results from the *crossmodal* alignment trials in Fig. [1](#fig_crossmodal){reference-type="ref" reference="fig_crossmodal"}. Both sets of results show significant correlations between MKNN score and model size.

**We find statistically significant evidence that larger models, even when trained across different data modalities, converge towards more similar representations.** Table [\[tab_intramodal\]](#tab_intramodal){reference-type="ref" reference="tab_intramodal"} shows that intramodal MKNN scores increase for 14 of the 18 pairwise comparisons. For example, we see an increase for JWST between AstroPTv2 Small vs Base (49.7%) and Base vs Large (56.2%). Under a random binomial test, $p = 1.54 \times 10^{-2}$. Meanwhile, crossmodal MKNN scores increase for 28 out of 33 trials, with a binomial test $p = 3.31 \times 10^{-5}$.

<figure id="fig_crossmodal" data-latex-placement="htbp">
<embed src="figures/mknn_no_sdss.pdf" />
<figcaption> Model size vs crossmodal embedding alignment for our tested models. Each of our modality embeddings are compared to crossmatched embeddings from a paired HSC image dataset. <span class="math inline"><em>π</em></span> denotes a random permutation along the zeroth axis, such that <span class="math inline"><em>π</em></span>(HSC) denotes a comparison to a randomised HSC embedding dataset. A table of the MKNN scores can be found in Tab. <a href="#tab_crossmodal_app" data-reference-type="ref" data-reference="tab_crossmodal_app">[tab_crossmodal_app]</a> of Appendix <a href="#sec_full_results" data-reference-type="ref" data-reference="sec_full_results">4</a>. </figcaption>
</figure>

**Discussion.** Aside from AstroPT and Specformer, our tested models are not significantly pre-trained on astronomical data. That these models identify any correspondence between fundamentally different astronomical observations supports the PRH: suggesting that sufficiently scaled up neural networks learn universal structural patterns transcending their training domains. We can also see that our natural image-trained models achieve embedding alignment that increases with model size with Specformer's DESI spectral embeddings, therefore showing correspondence between fundamentally different modalities and data types they have never encountered. Interestingly, AstroPTv2 (pre-trained on imaging from the DESI Legacy Survey) yields comparable rather than dominant performance, also suggesting convergence toward shared representations rather than domain-specific features.

While our results provide compelling evidence for the PRH, we note several limitations that suggest avenues for future exploration. Primarily, some modality comparisons rely on relatively small datasets (e.g. 1.67k objects in the case of JWST vs HSC), which may not capture the full diversity of astronomical phenomena. Future work will prioritise increasing the size of crossmatched catalogues and adding additional testing modalities to the MMU. Our choice of the MKNN metric provides only one perspective on representation similarity; future work will explore further similarity metrics like centred kernel alignment [@ref_kornblith2019] and mutual information [@ref_li15] measurements. We also plan on comparing more diverse modalities and architectures, as our tested astronomical modes and architectures represent only a small slice of all possible varieties. Notable absences include but are not limited to LLMs, diffusion models, and multimodal architectures on the modelling side, and time series and tabular data, and other wavelength regimes as new astronomical modalities.

**Summary.** We observe general improvement in representational alignment at larger model scales, suggesting that each architecture is converging towards a shared representation. Taken to its conclusion, this convergence implies that future efforts in astronomical foundation modelling should focus less on astronomy-specific architectures and more on scale and data diversity. It also follows that the astronomy community should embrace pre-trained foundation models rather than training from scratch: if all architectures converge toward the same representations, then starting from models pre-trained on natural images or text---with their billions of parameters and massive computational investment already spent---offers both superior performance and dramatic reductions in environmental impact. The broader open source machine learning community has already invested the GPU-centuries needed to learn general-purpose representations, we need now only gently guide these models toward astronomical use-cases.

# Acknowledgements {#acknowledgements .unnumbered}

This work has made use of the University of Hertfordshire's high-performance computing facility (<http://uhhpc.herts.ac.uk/>).

::: refcontext
:::

# Full Crossmodal Results Table {#sec_full_results}

We list the full results across pairs of data modalities for each model variant in Table [\[tab_crossmodal_app\]](#tab_crossmodal_app){reference-type="ref" reference="tab_crossmodal_app"}. Each entry is the MKNN score as a percentage. Our main experimental results are shown in the first three columns (JWST imaging vs HSC imaging, Legacy imaging vs HSC imagina, and DESI spectra vs HSC imaging). The fourth column shows results from the SDSS spectra vs HSC imaging; however, SDSS is out-of-domain for the Specformer embedding model, and we only include these results for transparency (see Appendix [5](#sec_sdss){reference-type="ref" reference="sec_sdss"} for more details).

As a null test, we include a final column that shows MKNN scores for randomly shuffled HSC embeddings, $\pi$(HSC), against unshuffled HSC embeddings. Here we use the same dataset as in the 'DESI vs HSC' experiment (with 18.6k galaxies).

[]{#tab_crossmodal_app label="tab_crossmodal_app"}

# SDSS Spectra and Domain Shift Challenges {#sec_sdss}

In addition to the datasets listed in Section [2](#sec_data){reference-type="ref" reference="sec_data"}, we also initially experimented with SDSS galaxy spectra [@ref_york2000]. However, the Specformer embedding model is trained on DESI spectra [@ref_parker2024], which requires a different wavelength grid (i.e., with different wavelength ranges and with a different spectral resolution). We attempted to circumvent this by interpolating SDSS spectra via the (Astropy-affiliated; @astropy:2013 [@astropy:2018; @astropy:2022]) Specutils package [@ref_nicholas_earl_2025_16615456] to resample SDSS spectra onto DESI's wavelength grid (7781 pixels, 3600--9800 Å), transforming them from their native format (4000 pixels, 3800--9200 Å), albeit with loss of spectral resolution.

However, DESI and SDSS spectra differ in terms of other observing systematics, e.g., observing conditions and sites, integration times and observing depth, detector systematics, and calibration pipelines, as well as galaxy population-level trends owing to survey design and selection effects. All of these subtle biases are implicitly encoded in the Specformer model, leaving the SDSS spectra significantly out of domain. This is true even when we attempt to correct the wavelength-dependent effects.

Thus, we do not expect the SDSS Specformer embeddings to be meaningful. Indeed, the crossmodal MKNN scores for SDSS vs HSC are no better than random. Table [\[tab_crossmodal_app\]](#tab_crossmodal_app){reference-type="ref" reference="tab_crossmodal_app"} shows that 6 of the 11 scores are increasing, with a binomial test $p=0.5$. While the magnitude of the MKNN scores for SDSS vs HSC are comparable to those of DESI vs HSC, this only suggests that spectral-to-imaging alignment typically falls in the MKNN $\sim 0.3 - 0.5\%$ range.

# Further Information On Used Datasets and Models {#sec_furtherinfo}

Here we list the datasets used, as well as the models used for this study. For each dataset and model we provide a link to the publicly available data or weights. Cross-matching between surveys is performed using the Multimodal Universe framework on MMU v1 [@ref_mmu2024] with a 1 arcsecond matching radius.

We release all of our code on Github at [`github.com/UniverseTBD/platonic-universe`](https://github.com/UniverseTBD/platonic-universe).

[^1]: Equal contribution.

[^2]: With---as always---many related rumblings preceding this work [e.g. @ref_lenc2014; @ref_bansal2021; @ref_liu2023llava].

[^3]: We define an astronomical 'mode' or 'modality' as the information captured by a specific instrument. Under this definition, for example, JWST and HSC imaging are separate modes.

[^4]: We also initially tested Sloan Digital Sky Survey spectra [SDSS I & II; @ref_york2000]. However, our embedding model exhibits out-of-domain behaviour for SDSS spectra (even when we reprocess the SDSS spectra to match DESI data). See Appendix [5](#sec_sdss){reference-type="ref" reference="sec_sdss"} for more details.
