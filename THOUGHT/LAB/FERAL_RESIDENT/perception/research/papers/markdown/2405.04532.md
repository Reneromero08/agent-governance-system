# QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving

Yujun Lin\*,1, Haotian Tang\*,1, Shang Yang\*,1, Zhekai Zhang1, Guangxuan Xiao1, Chuang Gan3,4, Song Han1,2
  
MIT1, NVIDIA2, UMass Amherst3, MIT-IBM Watson AI Lab4
  
{yujunlin,kentang,shangy,songhan}@mit.edu
  
<https://hanlab.mit.edu/projects/qserve>

###### Abstract

Quantization can accelerate large language model (LLM) inference. Going beyond INT8 quantization, the research community is actively exploring even lower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization techniques only accelerate low-batch, edge LLM inference, failing to deliver performance gains in large-batch, cloud-based LLM serving. We uncover a critical issue: existing INT4 quantization methods suffer from significant runtime overhead (20-90%) when dequantizing either weights or partial sums on GPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization algorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands for quattuor-octÅ-quattuor, which represents 4-8-4 in Latin. QoQ is implemented by the QServe inference library that achieves measured speedup. The key insight driving QServe is that the efficiency of LLM serving on GPUs is critically influenced by operations on low-throughput CUDA cores. Building upon this insight, in QoQ algorithm, we introduce progressive quantization that can allow low dequantization overhead in W4A8 GEMM. Additionally, we develop SmoothAttention to effectively mitigate the accuracy degradation incurred by 4-bit KV quantization. In the QServe system, we perform compute-aware weight reordering and take advantage of register-level parallelism to reduce dequantization latency. We also make fused attention memory-bound, harnessing the performance gain brought by KV4 quantization.
As a result, QServe improves the maximum achievable serving throughput of Llama-3-8B by 1.2Ã—\times on A100, 1.4Ã—\times on L40S; and Qwen1.5-72B by 2.4Ã—\times on A100, 3.5Ã—\times on L40S, compared to TensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput than TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of LLM serving by 3Ã—\times. Code is released at <https://github.com/mit-han-lab/qserve>.

â€ â€ footnotetext: \*: The first three authors contribute equally to this project and are listed in the alphabetical order. Yujun Lin leads the quantization algorithm, Haotian Tang and Shang Yang lead the GPU kernels and the serving system.

## I Introduction

![Refer to caption](/html/2405.04532/assets/x1.png)

Figure 1: QServe achieves higher throughput when running Llama models on L40S compared with TensorRT-LLM on A100, effectively saves the dollar cost for LLM serving by 3Ã—\times through system-algorithm codesign. See TableÂ [IV](#S6.T4 "TABLE IV â€£ Zero-shot accuracy â€£ VI-B Accuracy Evaluation â€£ VI Evaluation â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving") for absolute throughput numbers and precision choices in TensorRT-LLM.

Large language models (LLMs) have demonstrated remarkable capability across a broad spectrum of tasks, exerting a profound influence on our daily lives.

However, the colossal size of LLMs makes their deployment extremely challenging, necessitating the adoption of quantization techniques for efficient inference. State-of-the-art integer quantization algorithms can be divided into three categories: 8-bit weight and 8-bit activation (W8A8), 4-bit weight and 16-bit activation (W4A16), 4-bit weight 4-bit activation (W4A4) quantization. The former two methods are considered nearly lossless in terms of accuracy. In contrast, W4A4 quantization introduces a notable accuracy degradation, although it is anticipated to offer superior throughput in return by mapping its computations onto high-throughput 4-bit tensor cores. Unfortunately, this anticipated performance boost has not been consistently observed across current GPU platforms. For instance, the state-of-the-art W4A4 serving system, AtomÂ [[44](#bib.bib44)], exhibits 20-25% lower performance than its W4A16 and W8A8 counterpart in TensorRT-LLM when running the Llama-2-7BÂ [[34](#bib.bib34)] model on A100 GPUs. That said, the research community has yet to find a precision combination superior to W4A16 and W8A8 for efficient cloud LLM serving.

In this paper, we reveal a critical observation: current 4-bit integer quantization methods experience significant overhead, ranging from 20% to 90%, during the dequantization of weights or partial sums on current-generation GPUs. For example, W4A16 quantization performs computation on FP16 tensor cores while the weights are in INT4, so weight dequantization is required in the GEMM kernel. On the other hand, for W4A4 quantization, to achieve reasonable accuracy, W4A4 methods must apply per-group quantization to both weights and activation, sharing FP16 scaling factors on a sub-channel basis. For example, the state-of-the-art W4A4 quantization method, QuaRotÂ [[2](#bib.bib2)], reports a significant 0.2 perplexity degradation after switching from per-group quantization to per-channel quantization. This per-group quantization design requires an integer to floating-point dequantization for partial sums (since INT4 tensor cores produce INT32 partial sums), which operates on the slower CUDA cores within the sequential main loop of W4A4 GEMM. On data center GPUs like A100, a CUDA core operation is as expensive as 50 INT4 tensor core operations.

Therefore, reducing overhead on CUDA cores is crucial for achieving optimal throughput in LLM serving. Guided by this principle, we introduce QoQ (Quattuor-OctÅ-Quattuor, or 4-8-4 in Latin) algorithm which quantizes LLMs to W4A8KV4 precision: 4-bit weights, 8-bit activations and 4-bit KV caches. Additionally, we present QServe, which provides efficient system support for W4A8KV4 quantization.

In the QoQ algorithm, we introduce progressive group quantization. This method first quantizes weights to 8 bits using per-channel FP16 scales, then quantizes these 8-bit intermediates to 4 bits. This approach ensures that all GEMMs are performed on INT8 tensor cores. Additionally, we mitigate accuracy loss from KV4 quantization through SmoothAttention, which shifts the challenge of activation quantization from keys to queries, the latter of which are not quantized.

In the QServe system, the protective range in progressive group quantization enables full register-level parallelism during INT4 to INT8 dequantization, using a subtraction after multiplication computation order. Furthermore, we propose compute-aware weight reordering to minimize pointer arithmetic overhead on CUDA cores during W4A8 GEMM operations. Additionally, we delay the turning point of the CUDA core roofline and decrease the computational intensity of KV4 attention at the same time. This ensures that the attention operator remains within the memory-bound region, where low-bit quantization can effectively enhance throughput.

We evaluate seven widely-used LLMs using QServe on A100 and L40S GPUs, and compare their maximum achievable throughput against state-of-the-art systems, including TensorRT-LLM (in FP16, W8A8, and W4A16 configurations), AtomÂ [[44](#bib.bib44)] (in W4A4), and QuaRotÂ [[2](#bib.bib2)] (in W4A4). On A100 GPUs, QServe achieves 1.2-2.4Ã—\times higher throughput over the best-performing configuration of TensorRT-LLM, and 2.5-2.9Ã—\times higher throughput compared to Atom and QuaRot. On L40S GPUs, QServe records an even more significant 1.5-3.5Ã—\times throughput improvement over TensorRT-LLM. Notably, we manage to accommodate the same batch size on the L40S while consistently achieving higher serving throughput than TensorRT-LLM on A100 for six of the eight models tested, thereby significantly reducing LLM serving cost by 3Ã—\times.

## II Background

### II-A Large Language Models

Large Language Models (LLMs) are a family of causal transformer models with multiple identically-structured layers. Each layer combines an attention block, a feed-forward network (FFN) and normalization layers. The input of each layer, ğ±ğ±\mathbf{x}, is an NÃ—Hâ€‹Dğ‘ğ»ğ·N\times HD tensor, where Nğ‘N is the number of input tokens, Hğ»H represents the number of attention heads, and Dğ·D is the hidden dimension for each head. Serving LLMs involves two stages: the prefilling stage, where all prompt tokens are presented simultaneously (N>1ğ‘1N>1 for each request), and the decoding stage, where the model only processes one token at a time for each prompt (N=1ğ‘1N=1 for each request).

In attention blocks, ğ±ğ±\mathbf{x} first undergoes linear projection to obtain ğªâˆˆâ„NÃ—Hâ€‹D,ğ¤,ğ¯âˆˆâ„NÃ—HKâ€‹Vâ€‹Dformulae-sequenceğª

superscriptâ„ğ‘ğ»ğ·ğ¤ğ¯superscriptâ„ğ‘subscriptğ»ğ¾ğ‘‰ğ·\mathbf{q}\in\mathbb{R}^{N\times HD},\mathbf{k},\mathbf{v}\in\mathbb{R}^{N\times H\_{KV}D}, where HKâ€‹Vsubscriptğ»ğ¾ğ‘‰H\_{KV} is the number of key/value heads. We have H=HKâ€‹Vğ»subscriptğ»ğ¾ğ‘‰H=H\_{KV} in the standard multi-head attention (MHA), while recent methodsÂ [[34](#bib.bib34), [17](#bib.bib17), [18](#bib.bib18)] also employ grouped-query attention (GQA)Â [[1](#bib.bib1)] with H=râ€‹HKâ€‹Vâ€‹(râˆˆâ„¤)ğ»ğ‘Ÿsubscriptğ»ğ¾ğ‘‰ğ‘Ÿâ„¤H=rH\_{KV}(r\in\mathbb{Z}). We concatenate ğ¤,ğ¯

ğ¤ğ¯\mathbf{k},\mathbf{v} with pre-computed KV cache features of Sğ‘†S previous tokens to obtain ğŠ,ğ•âˆˆâ„(S+N)Ã—HKâ€‹Vâ€‹D

ğŠğ•
superscriptâ„ğ‘†ğ‘subscriptğ»ğ¾ğ‘‰ğ·\mathbf{K},\mathbf{V}\in\mathbb{R}^{(S+N)\times H\_{KV}D} and compute attention using:

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ¨h=softmaxâ€‹(ğªhâ€‹ğŠhKâ€‹VTD)â€‹ğ•hKâ€‹V,hKâ€‹V=âŒŠhrâŒ‹.formulae-sequencesubscriptğ¨â„softmaxsubscriptğªâ„superscriptsubscriptğŠsubscriptâ„ğ¾ğ‘‰ğ‘‡ğ·subscriptğ•subscriptâ„ğ¾ğ‘‰subscriptâ„ğ¾ğ‘‰â„ğ‘Ÿ\small\mathbf{o}\_{h}=\text{softmax}\left(\frac{\mathbf{q}\_{h}\mathbf{K}\_{h\_{KV}}^{T}}{\sqrt{D}}\right)\mathbf{V}\_{h\_{KV}},\quad h\_{KV}=\left\lfloor\frac{h}{r}\right\rfloor. |  | (1) |

The result ğ¨ğ¨\mathbf{o} is multiplied with an output projection matrix ğ–Oâˆˆâ„Hâ€‹DÃ—Hâ€‹Dsubscriptğ–ğ‘‚superscriptâ„ğ»ğ·ğ»ğ·\mathbf{W}\_{O}\in\mathbb{R}^{HD\times HD}, and the product is added to ğ±ğ±\mathbf{x} as the input of FFN. The FFN is composed of linear projection and activation layers and it does not mix features between tokens.

### II-B Integer Quantization

Integer quantization maps high-precision numbers to discrete levels. The process can be formulated as:

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğğ—=âŒˆğ—s+zâŒ‹,s=ğ—maxâˆ’ğ—minqmaxâˆ’qmin,z=âŒˆqminâˆ’ğ—minsâŒ‹,\small\mathbf{Q}\_{\mathbf{X}}=\left\lceil\frac{\mathbf{X}}{s}+z\right\rfloor,s=\frac{\mathbf{X}\_{\max}-\mathbf{X}\_{\min}}{q\_{\max}-q\_{\min}},z=\left\lceil q\_{\min}-\frac{\mathbf{X}\_{\min}}{s}\right\rfloor, |  | (2) |

where ğ—ğ—\mathbf{X} is the floating point tensor, ğğ—subscriptğğ—\mathbf{Q}\_{\mathbf{X}} is its nğ‘›n-bit quantized counterpart, sğ‘ s is the scaling factor and zğ‘§z is the zero point. Thus, the dequantized tensor can be represented as,

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ—^=Qâ€‹(ğ—)=(ğğ—âˆ’z)â‹…s^ğ—ğ‘„ğ—â‹…subscriptğğ—ğ‘§ğ‘ \small\hat{\mathbf{X}}=Q\left(\mathbf{X}\right)=\left(\mathbf{Q}\_{\mathbf{X}}-z\right)\cdot s |  | (3) |

This is known as asymmetric quantization, where ğ—max=maxâ¡(ğ—),ğ—min=minâ¡(ğ—)formulae-sequencesubscriptğ—ğ—subscriptğ—ğ—\mathbf{X}\_{\max}=\max\left(\mathbf{X}\right),\mathbf{X}\_{\min}=\min\left(\mathbf{X}\right), and qmaxâˆ’qmin=2nâˆ’1subscriptğ‘subscriptğ‘superscript2ğ‘›1q\_{\max}-q\_{\min}=2^{n}-1 for integer quantization. Equation [2](#S2.E2 "In II-B Integer Quantization â€£ II Background â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving") can be further simplied to symmetric quantization, where z=ğŸğ‘§0z=\mathbf{0}, ğ—max=âˆ’ğ—min=maxâ¡|ğ—|subscriptğ—subscriptğ—ğ—\mathbf{X}\_{\max}=-\mathbf{X}\_{\min}=\max\left|\mathbf{X}\right|, and qmaxâˆ’qmin=2nâˆ’2subscriptğ‘subscriptğ‘superscript2ğ‘›2q\_{\max}-q\_{\min}=2^{n}-2 .

In this paper, we denote xğ‘¥x-bit weight, yğ‘¦y-bit activation and zğ‘§z-bit KV cache quantization in LLMs as WxAyKVz, and use the abbreviated notation WxAy if y=z. Apart from bit precision, quantization can also be applied at various granularities. Per-tensor quantization shares sğ‘ s and zğ‘§z across the entire tensor. Per-channel quantization for weights or per-token quantization for activations means that sğ‘ s and zğ‘§z are shared within each row of tensor. Per-group quantization further reduces the degree of parameter sharing by using different sğ‘ s and zğ‘§z for every gğ‘”g columns within each row, where gğ‘”g is the group size.

## III Motivation

Weight and KV cache quantization (e.g. W4, KV4) can reduce the memory footprint in LLM serving. Quantizing both weight and activation (e.g. W8A8) can also improve the peak computation throughput. Choosing the right precision for LLM deployment is a difficult task. Existing solutions can be divided into three categories: W4A16 (per-group), W8A8 (per-channel weight + per-token activation), W4A4 (per-group). We will demonstrate in this section why W4A8KV4 is a superior choice.

### III-A W4A8KV4 Has Superior Roofline Over W8A8, W4A16

![Refer to caption](/html/2405.04532/assets/x2.png)

Figure 2: Left: Both attention and GEMM are crucial for end-to-end LLM latency. Right: Despite 2Ã—\times higher theoretical peak performance, W4A4 systems significantly lag behind TRT-LLM-W8A8 in efficiency.

![Refer to caption](/html/2405.04532/assets/x3.png)

Figure 3: A100 roofline for LLM serving: for GEMM layers, the W4A8 roofline dominates both W4A16 and W8A8 across different batch sizes; for attention layers, 4-bit quantization improves theoretical peak performance.

We begin our exploration through roofline analysis. As in Figure [2](#S3.F2 "Figure 2 â€£ III-A W4A8KV4 Has Superior Roofline Over W8A8, W4A16 â€£ III Motivation â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving")a, when considering real-world conversations with 1024 input tokens and 512 output tokens, attention and GEMM account for most of the runtime when deploying LLMs. Furthermore, the runtime of the decoding stage is approximately 6Ã—\times that of the prefilling stage. Therefore, we focus our analysis on the attention and GEMM within the decoding stage.

For an mÃ—nÃ—kğ‘šğ‘›ğ‘˜m\times n\times k GEMM problem, the computation intensity (defined as MACs/element) is approximately mğ‘šm when n,k

ğ‘›ğ‘˜n,k are much larger than mğ‘šm. This situation applies to LLM decoding stage, since mğ‘šm is number of sequences and n,k

ğ‘›ğ‘˜n,k are channel sizes. According to the A100 roofline111A100 has a peak FP16/INT8/INT4 tensor core performance of 312/624/1248 TOPS and a DRAM bandwidth of 2 TB/s. in Figure [3](#S3.F3 "Figure 3 â€£ III-A W4A8KV4 Has Superior Roofline Over W8A8, W4A16 â€£ III Motivation â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving"), W4A16 has a higher theoretical throughput when m<78ğ‘š78m<78, while W8A8 performs better when m>78ğ‘š78m>78. When the input batch size is small, GEMMs in LLMs are memory bound, and the memory bandwidth is dominated by weight traffic. Therefore, the smaller memory footprint of W4A16 leads to better performance. However, when mğ‘šm is large, the problem is compute bound. Thus, W8A8 has faster speed thanks to the higher throughput from INT8 tensor cores. Intuitively, one can expect W4A8 to combine the best of both worlds across all batch sizes. This is clearly demonstrated in Figure [3](#S3.F3 "Figure 3 â€£ III-A W4A8KV4 Has Superior Roofline Over W8A8, W4A16 â€£ III Motivation â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving"), as long as we can perform all computation on INT8 tensor cores.

Why KV4: attention workloads in LLM decoding can be formulated as a sequence of batched GEMV operations, with a computation intensity of 1 MAC / element regardless of input batch sizes. As in Equation [1](#S2.E1 "In II-A Large Language Models â€£ II Background â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving"), the memory traffic is dominated by KV cache access, since Sâ‰«N=1much-greater-thanğ‘†ğ‘1S\gg N=1 for each sequence. Quantizing the KV cache can be viewed as effectively increasing the memory bandwidth. Therefore, KV4 offers 2Ã—\times peak performance for attention over KV8. This improvement offers decent end-to-end speedup opportunities, since attention accounts for more than 50% of total runtime at batch=64 in Figure [2](#S3.F2 "Figure 2 â€£ III-A W4A8KV4 Has Superior Roofline Over W8A8, W4A16 â€£ III Motivation â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving")a.

### III-B Why Not W4A4KV4: Main Loop Overhead in GEMM

![Refer to caption](/html/2405.04532/assets/x4.png)

Figure 4: Illustration of mÃ—nÃ—kğ‘šğ‘›ğ‘˜m\times n\times k GPU GEMM: m,n

ğ‘šğ‘›m,n are parallel dimensions and the reduction dimension kğ‘˜k has a sequential main loop. In LLM serving, mğ‘šm is small and n,k

ğ‘›ğ‘˜n,k are large. Thus, the main loop is long.

![Refer to caption](/html/2405.04532/assets/x5.png)

Figure 5: Quantized GEMM on GPUs: W8A8 is fast because its main loop only contains tensor core operations and all dequantization operations are present in the epilogue. Atom-W4A4 and TensorRT-LLM-W4A16 suffer from significant partial sum or weight dequantization overhead in the main loop. Thanks to the two-level progressive quantiation algorithm, QServe-W4A8 reduces main loop dequantization overhead by introducing register-level parallelism.

A natural follow-up question would be: â€œWhy do we not choose the even more aggressive W4A4?â€ W4A4 starts to achieve better theoretical GEMM performance when mğ‘šm, the number of input sequences, exceeds 78, as 4-bit tensor cores are twice as performant compared to their 8-bit counterparts. However, apart from the significant accuracy degradation, which will be discussed in Section [VI](#S6 "VI Evaluation â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving"), we demonstrate that such theoretical performance gains cannot be realized on existing GPU architectures (Ampere and Hopper). As in Figure [2](#S3.F2 "Figure 2 â€£ III-A W4A8KV4 Has Superior Roofline Over W8A8, W4A16 â€£ III Motivation â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving")b, existing W4A4 serving systems AtomÂ [[44](#bib.bib44)] and QuaRotÂ [[2](#bib.bib2)] are even significantly slower than the W16A16 solution from TensorRT-LLM.

While this performance gap can be partially explained by the inefficient runtime in these two systems, the inherent difficulty in mapping per-group quantized W4A4 GEMM on GPUs has been overlooked in previous literature. State-of-the-art systems implement tensor core GEMM with an output stationary dataflow shown in Figure [4](#S3.F4 "Figure 4 â€£ III-B Why Not W4A4KV4: Main Loop Overhead in GEMM â€£ III Motivation â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving"). For an mÃ—nÃ—kğ‘šğ‘›ğ‘˜m\times n\times k problem, each thread block computes a tmÃ—tnsubscriptğ‘¡ğ‘šsubscriptğ‘¡ğ‘›t\_{m}\times t\_{n} output tile by iterating sequentially through the reduction dimension kğ‘˜k. This sequential loop is referred to as the main loop. The main loop comprises more than 100 iterations and dominates the runtime of the GEMM kernel. In both FP16 and W8A8 GEMM (Figure [5](#S3.F5 "Figure 5 â€£ III-B Why Not W4A4KV4: Main Loop Overhead in GEMM â€£ III Motivation â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving")a), the main loop is executed entirely on tensor cores. TensorRT-LLM-W4A16 (Figure [5](#S3.F5 "Figure 5 â€£ III-B Why Not W4A4KV4: Main Loop Overhead in GEMM â€£ III Motivation â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving")b) and Atom-W4A4 (Figure [5](#S3.F5 "Figure 5 â€£ III-B Why Not W4A4KV4: Main Loop Overhead in GEMM â€£ III Motivation â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving")c) both require dequantization operations in the main loop, which is running on the CUDA cores. W4A16 requires INT4 to FP16 weight conversion, while Atom-W4A4 requires INT32 to FP32 partial sum conversion and accumulation.

The dequantization process in Atomâ€™s main loop leads to two substantial efficiency bottlenecks. Firstly, on modern data center GPUs like the A100 and H100, the peak performance of FP32 CUDA cores is merely 2% of their INT4 tensor core counterparts. That said, de-quantizing one single partial sum in Atom is equivalent to 50 tensor core MACs. Therefore, the main loop is dominated by slow CUDA core operations rather than fast tensor core operations. Secondly, Atom creates two sets of registers (one for FP32 and one for INT32) to hold partial sums. Larger GEMM problems (e.g., prefilling stage) are typically register-bound on GPUs due to the nature of the output stationary dataflow, which results in high register consumption for storing partial sums. Consuming a large number of registers within each warp limits the number of warps that can be executed simultaneously on the streaming multiprocessor. It is important to note that GPUs rely on low-cost context switching between a large number of in-flight warps to hide latency. Consequently, a smaller number of concurrently executed warps limits the opportunity for latency hiding, further exacerbating the main loop overhead.

We preview our QServeâ€™s W4A8 per-group quantized GEMM kernel design in Figure [5](#S3.F5 "Figure 5 â€£ III-B Why Not W4A4KV4: Main Loop Overhead in GEMM â€£ III Motivation â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving")d. We employ a two-level progressive group quantization approach to ensure that all computations are performed on INT8 tensor cores. We opt for weight dequantization over partial sum dequantization due to its lower register pressure. Furthermore, we apply 4-way register-level parallelism to decode four INT4 weights simultaneously, further reducing the main loop overhead.

## IV QoQ Quantization

To this end, we have discussed why W4A8KV4 is a superior quantization precision choice. Yet, preserving model accuracy with such low-bit quantization remains a significant challenge. To unleash the full potential of W4A8KV4 without compromising the efficacy of large language models, we propose QoQ algorithm featuring progressive group quantization, SmoothAttention, and various general quantization optimizations.

### IV-A Progressive Group Quantization

![Refer to caption](/html/2405.04532/assets/x6.png)

![Refer to caption](/html/2405.04532/assets/x7.png)

Figure 6: Progressive Group Quantization first employs per-channel INT8 quantization with protective range [-119, 119], followed by per-group INT4 quantization, so that the dequantized intermediate values remain within the INT8 range for computation. Bottom: prior methods directly applies per-group INT4 quantization on weights, followed by per-channel INT8 quantization on scale factors. Thus the dequantized intermediate values may exceed the INT8 range, necessitating further dequantization to floating-point values for computation.

To enhance the accuracy of low-bit quantization, group quantization is commonly utilizedÂ [[44](#bib.bib44), [23](#bib.bib23), [12](#bib.bib12)]. However, as outlined in SectionÂ [5](#S3.F5 "Figure 5 â€£ III-B Why Not W4A4KV4: Main Loop Overhead in GEMM â€£ III Motivation â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving"), the dequantization overhead in the system implementation can negate these accuracy improvements. To tackle this issue, we introduce progressive group quantization, as depicted in FigureÂ [6](#S4.F6 "Figure 6 â€£ IV-A Progressive Group Quantization â€£ IV QoQ Quantization â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving").

Given the weight tensor ğ–âˆˆâ„kÃ—nğ–superscriptâ„ğ‘˜ğ‘›\mathbf{W}\in\mathbb{R}^{k\times n}, we first apply per-channel symmetric INT8 quantization:

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ–^=ğğ–s8(0)â‹…ğ¬fp16(0),^ğ–â‹…subscriptsuperscriptsubscriptğğ–0s8subscriptsuperscriptğ¬0fp16\small\hat{\mathbf{W}}={\mathbf{Q}\_{\mathbf{W}}}^{(0)}\_{\mathrm{s8}}\cdot\mathbf{s}^{(0)}\_{\mathrm{fp16}}, |  | (4) |

where ğğ–s8(0)âˆˆâ„•nÃ—ksuperscriptsubscriptsubscriptğğ–s80superscriptâ„•ğ‘›ğ‘˜{\mathbf{Q}\_{\mathbf{W}}}\_{\mathrm{s8}}^{(0)}\in\mathbb{N}^{n\times k} is the intermediate 8-bit quantized weight tensor, and ğ¬fp16(0)âˆˆâ„nÃ—1subscriptsuperscriptğ¬0fp16superscriptâ„ğ‘›1\mathbf{s}^{(0)}\_{\mathrm{fp16}}\in\mathbb{R}^{n\times 1} is the channel-wise quantization scales. We then further employ per-group asymmetric INT4 quantization on the intermediate weight tensor:

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğğ–s8(0)=(ğğ–u4âˆ’ğ³u4)â‹…ğ¬u8(1),superscriptsubscriptsubscriptğğ–s80â‹…subscriptsubscriptğğ–u4subscriptğ³u4subscriptsuperscriptğ¬1u8\small{{\mathbf{Q}}\_{\mathbf{W}}}\_{\mathrm{s8}}^{(0)}=\left({\mathbf{Q}\_{\mathbf{W}}}\_{\mathrm{u4}}-\mathbf{z}\_{\mathrm{u4}}\right)\cdot\mathbf{s}^{(1)}\_{\mathrm{u8}}, |  | (5) |

where ğğ–u4âˆˆâ„•nÃ—ksubscriptsubscriptğğ–u4superscriptâ„•ğ‘›ğ‘˜{\mathbf{Q}\_{\mathbf{W}}}\_{\mathrm{u4}}\in\mathbb{N}^{n\times k} is the unsigned 4-bit quantized weight tensor, ğ³u4âˆˆâ„•nÃ—k/gsubscriptğ³u4superscriptâ„•ğ‘›ğ‘˜ğ‘”\mathbf{z}\_{\mathrm{u4}}\in\mathbb{N}^{n\times k/g} is the unsigned 4-bit group-wise quantization zero points, and ğ¬u8(1)âˆˆâ„•nÃ—k/gsubscriptsuperscriptğ¬1u8superscriptâ„•ğ‘›ğ‘˜ğ‘”\mathbf{s}^{(1)}\_{\mathrm{u8}}\in\mathbb{N}^{n\times k/g} is the unsigned 8-bit group-wise quantization scales.

For W4A8 GEMM computation, the 4-bit quantized weight tensor ğğ–u4subscriptsubscriptğğ–u4{\mathbf{Q}\_{\mathbf{W}}}\_{\mathrm{u4}} will be first dequantized into intermediate 8-bit quantized weight tensor ğğ–s8(0)superscriptsubscriptsubscriptğğ–s80{\mathbf{Q}\_{\mathbf{W}}}\_{\mathrm{s8}}^{(0)} following EquationÂ [5](#S4.E5 "In IV-A Progressive Group Quantization â€£ IV QoQ Quantization â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving"), and then perform INT8 matrix multiplication as if it was W8A8 per-channel quantization.

##### Protective Quantization Range

naÃ¯vely applying EquationÂ [4](#S4.E4 "In IV-A Progressive Group Quantization â€£ IV QoQ Quantization â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving") andÂ [5](#S4.E5 "In IV-A Progressive Group Quantization â€£ IV QoQ Quantization â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving") does not guarantee that the intermediate dequantized weights perfectly lie in the 8-bit integer representation range. For example, after INT8 quantization, a group of 8-bit weights lie in [âˆ’113,120]113120[-113,120]. 4-bit asymmetric quantization will yield a scale factor of âŒˆ(120âˆ’âˆ’113)/(15âˆ’0)âŒ‹=16\lceil(120--113)/(15-0)\rfloor=16 and a zero point of âŒˆ0âˆ’âˆ’113/16âŒ‹=7\lceil 0--113/16\rfloor=7. Thus value 120 is quantized into âŒˆ120/16+7âŒ‹=15\lceil 120/16+7\rfloor=15. It will be dequantized into (15âˆ’7)âˆ—16=12815716128(15-7)\*16=128 which is beyond the max 8-bit integer 127. One straightforward solution is to turn on the saturation option in the arithmetic instructions during dequantization. However, simply applying saturation will severely damage the computation throughput, reducing speed by as much as 67%.

We reconsider the dequantization process. Take EquationÂ [2](#S2.E2 "In II-B Integer Quantization â€£ II Background â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving") into EquationÂ [5](#S4.E5 "In IV-A Progressive Group Quantization â€£ IV QoQ Quantization â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving"), we have,

|  |  |  |
| --- | --- | --- |
|  | q^sâ€‹8=âŒŠqsâ€‹8suâ€‹8âŒ‰â‹…suâ€‹8â‰¤qsâ€‹8+12suâ€‹8.\small\hat{q}\_{s8}=\lfloor\frac{{q}\_{s8}}{{s}\_{u8}}\rceil\cdot{{s}\_{u8}}\leq{q}\_{s8}+\frac{1}{2}{{s}\_{u8}}. |  |

Since suâ€‹8=qsâ€‹8maxâˆ’qsâ€‹8minquâ€‹4maxâˆ’quâ€‹4minâ‰¤127âˆ’(âˆ’128)15âˆ’0=17subscriptğ‘ ğ‘¢8subscriptsubscriptğ‘ğ‘ 8subscriptsubscriptğ‘ğ‘ 8subscriptsubscriptğ‘ğ‘¢4subscriptsubscriptğ‘ğ‘¢412712815017{s}\_{u8}=\frac{{{q}\_{s8}}\_{\max}-{{q}\_{s8}}\_{\min}}{{{q}\_{u4}}\_{\max}-{{q}\_{u4}}\_{\min}}\leq\frac{127-(-128)}{15-0}=17, we have,

|  |  |  |
| --- | --- | --- |
|  | q^sâ€‹8â‰¤127â†’qsâ€‹8â‰¤127âˆ’12â€‹suâ€‹8â†’qsâ€‹8â‰¤119.5subscript^ğ‘ğ‘ 8127â†’subscriptğ‘ğ‘ 812712subscriptğ‘ ğ‘¢8â†’subscriptğ‘ğ‘ 8119.5\small\hat{q}\_{s8}\leq 127\rightarrow{q}\_{s8}\leq 127-\frac{1}{2}{{s}\_{u8}}\rightarrow{q}\_{s8}\leq 119.5 |  |

Therefore, we shrink the INT8 symmetric quantization range from [-127, 127] to a protective range [-119, 119] in order to avoid the dequantization overflow, as shown in the top of FigureÂ [6](#S4.F6 "Figure 6 â€£ IV-A Progressive Group Quantization â€£ IV QoQ Quantization â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving").

##### Compared to previous two-level quantization

progressive group quantization introduces two levels of scales ğ¬fp16(0)subscriptsuperscriptğ¬0fp16\mathbf{s}^{(0)}\_{\mathrm{fp16}} and ğ¬u8(1)subscriptsuperscriptğ¬1u8\mathbf{s}^{(1)}\_{\mathrm{u8}}. Prior studies such as VSQuant and DoubleQuant in QLoRAÂ [[9](#bib.bib9)] also introduce two levels of scales to reduce the memory footprint of group-wise scaling factors. In contrast to our quantization flow, previous approaches directly apply group quantization with the target precision and then perform per-channel quantization on the group-wise floating-point scaling factors, as shown in the bottom of FigureÂ [6](#S4.F6 "Figure 6 â€£ IV-A Progressive Group Quantization â€£ IV QoQ Quantization â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving"):

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ–^=ğğ–s4â‹…ğ¬fp16,ğ¬^fp16=ğ¬u8(1)â‹…ğ¬fp16(0)formulae-sequence^ğ–â‹…subscriptsubscriptğğ–s4subscriptğ¬fp16subscript^ğ¬fp16â‹…subscriptsuperscriptğ¬1u8subscriptsuperscriptğ¬0fp16\small\hat{\mathbf{W}}={\mathbf{Q}\_{\mathbf{W}}}\_{\mathrm{s4}}\cdot\mathbf{s}\_{\mathrm{fp16}},\;\;\;\hat{\mathbf{s}}\_{\mathrm{fp16}}={\mathbf{s}}^{(1)}\_{\mathrm{u8}}\cdot\mathbf{s}^{(0)}\_{\mathrm{fp16}} |  | (6) |

Therefore, using the group-wise scaling factors ğ¬u8(1)subscriptsuperscriptğ¬1u8{\mathbf{s}}^{(1)}\_{\mathrm{u8}} to dequantize ğğ–s4subscriptğsubscriptğ–s4\mathbf{Q}\_{\mathbf{W}\_{\mathrm{s4}}} cannot yield the 8-bit weight tensor. During the computation on GPUs, these approaches usually first dequantize the scales and, subsequently, the weights into floating-point values, which ultimately limits the peak throughput.

DGQÂ [[43](#bib.bib43)] also follows the quantization scheme of VSQuant and DoubleQuant, but enforces restrictions on scaling factors to make sure that all computation can be mapped onto INT8 tensor cores. However, the DGQ serving system separates dequantization kernel with the GEMM kernel. Consequently, the end-to-end latency of W4A8 GEMM in DGQ is even slower than the W8A8 GEMM in cuBLAS, failing to demonstrate the memory bandwidth advantage of 4-bit weight quantization. In contrast, our QoQ introduces a protective range, allowing us to fuse dequantization operations into the W4A8 GEMM kernel with full register-level parallelism, minimizing CUDA core overhead. Thus, our QServeâ€™s W4A8 per-group GEMM achieves 1.5Ã—\times speedup over the W8A8 cuBLAS GEMM.

### IV-B SmoothAttention

![Refer to caption](/html/2405.04532/assets/x8.png)

Figure 7: SmoothAttention effectively smooths the outliers in Keys. Values doesnâ€™t suffer from outliers.

As illustrated in FigureÂ [16](#S6.F16 "Figure 16 â€£ VI-D Analysis and Discussion. â€£ VI Evaluation â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving"), directly reducing the KV cache to 4 bits significantly degrades the LLM accuracy. We visualize the magnitude distributions of the sampled Key and Value cache activations in FigureÂ [7](#S4.F7 "Figure 7 â€£ IV-B SmoothAttention â€£ IV QoQ Quantization â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving"). We observe that: the Value matrices show no significant outlier pattern, whereas Key matrices tend to have fixed outlier channels in each head. These outliers are âˆ¼similar-to\sim10Ã—\times larger than most of activation values. Though they can be easily handled KV8 quantization in prior worksÂ [[38](#bib.bib38)], it places challenging obstacle to KV4 quantization due to less quantization levels.

Inspired by SmoothQuantÂ [[38](#bib.bib38)], we propose SmoothAttention to scale down the outlier channels in Key cache by a per-channel factor Î»ğœ†\mathbf{\lambda}:

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ™=(ğâ€‹ğš²)â‹…(ğŠâ€‹ğš²âˆ’1)T,ğš²=diagâ€‹(Î»)formulae-sequenceğ™â‹…ğğš²superscriptğŠsuperscriptğš²1ğ‘‡ğš²diagğœ†\small\mathbf{Z}=\left(\mathbf{Q}\mathbf{\Lambda}\right)\cdot\left(\mathbf{K}\mathbf{\Lambda}^{-1}\right)^{T},\;\;\;\mathbf{\Lambda}=\mathrm{diag}\left(\mathbf{\lambda}\right) |  | (7) |

SmoothQuant migrates the quantization difficulty from activations to weights, and thus requires a dedicate balance between activation and weight quantization by searching the migration strength. In contrast, since we do not quantize Queries, we only need to concentrate on the Keys and simply choose the SmoothAttention scale factor as,

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î»i=max(|ğŠi|)Î±.\small\mathbf{\lambda}\_{i}=\max\left(|\mathbf{K}\_{i}|\right)^{\alpha}. |  | (8) |

In practice, Î±=0.5ğ›¼0.5\alpha=0.5 is good enough. As shown in FigureÂ [7](#S4.F7 "Figure 7 â€£ IV-B SmoothAttention â€£ IV QoQ Quantization â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving"), after SmoothAttention, the outliers in Key cache have been greatly smoothed.

In order to eliminate the extra kernel call overhead for SmoothAttention scaling, fusing the scale into preceding linear layerâ€™s weights is preferred. However, modern LLMs employ the rotary positional embedding (RoPE) to both Keys and Queries, which needs extra handling. In practice, rotary positional embedding pairs channel iğ‘–i with channel i+D2ğ‘–ğ·2i+\frac{D}{2} within each head. Consequently, to make SmoothAttention scaling commutative in terms of RoPE, we add a hard constraint that Î»i=Î»i+D2subscriptğœ†ğ‘–subscriptğœ†ğ‘–ğ·2\lambda\_{i}=\lambda\_{i+\frac{D}{2}}, and accordingly,

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î»i=Î»i+D2=max(max(|ğŠi|),max(|ğŠi+D2|))Î±\small\mathbf{\lambda}\_{i}=\lambda\_{i+\frac{D}{2}}=\max\left(\max\left(|\mathbf{K}\_{i}|\right),\max\left(|\mathbf{K}\_{i+\frac{D}{2}}|\right)\right)^{\alpha} |  | (9) |

Afterwards, we can easily fuse the SmoothAttention scale ğš²ğš²\mathbf{\Lambda} into previous layersâ€™ weights following ğ–Q=ğš²â€‹ğ–Qsubscriptğ–ğ‘„ğš²subscriptğ–ğ‘„\mathbf{W}\_{Q}=\mathbf{\Lambda}\mathbf{W}\_{Q} and ğ–K=ğš²âˆ’1â€‹ğ–Ksubscriptğ–ğ¾superscriptğš²1subscriptğ–ğ¾\mathbf{W}\_{K}=\mathbf{\Lambda}^{-1}\mathbf{W}\_{K}.

### IV-C General LLM Quantization Optimizations

One of the key challenges of low-bit LLM quantization is the activation outliers for every linear layers. We apply different optimizations for different types of linear layers as discussed below.

![Refer to caption](/html/2405.04532/assets/x9.png)

Figure 8: Rotate the block input activations to suppress the outliers: since rotation is a unitary transformation, the rotation matrix ğğ\mathbf{Q} can be absorbed by the weights of the output module in the previous block.

![Refer to caption](/html/2405.04532/assets/x10.png)

Figure 9: Smooth the block intermediate activations, migrating the quantization difficulty to weights: since smoothing is channel-independent, the smooth matrix ğš²ğš²\mathbf{\Lambda} is diagonal and can be absorbed by the weights of the previous modules.

#### IV-C1 Block Input Module Rotation

In transformer blocks, we define the components that take in the block inputs as input modules, such as the QKV Projection Layer and the FFN 1st Layer.
As shown in FigureÂ [9](#S4.F9 "Figure 9 â€£ IV-C General LLM Quantization Optimizations â€£ IV QoQ Quantization â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving"), inspired byÂ [[2](#bib.bib2), [4](#bib.bib4)], we rotate the block input activations by multiplying the rotation matrix. To keep mathematical equivalence of linear layers, we rotate the corresponding weights accordingly in the reversed direction.
After rotation, each channelâ€™s activations are linear combinations of all other channels, and thus outlier channels are effectively suppressed. Furthermore, since rotation is a unitary transformation, we can fuse the rotation matrix with the previous linear layersâ€™ weights. We simply choose the scaled Hadamard matrix as the rotation matrix.

#### IV-C2 Block Output Module Smoothing

Output modules refer to those layers that generate block outputs, such as the Output Projection Layer and FFN 2nd Layer. As shown in FigureÂ [9](#S4.F9 "Figure 9 â€£ IV-C General LLM Quantization Optimizations â€£ IV QoQ Quantization â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving"), inspired byÂ [[38](#bib.bib38)], we smooth the block intermediate activations through dividing them by a per-channel smoothing factor. Original SmoothQuant does not smooth the block intermediate activations; moreover, if we directly smooth these modules with the same migration strength as input modules (*e.g*., q\_proj, up\_proj), the evaluated Wikitext-2 perplexity of the Llama-2-7B model will drop by as much as 0.05. In practice, we find that the migration strength Î±ğ›¼\alpha should be near 0. That is, the smoothing factor Î»ğœ†\lambda is mostly determined by weights instead of activations, which is very different from the observations in SmoothQuant.

#### IV-C3 Activation-Aware Channel Reordering

![Refer to caption](/html/2405.04532/assets/x11.png)

Figure 10: Reorder weight input channels based on their salience in group quantization. Channel salience can be determined by the magnitude of input activations.

Both AWQÂ [[23](#bib.bib23)] and AtomÂ [[44](#bib.bib44)] have observed that maintaining the salient weights in FP16 can significantly improve model accuracy. These salient weights can be identified by the activation distribution. Instead of introducing mixed-precision quantization used by Atom, we propose activation-aware channel reordering as shown in FigureÂ [10](#S4.F10 "Figure 10 â€£ IV-C3 Activation-Aware Channel Reordering â€£ IV-C General LLM Quantization Optimizations â€£ IV QoQ Quantization â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving"). We use maxâ¡(|ğ—|)ğ—\max\left(|\mathbf{X}|\right) to determine the channel salience, and then reorder channels so that channels with similar salience are in the same quantization group.

#### IV-C4 Weight Clipping

Weight clipping is another popular quantization optimization technique. It applies a clip ratio Î±ğ›¼\alpha to the dynamic range in EquationÂ [2](#S2.E2 "In II-B Integer Quantization â€£ II Background â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving") by letting ğ–max=Î±â€‹maxâ¡(ğ–)subscriptğ–ğ›¼ğ–\mathbf{W}\_{\max}=\alpha\max\left(\mathbf{W}\right) and ğ–min=Î±â€‹minâ¡(ğ–)subscriptğ–ğ›¼ğ–\mathbf{W}\_{\min}=\alpha\min\left(\mathbf{W}\right).
Previous approachesÂ [[12](#bib.bib12), [44](#bib.bib44), [2](#bib.bib2), [23](#bib.bib23)] grid search the clip ratio Î±ğ›¼\alpha to minimize either quantization error of tensor itself (*i.e*., â€–ğ–âˆ’Qâ€‹(ğ–;Î±)â€–normğ–ğ‘„

ğ–ğ›¼\|\mathbf{W}-Q\left(\mathbf{W};\alpha\right)\|) or output mean square error (*i.e*., â€–ğ—ğ–Tâˆ’ğ—â€‹Qâ€‹(ğ–T;Î±)â€–normsuperscriptğ—ğ–ğ‘‡ğ—ğ‘„

superscriptğ–ğ‘‡ğ›¼\|\mathbf{X}\mathbf{W}^{T}-\mathbf{X}Q\left(\mathbf{W}^{T};\alpha\right)\|. In QServe, we minimize the layer output error for all linear layers, expect for q\_proj and k\_proj, for which we optimize block output mean square error:

|  |  |  |  |
| --- | --- | --- | --- |
|  | argâ¡minÎ±â¡â€–Blockâ€‹(ğ—;ğ–)âˆ’Blockâ€‹(ğ—;Qâ€‹(ğ–;Î±))â€–.subscriptğ›¼normBlock  ğ—ğ–Block  ğ—ğ‘„  ğ–ğ›¼\small\arg\min\_{\alpha}\|\mathrm{Block}\left(\mathbf{X};\mathbf{W}\right)-\mathrm{Block}\left(\mathbf{X};Q\left(\mathbf{W};\alpha\right)\right)\|. |  | (10) |

## V QServe Serving System

To this end, we have presented the QoQ quantization algorithm, which aims to minimize accuracy loss incurred by W4A8KV4 quantization. However, realizing the theoretical throughput benefits in Figure [3](#S3.F3 "Figure 3 â€£ III-A W4A8KV4 Has Superior Roofline Over W8A8, W4A16 â€£ III Motivation â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving") remains challenging. Thus, in this section, we will delve into the QServe system design, which is guided by two important principles: I. Reducing main loop overhead in GEMM kernels; II. Making fused attention kernels memory bound.

### V-A QServe System Runtime

![Refer to caption](/html/2405.04532/assets/x12.png)

Figure 11: QServeâ€™s precision mapping for an FP16 in, FP16 out LLM block. All GEMM operators take in W4A8 inputs and produce FP16 outputs. Activation quantization happens in normalization and activation layers.

We start by introducing the QServe runtime in Figure [11](#S5.F11 "Figure 11 â€£ V-A QServe System Runtime â€£ V QServe Serving System â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving"). All GEMM layers in QServe operate on W4A8 inputs, perform computation on INT8 tensor cores, and generate FP16 outputs.
All attention layers perform computation in FP16 on CUDA cores. Consequently, each LLM block in QServe has FP16 inputs and FP16 outputs.

Activation Quantization. To ensure that each GEMM takes in INT8 activation, we fuse activation quantization into the preceding layernorm for the QKV projection and the first FFN layer, or into the preceding activation kernel for the second FFN layer. Furthermore, a separate quantization node is inserted before output projection in the attention block.

KV Cache Management. To avoid memory fragmentation, we follow vLLMÂ [[21](#bib.bib21)] and TensorRT-LLMÂ [[25](#bib.bib25)] to adopt paged KV caches. In contrast to these two frameworks, which perform per-tensor, static quantization (*i.e*., scaling factors computed offline) on KV caches, QServe requires per-head, dynamic KV quantization to maintain competitive accuracy due to the lower bit precision (4 *vs*. 8). We therefore store FP16 scaling factors and zero points for each head immediately following the quantized KV features in each KV cache page, allowing these values to be updated on-the-fly. QServe also supports in-flight batching, similar to vLLM and TensorRT-LLM.

### V-B W4A8 GEMM in QServe

As discussed in Section [III](#S3 "III Motivation â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving"), the main loop overhead poses a significant obstacle in allowing quantized GEMMs to attain the theoretical performance gains projected by the roofline model (Figure [3](#S3.F3 "Figure 3 â€£ III-A W4A8KV4 Has Superior Roofline Over W8A8, W4A16 â€£ III Motivation â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving")). Therefore, the focus of QServe W4A8 GEMM is to reduce main loop overhead. Specifically, we address the costs of pointer arithmetic operations through compute-aware weight reorder, and reduce dequantization overhead through a subtraction after multiplication computation order and register-level parallelism.

#### V-B1 Compute-Aware Weight Reorder

![Refer to caption](/html/2405.04532/assets/x13.png)

Figure 12: QServe applies compute-aware weight reoder to minimize the pointer arithmetics in W4A8 GEMM main loop.

Prior to dequantization and tensor core computation, the operands must be loaded from global memory into the L1 shared memory during each main loop iteration. This loading process is non-trivial since the tensor core GEMM intrisics require a strided layout for each thread in computation, as demonstrated in Figure [12](#S5.F12 "Figure 12 â€£ V-B1 Compute-Aware Weight Reorder â€£ V-B W4A8 GEMM in QServe â€£ V QServe Serving System â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving")a. For instance, instead of loading consecutive eight INT8 weights, thread 0 first loads input channels 0-3, then skips ahead to input channels 16-19. That said, a naive weight loading implementation would require one address calculation per four channels, leading to two efficiency issues. First, pointer arithmetic operations are performed on CUDA cores, which have 32Ã—\times lower throughput than the INT8 tensor core on the A100. Consequently, the address calculation overhead becomes non-negligible. Second, strided memory access prevents achieving the highest HBM bandwidth through packed 128-bit loading, further slowing down the memory pipeline. This issue is addressed by the ldmatrix instruction when the storage and compute data types are the same. As illustrated in Figure [12](#S5.F12 "Figure 12 â€£ V-B1 Compute-Aware Weight Reorder â€£ V-B W4A8 GEMM in QServe â€£ V QServe Serving System â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving")a, thread iğ‘–i loads a consecutive 128 bits in output channel i%â€‹8percentğ‘–8i\%8, and the ldmatrix instruction automatically distributes the data in a strided manner, ensuring that each thread eventually obtains the required data for INT8 tensor core computation.

Unfortunately, the ldmatrix instruction will not work when the data types used for storage and computation differ (like in W4A8). Specifically, in Figure [12](#S5.F12 "Figure 12 â€£ V-B1 Compute-Aware Weight Reorder â€£ V-B W4A8 GEMM in QServe â€£ V QServe Serving System â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving")b, ldmatrix ensures that each thread obtains the same number of bytes, not the same number of elements, after data permutation in the register file. Consequently, thread 0 obtains the tiles needed by both itself and thread 1, while thread 1 obtains the tiles needed by thread 2 and thread 3 in the subsequent INT8 tensor core computation. This creates a mismatch between the data obtained by each thread and used in computation. That said, ldmatrix cannot be used for W4A8 GEMM and the aforementioned pointer arithmetic overhead persists. Worse still, memory bandwidth utilization deteriorates further as we consecutively load only 16 bits for 4-bit weights.

We address this challenge through compute-aware weight reordering (Figure [12](#S5.F12 "Figure 12 â€£ V-B1 Compute-Aware Weight Reorder â€£ V-B W4A8 GEMM in QServe â€£ V QServe Serving System â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving")c). The key insight is to store the weights in the order they are used during computation. We divide the entire GEMM problem into multiple 32Ã—\times32 tiles. Within each tile, thread 0 utilizes input channels 0-3 and 16-19 for output channels 0, 8, 16, and 24 (output channels 16-31 are omitted in Figure [12](#S5.F12 "Figure 12 â€£ V-B1 Compute-Aware Weight Reorder â€£ V-B W4A8 GEMM in QServe â€£ V QServe Serving System â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving")c). Consequently, we concatenate these 32 channels into a single 128-bit word. The 32 channels used by thread 1 are stored immediately following thread 0â€™s 32 channels. Since weights are static, such reordering does not introduce any runtime overhead. Additionally, it not only reduces the pointer arithmetic overhead to the same level as ldmatrix but also guarantees high-bandwidth 128-bit/thread memory transactions. We apply this reordering to zero points and scales as well to mitigate dequantization overhead.

#### V-B2 Fast Dequantization in Per-Channel W4A8 GEMM

![Refer to caption](/html/2405.04532/assets/x14.png)

Figure 13: QServe exploits register-level parallelism to significantly reduce the number of required logical operations in UINT4 to UINT8 weight unpacking.

As illustrated in Figure [5](#S3.F5 "Figure 5 â€£ III-B Why Not W4A4KV4: Main Loop Overhead in GEMM â€£ III Motivation â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving")d, dequantizing weights within the main loop becomes necessary when the bit precisions for weights and activations differ. In the case of per-channel W4A8 quantization, second-level scaling factors are omitted, and first-level FP16 scaling is efficiently fused into the GEMM epilogue. We therefore focus our discussion on the efficient conversion from ZINT4 (i.e., unsigned 4-bit integers with zero points) to SINT8 within the main loop. We further decompose this conversion into two steps: UINT4 to UINT8 (weight unpacking) and UINT8 to SINT8 (zero point subtraction). As depicted in Figure [13](#S5.F13 "Figure 13 â€£ V-B2 Fast Dequantization in Per-Channel W4A8 GEMM â€£ V-B W4A8 GEMM in QServe â€£ V QServe Serving System â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving"), we reorder every 32 UINT4 weights w0,w1,â€¦,w31

subscriptğ‘¤0subscriptğ‘¤1â€¦subscriptğ‘¤31w\_{0},w\_{1},...,w\_{31} into w0,w16,w1,w17,â€¦

subscriptğ‘¤0subscriptğ‘¤16subscriptğ‘¤1subscriptğ‘¤17â€¦w\_{0},w\_{16},w\_{1},w\_{17},... This allows us to exploit register-level parallelism and efficiently unpack them into UINT8 numbers with only three logical operations.

For the conversion from UINT8 to SINT8, the most intuitive approach is to introduce integer subtraction instructions within the main loop, which we refer to as subtraction before multiplication. Although straightforward, this approach inevitably introduces additional cost to the main loop, which is undesirable. Instead, we adopt a subtraction after multiplication approach to minimize the main loop overhead.

Specifically, a GEMM layer with per-channel quantized operands can be expressed as:

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ=ğ—^â€‹ğ–^=(ğğ—âŠ™ğ’ğ—)â€‹((ğğ–âˆ’ğ™ğ–)âŠ™ğ’ğ–),ğ^ğ—^ğ–direct-productsubscriptğğ—subscriptğ’ğ—direct-productsubscriptğğ–subscriptğ™ğ–subscriptğ’ğ–\small\mathbf{O}=\hat{\mathbf{X}}\hat{\mathbf{W}}=(\mathbf{Q}\_{\mathbf{X}}\odot\mathbf{S}\_{\mathbf{X}})((\mathbf{Q}\_{\mathbf{W}}-\mathbf{Z}\_{\mathbf{W}})\odot\mathbf{S}\_{\mathbf{W}}), |  | (11) |

where ğğ–subscriptğğ–\mathbf{Q}\_{\mathbf{W}} (ğğ—subscriptğğ—\mathbf{Q}\_{\mathbf{X}}) is the quantized weight (activation), ğ™ğ–subscriptğ™ğ–\mathbf{Z}\_{\mathbf{W}} expands the zero point vector ğ³ğ–subscriptğ³ğ–\mathbf{z}\_{\mathbf{W}} of size nğ‘›n (output channels) to kÃ—nğ‘˜ğ‘›k\times n (kğ‘˜k is input channels) and ğ’ğ–subscriptğ’ğ–\mathbf{S}\_{\mathbf{W}}, ğ’ğ—subscriptğ’ğ—\mathbf{S}\_{\mathbf{X}} are similarly obtained from scaling vectors ğ¬ğ–,ğ¬ğ—

subscriptğ¬ğ–subscriptğ¬ğ—\mathbf{s}\_{\mathbf{W}},\mathbf{s}\_{\mathbf{X}}. We denote ğ™ğ–âŠ™ğ’ğ–direct-productsubscriptğ™ğ–subscriptğ’ğ–\mathbf{Z}\_{\mathbf{W}}\odot\mathbf{S}\_{\mathbf{W}} as ğ™ğ’ğ–subscriptğ™ğ’ğ–\mathbf{ZS}\_{\mathbf{W}}, then we rewrite Equation [11](#S5.E11 "In V-B2 Fast Dequantization in Per-Channel W4A8 GEMM â€£ V-B W4A8 GEMM in QServe â€£ V QServe Serving System â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving") as:

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | ğğ\displaystyle\small\mathbf{O} | =(ğğ—âŠ™ğ’ğ—)â€‹(ğğ–âŠ™ğ’ğ–âˆ’ğ™ğ’ğ–)absentdirect-productsubscriptğğ—subscriptğ’ğ—direct-productsubscriptğğ–subscriptğ’ğ–subscriptğ™ğ’ğ–\displaystyle=(\mathbf{Q}\_{\mathbf{X}}\odot\mathbf{S}\_{\mathbf{X}})(\mathbf{Q}\_{\mathbf{W}}\odot\mathbf{S}\_{\mathbf{W}}-\mathbf{ZS}\_{\mathbf{W}}) |  | (12) |
|  |  | =(ğğ—â€‹ğğ–)âŠ™(ğ¬ğ–Ã—ğ¬ğ—)âˆ’(ğğ—âŠ™ğ’ğ—)â€‹ğ™ğ’ğ–.absentdirect-productsubscriptğğ—subscriptğğ–subscriptğ¬ğ–subscriptğ¬ğ—direct-productsubscriptğğ—subscriptğ’ğ—subscriptğ™ğ’ğ–\displaystyle=(\mathbf{Q}\_{\mathbf{X}}\mathbf{Q}\_{\mathbf{W}})\odot(\mathbf{s}\_{\mathbf{W}}\times\mathbf{s}\_{\mathbf{X}})-(\mathbf{Q}\_{\mathbf{X}}\odot\mathbf{S}\_{\mathbf{X}})\mathbf{ZS}\_{\mathbf{W}}. |  |

The first term, (ğğ—â€‹ğğ–)âŠ™(ğ¬ğ–Ã—ğ¬ğ—)direct-productsubscriptğğ—subscriptğğ–subscriptğ¬ğ–subscriptğ¬ğ—(\mathbf{Q}\_{\mathbf{X}}\mathbf{Q}\_{\mathbf{W}})\odot(\mathbf{s}\_{\mathbf{W}}\times\mathbf{s}\_{\mathbf{X}}), is analogous to the W8A8 GEMM in TensorRT-LLM, where the ğ¬ğ–Ã—ğ¬ğ—subscriptğ¬ğ–subscriptğ¬ğ—\mathbf{s}\_{\mathbf{W}}\times\mathbf{s}\_{\mathbf{X}} outer product scaling is performed in the epilogue. For the second term, we first replace ğğ—â€‹ğ’ğ—subscriptğğ—subscriptğ’ğ—\mathbf{Q}\_{\mathbf{X}}\mathbf{S}\_{\mathbf{X}} (ğ—^^ğ—\hat{\mathbf{X}}) with the unquantized ğ—ğ—\mathbf{X}. We then notice that:

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ—â€‹(ğ™ğ’ğ–)=ğ­ğ—Ã—(ğ³ğ–âŠ™ğ¬ğ–),ğ—subscriptğ™ğ’ğ–subscriptğ­ğ—direct-productsubscriptğ³ğ–subscriptğ¬ğ–\mathbf{X}(\mathbf{ZS}\_{\mathbf{W}})=\mathbf{t}\_{\mathbf{X}}\times(\mathbf{z}\_{\mathbf{W}}\odot\mathbf{s}\_{\mathbf{W}}), |  | (13) |

where ğ­ğ—=ğ—ğŸksubscriptğ­ğ—subscriptğ—ğŸğ‘˜\mathbf{t}\_{\mathbf{X}}=\mathbf{X}\mathbf{1}\_{k}, i.e., summing all input channels for each token. We observe that Equation [13](#S5.E13 "In V-B2 Fast Dequantization in Per-Channel W4A8 GEMM â€£ V-B W4A8 GEMM in QServe â€£ V QServe Serving System â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving") has a form similar to the outer product of scaling factors. Therefore, it can also be fused into the epilogue of the W4A8 GEMM, analogous to the first term in Equation [12](#S5.E12 "In V-B2 Fast Dequantization in Per-Channel W4A8 GEMM â€£ V-B W4A8 GEMM in QServe â€£ V QServe Serving System â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving"). To this end, we move the zero-point subtraction from the main loop to the epilogue, thereby largely eliminating its overhead in the GEMM kernel. This formulation of subtraction after multiplication necessitates precomputing ğ­ğ—subscriptğ­ğ—\mathbf{t}\_{\mathbf{X}}. Fortunately, each W4A8 kernel is always preceded by a memory-bound kernel, allowing us to fuse the precomputation kernel into it with negligible latency overhead.

#### V-B3 Fast Dequantization in Per-Group W4A8 GEMM

![Refer to caption](/html/2405.04532/assets/x15.png)

Figure 14: Our progressive quantization algorithm ensures that all intermediate results in the subtraction after multiplication computation order will not overflow, thereby enabling register-level parallelism and reducing main loop overhead.

The primary distinction between the per-group W4A8 GEMM and its per-channel counterpart lies in the second-level dequantization process in Figure [5](#S3.F5 "Figure 5 â€£ III-B Why Not W4A4KV4: Main Loop Overhead in GEMM â€£ III Motivation â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving")d. Firstly, since zero points are now defined on a per-group basis, it is no longer possible to merge zero point subtraction into the epilogue, as was done in the previous section. Secondly, due to the presence of level 2 scales, an additional INT8 multiplication is required for each weight. Akin to the previous section, we must determine whether to apply multiplication (scales) or subtraction (zeros) first during level 2 dequantization.

In this context, we contend that performing subtraction after multiplication remains the advantageous approach because it enables register-level parallelism (RLP). As shown in Figure [14](#S5.F14 "Figure 14 â€£ V-B3 Fast Dequantization in Per-Group W4A8 GEMM â€£ V-B W4A8 GEMM in QServe â€£ V QServe Serving System â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving"), NVIDIA GPUs provide the vadd4 instruction that performs four INT8 additions with a single INT32 ALU operation. However, there is no instruction that realizes similar effect for 4-way INT8 multiplication. Consequently, in order to achieve RLP, one has to simulate this by padding 24 zeros to the most significant bits (MSBs) of the 8-bit scaling factor. However, this simulation is valid only when the result of each INT8 multiplication remains within the INT8 range. This condition is not met for the subtraction-before-multiplication computation order. As illustrated in Figure [14](#S5.F14 "Figure 14 â€£ V-B3 Fast Dequantization in Per-Group W4A8 GEMM â€£ V-B W4A8 GEMM in QServe â€£ V QServe Serving System â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving")a, the result of the scale multiplication overflows, leading to an incorrect output. In the subtraction-before-multiplication approach, we can only perform multiplication one by one, which is extremely inefficient. On the other hand, with the subtraction-after-multiplication computation order, our progressive group quantization algorithm ensures that the result of the initial multiplication step never exceeds the INT8 range. This allows for fully leveraging the performance benefits of RLP in both multiplication and subtraction.

#### V-B4 General Optimizations

In our W4A8 kernel, we also employ general techniques for GEMM optimization. On the memory side, we apply multi-stage software pipelining and asynchronous memory copy to better overlap memory access with computation. Additionally, we swizzle the layout of the L1 shared memory to eliminate bank conflicts. To improve L2 cache utilization, we permute the computation partition across different thread blocks, allowing adjacent blocks to reuse the same weight. On the compute side, when the number of input tokens (mğ‘šm) is small, we found it beneficial to partition the reduction dimension kğ‘˜k into multiple slices and reduce the partial sums across different warps in the L1 shared memory.

### V-C KV4 Attention in QServe

TABLE I: A naive KV4 attention implementation is 1.7Ã—\times faster on L40S than TRT-LLM-KV8, but is 1.1-1.2Ã—\times slower on A100 due to earlier CUDA core roofline turning point.

|  |  |  |  |
| --- | --- | --- | --- |
| Seq\_len | 8-bit KV | 4-bit KV (Naive) | 4-bit KV (Ours) |
| 128 | 0.09 ms | 0.10 ms (0.87Ã—\times) | 0.07 ms (1.29Ã—\times) |
| 256 | 0.14 ms | 0.16 ms (0.86Ã—\times) | 0.11 ms (1.32Ã—\times) |
| 512 | 0.23 ms | 0.27 ms (0.87Ã—\times) | 0.16 ms (1.44Ã—\times) |
| 1024 | 0.42 ms | 0.48 ms (0.88Ã—\times) | 0.28 ms (1.49Ã—\times) |
| 1536 | 0.62 ms | 0.69 ms (0.90Ã—\times) | 0.41 ms (1.51Ã—\times) |

Attention accounts for 30-50% of the total LLM runtime, as depicted in Figure [2](#S3.F2 "Figure 2 â€£ III-A W4A8KV4 Has Superior Roofline Over W8A8, W4A16 â€£ III Motivation â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving")a. Although the roofline model in Figure [5](#S3.F5 "Figure 5 â€£ III-B Why Not W4A4KV4: Main Loop Overhead in GEMM â€£ III Motivation â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving") suggests that quantizing the KV cache to INT4 should automatically yield a 2Ã—\times speedup over the 8-bit KV baseline, this is not the case in real-world implementation.

We start with the KV8-attention decoding stage kernel from TensorRT-LLM as our baseline and replace all static, per-tensor quantized 8-bit KV cache accesses and conversions with their dynamic, per-head quantized 4-bit counterparts. This direct replacement immediately leads to 1.7Ã—\times speedup on L40S, but results in 1.2Ã—\times slowdown on A100 (Table [I](#S5.T1 "TABLE I â€£ V-C KV4 Attention in QServe â€£ V QServe Serving System â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving")), compared to the KV8 baseline.

Once again, our analysis reveals that the devil is in the slow CUDA cores, which are responsible for executing the attention kernels during the decoding stage. While each individual batched GEMV has a computation intensity of 1 MAC / element, the computation intensity escalates significantly for a fused attention kernel that combines all the arithmetics and KV cache updates. As an illustration, naively dequantizing a single INT4 number from the KV cache necessitates 5 ALU Ops. This includes mask and shift operations to isolate the operand, type conversion from integer to floating-point representation, and floating point mul and sub to obtain the final results. It is crucial to note that the roofline turning point for A100 FP32 CUDA cores is merely 9.8 Ops/Byte. That said, the dequantization of KV operands alone already saturates this bound, leading to the surprising observation that the fused KV4 attention kernel can become compute-bound on datacenter GPUs like A100. In fact, similar observations hold in other systems like QuaRotÂ [[2](#bib.bib2)] and AtomÂ [[44](#bib.bib44)]. Specifically, QuaRot introduces compute-intensive Hadamard transformationÂ [[4](#bib.bib4)] in the attention operator, making it hard to achieve real speedup over TRT-LLM-KV8 with 4-bit quantized KV caches.

To mitigate the compute-bound bottleneck, it is important to shift the decoding stage KV4 attention kernels away from the compute-bound region. We accomplish this objective through a bidirectional approach: Firstly, delaying the onset of the roofline turning point, and secondly, concurrently reducing the computation intensity within the fused kernel. For the first part, we replace all FP32 operations in the original TensorRT-LLM kernel with their FP16 counterpart, effectively doubling the computation roof. For the second part, we observe that the arithmetic intensity of dequantization can be significantly reduced to 2 operations per element by applying bit tricks proposed inÂ [[20](#bib.bib20)]. Furthermore, we note that simplifying the control logic and prefetching the scaling factors and zero values, thereby simplifying address calculations, contribute to performance improvements. After incorporating these enhancements, we observe a 1.5Ã—\times speedup over TensorRT-LLMâ€™s KV8 kernel on A100.

## VI Evaluation

### VI-A Evaluation Setup

##### Algorithm

The QoQ quantization algorithm is implemented using HuggingFaceÂ [[37](#bib.bib37)] on top of PyTorchÂ [[26](#bib.bib26)]. We use per-channel symmetric INT8 quantization on activations, and per-token asymmetric INT4 group quantization on KV cache.
â€œW4A8KV4 g128â€ refers to the case where QServe used progressive group quantization on weights: per-channel symmetric INT8 quantization followed by asymmetric INT4 quantization with a group size of 128, while â€œW4A8KV4â€ is the per-channel counterpart for weight quantization.

##### System

QServe serving system is implemented using CUDA and PTX assembly for high-performance GPU kernels. We also provide a purely PyTorch-based front-end framework for better flexibility.
For the throughput benchmarking, we perform all experiments under PyTorch 2.2.0 with CUDA 12.2, unless otherwise specified. The throughput numbers reported are real measurements on NVIDIA GPUs. For baseline systems, we use TensorRT-LLM v0.9.0 and latest main branches from QuaRot and Atom as of April 18th, 2024. Paged attention is enabled for all systems except QuaRot, which does not offer corresponding support.

### VI-B Accuracy Evaluation

##### Benchmarks

We evaluated QoQ on the Llama-1Â [[33](#bib.bib33)], Llama-2Â [[34](#bib.bib34)], Llama-3 families, Mistral-7BÂ [[17](#bib.bib17)], Mixtral-8x7BÂ [[18](#bib.bib18)] and Yi-34BÂ [[39](#bib.bib39)] models. Following previous literatureÂ [[8](#bib.bib8), [12](#bib.bib12), [44](#bib.bib44), [2](#bib.bib2), [38](#bib.bib38), [23](#bib.bib23)], we evaluated QoQ-quantized models on language modeling and zero-shot tasks. Specifically, we evaluated on WikiText2Â [[24](#bib.bib24)] for perplexity, and evaluated on PIQAÂ [[3](#bib.bib3)] (PQ), ARCÂ [[5](#bib.bib5)], HellaSwagÂ [[42](#bib.bib42)] (HS) and WinoGrandeÂ [[29](#bib.bib29)] (WG) with lm\_evalÂ [[13](#bib.bib13)].

##### Baselines

We compared QoQ to widely used post-training LLM quantization techiniques, SmoothQuantÂ [[38](#bib.bib38)], GPTQÂ [[12](#bib.bib12)], AWQÂ [[23](#bib.bib23)], and recently released state-of-the-art 4-bit weight-activation quantization frameworks, AtomÂ [[44](#bib.bib44)] and QuaRotÂ [[2](#bib.bib2)]. For SmoothQuant, we uses static per-tensor symmetric 8-bit quantization for KV cache following the settings in the TensorRT-LLMÂ [[25](#bib.bib25)].
For GPTQ, we use their latest version with â€œreorderâ€ trick, denoted as â€œGPTQ-Râ€. For QuaRot and Atom, we mainly evaluated using Pile validation dataset as calibration dataset. We also report their results with WikiText2 as calibration dataset in gray color. For â€œW4A8KV4 g128â€ setting, both QuaRot and Atom does not support progressive group quantization, and thus we evaluated them using ordinary group weight quantization (*i.e*., each group has one FP16 scale factor). Unsupported models and quantization settings will be reported as NaN.

TABLE II: WikiText2 perplexity with 2048 sequence length. The lower is the better.

|  |  |  |  |  |  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| WikiText2 Perplexity â†“ | | Llama-3 | Llama-2 | | | Llama | | | Mistral | Mixtral | Yi |
| Precision | Algorithm | 8B | 7B | 13B | 70B | 7B | 13B | 30B | 7B | 8x7B | 34B |
| FP16 | - | 6.14 | 5.47 | 4.88 | 3.32 | 5.68 | 5.09 | 4.10 | 5.25 | 3.84 | 4.60 |
| W8A8 | SmoothQuant | 6.28 | 5.54 | 4.95 | 3.36 | 5.73 | 5.13 | 4.23 | 5.29 | 3.89 | 4.69 |
| W4A16  g128 | GPTQ-R | 6.56 | 5.63 | 4.99 | 3.43 | 5.83 | 5.20 | 4.22 | 5.39 | 4.08 | 4.68 |
| AWQ | 6.54 | 5.60 | 4.97 | 3.41 | 5.78 | 5.19 | 4.21 | 5.37 | 4.02 | 4.67 |
| W4A4 | QuaRot | 8.20 | 6.10 | 5.40 | 3.79 | 6.26 | 5.55 | 4.60 | 5.71 | NaN | NaN |
| 8.33 | 6.19 | 5.45 | 3.83 | 6.34 | 5.58 | 4.64 | 5.77 | NaN | NaN |
| W4A4  g128 | QuaRotâ€ â€ \dagger | 7.32 | 5.93 | 5.26 | 3.61 | 6.06 | 5.40 | 4.44 | 5.54 | NaN | NaN |
| 7.51 | 6.00 | 5.31 | 3.64 | 6.13 | 5.43 | 4.48 | 5.58 | NaN | NaN |
| Atomâ€ â€ \dagger | 7.57 | 6.03 | 5.27 | 3.69 | 6.16 | 5.46 | 4.55 | 5.66 | 4.42 | 4.92 |
| 7.76 | 6.12 | 5.31 | 3.73 | 6.25 | 5.52 | 4.61 | 5.76 | 4.48 | 4.97 |
| W4A8KV4 | RTN | 9.50 | 6.51 | 5.40 | 3.90 | 6.51 | 5.71 | 4.91 | 6.18 | 5.02 | 6.52 |
| AWQ | 7.90 | 6.28 | 5.25 | 3.68 | 6.33 | 5.59 | 4.61 | 5.92 | 4.58 | 5.26 |
| Quarot | 6.75 | 5.73 | 5.07 | 3.46 | 5.93 | 5.29 | 4.32 | 5.41 | NaN | NaN |
| Atom | 7.37 | 5.91 | 5.16 | 3.60 | 6.03 | 5.41 | 4.49 | 5.55 | NaN | 4.84 |
| QoQ | 6.89 | 5.75 | 5.12 | 3.52 | 5.93 | 5.28 | 4.34 | 5.45 | 4.18 | 4.74 |
| W4A8KV4  g128 | RTN | 7.25 | 5.99 | 5.19 | 3.70 | 6.23 | 5.46 | 4.56 | 5.59 | 4.39 | 5.49 |
| AWQ | 6.94 | 5.83 | 5.12 | 3.51 | 5.93 | 5.36 | 4.39 | 5.50 | 4.23 | 4.78 |
| Quarotâ€¡â€¡\ddagger | 6.68 | 5.71 | 5.06 | 3.45 | 5.91 | 5.26 | 4.30 | 5.39 | NaN | NaN |
| Atomâ€¡â€¡\ddagger | 7.04 | 5.80 | 5.10 | 3.53 | 5.95 | 5.36 | 4.41 | 5.47 | 4.22 | 4.75 |
| QoQ | 6.76 | 5.70 | 5.08 | 3.47 | 5.89 | 5.25 | 4.28 | 5.42 | 4.14 | 4.76 |
| \* Grayed results use Wikitext2 as calibaration dataset. | | | | | | | | | | |  |
| â€ â€ \dagger QuaRot and Atom apply group quantization to activations as well. | | | | | | | | | | |  |
| â€¡â€¡\ddagger QuaRot and Atom use ordinary group quantization where each group has one FP16 scale factor. | | | | | | | | | | |  |

TABLE III: Zero-shot accuracy on five common sense tasks with 2048 sequence length.

|  |  |  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Llama-2 | Precision | Method | Zero-shot Accuracy â†‘ | | | | | |
| PQ | ARC-e | ARC-c | HS | WG | Avg. |
|  | FP16 | - | 79.05 | 74.58 | 46.25 | 76.05 | 68.98 | 68.98 |
|  | W4A4 | Quarot | 76.77 | 69.87 | 40.87 | 72.16 | 63.77 | 64.69 |
| 7B | W4A4 g128 | Atom | 75.14 | 52.99 | 38.40 | 69.37 | 62.75 | 59.73 |
|  | W4A8KV4 | QoQ | 77.64 | 72.81 | 43.60 | 74.00 | 68.03 | 67.22 |
|  | W4A8KV4 g128 | QoQ | 78.07 | 73.32 | 44.80 | 74.98 | 68.59 | 67.95 |
|  | FP16 | - | 80.52 | 77.44 | 49.06 | 79.38 | 72.22 | 71.72 |
|  | W4A4 | Quarot | 78.89 | 72.98 | 46.59 | 76.37 | 70.24 | 69.01 |
| 13B | W4A4 g128 | Atom | 76.50 | 57.49 | 42.32 | 73.84 | 67.40 | 63.51 |
|  | W4A8KV4 | QoQ | 79.71 | 75.97 | 48.38 | 77.80 | 70.96 | 70.56 |
|  | W4A8KV4 g128 | QoQ | 79.43 | 77.06 | 48.81 | 78.35 | 70.48 | 70.83 |
|  | FP16 | - | 82.70 | 81.02 | 57.34 | 83.82 | 77.98 | 76.57 |
|  | W4A4 | Quarot | 82.43 | 80.43 | 56.23 | 81.82 | 76.24 | 75.43 |
| 70B | W4A4 g128 | Atom | 79.92 | 58.25 | 46.08 | 79.06 | 74.27 | 67.52 |
|  | W4A8KV4 | QoQ | 82.64 | 79.80 | 56.83 | 82.78 | 77.51 | 75.91 |
|  | W4A8KV4 g128 | QoQ | 82.92 | 80.93 | 56.40 | 83.28 | 78.45 | 76.40 |
| \* For reference, using MX-FP4 for W4A4 quantizing Llama-7B model will decrease the | | | | | | | | |
| accuracy from 72.9 to 63.7 on ARC easy and from 44.7 to 35.5 on ARC challenge task.Â [[28](#bib.bib28)] | | | | | | | | |

##### WikiText2 perplexity

TableÂ [II](#S6.T2 "TABLE II â€£ Baselines â€£ VI-B Accuracy Evaluation â€£ VI Evaluation â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving") compares the Wikitext2 perplexity results between QoQ and other baselines. For Llama-2-7B, compared to W8A8 SmoothQuant and W4A16 AWQ, QoQ only increased perplexity by up to 0.16 QoQ consistently outperformed Atom with either W4A4 or W4A8KV4 quantization precision. QoQ also showed up to 0.49 perplexity improvement compared to W4A4 Quarot.

##### Zero-shot accuracy

we report the zero-shot accuracy of five common sense tasks in TableÂ [III](#S6.T3 "TABLE III â€£ Baselines â€£ VI-B Accuracy Evaluation â€£ VI Evaluation â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving"). QoQ significantly outperformed other 4-bit quantization methods.
Especially on the Winogrande task, compared to Quarot, QoQ accuracy is 4.82% higher.
Compared to FP16, QoQ only introduced 1.03%, 0.89% and 0.40% accuracy loss for Llama-2 at 7B, 13B and 70B size.

![Refer to caption](/html/2405.04532/assets/x16.png)

Figure 15: QServe significantly outperforms existing large language model (LLM) serving frameworks in batched generation tasks across different LLMs, ranging from 7B to 72B models. It achieves an average speedup of 2.36Ã—\times over the state-of-the-art LLM serving system, TensorRT-LLM v0.9.0, on the L40S GPU, and it is also 1.68Ã—\times faster on the A100 GPU. All experiments were conducted under the same device memory budget (*i.e*. 80GB on A100 and 48GB on L40S). We omit the geometric mean speedup of Atom since it only supports Llama2-7B. For absolute values, see Table [IV](#S6.T4 "TABLE IV â€£ Zero-shot accuracy â€£ VI-B Accuracy Evaluation â€£ VI Evaluation â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving").

TABLE IV: The absolute token generation throughput of QServe and TensorRT-LLM in Fig. [15](#S6.F15 "Figure 15 â€£ Zero-shot accuracy â€£ VI-B Accuracy Evaluation â€£ VI Evaluation â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving"). \*: we calculate the speedup over highest achieveable throughput from TensorRT-LLM across all three precision configurations. Our QServe system achieves competitive throughput on L40S GPU compared to TensorRT-LLM on A100, effectively reducing the dollar cost of LLM serving by 3Ã—\times. Unit: tokens/second.

|  |  |  |  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Device | System | Llama-3 | Llama-2 | Mistral | LLama-2 | LLaMA | Yi | Llama-2 | Qwen1.5 |
| 8B | 7B | 7B | 13B | 30B | 34B | 70B | 72B |
|  | TRT-LLM-FP16 | 1326 | 444 | 1566 | 92 | OOM | OOM | OOM | OOM |
|  | TRT-LLM-W4A16 | 1431 | 681 | 1457 | 368 | 148 | 313 | 119 | 17 |
| L40S | TRT-LLM-W8A8 | 2634 | 1271 | 2569 | 440 | 123 | 364 | OOM | OOM |
|  | QServe (Ours) | 3656 | 2394 | 3774 | 1327 | 504 | 869 | 286 | 59 |
|  | Speedup\* | 1.39Ã—\times | 1.88Ã—\times | 1.47Ã—\times | 3.02Ã—\times | 3.41Ã—\times | 2.39Ã—\times | 2.40Ã—\times | 3.47Ã—\times |
|  | TRT-LLM-FP16 | 2503 | 1549 | 2371 | 488 | 80 | 145 | OOM | OOM |
|  | TRT-LLM-W4A16 | 2370 | 1549 | 2403 | 871 | 352 | 569 | 358 | 143 |
| A100 | TRT-LLM-W8A8 | 2396 | 2334 | 2427 | 1277 | 361 | 649 | 234 | 53 |
|  | QServe (Ours) | 3005 | 2908 | 2970 | 1741 | 749 | 797 | 419 | 340 |
|  | Speedup\* | 1.20Ã—\times | 1.25Ã—\times | 1.22Ã—\times | 1.36Ã—\times | 2.07Ã—\times | 1.23Ã—\times | 1.17Ã—\times | 2.38Ã—\times |

### VI-C Efficiency Evaluation

We assessed the efficiency of QServe on A100-80G-SXM4 and L40S-48G GPUs by comparing it against TensorRT-LLM (using FP16, W8A8, and W4A16 precisions), Atom (W4A4), and QuaRot (W4A4). The primary metric for system evaluation is the maximum achievable throughput within the same memory constraints, where we use an input sequence length of 1024 and output sequence length of 512. We notice that Atom only supports Llama-2-7B, and QuaRot does not support GQA. Therefore, we skip these unsupported models when measuring the performance of baseline systems.

We present relative performance comparisons in Figure [15](#S6.F15 "Figure 15 â€£ Zero-shot accuracy â€£ VI-B Accuracy Evaluation â€£ VI Evaluation â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving") and absolute throughput values in Table [IV](#S6.T4 "TABLE IV â€£ Zero-shot accuracy â€£ VI-B Accuracy Evaluation â€£ VI Evaluation â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving"). We use per-channel quantization for A100 and per-group quantization for L40S. This is because L40S has stronger CUDA cores for dequantization. Relative to the best-performing configuration of TensorRT-LLM, QServe demonstrates significant improvements on A100: it achieves 2Ã—\times higher throughput for Llama-1-30B, 1.2-1.4Ã—\times higher throughput for Llama-2 models, 1.2Ã—\times higher throughput for Mistral and Yi, and 2.4Ã—\times higher throughput for Qwen-1.5. The performance improvements are particularly notable on the L40S GPUs, where we observed a throughput improvement ranging from 1.47Ã—\times to 3.47Ã—\times across all seven models evaluated. Remarkably, despite the L40Sâ€™s significantly smaller memory capacity compared to the A100, QServe effectively maintains the same batch size as TensorRT-LLM on the A100. This achievement is attributed to our aggressive 4-bit quantization applied to both weights and the KV cache. By examining Table [IV](#S6.T4 "TABLE IV â€£ Zero-shot accuracy â€£ VI-B Accuracy Evaluation â€£ VI Evaluation â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving"), we clearly observe that serving five of seven models under 34B on L40S with QServe achieves even higher throughput than serving them on A100 using TensorRT-LLM. Our performance gain over Atom and QuaRot on A100 is even more prominent since these systems did not outperform TensorRT-LLM. On L40S, QServe still achieves 10% higher throughput than Atom when running Llama-2-7B, the only model supported by their system despite the fact that we use higher quantization precision. Besides, the accuracy achieved by QServe is much better than Atom, as indicated in Table [III](#S6.T3 "TABLE III â€£ Baselines â€£ VI-B Accuracy Evaluation â€£ VI Evaluation â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving").

### VI-D Analysis and Discussion.

![Refer to caption](/html/2405.04532/assets/x17.png)

Figure 16: Ablation study on quantization techniques used in QoQ and the impact of serving throughput, GPU memory consumption in QServe. The model used here is Llama-2-7B.

![Refer to caption](/html/2405.04532/assets/x18.png)

Figure 17: Same-batch throughput comparison between QServe and baseline systems on L40S. We use an input sequence length of 1024 and output sequence length of 512.

##### Ablation study on quantization techniques

we examine the impact on accuracy of various quantization techniques implemented in QoQ.
Our analysis begins with round-to-nearest (RTN) W8A8 quantization on Llama-2-7B (per-channel + per-token). We then lower the quantization precision and apply different techniques step-by-step. For each step, we evaluated the WikiText2 perplexity and end-to-end inference performance on L40S with 64 requests of 1024 input tokens and 512 output tokens.
The results are detailed in FigureÂ [16](#S6.F16 "Figure 16 â€£ VI-D Analysis and Discussion. â€£ VI Evaluation â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving").
We see that reducing the weight precision to 4 bits significantly impaired the model performance, though it increased end-to-end processing speed by 1.12Ã—\times and saved 3.5GB GPU memory. Rotating the block input modules helped suppress the activation outliers, resulting in 0.18 perplexity improvement. In addition, minimizing the block output MSE through weight clipping further decreased the perplexity by 0.16. Consequently, our W4A8 configuration has achieved a perplexity comparable to that of W4A16. However, quantizing KV cache to 4 bits again deteriorated model performance by 0.14, although it substantially enhanced the end-to-end inference throughput by 1.47Ã—\times and halved GPU memory usage. To solve this problem, SmoothAttention reduced perplexity by 0.05, without adding system overhead. Progressive group quantization further improved perplexity by an additional 0.02, with only a negligible increase in dequantization overhead. Lastly, activation-aware channel reordering enhanced perplexity by 0.03.

![Refer to caption](/html/2405.04532/assets/x19.png)

Figure 18: The dequantization overhead in QServe is much smaller than that in Atom-W4A4 (up to 90%).

##### Ablation study on QServe system

Dequantization overhead: We measure the dequantization overhead of per-group QServe-W4A8 GEMM and other baselines in Figure [18](#S6.F18 "Figure 18 â€£ Ablation study on quantization techniques â€£ VI-D Analysis and Discussion. â€£ VI Evaluation â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving"). Our dequantization overhead is comparable with TRT-LLM-W4A16, but since we perform computation on INT8 tensor cores, we enjoy 2Ã—\times higher throughput.

Comparisons under the same batches: We demonstrate speedup results under the same batch sizes in Figure [17](#S6.F17 "Figure 17 â€£ VI-D Analysis and Discussion. â€£ VI Evaluation â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving"). For Llama-2-7B, we show that the 1.88Ã—\times speedup over TRT-LLM can be broken down to two parts: 1.45Ã—\times from same batch speedup and 1.3Ã—\times from the enlarged batch size. For larger models like Llama-2-13B, scaling up the batch size and single batch speedup are equally important (1.7Ã—\times improvement).

Improvement breakdown for KV4 attention: We detail the enhancements from attention optimizations in Section SectionÂ [V-C](#S5.SS3 "V-C KV4 Attention in QServe â€£ V QServe Serving System â€£ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving"). Starting with the basic KV4 implementation, which exhibits an A100 latency of 0.48ms for a 64Ã—\times1024 input, the application of bit tricks from [[20](#bib.bib20)] reduces the kernel latency to 0.44ms. Further improvements are achieved by simplifying the control flow, which reduces latency by an additional 0.05ms. Subsequently, converting the QK and SV products to FP16 each contributes a 0.03ms latency reduction. Asynchronous prefetching of dequantization parameters at the start of the attention kernel further enhances performance, ultimately reducing the latency to 0.28ms and achieving an end-to-end improvement of 1.7Ã—\times.

## VII Related Work

Quantization of LLMs.
Quantization reduces the size of LLMs and speedup inference. There are two primary quantization strategies:
(1) Weight-only quantizationÂ [[12](#bib.bib12), [23](#bib.bib23), [10](#bib.bib10), [19](#bib.bib19)] benefits edge devices where the workload is memory-bound, improving weight-loading speed. However, for cloud services with high user traffic and required batch processing, this method falls short as it does not accelerate computation in compute-bound scenarios.
(2) Weight-activation quantization accelerates computation in batch processing by quantizing both weights and activationsÂ [[8](#bib.bib8), [36](#bib.bib36), [38](#bib.bib38)]. OmniQuantÂ [[30](#bib.bib30)] and AtomÂ [[44](#bib.bib44)] exploring more aggressive quantizations (W4A4, W4A8) and mixed precision to enhance model quality and efficiency, though these can impact model accuracy and reduce serving throughput. QuaRotÂ [[2](#bib.bib2)] further refines W4A4 by rotating weights and activations at the cost of increased computational overhead due to additional transformations required during inference.

LLM serving systems.
Numerous systems have been proposed for efficient LLM deployment. OrcaÂ [[40](#bib.bib40)] employs iteration-level scheduling and selective batching in distributed systems. vLLMÂ [[22](#bib.bib22)] features virtual memory-inspired PagedAttention, optimizing KV cache management. SGLangÂ [[45](#bib.bib45)] enhances LLM programming with advanced primitives and RadixAttention. LMDeployÂ [[7](#bib.bib7)] offers persistent batching and blocked KV cache features to improve deployment efficiency. LightLLMÂ [[6](#bib.bib6)] manages GPU memory with token-wise KV cache control via Token Attention, increasing throughput. MLC-LLMÂ [[32](#bib.bib32)] utilizes compiler acceleration for versatile LLM deployment across edge devices. TensorRT-LLMÂ [[25](#bib.bib25)] is the leading industry solution and the most important baseline in this paper.

LLM Accelerators. Transformers and LLMs have also generated considerable research interest in domain-specific accelerator design. Several works, such as A3superscriptğ´3A^{3}Â [[14](#bib.bib14)], ELSAÂ [[15](#bib.bib15)], and SpAttenÂ [[35](#bib.bib35)], have applied pruning techniques to the attention module, while GOBOÂ [[41](#bib.bib41)] and EdgeBERTÂ [[31](#bib.bib31)] have investigated quantization approaches. Additionally, DOTAÂ [[27](#bib.bib27)] introduces a lightweight, runtime detector for omitting weak attention connections, coupled with specialized accelerators for transformer inference. Apart from attention optimizations, STAÂ [[11](#bib.bib11)] leverages Nğ‘N:Mğ‘€M sparsity and specialized softmax module to reduce off-chip communication. Moreover, DFXÂ [[16](#bib.bib16)] exploits model parallelism and optimized dataflow for low-latency generation. However, these accelerators have yet to be scaled up to recent LLMs with billions of parameters.

## VIII Conclusion

We introduce QServe, an algorithm and system co-design framework tailored to quantize large language models (LLMs) to W4A8KV4 precision, facilitating their efficient deployment on GPUs. On the algorithmic front, we design the QoQ quantization method that features progressive quantization, enabling W4A8 GEMM operations to be executed on INT8 tensor cores, and SmoothAttention, which significantly reduces accuracy loss resulting from KV4 quantization. Correspondingly, in the QServe system, we leverage the protective range established in the first level of progressive quantization to enable INT4 to INT8 dequantization. This process utilizes full register-level parallelism and employs a subtraction-after-multiplication computation sequence. Additionally, we implement compute-aware weight reordering to minimize the overhead associated with pointer arithmetic. As a result, when serving seven representative LLMs on A100 and L40S GPUs, QServe achieves up to 2.4-3.5Ã—\times higher throughput over the industrial standard for LLM serving, TensorRT-LLM.

## Acknowledgements

We thank MIT-IBM Watson AI Lab, MIT AI Hardware Program, MIT Amazon Science Hub, and NSF for supporting this research. We also thank Julien Demouth, June Yang, and Dongxu Yang from NVIDIA for their helpful discussions.

## References

* [1]

  J.Â Ainslie, J.Â Lee-Thorp, M.Â deÂ Jong, Y.Â Zemlyanskiy, F.Â LebrÃ³n, and S.Â Sanghai, â€œGqa: Training generalized multi-query transformer models from multi-head checkpoints,â€ *arXiv preprint arXiv:2305.13245*, 2023.
* [2]

  S.Â Ashkboos, A.Â Mohtashami, M.Â L. Croci, B.Â Li, M.Â Jaggi, D.Â Alistarh, T.Â Hoefler, and J.Â Hensman, â€œQuarot: Outlier-free 4-bit inference in rotated llms,â€ *arXiv preprint arXiv:2404.00456*, 2024.
* [3]

  Y.Â Bisk, R.Â Zellers, R.Â L. Bras, J.Â Gao, and Y.Â Choi, â€œPiqa: Reasoning about physical commonsense in natural language,â€ in *Thirty-Fourth AAAI Conference on Artificial Intelligence*, 2020.
* [4]

  J.Â Chee, Y.Â Cai, V.Â Kuleshov, and C.Â D. Sa, â€œQuip: 2-bit quantization of large language models with guarantees,â€ 2024.
* [5]

  P.Â Clark, I.Â Cowhey, O.Â Etzioni, T.Â Khot, A.Â Sabharwal, C.Â Schoenick, and O.Â Tafjord, â€œThink you have solved question answering? try arc, the ai2 reasoning challenge,â€ 2018.
* [6]

  L.Â Contributors, â€œLightllm: A light and fast inference service for llm,â€ <https://github.com/ModelTC/lightllm>, 2023.
* [7]

  L.Â Contributors, â€œLmdeploy: A toolkit for compressing, deploying, and serving llm,â€ <https://github.com/InternLM/lmdeploy>, 2023.
* [8]

  T.Â Dettmers, M.Â Lewis, Y.Â Belkada, and L.Â Zettlemoyer, â€œGPT3.int8(): 8-bit matrix multiplication for transformers at scale,â€ in *Advances in Neural Information Processing Systems*, A.Â H. Oh, A.Â Agarwal, D.Â Belgrave, and K.Â Cho, Eds., 2022.
* [9]

  T.Â Dettmers, A.Â Pagnoni, A.Â Holtzman, and L.Â Zettlemoyer, â€œQlora: Efficient finetuning of quantized llms,â€ *arXiv preprint arXiv:2305.14314*, 2023.
* [10]

  T.Â Dettmers, R.Â Svirschevski, V.Â Egiazarian, D.Â Kuznedelev, E.Â Frantar, S.Â Ashkboos, A.Â Borzunov, T.Â Hoefler, and D.Â Alistarh, â€œSpqr: A sparse-quantized representation for near-lossless llm weight compression,â€ 2023.
* [11]

  C.Â Fang, A.Â Zhou, and Z.Â Wang, â€œAn algorithmâ€“hardware co-optimized framework for accelerating n: M sparse transformers,â€ *IEEE Transactions on Very Large Scale Integration (VLSI) Systems*, vol.Â 30, no.Â 11, pp. 1573â€“1586, 2022.
* [12]

  E.Â Frantar, S.Â Ashkboos, T.Â Hoefler, and D.Â Alistarh, â€œGPTQ: Accurate post-training compression for generative pretrained transformers,â€ *arXiv preprint arXiv:2210.17323*, 2022.
* [13]

  L.Â Gao, J.Â Tow, B.Â Abbasi, S.Â Biderman, S.Â Black, A.Â DiPofi, C.Â Foster, L.Â Golding, J.Â Hsu, A.Â LeÂ Noacâ€™h, H.Â Li, K.Â McDonell, N.Â Muennighoff, C.Â Ociepa, J.Â Phang, L.Â Reynolds, H.Â Schoelkopf, A.Â Skowron, L.Â Sutawika, E.Â Tang, A.Â Thite, B.Â Wang, K.Â Wang, and A.Â Zou, â€œA framework for few-shot language model evaluation,â€ 12 2023. [Online]. Available: <https://zenodo.org/records/10256836>
* [14]

  T.Â J. Ham, S.Â J. Jung, S.Â Kim, Y.Â H. Oh, Y.Â Park, Y.Â Song, J.-H. Park, S.Â Lee, K.Â Park, J.Â W. Lee *etÂ al.*, â€œA^ 3: Accelerating attention mechanisms in neural networks with approximation,â€ in *2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)*.Â Â Â IEEE, 2020, pp. 328â€“341.
* [15]

  T.Â J. Ham, Y.Â Lee, S.Â H. Seo, S.Â Kim, H.Â Choi, S.Â J. Jung, and J.Â W. Lee, â€œElsa: Hardware-software co-design for efficient, lightweight self-attention mechanism in neural networks,â€ in *2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)*.Â Â Â IEEE, 2021, pp. 692â€“705.
* [16]

  S.Â Hong, S.Â Moon, J.Â Kim, S.Â Lee, M.Â Kim, D.Â Lee, and J.-Y. Kim, â€œDfx: A low-latency multi-fpga appliance for accelerating transformer-based text generation,â€ in *2022 55th IEEE/ACM International Symposium on Microarchitecture (MICRO)*.Â Â Â IEEE, 2022, pp. 616â€“630.
* [17]

  A.Â Q. Jiang, A.Â Sablayrolles, A.Â Mensch, C.Â Bamford, D.Â S. Chaplot, D.Â d.Â l. Casas, F.Â Bressand, G.Â Lengyel, G.Â Lample, L.Â Saulnier *etÂ al.*, â€œMistral 7b,â€ *arXiv preprint arXiv:2310.06825*, 2023.
* [18]

  A.Â Q. Jiang, A.Â Sablayrolles, A.Â Roux, A.Â Mensch, B.Â Savary, C.Â Bamford, D.Â S. Chaplot, D.Â d.Â l. Casas, E.Â B. Hanna, F.Â Bressand *etÂ al.*, â€œMixtral of experts,â€ *arXiv preprint arXiv:2401.04088*, 2024.
* [19]

  S.Â Kim, C.Â Hooper, A.Â Gholami, Z.Â Dong, X.Â Li, S.Â Shen, M.Â W. Mahoney, and K.Â Keutzer, â€œSqueezellm: Dense-and-sparse quantization,â€ 2024.
* [20]

  Y.Â J. Kim, R.Â Henry, R.Â Fahim, and H.Â H. Awadalla, â€œWho says elephants canâ€™t run: Bringing large scale moe models into cloud scale production,â€ *arXiv preprint arXiv:2211.10017*, 2022.
* [21]

  W.Â Kwon, Z.Â Li, S.Â Zhuang, Y.Â Sheng, L.Â Zheng, C.Â H. Yu, J.Â Gonzalez, H.Â Zhang, and I.Â Stoica, â€œEfficient memory management for large language model serving with pagedattention,â€ in *Proceedings of the 29th Symposium on Operating Systems Principles*, 2023, pp. 611â€“626.
* [22]

  W.Â Kwon, Z.Â Li, S.Â Zhuang, Y.Â Sheng, L.Â Zheng, C.Â H. Yu, J.Â E. Gonzalez, H.Â Zhang, and I.Â Stoica, â€œEfficient memory management for large language model serving with pagedattention,â€ in *Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles*, 2023.
* [23]

  J.Â Lin, J.Â Tang, H.Â Tang, S.Â Yang, W.-M. Chen, W.-C. Wang, G.Â Xiao, X.Â Dang, C.Â Gan, and S.Â Han, â€œAwq: Activation-aware weight quantization for llm compression and acceleration,â€ in *MLSys*, 2024.
* [24]

  S.Â Merity, C.Â Xiong, J.Â Bradbury, and R.Â Socher, â€œPointer sentinel mixture models,â€ 2016.
* [25]

  NVIDIA, â€œTensorRT-LLM: A TensorRT Toolbox for Optimized Large Language Model Inference,â€ 2023. [Online]. Available: <https://github.com/NVIDIA/TensorRT-LLM>
* [26]

  A.Â Paszke, S.Â Gross, F.Â Massa, A.Â Lerer, J.Â Bradbury, G.Â Chanan, T.Â Killeen, Z.Â Lin, N.Â Gimelshein, L.Â Antiga, A.Â Desmaison, A.Â KÃ¶pf, E.Â Yang, Z.Â DeVito, M.Â Raison, A.Â Tejani, S.Â Chilamkurthy, B.Â Steiner, L.Â Fang, J.Â Bai, and S.Â Chintala, â€œPytorch: An imperative style, high-performance deep learning library,â€ 2019.
* [27]

  Z.Â Qu, L.Â Liu, F.Â Tu, Z.Â Chen, Y.Â Ding, and Y.Â Xie, â€œDota: detect and omit weak attentions for scalable transformer acceleration,â€ in *Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems*, 2022, pp. 14â€“26.
* [28]

  B.Â D. Rouhani, R.Â Zhao, A.Â More, M.Â Hall, A.Â Khodamoradi, S.Â Deng, D.Â Choudhary, M.Â Cornea, E.Â Dellinger, K.Â Denolf *etÂ al.*, â€œMicroscaling data formats for deep learning,â€ *arXiv preprint arXiv:2310.10537*, 2023.
* [29]

  K.Â Sakaguchi, R.Â L. Bras, C.Â Bhagavatula, and Y.Â Choi, â€œWinogrande: An adversarial winograd schema challenge at scale,â€ *arXiv preprint arXiv:1907.10641*, 2019.
* [30]

  W.Â Shao, M.Â Chen, Z.Â Zhang, P.Â Xu, L.Â Zhao, Z.Â Li, K.Â Z. Zhang, P.Â Gao, Y.Â Qiao, and P.Â Luo, â€œOmniquant: Omnidirectionally calibrated quantization for large language models,â€ *arXiv preprint arXiv:2308.13137*, 2023.
* [31]

  T.Â Tambe, C.Â Hooper, L.Â Pentecost, T.Â Jia, E.-Y. Yang, M.Â Donato, V.Â Sanh, P.Â Whatmough, A.Â M. Rush, D.Â Brooks *etÂ al.*, â€œEdgebert: Sentence-level energy optimizations for latency-aware multi-task nlp inference,â€ in *MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture*, 2021, pp. 830â€“844.
* [32]

  M.Â team, â€œMLC-LLM,â€ 2023. [Online]. Available: <https://github.com/mlc-ai/mlc-llm>
* [33]

  H.Â Touvron, T.Â Lavril, G.Â Izacard, X.Â Martinet, M.-A. Lachaux, T.Â Lacroix, B.Â RoziÃ¨re, N.Â Goyal, E.Â Hambro, F.Â Azhar, A.Â Rodriguez, A.Â Joulin, E.Â Grave, and G.Â Lample, â€œLlama: Open and efficient foundation language models,â€ 2023.
* [34]

  H.Â Touvron, L.Â Martin, K.Â Stone, P.Â Albert, A.Â Almahairi, Y.Â Babaei, N.Â Bashlykov, S.Â Batra, P.Â Bhargava, S.Â Bhosale *etÂ al.*, â€œLlama 2: Open foundation and fine-tuned chat models,â€ *arXiv preprint arXiv:2307.09288*, 2023.
* [35]

  H.Â Wang, Z.Â Zhang, and S.Â Han, â€œSpatten: Efficient sparse attention architecture with cascade token and head pruning,â€ in *2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)*.Â Â Â IEEE, 2021, pp. 97â€“110.
* [36]

  X.Â Wei, Y.Â Zhang, X.Â Zhang, R.Â Gong, S.Â Zhang, Q.Â Zhang, F.Â Yu, and X.Â Liu, â€œOutlier suppression: Pushing the limit of low-bit transformer language models,â€ *arXiv preprint arXiv:2209.13325*, 2022.
* [37]

  T.Â Wolf, L.Â Debut, V.Â Sanh, J.Â Chaumond, C.Â Delangue, A.Â Moi, P.Â Cistac, T.Â Rault, R.Â Louf, M.Â Funtowicz, J.Â Davison, S.Â Shleifer, P.Â von Platen, C.Â Ma, Y.Â Jernite, J.Â Plu, C.Â Xu, T.Â L. Scao, S.Â Gugger, M.Â Drame, Q.Â Lhoest, and A.Â M. Rush, â€œHuggingfaceâ€™s transformers: State-of-the-art natural language processing,â€ 2020.
* [38]

  G.Â Xiao, J.Â Lin, M.Â Seznec, H.Â Wu, J.Â Demouth, and S.Â Han, â€œSmoothQuant: Accurate and efficient post-training quantization for large language models,â€ in *Proceedings of the 40th International Conference on Machine Learning*, 2023.
* [39]

  A.Â Young, B.Â Chen, C.Â Li, C.Â Huang, G.Â Zhang, G.Â Zhang, H.Â Li, J.Â Zhu, J.Â Chen, J.Â Chang, K.Â Yu, P.Â Liu, Q.Â Liu, S.Â Yue, S.Â Yang, S.Â Yang, T.Â Yu, W.Â Xie, W.Â Huang, X.Â Hu, X.Â Ren, X.Â Niu, P.Â Nie, Y.Â Xu, Y.Â Liu, Y.Â Wang, Y.Â Cai, Z.Â Gu, Z.Â Liu, and Z.Â Dai, â€œYi: Open foundation models by 01.ai,â€ 2024.
* [40]

  G.-I. Yu, J.Â S. Jeong, G.-W. Kim, S.Â Kim, and B.-G. Chun, â€œOrca: A distributed serving system for Transformer-Based generative models,â€ in *16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)*.Â Â Â Carlsbad, CA: USENIX Association, Jul. 2022, pp. 521â€“538. [Online]. Available: <https://www.usenix.org/conference/osdi22/presentation/yu>
* [41]

  A.Â H. Zadeh, I.Â Edo, O.Â M. Awad, and A.Â Moshovos, â€œGobo: Quantizing attention-based nlp models for low latency and energy efficient inference,â€ in *2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)*.Â Â Â IEEE, 2020, pp. 811â€“824.
* [42]

  R.Â Zellers, A.Â Holtzman, Y.Â Bisk, A.Â Farhadi, and Y.Â Choi, â€œHellaswag: Can a machine really finish your sentence?â€ *CoRR*, vol. abs/1905.07830, 2019. [Online]. Available: <http://arxiv.org/abs/1905.07830>
* [43]

  L.Â Zhang, W.Â Fei, W.Â Wu, Y.Â He, Z.Â Lou, and H.Â Zhou, â€œDual grained quantization: Efficient fine-grained quantization for llm,â€ *arXiv preprint arXiv:2310.04836*, 2023.
* [44]

  Y.Â Zhao, C.-Y. Lin, K.Â Zhu, Z.Â Ye, L.Â Chen, S.Â Zheng, L.Â Ceze, A.Â Krishnamurthy, T.Â Chen, and B.Â Kasikci, â€œAtom: Low-bit quantization for efficient and accurate llm serving,â€ in *MLSys*, 2023.
* [45]

  L.Â Zheng, L.Â Yin, Z.Â Xie, J.Â Huang, C.Â Sun, C.Â H. Yu, S.Â Cao, C.Â Kozyrakis, I.Â Stoica, J.Â E. Gonzalez, C.Â Barrett, and Y.Â Sheng, â€œEfficiently programming large language models using sglang,â€ 2023.