]FAIR at Meta
\contribution[\*]Core contributors, alphabetical order
\contribution[â€ ]Contributors to data preparation, LCM extensions and evaluation, alphabetical order
\contribution[â€¡]Research and project management, alphabetical order
\contribution[+]Initial work while at FAIR at Meta, new affiliation: INRIA, France
\correspondenceHolger Schwenk at

# Large Concept Models: Language Modeling in a Sentence Representation Space

LCM team
â€ƒâ€ƒ
LoÃ¯c Barrault
â€ƒâ€ƒ
Paul-Ambroise Duquenne
â€ƒâ€ƒ
Maha Elbayad
â€ƒâ€ƒ
Artyom Kozhevnikov
â€ƒâ€ƒ
Belen Alastruey
â€ƒâ€ƒ
Pierre Andrews
â€ƒâ€ƒ
Mariano Coria
â€ƒâ€ƒ
Guillaume Couairon
â€ƒâ€ƒ
Marta R. Costa-jussÃ 
â€ƒâ€ƒ
David Dale
â€ƒâ€ƒ
Hady Elsahar
â€ƒâ€ƒ
Kevin Heffernan
â€ƒâ€ƒ
JoÃ£o Maria Janeiro
â€ƒâ€ƒ
Tuan Tran
â€ƒâ€ƒ
Christophe Ropers
â€ƒâ€ƒ
Eduardo SÃ¡nchez
â€ƒâ€ƒ
Robin San Roman
â€ƒâ€ƒ
Alexandre Mourachko
â€ƒâ€ƒ
Safiyyah Saleem
â€ƒâ€ƒ
Holger Schwenk
[
[schwenk@meta.com](mailto:schwenk@meta.com)

(December 12, 2024)

###### Abstract

LLMs have revolutionized the field of artificial intelligence and have emerged as the de-facto tool for many tasks. The current established technology of LLMs is to process input and generate output at the token level.
This is in sharp contrast to humans who operate at multiple levels of abstraction, well beyond single words, to analyze information and to generate creative content.
In this paper, we present an attempt at an architecture which operates on an explicit higher-level semantic representation, which we name a â€œconceptâ€. Concepts are language- and modality-agnostic and represent a higher level idea or action in a flow. Hence, we build a â€œLarge Concept Modelâ€.
In this study, as proof of feasibility, we assume that a concept corresponds to a sentence, and use an existing sentence embedding space, SONAR, which supports up to 200 languages in both text and speech modalities.

The Large Concept Model is trained to perform autoregressive sentence prediction in an embedding space.
We explore multiple approaches, namely MSE regression,
variants of diffusion-based generation,
and models operating in a quantized SONAR space.
These explorations are performed using 1.6B parameter models and training data in the order of 1.3T tokens.
We then scale one architecture to a model size of 7B parameters and training data of about 2.7T tokens.
We perform an experimental evaluation on several generative tasks, namely summarization and a new task of summary expansion.
Finally, we show that our model exhibits impressive zero-shot generalization performance to many languages, outperforming existing LLMs of the same size.
The training code of our models is freely available.111<https://github.com/facebookresearch/large_concept_model>

## 1 Introduction

Large Language models (LLMs) are dominating current research in natural language processing, and with their recent extension to more modalities, namely images, video and speech, they seem to be considered as the de-facto technique to follow to approach human intelligence. LLMs achieve indeed impressive performance on a large variety of tasks, such as providing detailed answers for general knowledge questions, helping in performing long document analysis, or drafting different types of messages, and writing or debugging code.
Building an LLM from scratch requires access to enormous computational resources to process ever larger amounts of data and train models, the size of which now exceeds four hundred billion parameters.
Knowledge acquisition in LLMs is heavily data-driven and extending them to more languages or modalities usually requires injecting additional (synthetic) data to cover them.

The landscape of available LLMs can be structured into open models such as Llama (The Llama3 team, [2024](#bib.bib109)), Mistral (Jiang etÂ al., [2024](#bib.bib56)), Bloom (BigScience Workshop, [2023](#bib.bib17)) or Falcon (Almazrouei etÂ al., [2023](#bib.bib2)), on the one hand,
and closed models such as Gemini (Gemini Team Google, [2024](#bib.bib40)), GPT (OpenAI, [2024](#bib.bib86)) or Claude (Anthropic, [2024](#bib.bib4)), on the other.
It is striking that all these models are based on the same underlying architecture: a transformer-based, decoder-only language model, which is pretrained to predict the next token, given a long context of preceding tokens.
Despite the undeniable success of LLMs and continued progress, all current LLMs miss a crucial characteristic of human intelligence: explicit reasoning and planning at multiple levels of abstraction.
The human brain does not operate at the word level only.
We usually have a top-down process to solve a complex task or compose a long document: we first plan at a higher level the overall structure, and then step-by-step, add details at lower levels of abstraction.
One may argue that LLMs are implicitly learning a hierarchical representation, but we stipulate that models with an explicit hierarchical architecture are better suited to create coherent long-form output.

Imagine a researcher giving a fifteen-minute talk. In such a situation, researchers do not usually prepare detailed speeches by writing out every single word they will pronounce. Instead, they outline a flow of higher-level ideas they want to communicate. Should they give the same talk multiple times, the actual words being spoken may differ, the talk could even be given in different languages, but the flow of higher-level abstract ideas will remain the same.
Similarly, when writing a research paper or essay on a specific topic, humans usually start by preparing an outline that structures the whole document into sections, which they then refine iteratively.
Humans also detect and remember dependencies between the different parts of a longer document at an abstract level. If we expand on our previous research writing example, keeping track of dependencies means that we need to provide results for each of the experiment mentioned in the introduction. Finally, when processing and analyzing information, humans rarely consider every single word in a large document. Instead, we use a hierarchical approach: we remember which part of a long document we should search to find a specific piece of information.

To the best of our knowledge, this explicit hierarchical structure of information processing and generation, at an abstract level, independent of any instantiation in a particular language or modality, cannot be found in any of the current LLMs.

|  |
| --- |
| Refer to caption |

|  |
| --- |
| Refer to caption |

Figure 1: Left: visualization of reasoning in an embedding space of concepts (task of summarization).
  
Right: fundamental architecture of an Large Concept Model (LCM).
  
â‹†â‹†\star: concept encoder and decoder are frozen.

In this work, we present a new approach which moves away from processing at the token level and closer to (hierarchical) reasoning in an abstract embedding space. This abstract embedding space is designed to be independent of the language or modality in which the content is expressed; in other words, we aim to model the underlying reasoning process at a purely semantic level, not its instantiation in a specific language.
In order to verify our approach, we limit our study to two levels of abstraction: subword tokens and concepts. We define a concept as an abstract atomic idea.
In practice, a concept would often correspond to a sentence in a text document, or an equivalent speech utterance. We posit that a sentence is an appropriate unit to achieve language independence, in opposition to single words.
This is in sharp contrast to current LLMs techniques which are heavily English centric and token based.

Our fundamental idea could be based on any fixed-size sentence embedding space for which an encoder and decoder are available. In particular, we could aim to train a new embedding space specifically optimized to our reasoning architecture. In this work, we chose an existing and freely available sentence embedding, named SONAR (Duquenne etÂ al., [2023b](#bib.bib35)). SONAR supports text input and output in 200 languages, speech input in 76 languages, and speech output in English. We discuss the constraints and impact of this choice in [SectionÂ 2.1](#S2.SS1 "2.1 The SONAR embedding space â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space"), and share some ideas on alternative embedding spaces in [SectionÂ 6](#S6 "6 Limitations â€£ Large Concept Models: Language Modeling in a Sentence Representation Space").

[FigureÂ 1](#S1.F1 "In 1 Introduction â€£ Large Concept Models: Language Modeling in a Sentence Representation Space")-left visualizes reasoning in an embedding space with the example of a summarization task, which is materialized by a function on the embedding space, mapping five concept representations into two.
[FigureÂ 1](#S1.F1 "In 1 Introduction â€£ Large Concept Models: Language Modeling in a Sentence Representation Space")-right summarizes the overall architecture and processing flow. The input is first segmented into sentences, and each one is encoded with SONAR to achieve a sequence of concepts, i.e., sentence embeddings.
This sequence of concepts is then processed by a Large Concept Model (LCM) to generate at the output a new sequence of concepts. Finally, the generated concepts are decoded by SONAR into a sequence of subwords. The encoder and decoder are fixed and are not trained. It is important to highlight that the unchanged sequence of concepts at the output of the LCM can be decoded into other languages or modalities without performing again the whole reasoning process. In the same spirit, a particular reasoning operation such as summarization can be performed in a zero-shot setting on input in any language or modality, since it solely operates on concepts. To summarize, the LCM neither has information on the input language or modality nor generates output in a particular language or modality.
We explore multiple architectures to train the LCM, in particular several variants of diffusion.
Finally, we envision an additional level of abstraction beyond concepts which could correspond to a short description of a paragraph or small section. In [SectionÂ 4.3](#S4.SS3 "4.3 Exploring explicit planning â€£ 4 Large Concept Model Extensions â€£ Large Concept Models: Language Modeling in a Sentence Representation Space") we report initial ideas on how conditioning and predicting such higher-level representations can improve consistency of output generated by an LCM.

To some extent, the LCM architecture resembles the Jepa approach (LeCun, [2022](#bib.bib63)) that also aims to predict the representation of the next observation in an embedding space. However, unlike Jepa that places more emphasis on learning a representation space in a self-supervised way, the LCM focuses on accurate prediction in the existing embedding space.

The mains characteristics of our generic Large Concept Model approach are as follows:

* â€¢

  Reasoning at an abstract language- and modality-agnostic level beyond tokens:

  + â€“

    We model the underlying reasoning process, not its instantiation in a particular language.
  + â€“

    The LCM can be trained, i.e. acquire knowledge, on all languages and modalities at once, promising scalability in an unbiased way.
* â€¢

  Explicit hierarchical structure:

  + â€“

    Better readability of long-form output by a human.
  + â€“

    Facilitates local interactive edits by a user.
* â€¢

  Handling of long context and long-form output:

  + â€“

    The complexity of a vanilla transformer model increases quadratically with the sequence length. This makes handling of large context windows challenging and several techniques have been developed to alleviate this problem, e.g., sparse attention (Child etÂ al., [2019](#bib.bib21)) or LSH attention (Kitaev etÂ al., [2020](#bib.bib61)).
    Our LCM operates on sequences which are at least an order of magnitude shorter.222We assume an average sentence length of 10â€“20 tokens.
* â€¢

  Unparalleled zero-shot generalization:

  + â€“

    Independently of the language or modality the LCM is pre-trained and fine-tuned on, it can be applied to any language and modality supported by the SONAR encoders, without the need of additional data or fine-tuning. We report results for multiple languages in the text modality.
* â€¢

  Modularity and extensibility:

  + â€“

    Unlike multimodal LLMs that can suffer from modality competitionÂ (Aghajanyan etÂ al., [2023](#bib.bib1); Chameleon team, [2024](#bib.bib18)), concept encoders and decoders can be independently developed and optimized without any competition or interference.
  + â€“

    New languages or modalities can be easily added for an existing system.

The goal of this paper is to provide a proof of concept of this high-level vision of an alternative architecture to current best practice in language modeling.
In the next section we present the main design principles of our models and discuss several variants to build and train a Large Concept Model.
We discuss several designs to implement diffusion approaches with concept embeddings and carefully study noise scheduling. This section is completed by a compute complexity comparison with token-based LLMs.
[SectionÂ 3](#S3 "3 Scaling the model to 7B â€£ Large Concept Models: Language Modeling in a Sentence Representation Space") is dedicated to the analysis of a larger 7B parameter model. We discuss challenges when instruction fine-tuning this model on multiple generative tasks, and provide a comparison with existing LLMs of comparable size.
The paper concludes with a discussion of related work, the current limitations and perspectives of our approach.

To foster research in this area, we make our LCM training code333<https://github.com/facebookresearch/large_concept_model> as well as SONAR encoders and decoders444<https://github.com/facebookresearch/SONAR> for up to 200 languages and multiple modalities freely available.

## 2 Main Design Principles

In this section, we outline the main design principles of the LCM.
We first describe the SONAR embedding space with its encoders and decoders.
Then, we discuss details of data preparation, namely sentence segmentation i.e., how we split long documents into sentences.
And finally, we describe in details the different versions of LCMs introduced in this work.

### 2.1 The SONAR embedding space

The motivation of this work is to perform reasoning at a higher conceptual level than tokens. This requires an embedding space which is highly semantic. We chose SONAR (Duquenne etÂ al., [2023b](#bib.bib35)) since it achieves best performance on several semantic similarity metrics like xsim or xsim++ (Chen etÂ al., [2023b](#bib.bib20)), and it was successfully used in large-scale bitext mining for translationÂ (Seamless Communication etÂ al., [2023b](#bib.bib99)).

![Refer to caption](/html/2412.08821/assets/figures/fig-archi-sonar2.png)

Figure 2: Encoder/decoder bottleneck architecture to train the SONAR text embeddings (right part of figure). Teacher-student approach to extend SONAR to the speech modality (left part).

The SONAR text embedding space was trained as an encoder/decoder architecture, with a fixed-size bottleneck instead of cross-attention (see [FigureÂ 2](#S2.F2 "In 2.1 The SONAR embedding space â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space")). The criterion combines a machine translation objective for 200 languages into and out of English, denoising auto-encoding and an explicit MSE loss at the embedding bottleneck layer.
Once the text embedding space was trained, a teacher-student approach was applied to extend the SONAR space to the speech modality.
More details on the architecture and training procedure can be found in Duquenne etÂ al. ([2023b](#bib.bib35)), and detailed speech recognition and translation results in the appendix of Seamless Communication etÂ al. ([2023a](#bib.bib98)).

|  |  |  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
|  | Text | | Speech | | Image | | Video | |
| Model | Input | Output | Input | Output | Input | Output | Input | Output |
| Gemini | 47 | 47 | 62 | âœ“ | âœ“ | âœ“ | âœ“ | âœ— |
| GPT | 85 | 85 | âœ“ | âœ“ | âœ“ | âœ“ | ? | âœ— |
| Claude | 37 | 37 | âœ“ | âœ“ | âœ“ | âœ“ | âœ— | âœ— |
| Bloom | 46 | 46 | âœ— | âœ— | âœ“ | âœ“ | âœ— | âœ— |
| Llama 3-400B | 8 | 8 | 34 | âœ— | âœ“ | âœ“ | âœ— | âœ— |
| LCM-SONAR | 200 | 200 | 76 | 1 | âœ— | âœ— | (ASL) | âœ— |

Table 1: Comparison of language and modality coverage for several LLMs and our LCM operating on the SONAR embedding space. SONAR has an experimental support for American Sign Language (ASL) which is not used in this paper.

Our LCM operates directly on SONAR concepts embeddings, hence, it can perform reasoning on all supported languages and modalities. [TableÂ 1](#S2.T1 "In 2.1 The SONAR embedding space â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space") compares the language coverage of several other LLMs. The LCM supports substantially more languages than other models, in particular many low-resource languages. In addition to the text modality, SONAR supports 76 languages for speech input and speech output in English. We have also developed an experimental encoder for American Sign language (ASL).
All these encoders and decoders are freely available.555<https://github.com/facebookresearch/SONAR>
Exact listings of the supported languages can be found in the SONAR GitHub repository.

### 2.2 Data preparation

To train and evaluate the LCM, we need to convert raw text datasets into a sequence of SONAR embeddings, each one corresponding to a sentence.
Dealing with large text corpora presents several practical limitations.
First, the precise segmentation of a text into sentences can be challenging due to the presence of errors, specific formatting issues or any other sources of noise.
This requires us to apply robust automatic text segmentation techniques.
Second, some sentences (even well formed) can be very long and complex, which might negatively impact the quality of the encoded SONAR embeddings.
This is particularly prevalent for texts in the scientific domain.
In the following, we discuss strategies for sentence segmentation and
how they affect the SONAR encoding.

##### Sentence segmentation analysis

We have identified two potential sentence segmentation techniques; as we are exploring multilingual data, we focus on sentence segmenters with a large language coverage:

1. 1.

   SpaCy segmenter (SpaCy)Â (Honnibal etÂ al., [2020](#bib.bib51)) is a well established multilingual NLP toolkit that provides a rule-based approach to sentence segmentation. SpaCy is thoroughly tested for high-resource languages.
2. 2.

   Segment any Text (SaT)Â (Minixhofer etÂ al., [2023](#bib.bib77); Frohmann etÂ al., [2024](#bib.bib38)) offers a suite of models and adapters that predict sentence boundaries at the token level.
   SaT is designed to be resilient to perturbations, particularly avoiding the over-reliance on punctuation and capitalization. This is valuable in domains where these conventional markers are often missing. The quality of SaTâ€™s segmentation is however dependent on the choice of an â€œappropriateâ€ split probability threshold.

We additionally customize both methods by incorporating a maximum sentence length cap in characters. We refer to these extensions by SpaCy Capped and SaT Capped.
Long sentences are broken down into smaller, logically coherent fragments using a rule-based approach based on punctuation marks for SpaCy. For SaT, we leverage the provided splitting probability estimates to identify the next best potential split.

To measure the efficacy of a given segmenter,
we evaluate the quality of the reconstructed sentences with AutoBLEU.
It is defined as a BLEU scoreÂ (Papineni etÂ al., [2002](#bib.bib87)) comparing
the decoded text from a SONAR vector after encoding a segment,
to the the reference segment.
A good segmentation will yield segments that can be encoded and then decoded without loss of signal, and thus score a higher AutoBLEU.

For this analysis, we sample 10k documents from our pretraining datasets, representing approximately 500k sentences.
The documents are processed with each segmenter, the sentences are encoded then decoded and the AutoBLEU score is calculated.
We stratified the results based on the lengths of the original sentences.

![Refer to caption](/html/2412.08821/assets/x3.png)

Figure 3: Segmenters quality. Average Auto-BLEU scores for different sentence segmentation methods depending on sentence length, for both out of the box (left) and capped implementations (right).

As illustrated in [FigureÂ 3](#S2.F3 "In Sentence segmentation analysis â€£ 2.2 Data preparation â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space") and with a capping at 200 characters, the SaT Capped method demonstrates a slight but consistent advantage over SpaCy Capped.
Both out-of-the-box segmenters, however, exhibit significant under-performance across all sentence lengths.
This lower performance is especially pronounced for sentences exceeding 250 characters, underscoring the limitations of using the segmenters without capping.

Accordingly, we prepare the LCM training data with SaT Capped.
We discuss in [AppendixÂ A](#A1 "Appendix A Technical consideration for data preparation â€£ Large Concept Models: Language Modeling in a Sentence Representation Space") technical and engineering challenges faced when handling large amounts of SONAR embeddings.

### 2.3 Large Concept Model variants

The design of the LCM is driven by the need to conditionally generate a continuous sentence embedding.
This obviously contrasts with how current LLMs work, i.e., estimating a probability distribution over a vocabulary of discrete tokens.
A straightforward way of solving the task is to train a transformer model to generate an embedding with the objective of minimizing the MSE loss (see [SectionÂ 2.3.1](#S2.SS3.SSS1 "2.3.1 Base-LCM â€£ 2.3 Large Concept Model variants â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space")).
However, a given context may have many plausible, yet semantically different, continuations.
The model should thus be able to learn a conditional probability distribution over the continuous embedding of the next sentence.

There is a large body of work in computer vision aiming to learn such conditional probability distributions over continuous data (Dhariwal and Nichol, [2021](#bib.bib29); Rombach etÂ al., [2021](#bib.bib95)).
Models like Dall-E 3 (Betker etÂ al., [2023](#bib.bib16)) or Imagen Video (Ho etÂ al., [2022](#bib.bib50)) use a diffusion process to generate an image or video from a text prompt.
Many different real images may satisfy the same input prompt, hence the model has to learn a probability distribution over continuous pixel data.
This motivates the exploration of diffusion models for sentence embedding generation.
Two variants are presented in [SectionsÂ 2.3.3](#S2.SS3.SSS3 "2.3.3 One-Tower Diffusion LCM â€£ 2.3 Large Concept Model variants â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space") andÂ [2.3.4](#S2.SS3.SSS4 "2.3.4 Two-Tower Diffusion LCM â€£ 2.3 Large Concept Model variants â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space").
Another prevalent take on continuous data generation consists of quantizing said data to ultimately model with discrete units; we explore LCM modeling with quantization in [SectionÂ 2.3.5](#S2.SS3.SSS5 "2.3.5 Quantized LCM â€£ 2.3 Large Concept Model variants â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space").

#### 2.3.1 Base-LCM

![Refer to caption](/html/2412.08821/assets/x4.png)

Figure 4: TheBase-LCM. Illustration of the Base-LCM. At its core is a standard decoder-only Transformer surrounded with a PreNetPreNet\operatorname{PreNet} and a PostNetPostNet\operatorname{PostNet}.

Our baseline architecture for next-concept prediction is a standard decoder-only Transformer that transduces a sequence of preceding concepts (read sentence embeddings) into a sequence of future ones.
As illustrated in [FigureÂ 4](#S2.F4 "In 2.3.1 Base-LCM â€£ 2.3 Large Concept Model variants â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space"), the Base-LCM is equipped with a â€œPostNetPostNet\operatorname{PostNet}â€ and a â€œPreNetPreNet\operatorname{PreNet}â€. The PreNetPreNet\operatorname{PreNet} normalizes the input SONAR embeddings and maps them to the modelâ€™s hidden dimension dmodelsubscriptdmodel{\textnormal{d}}\_{\text{model}}.

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | PreNetâ¡(ğ±)PreNetğ±\displaystyle\operatorname{PreNet}({\mathbf{x}}) | =normalizeâ¡(ğ±)â€‹ğ–pret+ğ›pre,absentnormalizeğ±superscriptsubscriptğ–preğ‘¡subscriptğ›pre\displaystyle=\operatorname{normalize}({\mathbf{x}}){\mathbf{W}}\_{\text{pre}}^{t}+{\mathbf{b}}\_{\text{pre}}, |  | (1) |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | PostNetâ¡(ğ±)PostNetğ±\displaystyle\operatorname{PostNet}({\mathbf{x}}) | =denormalizeâ¡(ğ±ğ–postt+ğ›post),absentdenormalizesuperscriptsubscriptğ±ğ–postğ‘¡subscriptğ›post\displaystyle=\operatorname{denormalize}\left({\mathbf{x}}{\mathbf{W}}\_{\text{post}}^{t}+{\mathbf{b}}\_{\text{post}}\right), |  | (2) |

where ğ–postâˆˆâ„dSONARÃ—dmodelsubscriptğ–postsuperscriptâ„subscriptdSONARsubscriptdmodel{\mathbf{W}}\_{\text{post}}\in\mathbb{R}^{{\textnormal{d}}\_{\text{{SONAR}}}\times{\textnormal{d}}\_{\text{model}}},
ğ›postâˆˆâ„dSONARsubscriptğ›postsuperscriptâ„subscriptdSONAR{\mathbf{b}}\_{\text{post}}\in\mathbb{R}^{{\textnormal{d}}\_{\text{{SONAR}}}},
ğ–preâˆˆâ„dmodelÃ—dSONARsubscriptğ–presuperscriptâ„subscriptdmodelsubscriptdSONAR{\mathbf{W}}\_{\text{pre}}\in\mathbb{R}^{{\textnormal{d}}\_{\text{model}}\times{\textnormal{d}}\_{\text{{SONAR}}}} and
ğ›preâˆˆâ„dmodelsubscriptğ›presuperscriptâ„subscriptdmodel{\mathbf{b}}\_{\text{pre}}\in\mathbb{R}^{{\textnormal{d}}\_{\text{model}}}.

In order to learn the maps â€œnormalizenormalize\operatorname{normalize}â€ and its inverse â€œdenormalizedenormalize\operatorname{denormalize}â€ we fit a robust scaler to a set of randomly sampled SONAR vectors from different corpora and domains of text data. This scaler removes the median statistics and scales the data according to the interquartile range (IQR).

|  |  |  |  |
| --- | --- | --- | --- |
|  | normalizeâ¡(ğ±)=ğ±âˆ’ğğˆ,denormalizeâ¡(ğ±)=ğ+ğˆâ€‹ğ±.formulae-sequencenormalizeğ±ğ±ğğˆdenormalizeğ±ğğˆğ±\displaystyle\operatorname{normalize}({\mathbf{x}})=\frac{{\mathbf{x}}-{\bm{\mu}}}{{\bm{\sigma}}},\quad\operatorname{denormalize}({\mathbf{x}})={\bm{\mu}}+{\bm{\sigma}}{\mathbf{x}}. |  | (4) |

The Base-LCM is trained on the semi-supervised task of next concept prediction, that is, the model predicts the next concept ğ±^nsubscript^ğ±ğ‘›\hat{\mathbf{x}}\_{n} and its parameters ğœ½ğœ½{\bm{\theta}} are optimized to regress the ground truth next concept (ğ±nsubscriptğ±ğ‘›{\mathbf{x}}\_{n}).

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ±^n=fâ€‹(ğ±<n;ğœ½),MSEâ¡(ğ±^n,ğ±n)=â€–ğ±^nâˆ’ğ±nâ€–2.formulae-sequencesubscript^ğ±ğ‘›ğ‘“  subscriptğ±absentğ‘›ğœ½MSEsubscript^ğ±ğ‘›subscriptğ±ğ‘›superscriptnormsubscript^ğ±ğ‘›subscriptğ±ğ‘›2\displaystyle\hat{\mathbf{x}}\_{n}=f({\mathbf{x}}\_{<n};{\bm{\theta}}),\quad\operatorname{MSE}(\hat{\mathbf{x}}\_{n},{\mathbf{x}}\_{n})=\|\hat{\mathbf{x}}\_{n}-{\mathbf{x}}\_{n}\|^{2}. |  | (5) |

Given a data distribution q of documents (sequences of concepts), the training loss is evaluated as:

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„’Base-LCMâ€‹(ğœ½)=ğ”¼ğ±âˆ¼qâ€‹[âˆ‘n=1|ğ±|MSEâ¡(fâ€‹(ğ±<n;ğœ½),ğ±n)].subscriptâ„’Base-LCMğœ½subscriptğ”¼similar-toğ±ğ‘delimited-[]superscriptsubscriptğ‘›1ğ±MSEğ‘“  subscriptğ±absentğ‘›ğœ½subscriptğ±ğ‘›\displaystyle\mathcal{L}\_{\textsc{Base-LCM}}({\bm{\theta}})=\mathbb{E}\_{{\mathbf{x}}\sim q}\Big{[}\sum\_{n=1}^{|{\mathbf{x}}|}\operatorname{MSE}\left(f({\mathbf{x}}\_{<n};{\bm{\theta}}),{\mathbf{x}}\_{n}\right)\Big{]}. |  | (6) |

In order to enable the generation of variable length documents at inference time, we suffix training documents with the sentence â€œEnd of text.â€. Similar to any sentence in the document, this special suffix will be encoded with SONAR. This means that ğ±|ğ±|=eotâ†’â‰”encodeâ¡("End of text.")subscriptğ±ğ±â†’eotâ‰”encode"End of text."{\mathbf{x}}\_{|{\mathbf{x}}|}=\overrightarrow{\text{eot}}\coloneqq\operatorname{encode}(\text{"End of text."}). During inference, we implement two main early stopping mechanisms: the first one measures the similarity of the generated embedding ğ±^nsubscript^ğ±ğ‘›\hat{\mathbf{x}}\_{n} to eotâ†’â†’eot\overrightarrow{\text{eot}} and stops if the cosine similarity exceeds a threshold seotsubscriptğ‘ eots\_{\text{eot}}. The second mechanism compares the newly generated embedding ğ±^nsubscript^ğ±ğ‘›\hat{\mathbf{x}}\_{n} to the previous generation ğ±^nâˆ’1subscript^ğ±ğ‘›1\hat{\mathbf{x}}\_{n-1} and stops if their cosine similarity is higher than a threshold sprevsubscriptğ‘ prevs\_{\text{prev}}. We set both seotsubscriptğ‘ eots\_{\text{eot}} and sprevsubscriptğ‘ prevs\_{\text{prev}} to 0.9.

#### 2.3.2 Diffusion-based LCM

Diffusion-based LCMs are generative latent variable models that learn a model distribution pğœ½subscriptpğœ½{\textnormal{p}}\_{\bm{\theta}} approximating a data distribution q.
Similar to the Base-LCM, we model the diffusion LCMs as auto-regressive models that generate concepts in a document, one at a time.
The model distribution is thus expressed at each position nğ‘›n of the sequence as
pğœ½â€‹(ğ±n|ğ±<n)subscriptpğœ½conditionalsubscriptğ±ğ‘›subscriptğ±absentğ‘›{\textnormal{p}}\_{\bm{\theta}}({\mathbf{x}}\_{n}|{\mathbf{x}}\_{<n}) i.e., the generation of the next concept is conditioned on the preceding context.

In what follows we use a superscript for the denoising/diffusion step (tâˆˆ[0,1]ğ‘¡01t\in[0,1]) and a subscript (nğ‘›n) for indexing the sequence of concepts.
We simplify for a given nğ‘›n the conditional model distribution
pğœ½â€‹(ğ±n0|ğ±<n0)subscriptpğœ½conditionalsubscriptsuperscriptğ±0ğ‘›subscriptsuperscriptğ±0absentğ‘›{\textnormal{p}}\_{\bm{\theta}}({\mathbf{x}}^{0}\_{n}|{\mathbf{x}}^{0}\_{<n})
as
pğœ½â€‹(ğ±0)subscriptpğœ½superscriptğ±0{\textnormal{p}}\_{\bm{\theta}}({\mathbf{x}}^{0}),
and the conditional data distribution
qâ€‹(ğ±n0|ğ±<n0)qconditionalsubscriptsuperscriptğ±0ğ‘›subscriptsuperscriptğ±0absentğ‘›{\textnormal{q}}({\mathbf{x}}^{0}\_{n}|{\mathbf{x}}^{0}\_{<n})
as
qâ€‹(ğ±0)qsuperscriptğ±0{\textnormal{q}}({\mathbf{x}}^{0}).

Diffusion models involve two processes: a *forward* noising process and a *reverse* denoising processÂ (Ho etÂ al., [2020](#bib.bib49); Song etÂ al., [2020](#bib.bib102)):

##### Forward process and noise schedule

The forward process is a Gaussian diffusion process characterized by the marginal distribution qâ€‹(ğ±t|ğ±0)qconditionalsuperscriptğ±ğ‘¡superscriptğ±0{\textnormal{q}}({\mathbf{x}}^{t}|{\mathbf{x}}^{0}), given for every timestep tâˆˆ[0,1]ğ‘¡01t\in[0,1] as:

|  |  |  |  |
| --- | --- | --- | --- |
|  | qâ€‹(ğ±t|ğ±0)â‰”ğ’©â€‹(Î±tâ€‹ğ±0,Ïƒt2â€‹ğˆ).â‰”qconditionalsuperscriptğ±ğ‘¡superscriptğ±0ğ’©subscriptğ›¼ğ‘¡superscriptğ±0superscriptsubscriptğœğ‘¡2ğˆ\displaystyle{\textnormal{q}}({\mathbf{x}}^{t}|{\mathbf{x}}^{0})\coloneqq\mathcal{N}(\alpha\_{t}{\mathbf{x}}^{0},\sigma\_{t}^{2}{\mathbf{I}}). |  | (7) |

With the reparameterization trick, we can sample from this marginal distribution via:

|  |  |  |  |
| --- | --- | --- | --- |
|  | ğ±t=Î±tâ€‹ğ±0+Ïƒtâ€‹ÏµwhereÂ â€‹Ïµâˆ¼ğ’©â€‹(ğŸ,ğˆ)formulae-sequencesuperscriptğ±ğ‘¡subscriptğ›¼ğ‘¡superscriptğ±0subscriptğœğ‘¡bold-italic-Ïµsimilar-towhereÂ bold-italic-Ïµğ’©0ğˆ\displaystyle{\mathbf{x}}^{t}=\alpha\_{t}{\mathbf{x}}^{0}+\sigma\_{t}{\bm{\epsilon}}\quad\text{where }{\bm{\epsilon}}\sim\mathcal{N}({\mathbf{0}},{\mathbf{I}}) |  | (8) |

We use a variance-preserving forward processÂ (Karras etÂ al., [2022](#bib.bib58)) for which we have:

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î±t2=sigmoidâ¡(Î»t),Ïƒt2=sigmoidâ¡(âˆ’Î»t)=1âˆ’sigmoidâ¡(Î»t),Î»t=logâ¡(Î±t2/Ïƒt2),formulae-sequenceformulae-sequencesuperscriptsubscriptğ›¼ğ‘¡2sigmoidsubscriptğœ†ğ‘¡superscriptsubscriptğœğ‘¡2sigmoidsubscriptğœ†ğ‘¡1sigmoidsubscriptğœ†ğ‘¡subscriptğœ†ğ‘¡superscriptsubscriptğ›¼ğ‘¡2superscriptsubscriptğœğ‘¡2\displaystyle\alpha\_{t}^{2}=\operatorname{sigmoid}(\lambda\_{t}),\quad\quad\sigma\_{t}^{2}=\operatorname{sigmoid}(-\lambda\_{t})=1-\operatorname{sigmoid}(\lambda\_{t}),\quad\quad\lambda\_{t}=\log\left({\alpha\_{t}^{2}}/{\sigma\_{t}^{2}}\right), |  | (9) |

where Î»tsubscriptğœ†ğ‘¡\lambda\_{t} is the log signal-to-noise ratio (log-SNR) for timestep tğ‘¡t.

The noise schedule is a strictly monotonically decreasing function fÎ»subscriptğ‘“ğœ†f\_{\lambda} that maps from the timestep tâˆˆ[0,1]ğ‘¡01t\in[0,1] to a log-SNR level: Î»t=fÎ»â€‹(t)subscriptğœ†ğ‘¡subscriptğ‘“ğœ†ğ‘¡\lambda\_{t}=f\_{\lambda}(t).

It is common in previous work to also define the noise schedule based on a discrete variance schedule (Î²0,â€¦,Î²T)subscriptğ›½0â€¦subscriptğ›½ğ‘‡(\beta\_{0},\ldots,\beta\_{T}).
This stems from the formulation of the forward process as a discrete-time Markov chain that gradually adds Gaussian noise to the data according to said variance schedule:

|  |  |  |  |
| --- | --- | --- | --- |
|  | qâ€‹(ğ±1â€‹â€¦â€‹T|ğ±0)â‰”âˆt=1Tqâ€‹(ğ±t|ğ±tâˆ’1),qâ€‹(ğ±t|ğ±tâˆ’1)â‰”ğ’©â€‹(ğ±t;1âˆ’Î²tâ€‹ğ±tâˆ’1,Î²tâ€‹ğˆ),formulae-sequenceâ‰”qconditionalsuperscriptğ±1â€¦Tsuperscriptğ±0superscriptsubscriptproductğ‘¡1Tqconditionalsuperscriptğ±ğ‘¡superscriptğ±ğ‘¡1â‰”qconditionalsuperscriptğ±ğ‘¡superscriptğ±ğ‘¡1ğ’©  superscriptğ±ğ‘¡1subscriptğ›½ğ‘¡superscriptğ±ğ‘¡1subscriptğ›½ğ‘¡ğˆ\displaystyle{\textnormal{q}}({\mathbf{x}}^{1\ldots{\textnormal{T}}}|{\mathbf{x}}^{0})\coloneqq\prod\_{t=1}^{\textnormal{T}}{\textnormal{q}}({\mathbf{x}}^{t}|{\mathbf{x}}^{t-1}),\quad{\textnormal{q}}({\mathbf{x}}^{t}|{\mathbf{x}}^{t-1})\coloneqq\mathcal{N}({\mathbf{x}}^{t};\sqrt{1-\beta\_{t}}{\mathbf{x}}^{t-1},\beta\_{t}{\mathbf{I}}), |  | (10) |

where to simplify the notation, ğ±tsuperscriptğ±ğ‘¡{\mathbf{x}}^{t} is short for ğ±t/Tsuperscriptğ±ğ‘¡ğ‘‡{\mathbf{x}}^{t/T} as the timesteps are now discretized.

From the variance schedule (Î²t)tsubscriptsubscriptğ›½ğ‘¡ğ‘¡(\beta\_{t})\_{t}, the noise schedule can be expressed as:

|  |  |  |  |
| --- | --- | --- | --- |
|  | Î±t2=âˆs=1t(1âˆ’Î²s).superscriptsubscriptğ›¼ğ‘¡2superscriptsubscriptproductğ‘ 1ğ‘¡1subscriptğ›½ğ‘ \displaystyle\alpha\_{t}^{2}=\prod\_{s=1}^{t}(1-\beta\_{s}). |  | (11) |

Following Kingma and Gao ([2024](#bib.bib60)), for any given noise schedule, we visualize the distribution over noise levels pâ€‹(Î»)=âˆ’dâ€‹t/dâ€‹Î»ğ‘ğœ†ğ‘‘ğ‘¡ğ‘‘ğœ†p(\lambda)=-dt/d\lambda in order to characterize how much time we are spending at every noise level during training.

In this work, we consider three types of noise schedules:

Cosine.
:   The schedule formulated in Nichol and Dhariwal ([2021](#bib.bib82)) as:

    |  |  |  |  |
    | --- | --- | --- | --- |
    |  | Î±t2=f(t)/f(0),whereÂ f(t)=cos2(t+s1+s.Ï€2),Â whereÂ s=0.008.\displaystyle\alpha\_{t}^{2}=f(t)/f(0),\text{where }f(t)=\cos^{2}\left(\frac{t+s}{1+s}.\frac{\pi}{2}\right),\text{ where }s=0.008. |  | (12) |

Quadratic.
:   The schedule introduced in Ho etÂ al. ([2020](#bib.bib49)) where the variances (Î²t)tsubscriptsubscriptğ›½ğ‘¡ğ‘¡(\beta\_{t})\_{t} are set to constants increasing quadratically from Î²0subscriptğ›½0\beta\_{0} to Î²1subscriptğ›½1\beta\_{1}.

    |  |  |  |  |
    | --- | --- | --- | --- |
    |  | Î²t/T=(Î²0+tT.(Î²1âˆ’Î²0))2.\displaystyle\beta\_{t/{\textnormal{T}}}=\left(\sqrt{\beta\_{0}}+\frac{t}{{\textnormal{T}}}.\left(\sqrt{\beta\_{1}}-\sqrt{\beta\_{0}}\right)\right)^{2}. |  | (13) |

Sigmoid.
:   We introduce in this work, the *sigmoid* schedule as a means to study the impact of the SNR distribution on the training of our models. The schedule is parametrized by two hyper-parameters (Î³,Î´)\gamma,\delta) and is defined as:

    |  |  |  |  |
    | --- | --- | --- | --- |
    |  | Î±t2=fâ€‹(t)/fâ€‹(0),Â whereÂ â€‹fâ€‹(t)=sigmoidâ¡(Î´âˆ’Î³â€‹logitâ¡(t)),formulae-sequencesuperscriptsubscriptğ›¼ğ‘¡2ğ‘“ğ‘¡ğ‘“0Â whereÂ ğ‘“ğ‘¡sigmoidğ›¿ğ›¾logitğ‘¡\displaystyle\alpha\_{t}^{2}=f(t)/f(0),\text{ where }f(t)=\operatorname{sigmoid}\left(\delta-\gamma\operatorname{logit}(t)\right), |  | (14) |

    where â€œsigmoidsigmoid\operatorname{sigmoid}â€ is the sigmoid function sigmoidâ¡xâ†¦ex/(ex+1)maps-tosigmoidğ‘¥superscriptğ‘’ğ‘¥superscriptğ‘’ğ‘¥1\operatorname{sigmoid}x\mapsto e^{x}/(e^{x}+1)
    and â€œlogitlogit\operatorname{logit}â€ its inverse function logit:xâ†¦logâ¡(x/(1âˆ’x)):logitmaps-toğ‘¥ğ‘¥1ğ‘¥\operatorname{logit}:x\mapsto\log(x/(1-x)). The hyper-parameter Î³ğ›¾\gamma controls the scale of the log-SNR distribution pâ€‹(Î»)ğ‘ğœ†p(\lambda) and Î´ğ›¿\delta its center (see [FigureÂ 5](#S2.F5 "In Forward process and noise schedule â€£ 2.3.2 Diffusion-based LCM â€£ 2.3 Large Concept Model variants â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space")).

In all our experiments, we follow Lin etÂ al. ([2024](#bib.bib71)) and rescale the variance schedule (Î²1,â€¦â€‹Î²T)subscriptğ›½1â€¦subscriptğ›½ğ‘‡(\beta\_{1},\ldots\beta\_{T}) to enforce zero terminal SNR i.e., Î²T=1subscriptğ›½T1\beta\_{\textnormal{T}}=1.

![Refer to caption](/html/2412.08821/assets/x5.png)

![Refer to caption](/html/2412.08821/assets/x6.png)

![Refer to caption](/html/2412.08821/assets/x7.png)

Figure 5: Noise schedules. Illustrations of the different noise schedules explored in this work. Our default schedule being cosine. Quadratic-1 is characterized by (Î²0=0.001,Î²T=0.0012)formulae-sequencesubscriptğ›½00.001subscriptğ›½T0.0012(\beta\_{0}=0.001,\beta\_{\textnormal{T}}=0.0012) and Quadratic-2 by (Î²0=0.02,Î²T=0.022)formulae-sequencesubscriptğ›½00.02subscriptğ›½T0.022(\beta\_{0}=0.02,\beta\_{\textnormal{T}}=0.022) For each schedule we visualize the curve of (Î±t)tsubscriptsubscriptğ›¼ğ‘¡ğ‘¡(\alpha\_{t})\_{t} (see [EquationÂ 8](#S2.E8 "In Forward process and noise schedule â€£ 2.3.2 Diffusion-based LCM â€£ 2.3 Large Concept Model variants â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space")), the curve of the log-SNR and the associated distribution over noises levels pâ€‹(Î»)ğ‘ğœ†p(\lambda)Â (Kingma and Gao, [2024](#bib.bib60)).

##### Reverse process and objective function

The joint distribution of the diffusion model pğœ½â€‹(ğ±0â€‹â€¦â€‹1)subscriptpğœ½superscriptğ±0â€¦1{\textnormal{p}}\_{\bm{\theta}}({\mathbf{x}}^{0\ldots 1}) is called the reverse process and is defined as a Markov chain with learned Gaussian transitions starting at pâ€‹(ğ±1)=ğ’©â€‹(ğŸ,ğˆ)psuperscriptğ±1ğ’©0ğˆ{\textnormal{p}}({\mathbf{x}}^{1})=\mathcal{N}({\mathbf{0}},{\mathbf{I}}). In its discretized form:

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | pğœ½â€‹(ğ±0:T)subscriptpğœ½superscriptğ±:0T\displaystyle{\textnormal{p}}\_{\bm{\theta}}({\mathbf{x}}^{0:{\textnormal{T}}}) | â‰”pâ€‹(ğ±T)â€‹âˆt=1Tpğœ½â€‹(ğ±tâˆ’1|ğ±t),pğœ½â€‹(ğ±tâˆ’1|ğ±t)â‰”ğ’©â€‹(ğ±tâˆ’1;ğğœ½â€‹(ğ±t,t),ğšºğœ½â€‹(ğ±t,t)),formulae-sequenceâ‰”absentpsuperscriptğ±Tsuperscriptsubscriptproductğ‘¡1Tsubscriptpğœ½conditionalsuperscriptğ±ğ‘¡1superscriptğ±ğ‘¡â‰”subscriptpğœ½conditionalsuperscriptğ±ğ‘¡1superscriptğ±ğ‘¡ğ’©  superscriptğ±ğ‘¡1subscriptğğœ½superscriptğ±ğ‘¡ğ‘¡subscriptğšºğœ½superscriptğ±ğ‘¡ğ‘¡\displaystyle\coloneqq{\textnormal{p}}({\mathbf{x}}^{{\textnormal{T}}})\prod\_{t=1}^{\textnormal{T}}{\textnormal{p}}\_{\bm{\theta}}({\mathbf{x}}^{t-1}|{\mathbf{x}}^{t}),\qquad{\textnormal{p}}\_{\bm{\theta}}({\mathbf{x}}^{t-1}|{\mathbf{x}}^{t})\coloneqq\mathcal{N}({\mathbf{x}}^{t-1};{\bm{\mu}}\_{\bm{\theta}}({\mathbf{x}}^{t},t),{\mathbf{\Sigma}}\_{\bm{\theta}}({\mathbf{x}}^{t},t)), |  | (15) |

where ğğœ½subscriptğğœ½{\bm{\mu}}\_{\bm{\theta}} and ğšºğšº{\mathbf{\Sigma}} are predicted statistics. ğšºğšº{\mathbf{\Sigma}} is set to to the constant Ïƒt2â€‹ğˆsuperscriptsubscriptğœğ‘¡2ğˆ\sigma\_{t}^{2}{\mathbf{I}} (matching the transitions of the forward process).
ğğœ½subscriptğğœ½{\bm{\mu}}\_{\bm{\theta}} can be decomposed into a linear combination of ğ±tâˆ’1superscriptğ±ğ‘¡1{\mathbf{x}}^{t-1} and a noise approximation model Ïµğœ½subscriptbold-italic-Ïµğœ½{\bm{\epsilon}}\_{\bm{\theta}}. This prediction method is dubbed Ïµbold-italic-Ïµ{\bm{\epsilon}}-predictionÂ (Ho etÂ al., [2020](#bib.bib49); Nichol and Dhariwal, [2021](#bib.bib82); Nichol etÂ al., [2022](#bib.bib83)). In this work we adopt ğ±0superscriptğ±0{\mathbf{x}}^{0}-prediction i.e., we predict the noiseless state and optimize the simple reconstruction loss:

|  |  |  |  |
| --- | --- | --- | --- |
|  | â„’â€‹(ğœ½)â‰”ğ”¼tâˆ¼ğ’°â€‹(0,1)â€‹[Ï‰â€‹(t)â€‹â„’â€‹(t,ğœ½)],â„’â€‹(t,ğœ½)â‰”ğ”¼ğ±0,Ïµâ€‹[â€–ğ±0âˆ’ğğœ½â€‹(Î±tâ€‹ğ±0+Ïƒtâ€‹Ïµ,t)â€–22].formulae-sequenceâ‰”â„’ğœ½subscriptğ”¼similar-toğ‘¡ğ’°01delimited-[]ğœ”ğ‘¡â„’ğ‘¡ğœ½â‰”â„’ğ‘¡ğœ½subscriptğ”¼  superscriptğ±0bold-italic-Ïµdelimited-[]superscriptsubscriptnormsuperscriptğ±0subscriptğğœ½subscriptğ›¼ğ‘¡superscriptğ±0subscriptğœğ‘¡bold-italic-Ïµğ‘¡22\displaystyle\mathcal{L}({\bm{\theta}})\coloneqq\mathbb{E}\_{t\sim\mathcal{U}(0,1)}\big{[}\omega(t)\mathcal{L}(t,{\bm{\theta}})\big{]},\quad\mathcal{L}(t,{\bm{\theta}})\coloneqq\mathbb{E}\_{{\mathbf{x}}^{0},{\bm{\epsilon}}}\Bigl{[}\big{\|}{\mathbf{x}}^{0}-{\bm{\mu}}\_{\bm{\theta}}(\alpha\_{t}{\mathbf{x}}^{0}+\sigma\_{t}{\bm{\epsilon}},t)\big{\|}\_{2}^{2}\Bigr{]}. |  | (16) |

Different weighting strategies for the reconstruction loss were proposed in the literatureÂ (Ho etÂ al., [2020](#bib.bib49); Salimans and Ho, [2022](#bib.bib97); Hang etÂ al., [2023](#bib.bib44)). In this work, we default to the simple reconstruction loss (Ï‰(t)=1,âˆ€t)\omega(t)=1,\;\forall t) and we experiment with a clamped-SNR weighting strategy:

|  |  |  |  |
| --- | --- | --- | --- |
|  | Ï‰â€‹(t)=maxâ¡(minâ¡(expâ¡(Î»t),Î»max),Î»min),Î»t=logâ¡(Î±t2/Ïƒt2),formulae-sequenceğœ”ğ‘¡subscriptğœ†ğ‘¡subscriptğœ†subscriptğœ†subscriptğœ†ğ‘¡superscriptsubscriptğ›¼ğ‘¡2superscriptsubscriptğœğ‘¡2\displaystyle\omega(t)=\max(\min(\exp(\lambda\_{t}),\lambda\_{\max}),\lambda\_{\min}),\,\lambda\_{t}=\log(\alpha\_{t}^{2}/\sigma\_{t}^{2}), |  | (17) |

which is a generalization of Salimans and Ho ([2022](#bib.bib97))â€™s truncated-SNR weighting and Hang etÂ al. ([2023](#bib.bib44))â€™s min-SNR strategy where the SNR is clamped between a min- and max-value Î»minsubscriptğœ†\lambda\_{\min} and Î»maxsubscriptğœ†\lambda\_{\max}.

Additionally, we consider a weighting strategy that factors in the quality of the sample ğ±0superscriptğ±0{\mathbf{x}}^{0}. We use as sample weight a scalar Ï‰â€‹(ğ±0)âˆˆ[0,1]ğœ”superscriptğ±001\omega({\mathbf{x}}^{0})\in[0,1] correlated with the sampleâ€™s fragility score i.e., how easy it is to reconstruct a noised sample (see [SectionÂ 2.5.2](#S2.SS5.SSS2 "2.5.2 Fragility of SONAR space â€£ 2.5 Analysis â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space")). Fragile samples will be assigned a smaller weight and thus contribute less to the objective function.

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | â„’fragilityâ€‹(ğœ½)subscriptâ„’fragilityğœ½\displaystyle\mathcal{L}\_{\operatorname{fragility}}({\bm{\theta}}) | â‰”ğ”¼tâˆ¼ğ’°â€‹(0,1),ğ±0,Ïµâ€‹[Ï‰â€‹(ğ±0)â€‹â€–ğ±0âˆ’ğğœ½â€‹(Î±tâ€‹ğ±0+Ïƒtâ€‹Ïµ,t)â€–22],â‰”absentsubscriptğ”¼similar-toğ‘¡  ğ’°01superscriptğ±0bold-italic-Ïµdelimited-[]ğœ”superscriptğ±0superscriptsubscriptnormsuperscriptğ±0subscriptğğœ½subscriptğ›¼ğ‘¡superscriptğ±0subscriptğœğ‘¡bold-italic-Ïµğ‘¡22\displaystyle\coloneqq\mathbb{E}\_{t\sim\mathcal{U}(0,1),{\mathbf{x}}^{0},{\bm{\epsilon}}}\Bigl{[}\omega({\mathbf{x}}^{0})\leavevmode\nobreak\ \big{\|}{\mathbf{x}}^{0}-{\bm{\mu}}\_{\bm{\theta}}(\alpha\_{t}{\mathbf{x}}^{0}+\sigma\_{t}{\bm{\epsilon}},t)\big{\|}\_{2}^{2}\Bigr{]}, |  | (18) |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | Ï‰â€‹(ğ±0)ğœ”superscriptğ±0\displaystyle\omega({\mathbf{x}}^{0}) | =sigmoidâ¡(aâ€‹fragilityâ¡(ğ±0)+b),absentsigmoidğ‘fragilitysuperscriptğ±0ğ‘\displaystyle=\operatorname{sigmoid}(a\leavevmode\nobreak\ \operatorname{fragility}({\mathbf{x}}^{0})+b), |  | (19) |

where a<0ğ‘0a<0 and bğ‘b are hyper-parameters to tune.

##### Classifier-free diffusion guidance for the LCM

Classifier-free diffusion guidanceÂ (Ho and Salimans, [2022](#bib.bib48)) consists of jointly training a conditional and an unconditional diffusion model. The resulting conditional and unconditional score
estimates are combined at inference time to achieve a trade-off between sample quality and diversity. This combined score is defined as follows:

|  |  |  |  |
| --- | --- | --- | --- |
|  | âˆ‡xlogÎ³â¡pâ€‹(x|y)=(1âˆ’Î³)â€‹âˆ‡xlogâ¡pâ€‹(x)+Î³â€‹âˆ‡xlogâ¡pâ€‹(x|y),subscriptâˆ‡ğ‘¥subscriptğ›¾ğ‘conditionalğ‘¥ğ‘¦1ğ›¾subscriptâˆ‡ğ‘¥ğ‘ğ‘¥ğ›¾subscriptâˆ‡ğ‘¥ğ‘conditionalğ‘¥ğ‘¦\displaystyle\nabla\_{x}\log\_{\gamma}p(x|y)=(1-\gamma)\nabla\_{x}\log p(x)+\gamma\nabla\_{x}\log p(x|y), |  | (20) |

where ğ²ğ²{\mathbf{y}} is the conditioning variable, in our case the sequence of preceding embeddings (ğ±1,â€¦â€‹ğ±nâˆ’1)subscriptğ±1â€¦subscriptğ±ğ‘›1({\mathbf{x}}\_{1},\ldots{\mathbf{x}}\_{n-1}) when denoising ğ±nsubscriptğ±ğ‘›{\mathbf{x}}\_{n}.

The hyper-parameter Î³ğ›¾\gamma controls the contribution of the conditional score; For Î³=0ğ›¾0\gamma=0, this is equivalent to an unconditional model, and for Î³=1ğ›¾1\gamma=1, it is a fully conditional model. In practice for vision models, Î³ğ›¾\gamma is set to a value greater than 1, thus amplifying the signal from the conditioning model.

##### Inference

At inference time, the reverse process is applied.
ğ±Tsuperscriptğ±T{\mathbf{x}}^{{\textnormal{T}}} is obtained by sampling a random noise from pâ€‹(ğ±T)=ğ’©â€‹(ğŸ,ğˆ)psuperscriptğ±Tğ’©0ğˆ{\textnormal{p}}({\mathbf{x}}^{{\textnormal{T}}})=\mathcal{N}({\mathbf{0}},{\mathbf{I}}), and is then iteratively denoised by taking steps in the direction of the score function (i.e., the direction in which the log-likelihood increases fastest).
Additional noise is added during the process in order to avoid falling down into modes of the distribution.

Practically, we start from ğ±Tâˆ¼ğ’©â€‹(ğŸ,Ïƒinit2â€‹ğˆ)similar-tosuperscriptğ±Tğ’©0superscriptsubscriptğœinit2ğˆ{\mathbf{x}}^{{\textnormal{T}}}\sim\mathcal{N}({\mathbf{0}},\sigma\_{\text{init}}^{2}{\mathbf{I}}) and find that the quality of the sampled output is sensitive to the initial noise scale Ïƒinitsubscriptğœinit\sigma\_{\text{init}}.

Although we train the model on a large number of discretized timesteps, e.g., T=100T100{\textnormal{T}}{=}100, we only generate with a smaller number of steps, e.g., S=40S40{\textnormal{S}}{=}40, at inference via accelerated generation processesÂ (Song etÂ al., [2020](#bib.bib102)). We select the sample steps following the trailing method of Â Lu etÂ al. ([2022](#bib.bib75)) as it is found to be more efficient for smaller steps SÂ (Lin etÂ al., [2024](#bib.bib71)). That is we generate along the sampled steps
(Ï„1,â€¦â€‹Ï„S)=roundâ€‹(flipâ€‹(arangeâ€‹(T,0,âˆ’T/S)))subscriptğœ1â€¦subscriptğœSroundfliparangeT0TS(\tau\_{1},\ldots\tau\_{\textnormal{S}})=\textrm{round}(\textrm{flip}(\textrm{arange}({\textnormal{T}},0,-{\textnormal{T}}/{\textnormal{S}}))).
During inference, we employ the classifier-free guidance rescaling technique ofÂ Lin etÂ al. ([2024](#bib.bib71)) proven to alleviate the image over-exposure problem encountered in image synthesis diffusion models as the terminal SNR approaches zero. We denote with gscalesubscriptğ‘”scaleg\_{\text{scale}} and grescalesubscriptğ‘”rescaleg\_{\text{rescale}} the guidance scale and guidance rescale factors used at inference.

FollowingÂ Ning etÂ al. ([2023](#bib.bib84)), we perform Epsilon-scaling at inference time as it is shown to alleviate the exposure bias problem in diffusion models. In its simplified version, this is a training-free method that consists of scaling down the over-predicted magnitude of error by a scalar Î»epssubscriptğœ†eps\lambda\_{\text{eps}}.

We describe in [SectionÂ 2.3.3](#S2.SS3.SSS3 "2.3.3 One-Tower Diffusion LCM â€£ 2.3 Large Concept Model variants â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space") and [SectionÂ 2.3.4](#S2.SS3.SSS4 "2.3.4 Two-Tower Diffusion LCM â€£ 2.3 Large Concept Model variants â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space") two variants of diffusion LCM: One-Tower and Two-Tower.

![Refer to caption](/html/2412.08821/assets/x8.png)

Figure 6: Inference with diffusion-based LCMs. In the left-hand side, an illustration of the One-Tower LCM and on the right-hand side an illustration of the Two-Tower LCM.

![Refer to caption](/html/2412.08821/assets/x9.png)

Figure 7: Training of One-Tower diffusion LCM. Interleaving the clean and noisy embeddings and sampling different diffusion timesteps allows for efficient training.

#### 2.3.3 One-Tower Diffusion LCM

This model, depicted in the left panel of [FigureÂ 6](#S2.F6 "In Inference â€£ 2.3.2 Diffusion-based LCM â€£ 2.3 Large Concept Model variants â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space"), consists of a single transformer backbone whose task is to predict the clean next sentence embedding ğ±n0subscriptsuperscriptğ±0ğ‘›{\mathbf{x}}^{0}\_{n} given a noisy input ğ±ntsubscriptsuperscriptğ±ğ‘¡ğ‘›{\mathbf{x}}^{t}\_{n}, conditioned on previous clean sentence embeddings ğ±<n0subscriptsuperscriptğ±0absentğ‘›{\mathbf{x}}^{0}\_{<n}.
During training, self-attention can be dropped with a certain probability
for unconditional training.
This enables classifier-free guidance at inference time (see [SectionÂ 2.3.2](#S2.SS3.SSS2.Px3 "Classifier-free diffusion guidance for the LCM â€£ 2.3.2 Diffusion-based LCM â€£ 2.3 Large Concept Model variants â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space") for details).

Each input embedding is concatenated with the corresponding diffusion timestep embedding.
The learned position embeddings are added to the input vectors prior to being fed to LCM.
The backbone utilizes a causal multi-head self-attention.

For efficient training, the model is trained to predict each and every sentence in a document at once.
As depicted in [FigureÂ 7](#S2.F7 "In Inference â€£ 2.3.2 Diffusion-based LCM â€£ 2.3 Large Concept Model variants â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space"), during the diffusion process, the model attends to the clean sentences in the context using causal multi-head attention layers.
The input is specially prepared by interleaving the noisy (blue) and clean (light blue) sentence embeddings, and the attention mask is prepared accordingly to only attend to the clean sentence embeddings (gray arrows).

#### 2.3.4 Two-Tower Diffusion LCM

This model, depicted in the right panel of [FigureÂ 6](#S2.F6 "In Inference â€£ 2.3.2 Diffusion-based LCM â€£ 2.3 Large Concept Model variants â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space"), separates the encoding of the preceding context from the diffusion of the next embedding.
A first model, labeled *contextualizer*, takes as input the context vectors x<nsubscriptğ‘¥absentğ‘›x\_{<n} and encodes them causally i.e., we apply a decoder-only Transformer with causal self-attention.
The outputs of the contextualizer are then fed to a second model dubbed *denoiser*, which predicts the clean next sentence embedding ğ±n0subscriptsuperscriptğ±0ğ‘›{\mathbf{x}}^{0}\_{n} by iteratively denoising the latent ğ±n1âˆ¼ğ’©â€‹(ğŸ,ğˆ)similar-tosubscriptsuperscriptğ±1ğ‘›ğ’©0ğˆ{\mathbf{x}}^{1}\_{n}\sim\mathcal{N}({\mathbf{0}},{\mathbf{I}}).
The denoiser consists of a stack of Transformer blocks with cross-attention block to attend over the encoded context.
Both the denoiser and the contextualizer share the same Transformer hidden dimension dmodelsubscriptdmodel{\textnormal{d}}\_{\text{model}}.
Each block of each Transformer layer in the denoiser (including the cross-attention layer) is modulated with adaptive layer norm (AdaLNAdaLN\operatorname{AdaLN}, Â Perez etÂ al. ([2018](#bib.bib90)); Peebles and Xie ([2023](#bib.bib89))). The AdaLNAdaLN\operatorname{AdaLN} modulator of Two-Tower regresses channel-wise scale (ğœ¸ğœ¸{\bm{\gamma}}), shift (ğœ·ğœ·{\bm{\beta}}) and residual gates (ğœ¶ğœ¶{\bm{\alpha}}) from the embedding of the current diffusion timestep tğ‘¡t.

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | [ğœ·,ğœ¸,ğœ¶]  ğœ·ğœ¸ğœ¶\displaystyle[{\bm{\beta}},{\bm{\gamma}},{\bm{\alpha}}] | =SiLUâ¡(embedâ¡(t))â€‹ğ–t+ğ›,absentSiLUembedğ‘¡superscriptğ–ğ‘¡ğ›\displaystyle=\operatorname{SiLU}(\operatorname{embed}(t)){\mathbf{W}}^{t}+{\mathbf{b}}, |  | (21) |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | ğ²ğ²\displaystyle{\mathbf{y}} | =ğ±+ğœ¶â€‹Blockâ¡((1+ğœ¸)â€‹ğ±+ğœ·),absentğ±ğœ¶Block1ğœ¸ğ±ğœ·\displaystyle={\mathbf{x}}+{\bm{\alpha}}\,\operatorname{Block}((1+{\bm{\gamma}})\,{\mathbf{x}}+{\bm{\beta}}), |  | (22) |

Following Peebles and Xie ([2023](#bib.bib89)) and Goyal ([2017](#bib.bib42)) we initialize each residual block in a Transformer layer (â€œBlockBlock\operatorname{Block}â€) with the identity function via initializing ğ–ğ–{\mathbf{W}} and bğ‘b in [EquationÂ 21](#S2.E21 "In 2.3.4 Two-Tower Diffusion LCM â€£ 2.3 Large Concept Model variants â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space") to zero.
The diffusion timestep tğ‘¡t is embedded using a 256-dimensional frequency embeddingÂ (Dhariwal and Nichol, [2021](#bib.bib29); Peebles and Xie, [2023](#bib.bib89)) followed by a two-layer MLP with SiLUSiLU\operatorname{SiLU} as activation function. â€œembedembed\operatorname{embed}â€ maps to the denoiserâ€™s hidden dimension dmodelsubscriptdmodel{\textnormal{d}}\_{\text{model}}. The self-attention layers in the denoiser do only attend to the current position i.e., we do not attend to the preceding noised context. The self-attention layers were kept for consistency with a standard Transformer block and for the possible extension of denoising multiple vectors at once.

![Refer to caption](/html/2412.08821/assets/x10.png)

Figure 8: Training Two-Tower diffusion LCM. On the left panel, a Two-Tower forward pass in training time in order to denoise multiple embeddings in parallel. On the right side panel a visualization of the denoiserâ€™s cross-attention masks with the red highlighted row signaling a sample dropped to train the denoiser unconditionally. (h1,â€¦,h4)subscriptâ„1â€¦subscriptâ„4(h\_{1},\ldots,h\_{4}) denotes the sequence of intermediate representations in the denoiser right before the cross-attention layer.

###### Two-Tower training.

At training time, Two-Towerâ€™s parameters are optimized for the next-sentence prediction task on unsupervised sequences of embeddings. The causal embeddings from the contextualizer are shifted by one position in the denoiser and a causal mask is used in its cross-attention layers. A zero vector is prepended to the context vectors to enable the prediction of the first position in the sequence (see [FigureÂ 8](#S2.F8 "In 2.3.4 Two-Tower Diffusion LCM â€£ 2.3 Large Concept Model variants â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space")).
To train the model both conditionally and unconditionally in preparation for inference with classifier-free guidance scaling, we drop random rows from the cross-attention mask with a rate of pcfgsubscriptpcfg{\textnormal{p}}\_{\text{cfg}} and denoise the corresponding positions with only the zero vector as context.

#### 2.3.5 Quantized LCM

Two major approaches currently stand to deal with continuous data generation in the image or speech generation fields: one is diffusion modeling, the other is learning quantization of the data before modeling on top of these discrete units.

In addition, the text modality remains discrete, and despite dealing with continuous representations in the SONAR space, all possible text sentences (of less than a given number of characters) are a cloud of points rather than a real continuous distribution in the SONAR space.
These considerations motivate the exploration of quantization of SONAR representations and then modeling on these discrete units to address the next sentence prediction task. Finally, following such an approach enables the natural use of temperature, top-p or top-k sampling, to control the level of randomness and diversity in the sampling of the next sentence representation.

In this section, we learn residual quantizers for the SONAR space, and then build a Quantized Large Concept Model based on these discrete units. We tried to come up with an architecture as close as the diffusion LCM models, to be able to compare approaches.

##### Quantization of SONAR space.

We use Residual Vector Quantization (RVQ; Zeghidour etÂ al. ([2021](#bib.bib121))) as a coarse-to-fine quantization technique to discretize SONAR representations. Vector quantization maps continuous input embeddings to the nearest entry in a learnt codebook. RVQ iteratively quantize residual errors from previous quantizations using additional codebook for each iteration. We use FAISS implementationÂ (Douze etÂ al., [2024](#bib.bib31))
which performs iterative k-means clustering of residuals. We use the Improved Residual Vector Quantization (IRVQ) method from Liu etÂ al. ([2015](#bib.bib72)), with a beam size of 1 for memory efficiency. We trained the RVQ codebooks on 15 million English sentences extracted from Common Crawl using ncodebooks=64subscriptğ‘›codebooks64n\_{\text{codebooks}}=64 number of quantizers with nunits-per-codebook=8192subscriptğ‘›units-per-codebook8192n\_{\text{units-per-codebook}}=8192 units per codebook.

One property of RVQ is that the cumulative sum of centroid embeddings of the first codebooks are an intermediate coarse approximation of input SONAR vectors. In that way, we can report the evolution of auto-encoding BLEU scores with the increasing number of codebooks used to quantize SONAR embeddings, before using the SONAR text decoder to decode quantized embeddings. We notice in [FigureÂ 9](#S2.F9 "Figure 9 â€£ Quantization of SONAR space. â€£ 2.3.5 Quantized LCM â€£ 2.3 Large Concept Model variants â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space") that auto-encoding BLEU consistently improves as the number of codebooks increases , reaching around 70% of the auto-encoding BLEU score achieved with continuous SONAR embeddings, when using all 64 codebooks.

![Refer to caption](/html/2412.08821/assets/x11.png)

Figure 9: Auto-encoding BLEU scores on FLORES devtest set, encoding sentences with SONAR encoder, quantizing with a varying number of codebooks, dequantizing and decoding with SONAR decoder.

##### Finetuning the SONAR decoder on quantized representations.

We fine-tuned SONAR decoder on quantized representations to adjust it for the space created by the quantizers on 1.2M English sentences.
To make the decoder more robust against residual representations from intermediate codebooks, we randomly select a codebook number kâˆˆ[23â‹…ncodebooks,ncodebooks]k\in\bigl{[}\frac{2}{3}\cdot n\_{\text{codebooks}},n\_{\text{codebooks}}\bigl{]} during fine-tuning, with probability p=0.3ğ‘0.3p=0.3, and use the quantized representation with codebooks up to kğ‘˜k.
[FigureÂ 9](#S2.F9 "In Quantization of SONAR space. â€£ 2.3.5 Quantized LCM â€£ 2.3 Large Concept Model variants â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space") shows the improvement in auto-encoding performance when the decoder is adapted to quantized representations.

##### Quant-LCM architecture.

In the same spirit of diffusion LCM, we aim at coarse-to-fine generation of SONAR embeddings conditioned on left-context sentences. However, we do not follow a denoising task as in diffusion modeling, but an iterative generation of SONAR embeddings based on intermediate quantized representations instead. In order to generate a SONAR embedding conditioned on left-context sentences, the Quant-LCM model starts with the intermediate representation as a vector filled with zeros. We iteratively add to this intermediate representation the predicted residual centroid embeddings. In that way, the predicted SONAR embeddings are iteratively refined based on the growing cumulative sum of centroid embeddings of first codebooks, until all codebooks have been seen. We used the One-Tower architecture for Quant-LCM experiments even though it could be trained with Two-Tower architecture too. Compared to the diffusion LCM, noisy input representations are replaced with intermediate quantized representations and diffusion timestep embeddings as input are replaced by codebook index embeddings.

##### Discrete targets.

Following previous work on modeling discrete units from residual quantizers (Wang etÂ al., [2023](#bib.bib113); Rubenstein etÂ al., [2023](#bib.bib96); Lee etÂ al., [2022](#bib.bib65)), a Quant-LCM can be trained to predict the unit from the next codebook, parameterized with a softmax output layer. For parameter efficiency, we do not use ncodebooksâ‹…nunits-per-codebookâ‹…subscriptğ‘›codebookssubscriptğ‘›units-per-codebookn\_{\text{codebooks}}\cdot n\_{\text{units-per-codebook}} unique indices as discrete targets which would imply ncodebooksâ‹…nunits-per-codebookâ‹…subscriptğ‘›codebookssubscriptğ‘›units-per-codebookn\_{\text{codebooks}}\cdot n\_{\text{units-per-codebook}} output dimensions, but only nunits-per-codebooksubscriptğ‘›units-per-codebookn\_{\text{units-per-codebook}} output dimensions while inputting the information of the codebook index to the model. At training time, similarly to diffusion LCM training, we randomly sample codebook index kğ‘˜k between 1 and ncodebookssubscriptğ‘›codebooksn\_{\text{codebooks}}, and compute the cumulative sum of centroid embeddings of the first kâˆ’1ğ‘˜1k{-}1 codebooks as input. We use the unit from codebook kğ‘˜k of the target embedding as target index for cross entropy loss computation. At inference time, we iteratively predict the unit from the next codebook, get the corresponding centroid embedding and add it to the current intermediate representation as additional predicted residual embedding. Finally, we also enable classifier-free guidance on logits at inference timeÂ (Gafni etÂ al., [2022](#bib.bib39))
by randomly dropping left-context conditioning during training as previously described in [SectionÂ 2.3.3](#S2.SS3.SSS3 "2.3.3 One-Tower Diffusion LCM â€£ 2.3 Large Concept Model variants â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space"). This modeling approach with discrete targets is dubbed Quant-LCM-d in the following sections. The improved SONAR decoder for quantized representations is used to bridge the compression gap coming from SONAR quantization in following ablation studies when using Quant-LCM-d.

##### Continuous targets.

We also explored a modeling approach that predicts continuous target SONAR vectors based on left-context sentences and intermediate quantized representation of the target vector, minimizing the Mean Squared Error between prediction and target embeddings. At inference time, we can either iteratively add the closest centroid embedding based on the predicted residual ğ«^^ğ«\hat{{\mathbf{r}}} or sample a centroid ğœisubscriptğœğ‘–{\mathbf{c}}\_{i} from the following distribution:

|  |  |  |  |
| --- | --- | --- | --- |
|  | pâ€‹(ğœi|ğ«^)=eâˆ’Î²â‹…â€–ğœiâˆ’ğ«^â€–2âˆ‘keâˆ’Î²â‹…â€–ğœkâˆ’ğ«^â€–2,pconditionalsubscriptğœğ‘–^ğ«superscriptğ‘’â‹…ğ›½subscriptnormsubscriptğœğ‘–^ğ«2subscriptğ‘˜superscriptğ‘’â‹…ğ›½subscriptnormsubscriptğœğ‘˜^ğ«2\displaystyle{\textnormal{p}}({\mathbf{c}}\_{i}|\hat{\mathbf{r}})=\dfrac{e^{-\beta\cdot\|{\mathbf{c}}\_{i}-\hat{{\mathbf{r}}}\|\_{2}}}{\sum\_{k}{e^{-\beta\cdot\|{\mathbf{c}}\_{k}-\hat{{\mathbf{r}}}\|\_{2}}}}, |  | (23) |

where Î²ğ›½\beta is a temperature hyper-parameter. This modeling approach with continuous targets is denoted with Quant-LCM-c in the following sections.

### 2.4 Ablations

In this section, we delineate the ablations experiments conducted to evaluate the aforementioned LCM designs.
We compare all the variants of LCMs introduced above, namely, Base-LCM, One-Tower, Two-Tower and Quant-LCM.

#### 2.4.1 Experimental setup

For our ablation study and for the sake of reproducibility, we pre-train our models on the Fineweb-edu datasetÂ (Lozhkov etÂ al., [2024](#bib.bib74)).
All models are configured to have approximately 1.6B trainable parameters and are pre-trained on Metaâ€™s Research Super Cluster (RSC,Â Lee and Sengupta ([2022](#bib.bib67))) for 250k optimization steps spanning 32 A100 GPUs with a total batch size of 229k concepts.

##### Models architectures.

The Base-LCM has 32 layers and a model dimension dmodel=2048subscriptdmodel2048{\textnormal{d}}\_{\text{model}}=2048 with 16 attention heads. It uses rotary position embeddingsÂ (RoPE, Su etÂ al. ([2024](#bib.bib106))), applies pre-normalization using RMSNormÂ (Zhang and Sennrich, [2019](#bib.bib122)), uses the SwiGLU activation functionÂ (Shazeer, [2020](#bib.bib101)) and is trained with a dropout rate of p=0.1p0.1{\textnormal{p}}{=}0.1.

The One-Tower diffusion LCM is made of 32 transformer blocks,
each made of a self-attention layer with 32 attention heads and followed by a feed-forward neural network with inner size 8192.
It has a dimension dmodelsubscriptdmodel{\textnormal{d}}\_{\text{model}} of 2048 and uses learned position embeddings.
The noise scheduler is set with T=100T100{\textnormal{T}}{=}100 diffusion timesteps.
During training, self-attention is dropped with a probability of 0.15 for unconditional training, enabling classifier-free guidance at inference time.

The Two-Tower diffusion LCM has 5 layers in its contextualizer and 13 layers in its denoiser.
Similar to the Base-LCM, it has 16 attention heads, a model dimension dmodel=2048subscriptdmodel2048{\textnormal{d}}\_{\text{model}}=2048, and uses SwiGLU activations and RMSNorm in both contextualizer and denoiser. The contextualizer uses RoPE for embedding positions whereas the denoiser is without positional embeddings. We use by default the cosine noise schedule with T=100T100{\textnormal{T}}{=}100 and train with a dropout rate of p=0.1p0.1{\textnormal{p}}{=}0.1. For training the model unconditionally we use a cross-attention mask dropout of rate 0.15 (see [SectionÂ 2.3.4](#S2.SS3.SSS4 "2.3.4 Two-Tower Diffusion LCM â€£ 2.3 Large Concept Model variants â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space")). The pre-training documents are wrapped at 128 sentences.
Unless otherwise mentioned we decode with S=40S40{\textnormal{S}}{=}40 sample steps with a guidance scale gscale=3subscriptğ‘”scale3g\_{\text{scale}}=3, a guidance rescaling factor of grescale=0.7subscriptğ‘”rescale0.7g\_{\text{rescale}}=0.7, an initial noise scale Ïƒinit=0.6subscriptğœinit0.6\sigma\_{\text{init}}=0.6 and epsilon-scaling with Î»eps=1.00045subscriptğœ†eps1.00045\lambda\_{\text{eps}}=1.00045.

The Quant-LCM follows exactly the same architecture as the One-Tower diffusion LCM, except for Quant-LCM-d which differs only by its output dimension which is set to nunits-per-codebook=8192subscriptğ‘›units-per-codebook8192n\_{\text{units-per-codebook}}=8192 for softmax computation. For single sentence prediction tasks, we use topk=1subscripttopğ‘˜1\text{top}\_{k}=1 and gscale=2subscriptğ‘”scale2g\_{\text{scale}}=2 for Quant-LCM-d and topk=1subscripttopğ‘˜1\text{top}\_{k}=1 with gscale=3subscriptğ‘”scale3g\_{\text{scale}}=3 for Quant-LCM-c, while for multi-sentence generation tasks we used temperature of 1, topk=3subscripttopğ‘˜3\text{top}\_{k}=3, gscale=1subscriptğ‘”scale1g\_{\text{scale}}=1, for Quant-LCM-d and temperature of 0.005, topk=5subscripttopğ‘˜5\text{top}\_{k}=5, gscale=1.5subscriptğ‘”scale1.5g\_{\text{scale}}=1.5 for Quant-LCM-c, as higher guidance or lower temperature setups led to repeated generated sentences.

|  |  |  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
|  |  | #Llama2 Tokens | | | #Sentences | | | Total  sentences |
| Dataset | #Docs | Q1 | Q2 | Q3 | Q1 | Q2 | Q3 |
| ROC-stories (dev) | 2000 | 48 | 57 | 64 | 5 | 5 | 5 | 10K |
| ROC-stories (test) | 1871 | 50 | 56 | 62 | 5 | 5 | 5 | 9.4K |
| C4 (dev) | 1000 | 136 | 288 | 577 | 6 | 12 | 24 | 20.6K |
| C4 (test) | 1000 | 133 | 282 | 599 | 6 | 11 | 25 | 21.9K |
| Wikipedia-en (dev) | 1000 | 146 | 332 | 736 | 5 | 10 | 23 | 21.1K |
| Wikipedia-en (test) | 1000 | 147 | 312 | 673 | 5 | 9 | 21 | 19.1K |
| Gutenberg (dev) | 55 | 10297 | 15934 | 22259 | 328 | 530 | 687 | 216.2K |
| Gutenberg (test) | 61 | 10752 | 15204 | 23414 | 325 | 457 | 735 | 562.2K |

Table 2: Statistics of the pre-training evaluation datasets. For each subset we report the number of documents, the total number of sentences and document lengths quartiles in sentences and in Llama2 tokens for reference.

##### Pre-training evaluation.

Pre-trained token-level language models are typically evaluated with perplexity: a measure of how well each next token is predicted given a teacher-forced (i.e., ground truth) prefix of the document. In a similar spirit, we evaluate pre-trained LCMs in a teacher-forced mode. But as they cannot produce the probability explicitly, we resort to a custom set of metrics of the quality of next sentence prediction.

Each pre-trained model is initially evaluated on the quality of its predicted next sentence ğ±^nsubscript^ğ±ğ‘›\hat{\mathbf{x}}\_{n} given a ground truth context ğ±<nsubscriptğ±absentğ‘›{\mathbf{x}}\_{<n}. Practically, for a given document ğ±1:Nsubscriptğ±:1N{\mathbf{x}}\_{1:{\textnormal{N}}}, we run the LCM inference in teacher-forcing mode and evaluate the following metrics:

* â€¢

  L2 distance (â„“2subscriptâ„“2\operatorname{\ell\_{2}}). Euclidean distance in the SONAR space between the predicted embedding ğ±^nsubscript^ğ±ğ‘›\hat{\mathbf{x}}\_{n} and the ground truth continuation ğ±nsubscriptğ±ğ‘›{\mathbf{x}}\_{n}: â„“2â‰”â€–ğ±^nâˆ’ğ±nâ€–2â‰”subscriptâ„“2superscriptnormsubscript^ğ±ğ‘›subscriptğ±ğ‘›2\ell\_{2}\coloneqq\|\hat{\mathbf{x}}\_{n}-{\mathbf{x}}\_{n}\|^{2}.
* â€¢

  Round-trip L2 distance (â„“2âˆ’rsubscriptâ„“2r\operatorname{\ell\_{2-\text{r}}}). Euclidean distance in the SONAR space between the re-encoded sentence generated from the predicted embedding and the ground truth continuation ğ±nsubscriptğ±ğ‘›{\mathbf{x}}\_{n},

  |  |  |  |
  | --- | --- | --- |
  |  | â„“2âˆ’râ‰”â€–encodeâ¡(decodeâ¡(ğ±^n))âˆ’ğ±nâ€–2.â‰”subscriptâ„“2rsuperscriptnormencodedecodesubscript^ğ±ğ‘›subscriptğ±ğ‘›2\operatorname{\ell\_{2-\text{r}}}\coloneqq\|\operatorname{encode}(\operatorname{decode}(\hat{\mathbf{x}}\_{n}))-{\mathbf{x}}\_{n}\|^{2}. |  |

  Since an LCM can predict an embedding outside of the distribution of real embeddings (obtained by encoding natural sentences), the SONAR decoder might shift these embeddings to the nearest plausible embeddings subspace. The â„“2âˆ’rsubscriptâ„“2r\operatorname{\ell\_{2-\text{r}}} metric is introduced to capture the shift in embeddings after decoding them into text then re-embedding them again in the SONAR space. The more the generated embeddings are out-of-distribution, the higher the delta between â„“2âˆ’rsubscriptâ„“2r\operatorname{\ell\_{2-\text{r}}} and â„“2subscriptâ„“2\operatorname{\ell\_{2}} would be.
* â€¢

  Contrastive accuracy (CACA\operatorname{CA}). The ratio of embeddings in a batch that are further away (in terms of â„“2subscriptâ„“2\operatorname{\ell\_{2}}) from the predicted embedding ğ±^nsubscript^ğ±ğ‘›\hat{\mathbf{x}}\_{n} than the ground truth ğ±nsubscriptğ±ğ‘›{\mathbf{x}}\_{n} (for each nğ‘›n, we exclude ğ±nsubscriptğ±ğ‘›{\mathbf{x}}\_{n} and its two neighboring ground truth embeddings from the comparison). This metric naturally assigns higher penalty for large â„“2subscriptâ„“2\operatorname{\ell\_{2}} values in the regions with high density of sentence embeddings.
* â€¢

  Paraphrasing (PARPAR\operatorname{PAR}). The maximum cosine similarity (CSCS\operatorname{CS}) between the generated embedding ğ±^nsubscript^ğ±ğ‘›\hat{\mathbf{x}}\_{n} and the context embeddings ğ±<nsubscriptğ±absentğ‘›{\mathbf{x}}\_{<n},
  normalized by the score of the ground truth sentence.
  Thus, PAR=maxm<nâ¡CSâ¡(ğ±^n,ğ±m)/maxm<nâ¡CSâ¡(ğ±n,ğ±m)PARsubscriptğ‘šğ‘›CSsubscript^ğ±ğ‘›subscriptğ±ğ‘šsubscriptğ‘šğ‘›CSsubscriptğ±ğ‘›subscriptğ±ğ‘š\operatorname{PAR}=\max\limits\_{m<n}\operatorname{CS}(\hat{\mathbf{x}}\_{n},{\mathbf{x}}\_{m})/\max\limits\_{m<n}\operatorname{CS}({\mathbf{x}}\_{n},{\mathbf{x}}\_{m}).
  The goal of this metric is to capture if the model is simply copying or paraphrasing a sentence from the context more (>1absent1>1) or less (<1absent1<1) than the reference sentence.
* â€¢

  Mutual information (MIMI\operatorname{MI}). This metric of text coherence evaluates the mutual information between the next predicted sentence s^n=decodeâ¡(ğ±^n)subscript^sğ‘›decodesubscript^ğ±ğ‘›\hat{\textnormal{s}}\_{n}=\operatorname{decode}(\hat{\mathbf{x}}\_{n}) and the previous k=10ğ‘˜10k=10 ground truth sentences by computing the difference between the unconditional perplexity of s^nsubscript^ğ‘ ğ‘›\hat{s}\_{n} and its perplexity conditioned on the prompt:

  |  |  |  |
  | --- | --- | --- |
  |  | MI=1|s^n|â€‹(logâ¡pLMâ€‹(s^n)âˆ’logâ¡pLMâ€‹(s^n|snâˆ’k:nâˆ’1)).MI1subscript^sğ‘›subscriptpLMsubscript^ğ‘ ğ‘›subscriptpLMconditionalsubscript^sğ‘›subscripts:ğ‘›ğ‘˜ğ‘›1\operatorname{MI}=\frac{1}{|\hat{\textnormal{s}}\_{n}|}\left(\log{\textnormal{p}}\_{\text{LM}}(\hat{s}\_{n})-\log{\textnormal{p}}\_{\text{LM}}(\hat{\textnormal{s}}\_{n}|{\textnormal{s}}\_{n-k:n-1})\right). |  |

  We estimate the perplexity with a small language model, GPT-2 (Radford etÂ al., [2019](#bib.bib91)). We prepend a newline symbol to s^nsubscript^sğ‘›\hat{\textnormal{s}}\_{n}, so that a probability could be assigned to its first token, and we compute the average mutual information per-token by normalizing it with |s^n|subscript^sğ‘›|\hat{\textnormal{s}}\_{n}|, the length of s^nsubscript^sğ‘›\hat{\textnormal{s}}\_{n} in tokens. When averaging MIMI\operatorname{MI} over a dataset, |s^n|subscript^sğ‘›|\hat{\textnormal{s}}\_{n}| are used as weights.

##### Pre-training evaluation data.

The pre-training evaluation is performed on sampled subsets from four corpora covering different domains: ROC-storiesÂ (Mostafazadeh etÂ al., [2016](#bib.bib79)), C4Â (Raffel etÂ al., [2019](#bib.bib92)), Wikipedia-en (English Wikipedia dump) and Gutenberg.
We sample two distinct subsets (dev and test) from each corpus, we use the dev split for tuning inference hyper-parameters and report the results on the test splits.
The statistics of the evaluation corpora are presented in [TableÂ 2](#S2.T2 "In Models architectures. â€£ 2.4.1 Experimental setup â€£ 2.4 Ablations â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space").

|  |  |  |  |  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Model | ROC-stories | | | | | C4 | | | | |
| â„“2subscriptâ„“2\operatorname{\ell\_{2}} | â„“2âˆ’rsubscriptâ„“2r\operatorname{\ell\_{2-\text{r}}} | PARPAR\operatorname{PAR} | CACA\operatorname{CA} | MIMI\operatorname{MI} | â„“2subscriptâ„“2\operatorname{\ell\_{2}} | â„“2âˆ’rsubscriptâ„“2r\operatorname{\ell\_{2-\text{r}}} | PARPAR\operatorname{PAR} | CACA\operatorname{CA} | MIMI\operatorname{MI} |
| Base-LCM | 0.177 | 0.237 | 1.847 | 72.4% | 0.062 | 0.204 | 0.261 | 1.964 | 69.1% | -0.105 |
| One-Tower | 0.236 | 0.236 | 1.939 | 80.2% | 0.977 | 0.279 | 0.273 | 2.239 | 77.1% | 1.110 |
| Two-Tower | 0.233 | 0.231 | 2.088 | 80.6% | 1.137 | 0.265 | 0.261 | 2.265 | 75.4% | 1.134 |
| Quant-LCM-c | 0.236 | 0.237 | 1.683 | 76.0% | 0.610 | 0.279 | 0.283 | 2.013 | 77.2% | 0.715 |
| Quant-LCM-d | 0.240 | 0.246 | 1.871 | 81.1% | 0.682 | 0.270 | 0.282 | 1.808 | 75.0% | 0.359 |

|  |  |  |  |  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Model | Wikipedia-en | | | | | Gutenberg | | | | |
| â„“2subscriptâ„“2\operatorname{\ell\_{2}} | â„“2âˆ’rsubscriptâ„“2r\operatorname{\ell\_{2-\text{r}}} | PARPAR\operatorname{PAR} | CACA\operatorname{CA} | MIMI\operatorname{MI} | â„“2subscriptâ„“2\operatorname{\ell\_{2}} | â„“2âˆ’rsubscriptâ„“2r\operatorname{\ell\_{2-\text{r}}} | PARPAR\operatorname{PAR} | CACA\operatorname{CA} | MIMI\operatorname{MI} |
| Base-LCM | 0.229 | 0.283 | 1.770 | 69.6% | 0.071 | 0.207 | 0.264 | 1.780 | 67.8% | -0.184 |
| One-Tower | 0.324 | 0.311 | 2.087 | 80.9% | 1.202 | 0.284 | 0.281 | 2.051 | 75.1% | 0.725 |
| Two-Tower | 0.307 | 0.297 | 2.079 | 78.8% | 1.307 | 0.267 | 0.267 | 2.077 | 73.0% | 0.684 |
| Quant-LCM-c | 0.306 | 0.317 | 1.842 | 79.5% | 0.744 | 0.269 | 0.281 | 1.774 | 72.1% | 0.419 |
| Quant-LCM-d | 0.295 | 0.311 | 1.592 | 76.0% | 0.323 | 0.276 | 0.290 | 1.599 | 72.0% | 0.153 |

Table 3: Comparing architectures. Pre-training evaluation results on the four select corpora. For each subset, we report â„“2subscriptâ„“2\operatorname{\ell\_{2}} (L2 distance in SONAR space), â„“2âˆ’rsubscriptâ„“2r\operatorname{\ell\_{2-\text{r}}} (round-trip L2 distance after decoding and re-encoding the generated embeddings), PARPAR\operatorname{PAR} (similarity to preceding embeddings) and CACA\operatorname{CA} (contrastive accuracy)

The results of the pre-training evaluation are presented in [TableÂ 3](#S2.T3 "In Pre-training evaluation data. â€£ 2.4.1 Experimental setup â€£ 2.4 Ablations â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space").

First, diffusion-based LCM and Quant-LCM variants have similar â„“2subscriptâ„“2\operatorname{\ell\_{2}} and â„“2âˆ’rsubscriptâ„“2r\operatorname{\ell\_{2-\text{r}}} scores despite an important difference in their learning objectives.
The only model that shows substantially lower â„“2subscriptâ„“2\operatorname{\ell\_{2}} score is the Base-LCM.
This is expected since Base-LCM effectively optimizes â„“2subscriptâ„“2\operatorname{\ell\_{2}} score during training.
Yet, â„“2âˆ’rsubscriptâ„“2r\operatorname{\ell\_{2-\text{r}}} score is not improved compared to other models.
This could be explained by the fact that when many plausible next sentence continuations are possible, Base-LCM generates their average in SONAR space (instead of sampling one of plausible modes) which may not correspond to any relevant point in SONAR space.
This hypothesis is also highlighted by the poor Base-LCM performance in term of CACA\operatorname{CA} and MIMI\operatorname{MI} scores.

We do not notice any significant difference in CACA\operatorname{CA} scores between diffusion LCMs and Quant-LCM variants. MIMI\operatorname{MI} scores, on the contrary, are consistently higher for diffusion-based models compared to Quant-LCM. At the same time, diffusion LCMs tend to paraphrase more the context in the generated embeddings, which also correlates with an increased MIMI\operatorname{MI} score.
Still, Quant-LCM variants significantly outperform Base-LCM on MIMI\operatorname{MI} metric.
Now comparing the different variants, Quant-LCM-c outperforms Quant-LCM-d modeling variant: one hypothesis is that predicting codebook indices with cross-entropy loss is harder than MSEMSE\operatorname{MSE} objective where Quant-LCM-c can more easily learn combination of left-context vectors for next sentence embedding.

For diffusion LCMs, we donâ€™t observe any consistent difference between One-Tower and Two-Tower when looking across all metrics and datasets.
Note that overall, to tackle the next sentence prediction task in the SONAR space, diffusion-based methods give clearly better results compared to all other models.

##### Instruction-tuning evaluation.

Subsequently, the pre-trained models are instruction-tuned on the stories subset of CosmopediaÂ (BenÂ Allal etÂ al., [2024](#bib.bib15)) and are evaluated on a held-out subset of Cosmopedia itself. We aim with this finetuning to evaluate the ability of the models to follow instructions and generate consistent stories.

For the sake of comparison, we trained a small LlamaÂ (Touvron etÂ al., [2023](#bib.bib110)) on the same training data (Fineweb-edu) and finetuned it on Cosmopedia.
This model has 24 transformer layers, each with 16 attention heads and a model dimension of 2048 for a total of 1.4B parameters. This model will be referred to as smaLlama.

We evaluate the following metrics:

* â€¢

  ROUGE-L (Râˆ’LRL\operatorname{R-L}). ROUGE-L (F-measure)Â (Lin, [2004](#bib.bib70)) between the generated and reference stories.
* â€¢

  Coherence (CoherenceCoherence\operatorname{Coherence}). This reference-free metric is computed with a bidirectional transformer model fine-tuned by Jwalapuram etÂ al. ([2022](#bib.bib57)) to assign higher scores to positive â€œnaturalâ€ documents than to negative examples with permuted sentences. For reporting, we normalize it with a sigmoid (with a temperature 3.0, empirically set to make the scores of â€œcertainly incoherentâ€ documents close to 0 and those of â€œcertainly coherentâ€ documents close to 1).

|  |  |  |
| --- | --- | --- |
| Model | Râˆ’Lâ†‘â†‘RLabsent\operatorname{R-L}\uparrow | Coherenceâ†‘â†‘Coherenceabsent\operatorname{Coherence}\uparrow |
| Base-LCM | 23.69 | 0.482 |
| One-Tower | 33.40 | 0.968 |
| Two-Tower | 33.64 | 0.938 |
| Quant-LCM-c | 30.87 | 0.847 |
| Quant-LCM-d | 28.01 | 0.704 |
| smaLlama | 34.88 | 0.984 |

Table 4: Comparing architectures. Instruction-tuning evaluation results. For each model we score the generated stories on the held-out test prompts and report Râˆ’LRL\operatorname{R-L} (ROUGE-L) scores.

The scores in terms of Râˆ’LRL\operatorname{R-L} and CoherenceCoherence\operatorname{Coherence} of the finetuning evaluation are presented in [TableÂ 4](#S2.T4 "In Instruction-tuning evaluation. â€£ 2.4.1 Experimental setup â€£ 2.4 Ablations â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space").
Those quantitative results are in line with the pretraining evaluation ones.
Both Râˆ’LRL\operatorname{R-L} and CoherenceCoherence\operatorname{Coherence} scores correlate with the model ordering based from MIMI\operatorname{MI} scores, mainly Quant-LCM is outperformed by diffusion-based models, and both outperform Base-LCM by a large margin.

We also note that smaLlama outperforms the LCMs on this downstream task on both metrics.
It is well known that LLMs produce very fluent outputs, that explains the higher Rouge-L score.
We also note that the One-Tower and Two-Tower produce coherent outputs, on par with the smaLlama outputs.

#### 2.4.2 Importance of the diffusion inference hyper-parameters

In this section we study the effect of different inference hyper-parameters on the quality of the generated text. To this end, we generate outputs for the C4 test split with the Two-Tower LCM model above, while varying the following hyper-parameters: the guidance scale gscalesubscriptğ‘”scaleg\_{\text{scale}}, the initial noise scale Ïƒinitsubscriptğœinit\sigma\_{\text{init}}, and the number of inference sample steps S. We score the generations following the same protocol above and report the results in [FigureÂ 10](#S2.F10 "In 2.4.2 Importance of the diffusion inference hyper-parameters â€£ 2.4 Ablations â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space"). We note that as we increase the guidance scale, the mutual information between the prefix and the generated suffix increases, and so does paraphrasing as we are paying more attention to the conditioning context variables. In the opposite direction of the mutual information, the â„“2subscriptâ„“2\operatorname{\ell\_{2}} distance from the ground truth continuation increases as the model prefers to stay close to the prefix. Regarding the initial noise scale, we observe that values between 0.5 and 0.7 achieve the best MIMI\operatorname{MI} score. In particular, the generated individual sentences are usually longer with a higher Ïƒinitsubscriptğœinit\sigma\_{\text{init}}. The â„“2subscriptâ„“2\operatorname{\ell\_{2}} distance on the other hand does not reflect this trend. Lastly, we increase the number of inference steps and measure the mutual information (MIMI\operatorname{MI}) of the generated texts. With more inference steps, we can improve the prefix-suffix mutual information, but there is diminishing returns to increasing the inference cost with little qualitative improvement.

![Refer to caption](/html/2412.08821/assets/x12.png)

Figure 10: Importance of inference hyper-parameters. The first panel shows the quality of the generated output measured with MIMI\operatorname{MI} and â„“2subscriptâ„“2\operatorname{\ell\_{2}} as we vary the guidance scale gscalesubscriptğ‘”scaleg\_{\text{scale}} with fixed Ïƒinit=0.6subscriptğœinit0.6\sigma\_{\text{init}}=0.6 and S=40S40{\textnormal{S}}=40. The second panel varies the initial noise scale Ïƒinitsubscriptğœinit\sigma\_{\text{init}} with fixed guidance gscale=3subscriptğ‘”scale3g\_{\text{scale}}=3 and S=40S40{\textnormal{S}}=40. The third panel varies the inference steps S while holding the guidance scale gscale=1.5subscriptğ‘”scale1.5g\_{\text{scale}}=1.5 and Ïƒinit=0.6subscriptğœinit0.6\sigma\_{\text{init}}=0.6 fixed. We consider 3 values for Î»epssubscriptğœ†eps\lambda\_{\text{eps}} to see the impact of epsilon-scaling in the regime of large inference steps.

#### 2.4.3 Studying the noise schedules

In this section, we compare different Two-Tower diffusion LCMs trained with different noise schedules, namely:

Cosine.
:   Our default cosine noise schedule.

Quadratic.
:   The first quadratic schedule (Quadratic-1) has Î²0=0.001subscriptğ›½00.001\beta\_{0}=0.001 and Î²T=0.0012subscriptğ›½ğ‘‡0.0012\beta\_{T}=0.0012, whereas the second quadratic schedule (Quadratic-2) has Î²0=0.02subscriptğ›½00.02\beta\_{0}=0.02 and Î²T=0.022subscriptğ›½ğ‘‡0.022\beta\_{T}=0.022.

Sigmoid.
:   Four different sigmoid schedules with with (Î±,Î²)âˆˆ{(1.5,âˆ’1),(1.5,âˆ’2),(0.8,âˆ’1),(3.5,0)}ğ›¼ğ›½1.511.520.813.50(\alpha,\beta)\in\{(1.5,-1),(1.5,-2),(0.8,-1),(3.5,0)\}.

All schedules are set with the default T=100T100{\textnormal{T}}{=}100. For the exact description of each noise schedule refer to [SectionÂ 2.3.2](#S2.SS3.SSS2 "2.3.2 Diffusion-based LCM â€£ 2.3 Large Concept Model variants â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space") and [FigureÂ 5](#S2.F5 "In Forward process and noise schedule â€£ 2.3.2 Diffusion-based LCM â€£ 2.3 Large Concept Model variants â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space").
We selected the quadratic schedule as a commonly used schedule for reference with two configurations, Quadratic-1 closer to the cosine schedule and Quadratic-2 with more weight given to lower log-SNR. The selected sigmoid schedules with Î´=1.5ğ›¿1.5\delta=1.5 are configured with Î³=âˆ’1ğ›¾1\gamma=-1 and Î³=âˆ’2ğ›¾2\gamma=-2, Î³=âˆ’2ğ›¾2\gamma=-2 being slightly shifted to the left on the log-SNR distribution i.e., more weight to lower log-SNR regimes. We then change the Î´ğ›¿\delta parameter of the sigmoid schedule to choose (Î´=0.8,Î³=âˆ’1formulae-sequenceğ›¿0.8ğ›¾1\delta=0.8,\gamma=-1) for a peaked distribution of log-SNR around -1 and (Î´=3.5,Î³=0)\delta=3.5,\gamma=0) for a flat distribution over noise levels.

We follow the experimental setup described in [SectionÂ 2.4.1](#S2.SS4.SSS1 "2.4.1 Experimental setup â€£ 2.4 Ablations â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space") and report the results of the pre-training evaluation in [TableÂ 5](#S2.T5 "In 2.4.3 Studying the noise schedules â€£ 2.4 Ablations â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space").
Both quadratic schedules achieve a better MIMI\operatorname{MI} score while the wide sigmoid schedule (Î´,Î³)=(3.5,0)ğ›¿ğ›¾3.50(\delta,\gamma)=(3.5,0) achieves the highest accuracy CACA\operatorname{CA} on both C4 and Wikipedia-en. To further understand the differences between those schedules we re-evaluate on C4 while varying the guidance scale gscalesubscriptğ‘”scaleg\_{\text{scale}}. The results in [FigureÂ 11](#S2.F11 "In 2.4.3 Studying the noise schedules â€£ 2.4 Ablations â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space") confirm that
the wide sigmoid schedule (Î´=3.5,Î³=0formulae-sequenceğ›¿3.5ğ›¾0\delta=3.5,\gamma=0) has a much higher contrastive accuracy across the board (rightmost panel) while closely matching the cosine schedule in terms of mutual information (MIMI\operatorname{MI}).
This schedule being trained on a wider spectrum of log-SNR learns to contrast more than to regress.
Contrast this behavior to the peaked sigmoid schedule (Î´=0.8,Î³=âˆ’1)\delta=0.8,\gamma=-1) where the model focuses on regressing to the target (lower â„“2subscriptâ„“2\operatorname{\ell\_{2}}), akin to a Base-LCM, resulting in a lower contrastive accuracy.

|  |  |  |  |  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Model | C4 | | | | | Wikipedia-en | | | | |
| â„“2subscriptâ„“2\operatorname{\ell\_{2}} | â„“2âˆ’rsubscriptâ„“2r\operatorname{\ell\_{2-\text{r}}} | PARPAR\operatorname{PAR} | CACA\operatorname{CA} | MIMI\operatorname{MI} | â„“2subscriptâ„“2\operatorname{\ell\_{2}} | â„“2âˆ’rsubscriptâ„“2r\operatorname{\ell\_{2-\text{r}}} | PARPAR\operatorname{PAR} | CACA\operatorname{CA} | MIMI\operatorname{MI} |
| Cosine | 0.265 | 0.261 | 2.265 | 75.4% | 1.134 | 0.307 | 0.297 | 2.079 | 78.8% | 1.307 |
| Quadratic-1 | 0.268 | 0.264 | 2.341 | 75.7% | 1.252 | 0.309 | 0.300 | 2.202 | 79.1% | 1.409 |
| Quadratic-2 | 0.270 | 0.265 | 2.320 | 76.2% | 1.252 | 0.312 | 0.303 | 2.185 | 79.7% | 1.399 |
| Sigmoid(1.5, -1) | 0.257 | 0.259 | 2.226 | 74% | 1.083 | 0.298 | 0.292 | 2.110 | 77% | 1.271 |
| Sigmoid(1.5, -2) | 0.277 | 0.267 | 2.291 | 77.2% | 1.173 | 0.321 | 0.303 | 2.179 | 80.3% | 1.308 |
| Sigmoid(0.8, -1) | 0.252 | 0.255 | 2.053 | 70.6% | 0.936 | 0.285 | 0.283 | 1.883 | 71.7% | 1.127 |
| Sigmoid(3.5, 0) | 0.307 | 0.265 | 2.347 | 80.3% | 1.154 | 0.347 | 0.303 | 2.187 | 83.7% | 1.288 |

Table 5: Comparing noise schedules. Results of the pre-training evaluation on two corpora, C4 and Wikipedia-en.

![Refer to caption](/html/2412.08821/assets/x13.png)

Figure 11: Comparing noise schedules. The prefix-suffix mutual information (MIMI\operatorname{MI}), the â„“2subscriptâ„“2\operatorname{\ell\_{2}} distance and contrastive accuracy (CACA\operatorname{CA}) scores of evaluated C4 documents while varying the guidance scale (gscale)subscriptğ‘”scale(g\_{\text{scale}}) under different schedules.

#### 2.4.4 Studying the loss weighting strategies

In this section we compare the baseline Two-Tower diffusion LCM trained with the simplified objective (i.e., Ï‰(t)=1,âˆ€t)\omega(t)=1,\;\forall t)) to models trained with the clamped-SNR weighting strategy. We consider two sets of (Î»min,Î»max)subscriptğœ†subscriptğœ†(\lambda\_{\min},\lambda\_{\max}): (Î»min,Î»max)=(0,10)subscriptğœ†subscriptğœ†010(\lambda\_{\min},\lambda\_{\max})=(0,10) and (Î»min,Î»max)=(0.001,5)subscriptğœ†subscriptğœ†0.0015(\lambda\_{\min},\lambda\_{\max})=(0.001,5). All models in this section are trained with the cosine noise schedule.

##### Fragility as a sample weighing strategy

As introduced in [EquationÂ 19](#S2.E19 "In Reverse process and objective function â€£ 2.3.2 Diffusion-based LCM â€£ 2.3 Large Concept Model variants â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space"), we train in this section a Two-Tower LCM model with loss terms weighted by the sampleâ€™s fragility.

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | Ï‰â€‹(ğ±0)ğœ”superscriptğ±0\displaystyle\omega({\mathbf{x}}^{0}) | =sigmoidâ¡(aâ€‹fragilityâ¡(ğ±0)+b)absentsigmoidğ‘fragilitysuperscriptğ±0ğ‘\displaystyle=\operatorname{sigmoid}(a\leavevmode\nobreak\ \operatorname{fragility}({\mathbf{x}}^{0})+b) |  | (24) |

Given that estimating the fragility score of each sample as defined in [SectionÂ 2.5.2](#S2.SS5.SSS2 "2.5.2 Fragility of SONAR space â€£ 2.5 Analysis â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space") can be very costly, we resort to training a simple MLP (3-layers) on 50M sampled sentences to approximate these fragility scores. This model is referred to as F. Henceforth, the sample weight is:

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | Ï‰â€‹(ğ±0)ğœ”superscriptğ±0\displaystyle\omega({\mathbf{x}}^{0}) | =sigmoidâ¡(aâ€‹Fâ€‹(ğ±)+b)absentsigmoidğ‘Fğ±ğ‘\displaystyle=\operatorname{sigmoid}(a\leavevmode\nobreak\ {\textnormal{F}}({\mathbf{x}})+b) |  | (25) |

The two hyper-parameters (a,b)a,b) are chosen so that extremely fragile sentences contribute less to the loss (Ï‰â€‹(ğ±0)â‰ˆ0ğœ”superscriptğ±00\omega({\mathbf{x}}^{0})\approx 0),
and so that sample weights should increase smoothly as sample robustness improves. [FigureÂ 12](#S2.F12 "In Fragility as a sample weighing strategy â€£ 2.4.4 Studying the loss weighting strategies â€£ 2.4 Ablations â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space") plots the sample weights distribution evaluated on a pre-training
dataset with a=âˆ’4ğ‘4a=-4 and b=3.5ğ‘3.5b=3.5.

![Refer to caption](/html/2412.08821/assets/x14.png)

Figure 12: Resulting distribution of fragility sample weights Ï‰â€‹(ğ±0)ğœ”superscriptğ±0\omega({\mathbf{x}}^{0}) with Ï‰â€‹(ğ±0)=sigmoidâ¡(âˆ’4â€‹Fâ€‹(ğ±)+3.5)ğœ”superscriptğ±0sigmoid4Fğ±3.5\omega({\mathbf{x}}^{0})=\operatorname{sigmoid}(-4\leavevmode\nobreak\ {\textnormal{F}}({\mathbf{x}})+3.5).

We follow the experimental setup described in [SectionÂ 2.4.1](#S2.SS4.SSS1 "2.4.1 Experimental setup â€£ 2.4 Ablations â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space") and report the results of the pre-training evaluation in [TableÂ 6](#S2.T6 "In Fragility as a sample weighing strategy â€£ 2.4.4 Studying the loss weighting strategies â€£ 2.4 Ablations â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space"). We observe that weighting with clamped SNRs does not improve the quality of generated texts as measured with our pre-training evaluation metrics.
When it comes to the fragility-aware weighting strategy, we observe an improvement in the contrastive accuracy of the model.
In the remainder of this work we default to the simplified training objective (Ï‰(t)=1,âˆ€t)\omega(t)=1,\,\forall t).

|  |  |  |  |  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Model | C4 | | | | | Wikipedia-en | | | | |
| â„“2subscriptâ„“2\operatorname{\ell\_{2}} | â„“2âˆ’rsubscriptâ„“2r\operatorname{\ell\_{2-\text{r}}} | PARPAR\operatorname{PAR} | CACA\operatorname{CA} | MIMI\operatorname{MI} | â„“2subscriptâ„“2\operatorname{\ell\_{2}} | â„“2âˆ’rsubscriptâ„“2r\operatorname{\ell\_{2-\text{r}}} | PARPAR\operatorname{PAR} | CACA\operatorname{CA} | MIMI\operatorname{MI} |
| Baseline Ï‰â€‹(t)=1ğœ”ğ‘¡1\omega(t)=1 | 0.265 | 0.261 | 2.265 | 75.4% | 1.134 | 0.307 | 0.297 | 2.079 | 78.8% | 1.307 |
| SNR (0,10) | 0.280 | 0.264 | 2.334 | 74.8% | 1.107 | 0.320 | 0.296 | 2.102 | 77.9% | 1.212 |
| SNR (0.001,5) | 0.266 | 0.261 | 2.269 | 73.4% | 1.094 | 0.304 | 0.291 | 2.007 | 76.6% | 1.295 |
| Fragility | 0.2815 | 0.273 | 2.306 | 76.5% | 1.103 | 0.321 | 0.308 | 2.118 | 80.7% | 1.193 |

Table 6: Comparing weighting strategies. Results of the pre-training evaluation on two corpora, C4 and Wikipedia-en.

### 2.5 Analysis

#### 2.5.1 Inference efficiency of LCMs

We compare in this section the inference computational cost of the Two-Tower LCM to that of a vanilla LLM as a function of the total length in tokens of the prompt and output combined. We chose the theoretical number of FLOPs, independent of any specific optimization. These optimizations are generic to the transformer architecture and also apply to our LCM.
We include in this comparison two configurations of LCMs; the 1.6B used in the previous ablation studies and a 7B model we scale up to in the following sections. For both LCMs, we estimate the inference cost with inference sample steps S=40S40{\textnormal{S}}=40.
Given the quadratic complexity of the attention mechanism in transformers, the complexity sharply increases with the context size (see upper right corner of [FigureÂ 13](#S2.F13 "In 2.5.1 Inference efficiency of LCMs â€£ 2.5 Analysis â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space")â€™s left panel).
The complexity of the LCM depends on how the context is sentencized: a context length of 200 tokens split into 10 sentences (20 tokens each) will incur a higher cost than the same 200 tokens split into 5 sentences (40 tokens each). We account for this by computing the cost on a range of sentence lengths but report the total context size on the x-axis (context size = sentence length Ã—\times number of sentences).
The LCM shows substantially better scalability with respect to increasing context size. The inference computational cost of the LCM includes the three steps of (1) encoding into SONAR, (2) LCM prediction in the sentence space then (3) decoding with a SONAR decoder. The inference cost of LCMs varies significantly depending on the average length in tokens per sentence.
For extremely short sentences (less than 10 tokens), an LLM is more computationally efficient (see lower left corner of [FigureÂ 13](#S2.F13 "In 2.5.1 Inference efficiency of LCMs â€£ 2.5 Analysis â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space")â€™s right panel).

![Refer to caption](/html/2412.08821/assets/x15.png)

Figure 13: Theoretical inference Flops of LCMs and LLLms. We evaluate the inference flops for different text lengths (in Llama2 tokens) with a variable average sentence length. Only extremely short sentences (â‰¤10absent10\leq 10 tokens) favor LLMs.

#### 2.5.2 Fragility of SONAR space

When we perform modeling in a latent space, we primarily rely on the induced geometry (Lâ€‹2ğ¿2L2-distance).
However, the homogeneous Euclidean geometry of any latent representation will not perfectly match the underlying text semantics. This is evidenced by the fact that
a small perturbation in the embedding space may result in a drastic loss of semantic information after decoding.
We dub such embeddings *â€œfragileâ€*.
For this reason, we aim to quantify the fragility of semantic embeddings (namely SONAR codes) to understand the quality of the LCM training data and how this fragility can hinder the LCM training dynamics.

Given a text fragment wğ‘¤w and its SONAR code ğ±=encodeâ¡(w)ğ±encodeğ‘¤{\mathbf{x}}=\operatorname{encode}(w), we define the fragility of wğ‘¤w as:

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | fragilityâ¡(w)fragilityğ‘¤\displaystyle\operatorname{fragility}(w) | â‰”âˆ’ğ”¼Î±âˆ¼ğ’°â€‹([0,1]),Ïµâˆ¼ğ’©â€‹(ğŸ,ğˆ)â€‹[scoreâ¡(w,wÎ±,Ïµ)],â‰”absentsubscriptğ”¼formulae-sequencesimilar-toğ›¼ğ’°01similar-tobold-italic-Ïµğ’©0ğˆdelimited-[]scoreğ‘¤subscriptğ‘¤  ğ›¼bold-italic-Ïµ\displaystyle\coloneqq-\displaystyle\mathbb{E}\_{\alpha\sim\mathcal{U}([0,1]),\ {\bm{\epsilon}}\sim\mathcal{N}({\mathbf{0}},{\mathbf{I}})}\left[\operatorname{score}(w,w\_{\alpha,{\bm{\epsilon}}})\right], |  | (26) |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | ğ±Î±,Ïµsubscriptğ±  ğ›¼bold-italic-Ïµ\displaystyle{\mathbf{x}}\_{\alpha,{\bm{\epsilon}}} | =denormalizeâ¡(1âˆ’Î±â€‹normalizeâ¡(ğ±)+Î±â€‹Ïµ),absentdenormalize1ğ›¼normalizeğ±ğ›¼bold-italic-Ïµ\displaystyle=\operatorname{denormalize}\left(\sqrt{1-\alpha}\leavevmode\nobreak\ \operatorname{normalize}({\mathbf{x}})+\sqrt{\alpha}\leavevmode\nobreak\ {\bm{\epsilon}}\right), |  | (27) |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | wÎ±,Ïµsubscriptğ‘¤  ğ›¼bold-italic-Ïµ\displaystyle w\_{\alpha,{\bm{\epsilon}}} | =decodeâ¡(ğ±Î±,Ïµ),absentdecodesubscriptğ±  ğ›¼bold-italic-Ïµ\displaystyle=\operatorname{decode}({\mathbf{x}}\_{\alpha,{\bm{\epsilon}}}), |  | (28) |

where normalizenormalize\operatorname{normalize} and denormalizedenormalize\operatorname{denormalize} are the normalization and denormalization operators introduced in [EquationÂ 4](#S2.E4 "In 2.3.1 Base-LCM â€£ 2.3 Large Concept Model variants â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space") with the goal of making ğ±ğ±{\mathbf{x}}â€™s coordinates scale-independent.
The â€œencodeencode\operatorname{encode}â€œ operation maps text fragments into SONAR space, and the â€œdecodedecode\operatorname{decode}â€œ operation produces a text fragment from a given vector in the SONAR space.
For each Î±ğ›¼\alpha in [0,1]01[0,1], ğ±Î±,Ïµsubscriptğ±

ğ›¼bold-italic-Ïµ{\mathbf{x}}\_{\alpha,{\bm{\epsilon}}} is the perturbed version of ğ±ğ±{\mathbf{x}} where a noise vector of variance Î±ğ›¼\alpha is linearly combined with ğ±ğ±{\mathbf{x}}. The perturbed vector is then decoded into a text fragment wÎ±,Ïµsubscriptğ‘¤

ğ›¼bold-italic-Ïµw\_{\alpha,{\bm{\epsilon}}}.
This perturbation is similar to the variance-preserving noising used in diffusion LCMs (see [SectionÂ 2.3.2](#S2.SS3.SSS2 "2.3.2 Diffusion-based LCM â€£ 2.3 Large Concept Model variants â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space")).

The â€œscorescore\operatorname{score}â€ operator in [EquationÂ 26](#S2.E26 "In 2.5.2 Fragility of SONAR space â€£ 2.5 Analysis â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space") is set to be a semantic similarity metric comparing the perturbed text wÎ±,Ïµsubscriptğ‘¤

ğ›¼bold-italic-Ïµw\_{\alpha,{\bm{\epsilon}}} to the original wğ‘¤w. We considered the following options:

* â€¢

  Auto-Encoding BLEU. scoreâ¡(w,wÎ±,Ïµ)=BLEUâ€‹(w,wÎ±,Ïµ)scoreğ‘¤subscriptğ‘¤
  ğ›¼bold-italic-ÏµBLEUğ‘¤subscriptğ‘¤
  ğ›¼bold-italic-Ïµ\operatorname{score}(w,w\_{\alpha,{\bm{\epsilon}}})=\textsc{BLEU}(w,w\_{\alpha,{\bm{\epsilon}}}).
* â€¢

  External cosine similarity. Provided an external text encoder (read unrelated to SONAR) that encodes a text fragment wğ‘¤w into encodeextâ¡(w)subscriptencodeextğ‘¤\operatorname{encode\_{\text{ext}}}(w)
  scoreâ¡(w,wÎ±,Ïµ)=CSâ¡(encodeextâ¡(w),encodeextâ¡(wÎ±,Ïµ))scoreğ‘¤subscriptğ‘¤
  ğ›¼bold-italic-ÏµCSsubscriptencodeextğ‘¤subscriptencodeextsubscriptğ‘¤
  ğ›¼italic-Ïµ\operatorname{score}(w,w\_{\alpha,{\bm{\epsilon}}})=\operatorname{CS}(\operatorname{encode\_{\text{ext}}}(w),\operatorname{encode\_{\text{ext}}}(w\_{\alpha,\epsilon})), where CSCS\operatorname{CS} is the cosine similarity measure.
  Compared to Auto-Encoding BLEU, this method is typically more robust to paraphrasing.

##### Finetuned robust decoder.

To serve as a testbed for our fragility analysis, a new SONAR decoder for English text is finetuned on a sample of our pre-training data. In order to improve the decoderâ€™s robustness to imperfectly generated embeddings from the LCM, we follow [EquationÂ 27](#S2.E27 "In 2.5.2 Fragility of SONAR space â€£ 2.5 Analysis â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space") and add random noise vectors to SONAR embeddings during training.
As reported in [TableÂ 7](#S2.T7 "In Finetuned robust decoder. â€£ 2.5.2 Fragility of SONAR space â€£ 2.5 Analysis â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space"), the finetuned SONAR decoder exhibits stronger performance across a range of public corpora.

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
| Model | Flores | CNN DailyMail | Gutenberg | C4 |
| Base SONAR decoder | 79.5 | 75.9 | 70.5 | 75.7 |
| Finetuned SONAR decoder | 88.0 | 87.6 | 85.6 | 87.5 |

Table 7: Comparing SONAR decoders. Raw reconstruction performance of our base SONAR decoder vs. the new decoder trained with noised embeddings. Scores are Auto-Encoding BLEU on random subsets of 10k sentences from each dataset, except for Flores where we use the entire dev split.

##### Fragility study.

We sample 50M random text fragments, and for each sample we generate 9 perturbations corresponding to different noise levels Î±âˆˆ[0.1,0.2,â€¦,0.9]ğ›¼

0.10.2â€¦0.9\alpha\in[0.1,0.2,\ldots,0.9].
For the external cosine similarity metric we use mGTE as external encoderÂ (Zhang etÂ al., [2024](#bib.bib123)).

![Refer to caption](/html/2412.08821/assets/x16.png)

Figure 14: Fragility scores. Auto-Encoding BLEU and external cosine similarity. In the left-hand panel as a function of the text length (Î±ğ›¼\alpha-averaged) and in the right-hand panel as a function of the noise variance Î±ğ›¼\alpha.

![Refer to caption](/html/2412.08821/assets/x17.png)

Figure 15: Auto-Encoding BLEU scores and cosine similarity (Î±ğ›¼\alpha-averaged) distributions.

We depict in the right panel of [FigureÂ 14](#S2.F14 "In Fragility study. â€£ 2.5.2 Fragility of SONAR space â€£ 2.5 Analysis â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space") the curves of both score functions with respect to the noise level Î±ğ›¼\alpha.
We observe that BLEU scores decrease faster than the cosine similarity.
Most importantly, fragility scores are sensitive to the choice of the decoder. In particular,
both Auto-Encoding BLEU and cosine similarity scores decrease at a markedly slower rate for the Finetuned decoder than for the Base one as the amount of noise increases.
We note also that the overall score distribution (after averaging over all Î±ğ›¼\alpha), shown in [FigureÂ 15](#S2.F15 "In Fragility study. â€£ 2.5.2 Fragility of SONAR space â€£ 2.5 Analysis â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space"), exhibits a large spread of fragility scores across SONAR samples.

One factor that can explain such a discrepancy is the text length. Compared to the Auto-Encoding BLEU metric (which drops only by 1â€“2% for long sentences),
fragility is more sensitive to the length of sentences and drops faster for both similarity metrics.
This shows that using a max sentence length over 250250250 can be extremely challenging for SONAR and the LCM model.
On the other hand, even if short sentences are on average more robust,
splitting a long sentence in the wrong place may result in shorter but more fragile sub-sentences.

Taking a closer look at the 5% most fragile embeddings, we notice that they are very noisy.
Typically, they correspond to hyperlinks, references, unique ids, code-switched or numerical entries. These are likely artifacts that the SONAR models were not exposed to during training or where the SONAR tokenizer fails.
Fragility can thus be used to filter out hard samples from the training data.
We also observe that short but complex technical phrases can be more fragile than common language phrases of similar length.

## 3 Scaling the model to 7B

This section describes our effort to scale our model to 7B parameters and compare the performance against other approaches such as token-based LLMs on more challenging tasks such as summarization and summarization expansion (detailed in [SectionÂ 3.1](#S3.SS1 "3.1 Evaluation Tasks and Data â€£ 3 Scaling the model to 7B â€£ Large Concept Models: Language Modeling in a Sentence Representation Space")).

Based on the results in [SectionÂ 2.4](#S2.SS4 "2.4 Ablations â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space") where the two diffusion-based LCMs (One-Tower and Two-Tower) outperform the other variants, we decided to scale a diffusion model to 7B parameters.
We chose to scale Two-Tower given its smaller memory footprint, particularly when processing long contexts with a shallower contextualizer tower

The large 7B Two-Tower diffusion LCM has 5 layers in its contextualizer and 14 layers in its denoiser.
Its dimension has been extended to dmodel=4096subscriptdmodel4096{\textnormal{d}}\_{\text{model}}{=}4096.
Each self-attention layer has 32 attention heads.
All other parameters are kept the same as for the 1.6B Two-Tower model.
The model is pre-trained on a dataset of 2.3B documents, representing 2.7T tokens and 142.4B concepts/sentences.
We pre-trained this model on Metaâ€™s RSC for 124k optimization steps spanning 256 A100 GPUs with a total batch size of 1M concepts. We further extend the context length of this model to cover 2048 concepts instead of the 128 concepts in the ablation experiments.
We trained using the AdamW optimizer with (Î²1,Î²2)=(0.9,0.95)subscriptğ›½1subscriptğ›½20.90.95(\beta\_{1},\beta\_{2})=(0.9,0.95), Ïµ=1â€‹eitalic-Ïµ1ğ‘’\epsilon=1e-555 and a weight decay of 0.1. We use a cosine learning rate schedule, with warm-up of 10,000 steps up to LR=3â€‹eLR3ğ‘’\text{LR}=3e-444. To improve training stability we clip gradients at a maximum norm of g=10g10{\textnormal{g}}=10.
We subsequently finetune the 7B Two-Tower LCM on publicly available instruction tuning datasets followingÂ Chung etÂ al. ([2024](#bib.bib22)).
Each sample consists of a prompt and an answer and we back-propagate on answer sentences only. Each answer sequence is suffixed with the phrase â€œEnd of response.â€ to teach the model when to stop generating.
The finetuning data totals
389M sentences, of which 53M are answers
(i.e., targets).
For supervised finetuning, we use a cosine learning rate schedule with an initial rate of LR=3â€‹eLR3ğ‘’\text{LR}=3e-555 and finetune the model for 7 epochs with a batch size of
262K sentences (prompts and answers combined).
We will refer to the pre-trained model as Two-Tower-7B and the finetuned model as Two-Tower-7B-IT.

### 3.1 Evaluation Tasks and Data

This section describes the tasks on which we are evaluating and benchmarking our proposed model. We detail datasets, baselines and metrics.
For each task, the dataset was processed with the same sentence splitter and SONAR encoder as used in the LCM training.

#### 3.1.1 Metrics

As longform text generation is the main challenge for LCM, our benchmarking is mainly focused on generative tasks, which are notoriously more difficult to evaluate automatically. Therefore, we evaluate them with multiple automatic metrics, chosen to focus on complementary aspects on generation quality. All metrics used in this section are summarized in [TableÂ 8](#S3.T8 "In 3.1.1 Metrics â€£ 3.1 Evaluation Tasks and Data â€£ 3 Scaling the model to 7B â€£ Large Concept Models: Language Modeling in a Sentence Representation Space").

For summarization and summary expansion (defined below), we report the traditional reference-based Rouge-L metric (Lin, [2004](#bib.bib70)). As summarization models have a tendency to copy content from the source or from its own generated prefix, we report two additional word-based metrics. To evaluate how much content is directly copied from the source, we report the proportion of word 3-grams of the source that are present in the output (OVL-3). To evaluate repetitiveness of the generated texts, we report the portion of duplicated word 4-grams in the output (REP-4).

To complement word-based metrics with summarization-focused neural evaluation, we use two metrics introduced by Clark etÂ al. ([2023](#bib.bib24)): average probabilities of the SEAHORSE classifiers for Q4 (whether all the information in the summary is fully attributable to the source), denoted as SH-4 in the following and Q5 (whether the summary captures the main ideas of the source), denoted as SH-5.

As a metric of the overall fluency of the generated sentences, we report an average probability that the sentence is linguistically acceptable, as predicted by a classifier trained by Krishna etÂ al. ([2020](#bib.bib62)) on the CoLA dataset (Warstadt etÂ al., [2019](#bib.bib116)), further referred to as CoLA.
To evaluate the local coherence of the generated text, we report the average cosine similarity between each nğ‘›nâ€™th and n+2ğ‘›2n+2â€™th sentence (Parola etÂ al., [2023](#bib.bib88)).

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
| Task | Area | Metric | Description | Reference |
| Summarization | Target similarity | Râˆ’LRL\operatorname{R-L} | ROUGE-L | Lin ([2004](#bib.bib70)) |
|  | Source similarity | OVL-3 | N-grams overlap (N=3) |  |
|  | Grammaticality | REP-4 | Portion of duplicated N-grams (N=4) | Welleck etÂ al. ([2019](#bib.bib117)) |
|  | Fluency | CoLA | Sentence fluency classifier score | Krishna etÂ al. ([2020](#bib.bib62)) |
|  | Attribution | SH-4 | Seahorse-Large-Q4 score | Clark etÂ al. ([2023](#bib.bib24)) |
|  | Semantic coverage | SH-5 | Seahorse-Large-Q5 coverage score | Clark etÂ al. ([2023](#bib.bib24)) |
| Summary Expansion | Grammaticality | REP-4 | (see above) | Welleck etÂ al. ([2019](#bib.bib117)) |
| Fluency | CoLA | (see above) | Krishna etÂ al. ([2020](#bib.bib62)) |

Table 8: Summary of automatic metrics used in different tasks in [SectionÂ 3.1](#S3.SS1 "3.1 Evaluation Tasks and Data â€£ 3 Scaling the model to 7B â€£ Large Concept Models: Language Modeling in a Sentence Representation Space"). Order mostly follows paperâ€™s narrative.

#### 3.1.2 Summarization

##### Task and datasets.

When considering a relatively long document, a summarization task can be described as the act of generating a much shorter corresponding document that includes the essential information contained in the long document and the same logical structure linking the various pieces of essential information.

Summarization techniques can range from more extractive to more abstractive. Extractive techniques attempt to preserve in the summary the same vocabulary as that found in the long document, thereby shortening the long by removing details and superfluous wording. Abstractive techniques, on the other hand, attempt to produce the summary by rephrasing the essential pieces of information found in the long document. Our work focuses more on abstractive summarization, as such type of summarization cannot be performed without some form of understanding and reasoning.

We use the CNN DailyMailÂ (Hermann etÂ al., [2015](#bib.bib47)) and XSumÂ (Narayan etÂ al., [2018](#bib.bib80)) datasets. We also report results on the challenging LCFO corpus which takes long documents as input, approx. 5k words (Costa-jussÃ  etÂ al., [2024](#bib.bib27)). The task is to provide abstractive summaries with lengths representing 20%, 10%, and 5% of the input document.
Detailed statistics are provided in [TableÂ 9](#S3.T9 "In Task and datasets. â€£ 3.1.2 Summarization â€£ 3.1 Evaluation Tasks and Data â€£ 3 Scaling the model to 7B â€£ Large Concept Models: Language Modeling in a Sentence Representation Space").

|  |  |  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
|  |  | #Llama2 Tokens | | | #Sentences | | |  |
| Dataset | #Docs | Q1 | Q2 | Q3 | Q1 | Q2 | Q3 |
| CNN DailyMail | 11.5k | 605/61 | 892/78 | 1266/97 | 10/3 | 14/4 | 21/4 |  |
| XSum | 11.3k | 273/25 | 445/30 | 735/35 | 25/1 | 30/1 | 35/1 |  |
| LCFO.5% | 249 | 6559/341 | 7214/378 | 7916/418 | 209/12 | 295/15 | 527/18 |  |
| LCFO.10% | 249 | 6559/654 | 7214/718 | 7916/796 | 209/22 | 295/27 | 527/32 |  |
| LCFO.20% | 249 | 6559/1276 | 7214/1403 | 7916/1524 | 209/41 | 295/48 | 527/59 |  |

Table 9: Statistics of the test split of evaluation benchmarks. For each subset we report the number of documents and statistics of document and summary length in terms of sentences and Llama2 tokens. Each table cell shows â€œdocument/summaryâ€ length quartiles.

##### Baselines.

For CNN DailyMail and XSum, we compare against several baselines of different architectures (encoder-decoder transformer, decoder-only LLMs) that are known to perform well on summarization tasks. For encoder-decoder transformer models, we use T5 (Raffel etÂ al., [2020](#bib.bib93)).
For decoder-only LLMs, we choose Gemma-7B, Llama-3.1-8B and Mistral-7B-v0.3. We chose the published instruction-tuned models to compare with the LCM with the same training regime, and have a similar size (7B). Note that while T5 has much smaller sizes than the LCM, this is compensated by using models that are fine-tuned explicitly on the target evaluation dataset.

|  |  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- | --- |
| Model | Paradigm | CNN DailyMail | | | | | |
|  |  | Râˆ’LRL\operatorname{R-L}(â†‘â†‘\uparrow) | OVL-3 (â†‘â†‘\uparrow) | REP-4 (â†“â†“\downarrow) | CoLA (â†‘â†‘\uparrow) | SH-4 (â†‘â†‘\uparrow) | SH-5 (â†‘â†‘\uparrow) |
| Ground truth | â€” | 100.00 | 0.170 | 0.684 | 0.850 | 0.683 | 0.586 |
| T5-3B | SFT | 37.56 | 0.174 | 0.854 | 0.946 | 0.773 | 0.503 |
| Gemma-7B-IT | IFT | 31.14 | 0.245 | 1.032 | 0.963 | 0.740 | 0.560 |
| Mistral-7B-v0.3-IT | IFT | 36.06 | 0.200 | 0.780 | 0.972 | 0.780 | 0.676 |
| Llama-3.1-8B-IT | IFT | 34.97 | 0.248 | 0.928 | 0.973 | 0.763 | 0.692 |
| Two-Tower-7B-IT | IFT | 36.47 | 0.177 | 0.757 | 0.767 | 0.723 | 0.459 |
| Model | Paradigm | XSum | | | | | |
|  |  | Râˆ’LRL\operatorname{R-L}(â†‘â†‘\uparrow) | OVL-3 (â†‘â†‘\uparrow) | REP-4 (â†“â†“\downarrow) | CoLA (â†‘â†‘\uparrow) | SH-4 (â†‘â†‘\uparrow) | SH-5 (â†‘â†‘\uparrow) |
| Ground truth | â€” | 100.00 | 0.108 | 0.399 | 0.987 | 0.352 | 0.418 |
| T5-3B | â€” | 17.11 | 0.221 | 0.671 | 0.939 | 0.680 | 0.450 |
| Gemma-7B-IT | IFT | 18.20 | 0.177 | 0.620 | 0.769 | 0.546 | 0.446 |
| Mistral-7B-v0.3-IT | IFT | 21.22 | 0.162 | 0.480 | 0.922 | 0.633 | 0.621 |
| Llama-3.1-8B-IT | IFT | 20.35 | 0.186 | 0.501 | 0.941 | 0.687 | 0.658 |
| Two-Tower-7B-IT | IFT | 23.71 | 0.106 | 0.464 | 0.683 | 0.358 | 0.284 |

Table 10: Performance on the CNN DailyMail and XSum summarization tasks.

##### Summarization results.

[TableÂ 10](#S3.T10 "In Baselines. â€£ 3.1.2 Summarization â€£ 3.1 Evaluation Tasks and Data â€£ 3 Scaling the model to 7B â€£ Large Concept Models: Language Modeling in a Sentence Representation Space") contains the results of different baselines and our LCM model for summarization (CNN DailyMail and XSum).
We can notice that the LCM produces competitive Rouge-L scores when compared to a specifically tuned LLM (T5-3B) and even surpasses the instruct-finetuned LLMs.
Our model tends to generate more abstractive summaries rather than extractive ones, as shown by the lower OVL-3 scores.
The LCM produces fewer repetitions compared to LLMs, and more importantly, the repetition rate is closer to the ground truth one.
The LCM generates globally less fluent summaries according to CoLA classifier.
However, we can remark that even the human generated ground truth gets a lower score compared to the LLM.
A similar behavior is observed for the source attribution (SH-4) and semantic coverage (SH-5).
This may be explained by model-based metrics that are more biased towards LLM generated content.

##### Long-context summarization results.

[TableÂ 11](#S3.T11 "In Long-context summarization results. â€£ 3.1.2 Summarization â€£ 3.1 Evaluation Tasks and Data â€£ 3 Scaling the model to 7B â€£ Large Concept Models: Language Modeling in a Sentence Representation Space") presents the results for long-context summarization (LCFO.5%, LCFO.10% and LCFO.20%).
This is a challenging task for most of the models.
For example, Mistral-7B-v0.3-IT seems to be unable to follow the length instruction of the summaryâ€“it always generates summaries which length is about 50% of the source. Mistral-7B-v0.3-IT also has the highest SH-4 score, i.e., source attribution.
The summaries generated by Gemma-7B-IT tend to be longer than requested, while Llama-3.1-8B-IT generates summaries which length is the closest to the requested size.

The LCM has only seen a limited amount of long documents in the pretraining and fine-tuning data. Nevertheless, it performs well for this task.
It outperforms Mistral-7B-v0.3-IT and Gemma-7B-IT in the metric Rouge-L for the 5 and 10% conditions, and is close to Gemma-7B-IT for the 20% condition. We also observe that the LCM yields high SH-5 scores for all conditions, i.e., the summaries can be attributed to the source.

|  |  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- | --- |
| Method | WR | LCFO.5% | | | | | |
|  |  | Râˆ’LRL\operatorname{R-L}(â†‘â†‘\uparrow) | OVL-3 (â†‘â†‘\uparrow) | REP-4 (â†“â†“\downarrow) | CoLA (â†‘â†‘\uparrow) | SH-4 (â†‘â†‘\uparrow) | SH-5 (â†‘â†‘\uparrow) |
| Gemma-7B-IT | 0.107 | 25.21 | 0.151 | 4.711 | 0.688 | 0.357 | 0.174 |
| Mistral-7B-v0.3-IT | 0.512 | 21.36 | 0.532 | 5.997 | 0.854 | 0.656 | 0.296 |
| Llama-3.1-8B-IT | 0.076 | 37.67 | 0.190 | 2.767 | 0.931 | 0.488 | 0.314 |
| Two-Tower-7B-IT | 0.060 | 26.88 | 0.162 | 2.473 | 0.796 | 0.628 | 0.196 |
|  |  | LCFO.10% | | | | | |
|  |  | Râˆ’LRL\operatorname{R-L}(â†‘â†‘\uparrow) | OVL-3 (â†‘â†‘\uparrow) | REP-4 (â†“â†“\downarrow) | CoLA (â†‘â†‘\uparrow) | SH-4 (â†‘â†‘\uparrow) | SH-5 (â†‘â†‘\uparrow) |
| Gemma-7B-IT | 0.150 | 29.25 | 0.164 | 6.427 | 0.667 | 0.377 | 0.194 |
| Mistral-7B-v0.3-IT | 0.549 | 25.00 | 0.537 | 6.289 | 0.848 | 0.660 | 0.306 |
| Llama-3.1-8B-IT | 0.128 | 42.85 | 0.243 | 3.804 | 0.907 | 0.486 | 0.310 |
| Two-Tower-7B-IT | 0.089 | 29.38 | 0.202 | 3.00 | 0.791 | 0.623 | 0.183 |
|  |  | LCFO.20% | | | | | |
|  |  | Râˆ’LRL\operatorname{R-L}(â†‘â†‘\uparrow) | OVL-3 (â†‘â†‘\uparrow) | REP-4 (â†“â†“\downarrow) | CoLA (â†‘â†‘\uparrow) | SH-4 (â†‘â†‘\uparrow) | SH-5 (â†‘â†‘\uparrow) |
| Gemma-7B-IT | 0.257 | 33.32 | 0.201 | 9.188 | 0.603 | 0.425 | 0.239 |
| Mistral-7B-v0.3-IT | 0.493 | 28.82 | 0.527 | 5.806 | 0.858 | 0.658 | 0.293 |
| Llama-3.1-8B-IT | 0.179 | 46.92 | 0.272 | 4.783 | 0.888 | 0.485 | 0.315 |
| Two-Tower-7B-IT | 0.140 | 31.74 | 0.253 | 3.664 | 0.779 | 0.613 | 0.187 |

Table 11: Performance on the long-context summarization task of LCFO. WR is the word count ratio between the generated text and the source document.

Finally, we observe that Llama-3.1-8B-IT performs substantially better than the other LLMs, according to Rouge-L, while all LLMs have similar performance on the CNN DailyMail and XSum summarization tasks. This could be explained by training data contamination for Llama-3.1-8B-IT, or by the fact that the other two LLMs struggle to handle the long input context.

## 4 Large Concept Model Extensions

In this section, we explore several extension of the Large Concept Model. First, we evaluate the LCM on the new task of summary expansion, i.e., given a summary, create a longer text. We then showcase the good zero-shot generalization performance of the LCM. Finally, we explore an approach to add higher level information beyond sentences.

### 4.1 Summary Expansion

##### Task and datasets.

When considering a short and concise document that has similar properties to those of a summary (i.e., mainly a stand-alone document that abstracts from details), a summary expansion task can be described as the act of generating a much longer document that preserves the essential elements found in the corresponding short document, as well as the logical structure that connects such elements. As this is a more freely generative task, an additional requirement to be taken into consideration is that of coherence (for example, the detailed information included in one generated sentence should not contradict that included in another sentence). The summary expansion task presented here consists in taking summaries as inputs, from CNN DailyMail and XSum, and generating a long document.
Note that the goal is not to recreate the factual information of the initial document rather than evaluating the capability of the model to extend the input text in a meaningful and fluent way.
We use similar baselines and metrics as in the previous section [3.1.2](#S3.SS1.SSS2 "3.1.2 Summarization â€£ 3.1 Evaluation Tasks and Data â€£ 3 Scaling the model to 7B â€£ Large Concept Models: Language Modeling in a Sentence Representation Space").

|  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- |
| CNN DailyMail | | | | | |
| Method | WR | Râˆ’LRL\operatorname{R-L}(â†‘â†‘\uparrow) | OVL-3 (â†‘â†‘\uparrow) | REP-4 (â†“â†“\downarrow) | CoLA (â†‘â†‘\uparrow) |
| Gemma-7B-IT | 6.8 | 35.54 | 0.801 | 2.104 | 0.951 |
| Mistral-7B-v0.3-IT | 6.4 | 34.24 | 0.817 | 2.063 | 0.959 |
| Llama-3.1-8B-IT | 8.5 | 37.76 | 0.822 | 2.582 | 0.844 |
| Two-Tower-7B-IT | 6.3 | 30.85 | 0.726 | 2.911 | 0.474 |
| XSum | | | | | |
| Method | WR | Râˆ’LRL\operatorname{R-L}(â†‘â†‘\uparrow) | OVL-3 (â†‘â†‘\uparrow) | REP-4 (â†“â†“\downarrow) | CoLA (â†‘â†‘\uparrow) |
| Gemma-7B-IT | 19.5 | 17.89 | 0.963 | 10.238 | 0.116 |
| Mistral-7B-v0.3-IT | 1.6 | 29.31 | 0.893 | 2.268 | 0.939 |
| Llama-3.1-8B-IT | 19.8 | 28.84 | 0.915 | 2.543 | 0.898 |
| Two-Tower-7B-IT | 7.1 | 23.82 | 0.561 | 1.542 | 0.603 |

Table 12: Performance on the summary expansion tasks of CNN DailyMail and XSum, evaluated with the metrics described in [TableÂ 8](#S3.T8 "In 3.1.1 Metrics â€£ 3.1 Evaluation Tasks and Data â€£ 3 Scaling the model to 7B â€£ Large Concept Models: Language Modeling in a Sentence Representation Space"). WR is the word count ratio between the hypothesis and the source summary.

##### Results.

[TableÂ 12](#S4.T12 "In Task and datasets. â€£ 4.1 Summary Expansion â€£ 4 Large Concept Model Extensions â€£ Large Concept Models: Language Modeling in a Sentence Representation Space") shows the results of the summary expansion for CNN DailyMail and XSum.
First of all, regarding the word count ratio, we can see different behaviours for the two corpora.
For CNN DailyMail, the models tend to generate texts that are 6 times larger than the input. Llama-3.1-8B-IT produces even longer outputs (factor 8 instead of 6).
But for XSum, while Gemma-7B-IT and Llama-3.1-8B-IT generate very long texts (almost 20 times longer than the prompt), the LCM generates output of approximately the same length ratio as for CNN DailyMail. Only Mistral-7B-v0.3-IT fails to generate long outputs for this corpus.

Then, we clearly see a different trend compared to the summarization task.
The LLMs get higher Rouge-L scores compared to the LCM.
As mentioned above, the goal of this task is not to recreate the original document. However, the Râˆ’LRL\operatorname{R-L} score tells us how much of the content of the full document can be recreated.
Contrary to the LLMs, our model tends to generate different sentences compared to the original document from which the summary has been created (with an exception for Gemma-7B-IT on XSum).
This is expected since our model generates embeddings that are then processed by a decoder trained on a translation task that tends to paraphrase the initial content.
However, the CoLA results show that this comes along with lower fluency, especially for CNN DailyMail.

### 4.2 Zero-shot generalization performance

SONAR is a semantic space that can represent 200 languages.
In this paper, all experiments presented so far have been done on English text.
In this section, we explore the capability of our proposed LCM approach to process other languages in a zero-shot fashion by leveraging SONARâ€™s ability to represent multilingual data.

We use the XLSumÂ (Hasan etÂ al., [2021](#bib.bib45)) corpus, a large scale multilingual abstractive news summarization benchmark covering 45 languages. We score model outputs using the multilingual rouge scoring scripts released with the benchmark.666<https://github.com/csebuetnlp/xl-sum/tree/master/multilingual_rouge_scoring> Note that Rouge-L scores are heavily dependent on language-specific text tokenization and stemming. Unless provided in the aforementioned scripts, we tokenize the model outputs and references with the default tokenization from Lin ([2004](#bib.bib70)). Languages like Korean, Telugu and Tamil can benefit from a more appropriate stemming and tokenization.

We compare the LCM performance with Llama-3.1-8B-IT which officially supports eight languages: English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai. According to The Llama3 team ([2024](#bib.bib109)), the model has seen many additional languages during pretraining, but was instruction finetuned on those eight languages only. The LCM, on the other hand, has never seen any language other than English, and we do not use the English XLSum training data either.

![Refer to caption]()

Figure 16: Rouge-L scores on XLSum for Llama-3.1-8B-IT and Two-Tower-7B-IT.

We report Rouge-L scores for 42 languages in [FigureÂ 16](#S4.F16 "In 4.2 Zero-shot generalization performance â€£ 4 Large Concept Model Extensions â€£ Large Concept Models: Language Modeling in a Sentence Representation Space"). Three languages were excluded since they are currently not supported by SONAR: Pidgin, Serbian in Latin script and Uzbek in Cyrillic script.
The LCM substantially outperforms Llama-3.1-8B-IT on English (23.5 compared to 20.7 Rouge-L) and on the average over all six languages officially supported by both models and included in XLSum (20.2 versus 19.7 Rouge-L).777English, French, Hindi, Portuguese, Spanish and Thai.
We also observe that the LCM generalizes very well to many other languages, in particular low-resource languages like Southern Pashto, Burmese, Hausa or Welsch which all have Rouge-L scores greater than 20. Other well performing low-resource languages are Somali, Igbo or Kirundi. Finally, the LCM obtains a Rouge-L score of 30.4 on Vietnamese.
Overall, these results highlight the impressive zero-shot generalization performance of the LCM to languages it has never seen.

### 4.3 Exploring explicit planning

When writing long-form text, it is often practical to first think about how to structure our narrative. Indeed Dijk ([1977](#bib.bib30)) states that all texts have an innate macrostructure i.e., a global discourse structure spanning the entirety of the text. In general scientific discourse, one such macrostructure is that of problem-solving (Heffernan and Teufel, [2022](#bib.bib46)), where an author must first motivate and describe the problem, before proposing an appropriate solution.

Composing such a macrostructure is no easy task. Yet in the realm of LLMs, there have been many recent efforts to help guide model generation using similar structures. One such popular approach is that of creating outlines, which provide a high-level overview of the desired narrative (Li etÂ al., [2024](#bib.bib68)). An alternative approach to outlines is creating summaries of future targets, which the model can then expand upon (Sun etÂ al., [2022](#bib.bib107)).

Given the nature of the LCM operating at the concept level, it naturally creates long-form output. Therefore, it is important to ensure that the model is capable of creating coherent generations given the multitudes of possibilities for next concept prediction. In order to address this, we envision an
explicit capability for planning. Similar to creating summaries, we propose a complementary *planning model* which creates a high-level overview of what should be generated next, given the prior context. The proposed plan could span multiple concepts, such as a paragraph. The LCM is conditioned on this plan, before it then generates the subsequent output sequence.

Operationally the model predicts auto-regressively a sequence of concepts followed by a *break* concept, which represents a natural topic cadence such as a paragraph break. Once the *break* concept is predicted, the large planning model (LPM) generates a plan in order to condition the LCM for prediction of the subsequent sequence. The model then continues generation as usual conditioned on both the prior concepts and the proposed plan. An overview of the approach is shown in [FigureÂ 17](#S4.F17 "In 4.3 Exploring explicit planning â€£ 4 Large Concept Model Extensions â€£ Large Concept Models: Language Modeling in a Sentence Representation Space").

![Refer to caption](/html/2412.08821/assets/figures/LPCM.png)

Figure 17: LCM conditioned on both the prior context and a high-level plan for the next sequence.

Although we envision a separate (but complementary) model for planning,
we present here an initial experiment using a simplified single-model approach
where the LCM is trained in a multitask setting to also predict both the *break* concepts and plans.
Additionally, instead of the idealized plan spanning multiple concepts (such as a paragraph),
we use a single concept in order to capture what should come next (i.e. a *plan* concept).
We call this simplified multitask approach a Large Planning Concept Model (LPCM).

#### Methodology

In order to evaluate this single-model approach, we perform an ablation study. As a baseline method, we train a One-Tower LCM (cf. [SectionÂ 2.3.3](#S2.SS3.SSS3 "2.3.3 One-Tower Diffusion LCM â€£ 2.3 Large Concept Model variants â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space")) without any visibility to *break* or *plan* concepts. We then subsequently train a LPCM with the same number of parameters as the baseline. Both models were trained on the same data888Compared to previous experiments, we use a different data mixture more favorable for long-form generation. for the same number of steps.

##### Data preprocessing.

In order to represent *break* concepts, we begin by first segmenting the data into paragraphs.
Given that most real world datasets are absent of paragraph structure (or it is not easy to recover),
we apply the Segment Any Text (Frohmann etÂ al., [2024](#bib.bib38)) paragraph splitting API999<https://github.com/segment-any-text/wtpsplit>.
We additionally force each paragraph to be less than 10 sentences, and merge small (e.g. one sentence) consecutive paragraphs together.
In order to represent *plan* concepts, we generate synthetic high-level topic description for each preceding segmented paragraph using an existing open-sourced LLM, namely Llama-3.1-8B-IT,
which offers a good trade-off between the generated topic quality and the generation speed.
The system prompt used to generate these topic descriptions is listed in [AppendixÂ C](#A3 "Appendix C System prompt: Generation of Topic Descriptions â€£ Large Concept Models: Language Modeling in a Sentence Representation Space").
In total we process approximately 320M
paragraphs with topic descriptions, spanning 1.5B segmented concepts (i.e. approximately 30B tokens).

##### Metrics.

We focus on coherence as our main measure of evaluation. Previous ablations (cf. [SectionÂ 2.4](#S2.SS4 "2.4 Ablations â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space")) used the coherence metric introduced by Jwalapuram etÂ al. ([2022](#bib.bib57)). However, we explore here LLM-as-a-judge as an alternative. Specifically, we use Llama-3.1-8B-IT in order to evaluate the coherence of the generated model outputs, which is prompted to return an overall coherence score between [0,5]05\left[0,5\right].
The prompt used is listed in [AppendixÂ D](#A4 "Appendix D User prompt: LLM As a Judge - Coherence â€£ Large Concept Models: Language Modeling in a Sentence Representation Space").
In order to validate this prompt, we evaluate it against a dataset of human judgements introduced by Jwalapuram etÂ al. ([2022](#bib.bib57)), and observed it reported an agreement101010Krippendorffâ€™s Î±ğ›¼\alpha = 0.48 with human annotators which improves upon their coherence model. We therefore choose this metric for our evaluation.
To be consistent across both model results, we do not include the special *break* or *plan* concepts generated by the LPCM when calculating coherence scores.

|  |  |
| --- | --- |
|  | Llama-3.1-8B-IT (â†‘â†‘\uparrow) |
| LPCM | 2.82 Â±plus-or-minus\pm 0.62 |
| Baseline | 2.74 Â±plus-or-minus\pm 0.70 |

Table 13: LPCM ablation coherence score results.

#### Results

We provide the results of our ablation experiment in [TableÂ 13](#S4.T13 "In Metrics. â€£ Methodology â€£ 4.3 Exploring explicit planning â€£ 4 Large Concept Model Extensions â€£ Large Concept Models: Language Modeling in a Sentence Representation Space"). Results are reported over a held-out subset of Cosmopedia (BenÂ Allal etÂ al., [2024](#bib.bib15)) following instruction fine-tuning, similar to previous ablations (cf. [SectionÂ 2.4](#S2.SS4 "2.4 Ablations â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space")).
We observe that the LPCM achieves significantly111111Significance observed at the 99.9% level. higher coherence scores (significance was measured using a paired t-test) than the baseline One-Tower LCM. This finding suggests that the LPCM is capable of producing significantly more coherent outputs than the LCM as a result of the additional structure coming from predicted plan concepts, helping the LPCM produce a more coherent narrative, which is essential for the objective of generating long-form output.

## 5 Related work

### 5.1 Sentence representations

##### Multilingual sentence representations

Learning effective sentence embeddings has been a well studied subject in recent years.
Significant progress has been made in this field, largely due to the capabilities of transformer-based language models that by learning contextual representations for individual tokens (Devlin etÂ al., [2018](#bib.bib28); Conneau etÂ al., [2020](#bib.bib25)), are able to effectively learn the semantics of language.
However, such models are not optimal to create sentence representations.

Following approaches built upon these initial works, and aimed at learning general sentence representations, leveraging dual encoder architectures (Guo etÂ al., [2018](#bib.bib43); Reimers and Gurevych, [2019](#bib.bib94); Ni etÂ al., [2021](#bib.bib81)).
These architectures encode the source and target into a common embedding space, and use a distance-based metric to create an alignment loss that approximate semantically identical sentences.
Such architectures have been extended to leverage multilingual data to create general, aligned embedding spaces across languages (Feng etÂ al., [2020](#bib.bib37); Janeiro etÂ al., [2024](#bib.bib54); Sturua etÂ al., [2024](#bib.bib104)).
Initial approaches leveraged the contrastive loss to align translations across languages (Feng etÂ al., [2020](#bib.bib37); Yang etÂ al., [2019](#bib.bib118)), using only translation data to train.
Other architectural changes, namely using token-level objectives combined with the sentence level objectives, have proven useful to improve the quality of multilingual sentence representations based on translation data only (Li etÂ al., [2023](#bib.bib69); Janeiro etÂ al., [2024](#bib.bib54)).
Recent approaches explore using data from other tasks, besides translation data, to increase the generality of the sentence representations (Wang etÂ al., [2024b](#bib.bib115); Mohr etÂ al., [2024](#bib.bib78)).
Other approaches change their embeddings per task, either with task-specific prompts (Wang etÂ al., [2024b](#bib.bib115); Su etÂ al., [2022](#bib.bib105); Lee etÂ al., [2024b](#bib.bib66)) or with task-specific parameters (Sturua etÂ al., [2024](#bib.bib104)).

Another successful line of work to create general purpose, multilingual, sentence representations is to leverage the translation objective.
LASER (Artetxe and Schwenk, [2019](#bib.bib5)), and SONAR (Duquenne etÂ al., [2023b](#bib.bib35)) leverage an encoder-decoder architecture, with a fixed-size sentence representation between the encoder and the decoder, trained with a translation objective.
SONAR is initialized from the NLLB-200 model (NLLB Team etÂ al., [2022](#bib.bib85)), and covers 200 languages, making it one of the open-source models with the widest language coverage.
SONAR also provides open-source speech encoders aligned to their sentence encoders for 73 languages (Seamless Communication etÂ al., [2023a](#bib.bib98)), aligned through a teacher-student approach.
SONAR has been used as the basis for several works (Seamless Communication etÂ al., [2023a](#bib.bib98), [b](#bib.bib99); Chen etÂ al., [2023a](#bib.bib19)), and its speech decoders have been extended to keep the expressiveness of the original speech (Duquenne etÂ al., [2023a](#bib.bib34)).

##### Joint speech/text sentence representations

There has been a large body of research on unsupervised representation learning for monolingual (Baevski etÂ al., [2020](#bib.bib10)) and multilingual speech (Babu etÂ al., [2022](#bib.bib8)), with recently w2v-bert (Chung etÂ al., [2021](#bib.bib23)) that combines contrastive learning and masked language modeling to learn self-supervised representations from speech. Other works explored multilingual and multimodal (speech/text) pre-training methods, including mSLAM (Bapna etÂ al., [2022](#bib.bib12)). Finally, Duquenne etÂ al. ([2021](#bib.bib32)), followed by Khurana etÂ al. ([2022](#bib.bib59)), introduced multilingual and multimodal sentence embeddings, extending a pre-existing multilingual text sentence embedding space to the speech modality with a distillation approach. Duquenne etÂ al. ([2022](#bib.bib33), [2023c](#bib.bib36)) also showed that it is possible to efficiently decode multilingual speech sentence embeddings with decoders trained on text sentence embeddings into different languages, to perform zero-shot speech translation.

##### LLM based sentence representations

Several text representation methods have been proposed which are based on existing LLMs.
Wang etÂ al. ([2024a](#bib.bib114)) proposed extracting text embeddings from the last token of LLMs fine-tuned with instructions on contrastive data. Lee etÂ al. ([2024a](#bib.bib64)) improved text embedding capabilities of fine-tuned LLMs by removing the causal attention mask and applying extra nonlinear layers before pooling the token embeddings.
Embeddings as a service are supported by some commercial LLM providers, for example, Mistral-embed.121212<https://docs.mistral.ai/capabilities/embeddings/>
Such embeddings proved competitive on retrieval benchmarks; however, to the best of our knowledge, their applicability to reconstructing the texts back from the embedding space has not been demonstrated.

### 5.2 Multilingual LLMs

Most of the leading LLMs have been trained on texts in several languages. [TableÂ 1](#S2.T1 "In 2.1 The SONAR embedding space â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space") summarizes the coverage of several of them. Nevertheless, the pretraining data of these LLMs seems to be mainly English texts. For example, The Llama3 team ([2024](#bib.bib109)) mentions that pretraining data contains significantly more English texts, requiring continued pre-training with multilingual data, out of which 34.6% is translated reasoning data.

There are also several efforts to train LLMs optimized on specific languages, e.g. LeoLM for German,131313<https://laion.ai/blog/leo-lm/> Fuano
for Italian (Bacciu etÂ al., [2024](#bib.bib9)), ALLaM for Arabic (Bari etÂ al., [2024](#bib.bib14)), and several models for Chinese: ErniBot,141414<http://research.baidu.com/Blog/index-view?id=183> Tongyi Qianwen,151515<https://www.alibabacloud.com/en/solutions/generative-ai> or ChatGLM (Team GLM etÂ al., [2024](#bib.bib108)). Some adaptations of LLMs to a massive number of languages also exist. LOLA (Srivastava etÂ al., [2024](#bib.bib103)) is a recent mixture-of-experts LLM supporting 160 languages, MALA-500 (Ji etÂ al., [2024](#bib.bib55)) adapts LLaMA2 to 546 languages. However, such models typically face a trade-off between language coverage and other capabilities. For example, the Aya model (ÃœstÃ¼n etÂ al., [2024](#bib.bib112)), following instructions in 101 languages, was superseded by Aya-23 (Aryabumi etÂ al., [2024](#bib.bib6)) that exchanged some breadth for depth, focusing on 23 languages only. The LCM architecture, combining a language-agnostic model for knowledge and reasoning with potentially language-specialized encoders and decoders, is expected to exhibit this trade-off to a lesser extent.

### 5.3 Alternative LLM architectures

Predicting the next state in the embedding space is a core idea of the Joint Embedding Predictive Architecture (Jepa) proposed by LeCun ([2022](#bib.bib63)). This idea has been implemented for images (I-JEPA by Assran etÂ al. ([2024](#bib.bib7))) and video (V-JEPA by Bardes etÂ al. ([2024](#bib.bib13))) as a self-supervised approach to learning representations. For language, equivalent models have not yet been explored.

##### Sentence embeddings for language modeling.

For text completion, Ippolito etÂ al. ([2020](#bib.bib53)) proposed a sentence-level language model operating by choosing the next sentence from a finite set of candidates. Their model demonstrated success in selecting appropriate continuations for short stories, but it has not been scaled to longer inputs or to fully generative outputs. Golestani etÂ al. ([2021](#bib.bib41)) studied a similar problem in the even more restrictive sentence ordered setting, but with a more thorough study of architectural choices. The INSET architecture (Huang etÂ al., [2020](#bib.bib52)) solves the sentence infilling task by combining a denoising autoencoder that encodes sentences into fixed-size vectors and decodes them back and a bidirectional transformer that predicts the embedding of a missing sentence.

Marfurt and Henderson ([2021](#bib.bib76)) and Cornille etÂ al. ([2024](#bib.bib26)) used predicted next sentence embeddings in a fully generative setting, for summarization and generic language modeling, respectively. However, their architectures considered sentence-level connections only as an addition to the token-level connections across sentences, not as their replacement.

In a recent work of An etÂ al. ([2024](#bib.bib3)), the SentenceVAE architecture performs language modeling on the sentence level using a sentence encoder to prepare the inputs and a sentence decoder to produce the outputs. However, its input and output embedding spaces are not tied, so the inference is only possible by decoding each predicted sentence into text and then re-encoding it for adding it to the context.

##### Language modeling with diffusion.

A series of more recent works tried adapting diffusion modeling, originally developed for continuous data, to the discrete text domain. The PLANNER architecture (Zhang etÂ al., [2023](#bib.bib124)) consists of a variational autoencoder for paragraphs and a diffusion model trained to predict latent autoencoder representations conditional on the textual context or on the class label. Lovelace etÂ al. ([2024](#bib.bib73)) augmented a decoder-only language model with an encoded semantic proposal of the continuation text, with an easily guidable diffusion model predicting the embedding of the next proposal. A TEncDM model (Shabalin etÂ al., [2024](#bib.bib100)) performs diffusion in the space of contextual token embeddings which are then decoded non-autoregressively.

Some applications of diffusion to sequence modeling have targeted the planning capabilities of the sequence models. Semformer (Yin etÂ al., [2024](#bib.bib120)) proposed training transformers language models to plan several steps ahead by including special planning tokens, the representations of which are trained to be informative about the future tokens. Ye etÂ al. ([2024](#bib.bib119)) applied discrete diffusion to language models as an alternative to autoregressive generation, more suitable for tasks that require multi-step planning. Ubukata etÂ al. ([2024](#bib.bib111)) give an overview of applications of diffusion for planning tasks, but most of them are not concerned with the language domain.

Overall, while many of the previous works used hidden representations for language modeling or related tasks, all of them either relied on token-level inputs or outputs, or were not intented for generating texts of arbitrary length. The LCM seems to be the first fully generative language model implemented fully in a highly semantic, reconstructable sentence representation space.

## 6 Limitations

In this section we discuss the possible limitations of the presented Large Concept Modeling approach.

##### Choice of the embedding space.

The choice and design of the embedding space plays a crucial role in the LCM modeling approach.

* â€¢

  The SONAR embedding space was chosen for its good multilingual and multimodal representations, as well as the availability of a massively multilingual decoder, which achieves excellent results in both translation and auto-encoding. However, the SONAR model was trained on very specific training data, namely bitext machine translation data containing rather short sentences. This has several consequences:

  1. 1.

     SONAR is trained to sustain a local geometry (sentences with very similar meanings are geometrically close) with no special guarantees for sentences that are only loosely related.
     Yet, predicting next sentences distribution requires the space to operate well globally.
  2. 2.

     SONAR auto-encodes surprisingly well texts containing links, references, or merely numbers or code data.
     Yet, such texts tend to be fragile, highlighting a distribution mismatch between the SONAR training data and commonly used LLM pre-training text corpora.
     Therefore, the accurate prediction of the sentences containing such a content (non-negligible in LCM pre-training data) will be hard for any LCM SONAR based model. For instance, the factuality of fragile generated sentences may easily be compromised.
* â€¢

  Using a frozen encoder represents some interesting trade-offs.
  Any frozen encoder which is learned in a different data context, and with no a-priori strong connection to LCM modeling, may be suboptimal compared to encoders that are learned in an end-to-end fashion (with the loss coming from the decoder).
  At the same time, learning an encoder within end-to-end training can be challenging and the resulting space is not guaranteed to result in good semantic representations shared across languages and modalities.

  Training the concept representation and the LCM end-to-end would also be less data and compute efficient since all modeling data should be multilingual and -modal, bearing the risk of modality competition.

##### Concept granularity

* â€¢

  In this work, the definition of concepts is interpreted at sentence level. However, the manifold of possible next sentences is very wide, attributing a proper probability to each of such sentences is much harder (even with a modeling within the latent space) that to the discrete set of tokens.
* â€¢

  In NLP, we encounter sentences of variable length. Combinatorial complexity of possible next sentences grows exponentially with the maximum character length.
  The choice of granularity for LCM is not trivial as long sentences (>120 characters) could reasonably be considered as several concepts.
  However, any finer splitting of such sentences does not necessary separate well these concepts. This shows the limitation of a fixed size embedding representation for one sentence. Text splitting (such as sentence splitting) or one-to-many mapping of a sentence into several embeddings is a major future direction of research.
* â€¢

  Each document in a training corpus typically contains a sequence of unique sentences or a little number of repetitions. This data sparsity effect manifests as well at large corpora level: the large majority of sentences are merely unique. In principle, this issue can be addressed with higher-level semantic embedding representations. These higher-level representations come with trade-off between requirement of lossless data encoding (think of named-entities or numbers, critical in many language modeling tasks) and good level of abstraction to enable reasoning capabilities. Compared to a monolingual auto-encoder which would simply compress input sentences, SONAR offers semantic representations with good auto-encoding quality but still certainly sacrificing generalization capacities.
* â€¢

  This generalization issue can be partially mitigated by splitting or encoding input text as new conceptual units which are more commonly shared across source documents. This is in the spirit of stemming or lemmatization techniques studied in NLP for words.
  That being said, building such conceptual units that are also language and modality agnostic is a challenging task. Such shared multilingual and multimodal conceptual units are also key for generalization across languages and across modalities. To maximize cross-lingual and cross-modal transfers, Large Concept Models should be exposed to a richer variety of multilingual and multi-modal data.

##### Continuous versus discrete

* â€¢

  Diffusion modeling has proven to be very efficient in generative modeling of continuous data like images or speech. As previously stated, sentences in the SONAR space, despite being represented as continuous vectors, remain discrete combinatorial objects. This makes diffusion modeling struggle on the text modality (either at word or sentence embedding level).
* â€¢

  The contrastive nature of cross-entropy loss based on softmax outputs which is used for next token prediction plays a critical role for many downstream task where higher accuracy is required (e.g. MCQ tasks, code or math generation).
  On the opposite, continuous diffusion modeling does not allow to integrate such a contrastive objective.
* â€¢

  The Quant-LCM could be a way to address the discrete nature of text while modeling on coarse-to-fine semantic units shared across languages and modalities. The limited performance of the Quant-LCM approaches presented in this paper may be explained by the fact that SONAR space was not trained to be efficiently quantizable, yielding a significant number of codebooks and a large amount of units per codebook. Therefore, the current SONAR quantization suffers from the exponentially increasing number of RVQ units combinations which does not solve the data sparsity/uniqueness issue discussed earlier.
  This indicates once again the importance of developing a new representation space, either continuous or discrete, for the Large Concept Model.

## 7 Acknowledgments

We would like to thank
Robbie Adkins,
Can Balioglu,
Joy Chen,
Pascale Fung,
Jason Holland,
Amita Kamath,
Justine Kao,
Sagar Miglani,
Alice Rakotoarison,
Abhilasha Sancheti,
Arjang Talattof,
Ellen Tan,
Carleigh Wood,
Shireen Yates,
Bokai Yu and
Luke Zettlemoyer
for comments and suggestions on this work, as well helping us to improve this paper.

## 8 Conclusion and Future Work

Current best practice for large scale language modeling is to operate at the token level, i.e. to learn to predict the next tokens given a sequence of preceding tokens. There is a large body of research on improvements of LLMs, but most works concentrate on incremental changes and do not question the main underlying architecture.
In this paper, we have proposed a new architecture, named a Large Concept Model (LCM), which substantially differs from current LLMs in two aspects:
1)Â all modeling is performed in a high-dimensional embedding space instead of on a discrete token representation;
and 2)Â modeling is not instantiated in a particular language or modality, but at a higher semantic and abstract level. We have named the general form of this representation aÂ â€œconceptâ€.

In this paper, to verify the feasibility of the high-level idea, we have assumed that a concept corresponds to a sentence in the text domain, or an equivalent speech segment, and that the embeddings are obtained by the freely available SONAR sentence encoder (Duquenne etÂ al., [2023b](#bib.bib35)). With respect to the specific architecture of the LCM, we have first shown that directly minimizing the MSE loss in the embedding space does not yield good results. We then explored several architectures based on a diffusion process: the One-Tower and Two-Tower LCM, as well as a Quant-LCM which uses quantization of SONAR representations and then modeling on these discrete units.
These ablation experiments were performed with models with 1.6B parameters and focused on the generative task of continuing a sequence of sentences.
We have then scaled our models to a size of 7B parameters and instruction-finetuned them on several summarization and summary expansion tasks. We provide a detailed comparison to other public models of the same size, namely Gemma, Mistral and Llama.

By design, a LCM exhibits strong zero-shot generalization performance. In this paper, we trained models on English texts only, and applied them to text in other languages, without any additional training data, neither aligned nor unlabeled.
The LCM outperforms Llama-3.1-8B-IT on English and on the average over foreign languages officially supported by the LLM.
The LCM itself could also be trained on multilingual- and model data to acquire knowledge from these sources. We will explore this in future versions of the LCM.
In short, all languages and modalities are first class citizens and handled equally at all stages of a LCM.

We have observed that next sentence prediction is substantially more challenging than next token prediction.
First, given that we operate in an embedding space and at a higher semantic level, the number of possible sentences is virtually unlimited, while token vocabularies are usually in the range of 100k. Second, even given a long context, there is unavoidably more ambiguity in choosing the next sentence than the next token. And third, the usual softmax output layer over the fixed size token vocabulary provides a normalized probability distribution over all possible token continuations. Theoretically, a diffusion process should be able to learn a probability distribution over an output embedding space, but our current experimental evidence indicates that more research is needed to take full advantage of the properties of Large Concept Models. As an example, the ability to sample multiple embeddings and associate a score would enable beam search to find the best sequence of sentences.
Finally, small modeling errors could yield predictions in the embedding space which do not correspond to valid sentences, i.e. that cannot be decoded into a syntactically and semantically correct sentence. We will work on alternative concept embeddings to SONAR which would be better suited to the next sentence prediction task, and would improve modeling approaches in that concept embedding space.

We see the models and results discussed in this paper as a step towards increasing scientific diversity and a move away from current best practice in large scale language modeling.
We acknowledge that there is still a long path to reach the performance of current flagship LLMs. This will require of course further improving the core architecture, but also careful data selection and curation, extensive ablations, optimized and diverse instruction fine-tuning, and finally, scaling to models with more than 70B parameters.

We open-source the full training code of all our LCM variants, together with a set of supporting scripts,161616<https://github.com/facebookresearch/large_concept_model> to make it easy for other teams to train LCM models.
By these means, we hope to foster research on alternative LLMs and contribute to advance the field of machine intelligence.

## References

* Aghajanyan etÂ al. (2023)

  A.Â Aghajanyan, L.Â Yu, A.Â Conneau, W.-N. Hsu, K.Â Hambardzumyan, S.Â Zhang,
  S.Â Roller, N.Â Goyal, O.Â Levy, and L.Â Zettlemoyer.
  Scaling laws for generative mixed-modal language models.
  In *International Conference on Machine Learning*, pages
  265â€“279. PMLR, 2023.
* Almazrouei etÂ al. (2023)

  E.Â Almazrouei, H.Â Alobeidli, A.Â Alshamsi, A.Â Cappelli, R.Â Cojocaru, M.Â Debbah,
  Ã‰tienne Goffinet, D.Â Hesslow, J.Â Launay, Q.Â Malartic, D.Â Mazzotta, B.Â Noune,
  B.Â Pannier, and G.Â Penedo.
  The Falcon series of open language models.
  *ArXiv*, abs/2311.16867, 2023.
  URL <https://arxiv.org/pdf/2311.16867>.
* An etÂ al. (2024)

  H.Â An, Y.Â Chen, Z.Â Sun, and X.Â Li.
  SentenceVAE: Enable next-sentence prediction for large language
  models with faster speed, higher accuracy and longer context.
  *arXiv preprint arXiv:2408.00655*, 2024.
* Anthropic (2024)

  Anthropic.
  The Claude 3 model family: Opus, sonnet, haiku, 2024.
  URL
  <https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf>.
* Artetxe and Schwenk (2019)

  M.Â Artetxe and H.Â Schwenk.
  Massively multilingual sentence embeddings for zero-shot
  cross-lingual transfer and beyond.
  *TACL*, pages 597â€“610, 2019.
* Aryabumi etÂ al. (2024)

  V.Â Aryabumi, J.Â Dang, D.Â Talupuru, S.Â Dash, D.Â Cairuz, H.Â Lin, B.Â Venkitesh,
  M.Â Smith, K.Â Marchisio, S.Â Ruder, etÂ al.
  Aya 23: Open weight releases to further multilingual progress.
  *arXiv preprint arXiv:2405.15032*, 2024.
* Assran etÂ al. (2024)

  M.Â Assran, Q.Â Duval, I.Â Misra, P.Â Bojanowski, P.Â Vincent, M.Â Rabbat, Y.Â LeCun,
  and N.Â Ballas.
  Self-supervised learning from images with a joint-embedding
  predictive architecture.
  *ArXiv*, abs/2301.08243, 2024.
  URL <https://arxiv.org/pdf/2301.08243>.
* Babu etÂ al. (2022)

  A.Â Babu, C.Â Wang, A.Â Tjandra, K.Â Lakhotia, Q.Â Xu, N.Â Goyal, K.Â Singh, P.Â von
  Platen, Y.Â Saraf, J.Â Pino, A.Â Baevski, A.Â Conneau, and M.Â Auli.
  XLS-R: Self-supervised Cross-lingual Speech Representation Learning
  at Scale.
  In *Proc. Interspeech 2022*, pages 2278â€“2282, 2022.
  [10.21437/Interspeech.2022-143](https:/doi.org/10.21437/Interspeech.2022-143).
* Bacciu etÂ al. (2024)

  A.Â Bacciu, G.Â Trappolini, A.Â Santilli, E.Â RodolÃ , and F.Â Silvestri.
  Fauno: The italian large language model that will leave you senza
  parole!
  *ArXiv*, abs/2306.14457, 2024.
  URL <https://arxiv.org/pdf/2306.14457>.
* Baevski etÂ al. (2020)

  A.Â Baevski, Y.Â Zhou, A.Â Mohamed, and M.Â Auli.
  wav2vec 2.0: A framework for self-supervised learning of speech
  representations.
  *NeurIPS*, 33:12449â€“12460, 2020.
* Balioglu (2023)

  C.Â Balioglu.
  fairseq2, 2023.
  URL <http://github.com/facebookresearch/fairseq2>.
* Bapna etÂ al. (2022)

  A.Â Bapna, C.Â Cherry, Y.Â Zhang, Y.Â Jia, M.Â Johnson, Y.Â Cheng, S.Â Khanuja,
  J.Â Riesa, and A.Â Conneau.
  mslam: Massively multilingual joint pre-training for speech and text.
  *arXiv preprint arXiv:2202.01374*, 2022.
* Bardes etÂ al. (2024)

  A.Â Bardes, Q.Â Garrido, J.Â Ponce, X.Â Chen, M.Â Rabbat, Y.Â LeCun, M.Â Assran, and
  N.Â Ballas.
  Revisiting feature prediction for learning visual representations
  from video.
  *ArXiv*, abs/2404.08471, 2024.
  URL <https://arxiv.org/pdf/2404.08471>.
* Bari etÂ al. (2024)

  M.Â S. Bari, Y.Â Alnumay, N.Â A. Alzahrani, N.Â M. Alotaibi, H.Â A. Alyahya,
  S.Â AlRashed, F.Â A. Mirza, S.Â Z. Alsubaie, H.Â A. Alahmed, G.Â Alabduljabbar,
  R.Â Alkhathran, Y.Â Almushayqih, R.Â Alnajim, S.Â Alsubaihi, M.Â A. Mansour,
  M.Â Alrubaian, A.Â Alammari, Z.Â Alawami, A.Â Al-Thubaity, A.Â Abdelali,
  J.Â Kuriakose, A.Â Abujabal, N.Â Al-Twairesh, A.Â Alowisheq, and H.Â Khan.
  ALLaM: Large language models for arabic and english.
  *ArXiv*, abs/2407.15390, 2024.
  URL <https://arxiv.org/pdf/2407.15390>.
* BenÂ Allal etÂ al. (2024)

  L.Â BenÂ Allal, A.Â Lozhkov, G.Â Penedo, T.Â Wolf, and L.Â von Werra.
  Cosmopedia, 2024.
  URL <https://huggingface.co/datasets/HuggingFaceTB/cosmopedia>.
* Betker etÂ al. (2023)

  J.Â Betker, G.Â Goh, L.Â Jing, TimBrooks, J.Â Wang, L.Â Li, LongOuyang,
  JuntangZhuang, JoyceLee, YufeiGuo, WesamManassra, PrafullaDhariwal, CaseyChu,
  YunxinJiao, and A.Â Ramesh.
  Improving image generation with better captions, 2023.
  URL <https://api.semanticscholar.org/CorpusID:264403242>.
* BigScience Workshop (2023)

  BigScience Workshop.
  BLOOM: a 176b-parameter open-access multilingual language model.
  *ArXiv*, abs/2211.05100, 2023.
  URL <https://arxiv.org/pdf/2211.05100>.
* Chameleon team (2024)

  Chameleon team.
  Chameleon: Mixed-modal early-fusion foundation models.
  *ArXiv*, abs/2405.09818, 2024.
  URL <https://arxiv.org/pdf/2405.09818>.
* Chen etÂ al. (2023a)

  M.Â Chen, P.-A. Duquenne, P.Â Andrews, J.Â Kao, A.Â Mourachko, H.Â Schwenk, and
  M.Â R. Costa-jussÃ .
  BLASER: A text-free speech-to-speech translation evaluation metric.
  In *ACL*, pages 9064â€“9079, 2023a.
  URL <https://aclanthology.org/2023.acl-long.504>.
* Chen etÂ al. (2023b)

  M.Â Chen, K.Â Heffernan, O.Â Ã‡elebi, A.Â Mourachko, and H.Â Schwenk.
  xSIM++: An improved proxy to bitext mining performance for
  low-resource languages.
  In *ACL*, pages 101â€“109, 2023b.
  URL <https://aclanthology.org/2023.acl-short.10>.
* Child etÂ al. (2019)

  R.Â Child, S.Â Gray, A.Â Radford, and I.Â Sutskever.
  Generating long sequences with sparse transformers.
  *arXiv preprint arXiv:1904.10509*, 2019.
* Chung etÂ al. (2024)

  H.Â W. Chung, L.Â Hou, S.Â Longpre, B.Â Zoph, Y.Â Tay, W.Â Fedus, Y.Â Li, X.Â Wang,
  M.Â Dehghani, S.Â Brahma, etÂ al.
  Scaling instruction-finetuned language models.
  *Journal of Machine Learning Research*, 25(70):1â€“53, 2024.
* Chung etÂ al. (2021)

  Y.-A. Chung, Y.Â Zhang, W.Â Han, C.-C. Chiu, J.Â Qin, R.Â Pang, and Y.Â Wu.
  W2v-bert: Combining contrastive learning and masked language
  modeling for self-supervised speech pre-training.
  In *2021 IEEE Automatic Speech Recognition and Understanding
  Workshop (ASRU)*, pages 244â€“250. IEEE, 2021.
* Clark etÂ al. (2023)

  E.Â Clark, S.Â Rijhwani, S.Â Gehrmann, J.Â Maynez, R.Â Aharoni, V.Â Nikolaev,
  T.Â Sellam, A.Â Siddhant, D.Â Das, and A.Â Parikh.
  Seahorse: A multilingual, multifaceted dataset for summarization
  evaluation.
  In *Proceedings of the 2023 Conference on Empirical Methods in
  Natural Language Processing*, pages 9397â€“9413, 2023.
* Conneau etÂ al. (2020)

  A.Â Conneau, K.Â Khandelwal, N.Â Goyal, V.Â Chaudhary, G.Â Wenzek, F.Â GuzmÃ¡n,
  E.Â Grave, M.Â Ott, L.Â Zettlemoyer, and V.Â Stoyanov.
  Unsupervised cross-lingual representation learning at scale.
  In *ACL*, 2020.
* Cornille etÂ al. (2024)

  N.Â Cornille, M.-F. Moens, and F.Â Mai.
  Learning to plan for language modeling from unlabeled data.
  *arXiv preprint arXiv:2404.00614*, 2024.
* Costa-jussÃ  etÂ al. (2024)

  M.Â R. Costa-jussÃ , P.Â Andrews, M.Â C. Megliogli, J.Â Chen, J.Â Chuang, D.Â Dale,
  C.Â Ropers, A.Â Mourachko, E.Â SÃ¡nchez, H.Â Schwenk, T.Â Tran, A.Â Turkatenko, and
  C.Â Wood.
  LCFO: Long context and long form output dataset and benchmarking.
  *ArXiv*, 2024.
* Devlin etÂ al. (2018)

  J.Â Devlin, M.-W. Chang, K.Â Lee, and K.Â Toutanova.
  BERT: Pre-training of deep bidirectional transformers for language
  understanding.
  *arXiv preprint arXiv:1810.04805*, 2018.
* Dhariwal and Nichol (2021)

  P.Â Dhariwal and A.Â Nichol.
  Diffusion models beat gans on image synthesis.
  *Advances in neural information processing systems*,
  34:8780â€“8794, 2021.
* Dijk (1977)

  V.Â Dijk.
  *Text and Context: Explorations in the Semantics and Pragmatics
  of Discourse.*
  Longman, 1977.
* Douze etÂ al. (2024)

  M.Â Douze, A.Â Guzhva, C.Â Deng, J.Â Johnson, G.Â Szilvasy, P.-E. MazarÃ©,
  M.Â Lomeli, L.Â Hosseini, and H.Â JÃ©gou.
  The Faiss library.
  *ArXiv*, abs/2401.08281, 2024.
  URL <https://arxiv.org/pdf/2401.08281>.
* Duquenne etÂ al. (2021)

  P.-A. Duquenne, H.Â Gong, and H.Â Schwenk.
  Multimodal and multilingual embeddings for large-scale speech mining.
  In *NeurIPS*, volumeÂ 34, pages 15748â€“15761, 2021.
  URL
  <https://proceedings.neurips.cc/paper/2021/file/8466f9ace6a9acbe71f75762ffc890f1-Paper.pdf>.
* Duquenne etÂ al. (2022)

  P.-A. Duquenne, H.Â Gong, B.Â Sagot, and H.Â Schwenk.
  T-modules: Translation modules for zero-shot cross-modal machine
  translation.
  In *EMNLP*, pages 5794â€“5806, 2022.
  URL <https://aclanthology.org/2022.emnlp-main.391.pdf>.
* Duquenne etÂ al. (2023a)

  P.-A. Duquenne, K.Â Heffernan, A.Â Mourachko, B.Â Sagot, and H.Â Schwenk.
  Sonar expressive: Zero-shot expressive speech-to-speech translation,
  2023a.
* Duquenne etÂ al. (2023b)

  P.-A. Duquenne, H.Â Schwenk, and B.Â Sagot.
  SONAR: sentence-level multimodal and language-agnostic
  representations, 2023b.
  URL <https://arxiv.org/abs/2308.11466>.
* Duquenne etÂ al. (2023c)

  P.-A. Duquenne, H.Â Schwenk, and B.Â Sagot.
  Modular speech-to-text translation for zero-shot cross-modal
  transfer.
  In *Interspeech*, 2023c.
* Feng etÂ al. (2020)

  F.Â Feng, Y.Â Yang, D.Â Cer, N.Â Arivazhagan, and W.Â Wang.
  Language-agnostic BERT sentence embedding.
  *arXiv preprint arXiv:2007.01852*, 2020.
* Frohmann etÂ al. (2024)

  M.Â Frohmann, I.Â Sterner, I.Â VuliÄ‡, B.Â Minixhofer, and M.Â Schedl.
  Segment Any Text: A universal approach for robust, efficient and
  adaptable sentence segmentation.
  In *EMNLP*, pages 11908â€“11941, 2024.
  URL <https://aclanthology.org/2024.emnlp-main.665>.
* Gafni etÂ al. (2022)

  O.Â Gafni, A.Â Polyak, O.Â Ashual, S.Â Sheynin, D.Â Parikh, and Y.Â Taigman.
  Make-a-scene: Scene-based text-to-image generation with human priors.
  In *European Conference on Computer Vision*, pages 89â€“106.
  Springer, 2022.
* Gemini Team Google (2024)

  Gemini Team Google.
  Gemini 1.5 unlocking multimodal understanding across millions of
  tokens of conte.
  *ArXiv*, abs/2403.05530, 2024.
  URL <https://arxiv.org/pdf/2403.05530>.
* Golestani etÂ al. (2021)

  M.Â Golestani, S.Â Z. Razavi, Z.Â Borhanifard, F.Â Tahmasebian, and H.Â Faili.
  Using BERT encoding and sentence-level language model for sentence
  ordering.
  In *International Conference on Text, Speech, and Dialogue*,
  pages 318â€“330. Springer, 2021.
* Goyal (2017)

  P.Â Goyal.
  Accurate, large minibatch sg d: training imagenet in 1 hour.
  *arXiv preprint arXiv:1706.02677*, 2017.
* Guo etÂ al. (2018)

  M.Â Guo, Q.Â Shen, Y.Â Yang, H.Â Ge, D.Â Cer, G.Â H. Abrego, K.Â Stevens, N.Â Constant,
  Y.-H. Sung, B.Â Strope, etÂ al.
  Effective parallel corpus mining using bilingual sentence embeddings.
  *arXiv preprint arXiv:1807.11906*, 2018.
* Hang etÂ al. (2023)

  T.Â Hang, S.Â Gu, C.Â Li, J.Â Bao, D.Â Chen, H.Â Hu, X.Â Geng, and B.Â Guo.
  Efficient diffusion training via min-snr weighting strategy.
  In *Proceedings of the IEEE/CVF International Conference on
  Computer Vision*, pages 7441â€“7451, 2023.
* Hasan etÂ al. (2021)

  T.Â Hasan, A.Â Bhattacharjee, M.Â S. Islam, K.Â Mubasshir, Y.-F. Li, Y.-B. Kang,
  M.Â S. Rahman, and R.Â Shahriyar.
  XL-sum: Large-scale multilingual abstractive summarization for 44
  languages.
  In *Findings of the Association for Computational Linguistics:
  ACL-IJCNLP 2021*, pages 4693â€“4703, Online, Aug. 2021. Association for
  Computational Linguistics.
  URL <https://aclanthology.org/2021.findings-acl.413>.
* Heffernan and Teufel (2022)

  K.Â Heffernan and S.Â Teufel.
  Problem-solving recognition in scientific text.
  In *Proceedings of the Thirteenth Language Resources and
  Evaluation Conference*, pages 6045â€“6058, Marseille, France, June 2022.
  European Language Resources Association.
  URL <https://aclanthology.org/2022.lrec-1.650>.
* Hermann etÂ al. (2015)

  K.Â M. Hermann, T.Â Kocisky, E.Â Grefenstette, L.Â Espeholt, W.Â Kay, M.Â Suleyman,
  and P.Â Blunsom.
  Teaching machines to read and comprehend.
  *Advances in neural information processing systems*, 28, 2015.
* Ho and Salimans (2022)

  J.Â Ho and T.Â Salimans.
  Classifier-free diffusion guidance.
  *arXiv preprint arXiv:2207.12598*, 2022.
* Ho etÂ al. (2020)

  J.Â Ho, A.Â Jain, and P.Â Abbeel.
  Denoising diffusion probabilistic models.
  *CoRR*, abs/2006.11239, 2020.
  URL <https://arxiv.org/abs/2006.11239>.
* Ho etÂ al. (2022)

  J.Â Ho, W.Â Chan, C.Â Saharia, J.Â Whang, R.Â Gao, A.Â Gritsenko, D.Â P. Kingma,
  B.Â Poole, M.Â Norouzi, D.Â J. Fleet, and T.Â Salimans.
  Imagen video: High definition video generation with diffusion models.
  *ArXiv*, abs/2210.02303, 2022.
  URL <https://arxiv.org/abs/2210.02303>.
* Honnibal etÂ al. (2020)

  M.Â Honnibal, I.Â Montani, S.Â VanÂ Landeghem, and A.Â Boyd.
  spaCy: Industrial-strength Natural Language Processing in Python,
  2020.
* Huang etÂ al. (2020)

  Y.Â Huang, Y.Â Zhang, O.Â Elachqar, and Y.Â Cheng.
  INSET: Sentence infilling with inter-sentential transformer.
  In *ACL*, pages 2502â€“2515, 2020.
  URL <https://aclanthology.org/2020.acl-main.226.pdf>.
* Ippolito etÂ al. (2020)

  D.Â Ippolito, D.Â Grangier, D.Â Eck, and C.Â Callison-Burch.
  Toward better storylines with sentence-level language models.
  *ArXiv*, abs/2005.05255, 2020.
  URL <https://arxiv.org/pdf/2005.05255>.
* Janeiro etÂ al. (2024)

  J.Â M. Janeiro, B.Â Piwowarski, P.Â Gallinari, and L.Â Barrault.
  Mexma: Token-level objectives improve sentence representations, 2024.
  URL <https://arxiv.org/abs/2409.12737>.
* Ji etÂ al. (2024)

  S.Â Ji, Z.Â Li, I.Â Paul, J.Â Paavola, P.Â Lin, P.Â Chen, D.Â Oâ€™Brien, H.Â Luo,
  H.Â SchÃ¼tze, J.Â Tiedemann, etÂ al.
  Emma-500: Enhancing massively multilingual adaptation of large
  language models.
  *arXiv preprint arXiv:2409.17892*, 2024.
* Jiang etÂ al. (2024)

  A.Â Q. Jiang, A.Â Sablayrolles, A.Â Roux, A.Â Mensch, B.Â Savary, C.Â Bamford, D.Â S.
  Chaplot, D.Â deÂ las Casas, E.Â B. Hanna, F.Â Bressand, G.Â Lengyel, G.Â Bour,
  G.Â Lample, L.Â R. Lavaud, L.Â Saulnier, M.-A. Lachaux, P.Â Stock,
  S.Â Subramanian, S.Â Yang, S.Â Antoniak, T.Â L. Scao, T.Â Gervet, T.Â Lavril,
  T.Â Wang, T.Â Lacroix, and W.Â E. Sayed.
  Mixtral.
  *ArXiv*, abs/2401.04088, 2024.
  URL <https://arxiv.org/pdf/2401.04088>.
* Jwalapuram etÂ al. (2022)

  P.Â Jwalapuram, S.Â Joty, and X.Â Lin.
  Rethinking self-supervision objectives for generalizable coherence
  modeling.
  In *ACL*, pages 6044â€“6059, 2022.
  URL <https://aclanthology.org/2022.acl-long.418>.
* Karras etÂ al. (2022)

  T.Â Karras, M.Â Aittala, T.Â Aila, and S.Â Laine.
  Elucidating the design space of diffusion-based generative models.
  *Advances in neural information processing systems*,
  35:26565â€“26577, 2022.
* Khurana etÂ al. (2022)

  S.Â Khurana, A.Â Laurent, and J.Â Glass.
  Samu-xlsr: Semantically-aligned multimodal utterance-level
  cross-lingual speech representation.
  *arXiv preprint arXiv:2205.08180*, 2022.
* Kingma and Gao (2024)

  D.Â Kingma and R.Â Gao.
  Understanding diffusion objectives as the elbo with simple data
  augmentation.
  *Advances in Neural Information Processing Systems*, 36, 2024.
* Kitaev etÂ al. (2020)

  N.Â Kitaev, Å.Â Kaiser, and A.Â Levskaya.
  Reformer: The efficient transformer.
  *arXiv preprint arXiv:2001.04451*, 2020.
* Krishna etÂ al. (2020)

  K.Â Krishna, J.Â Wieting, and M.Â Iyyer.
  Reformulating unsupervised style transfer as paraphrase generation.
  In *Empirical Methods in Natural Language Processing*, 2020.
* LeCun (2022)

  Y.Â LeCun.
  A path towards autonomous machine intelligence, 2022.
  URL <https://openreview.net/pdf?id=BZ5a1r-kVsf>.
* Lee etÂ al. (2024a)

  C.Â Lee, R.Â Roy, M.Â Xu, J.Â Raiman, M.Â Shoeybi, B.Â Catanzaro, and W.Â Ping.
  NV-Embed: Improved techniques for training LLMs as generalist
  embedding models.
  *arXiv preprint arXiv:2405.17428*, 2024a.
* Lee etÂ al. (2022)

  D.Â Lee, C.Â Kim, S.Â Kim, M.Â Cho, and W.-S. Han.
  Autoregressive image generation using residual quantization.
  In *Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition*, pages 11523â€“11532, 2022.
* Lee etÂ al. (2024b)

  J.Â Lee, Z.Â Dai, X.Â Ren, B.Â Chen, D.Â Cer, J.Â R. Cole, K.Â Hui, M.Â Boratko,
  R.Â Kapadia, W.Â Ding, Y.Â Luan, S.Â M.Â K. Duddu, G.Â H. Abrego, W.Â Shi, N.Â Gupta,
  A.Â Kusupati, P.Â Jain, S.Â R. Jonnalagadda, M.-W. Chang, and I.Â Naim.
  Gecko: Versatile text embeddings distilled from large language
  models, 2024b.
  URL <https://arxiv.org/abs/2403.20327>.
* Lee and Sengupta (2022)

  K.Â Lee and S.Â Sengupta.
  Introducing the ai research supercluster â€” metaâ€™s cutting-edge ai
  supercomputer for ai research, 2022.
  URL <https://ai.facebook.com/blog/ai-rsc/>.
* Li etÂ al. (2024)

  Y.Â Li, Q.Â Chen, W.Â Yan, W.Â Wang, Q.Â Zhang, and H.Â Sundaram.
  Advancing precise outline-conditioned text generation with task
  duality and explicit outline control, 2024.
  URL <https://arxiv.org/abs/2305.14459>.
* Li etÂ al. (2023)

  Z.Â Li, S.Â Huang, Z.Â Zhang, Z.-H. Deng, Q.Â Lou, H.Â Huang, J.Â Jiao, F.Â Wei,
  W.Â Deng, and Q.Â Zhang.
  Dual-alignment pre-training for cross-lingual sentence embedding.
  In A.Â Rogers, J.Â Boyd-Graber, and N.Â Okazaki, editors,
  *Proceedings of the 61st Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)*, pages 3466â€“3478,
  Toronto, Canada, July 2023. Association for Computational Linguistics.
  [10.18653/v1/2023.acl-long.191](https:/doi.org/10.18653/v1/2023.acl-long.191).
  URL <https://aclanthology.org/2023.acl-long.191>.
* Lin (2004)

  C.-Y. Lin.
  Rouge: A package for automatic evaluation of summaries.
  In *Text summarization branches out*, pages 74â€“81, 2004.
* Lin etÂ al. (2024)

  S.Â Lin, B.Â Liu, J.Â Li, and X.Â Yang.
  Common diffusion noise schedules and sample steps are flawed.
  In *Proceedings of the IEEE/CVF winter conference on
  applications of computer vision*, pages 5404â€“5411, 2024.
* Liu etÂ al. (2015)

  S.Â Liu, H.Â Lu, and J.Â Shao.
  Improved residual vector quantization for high-dimensional
  approximate nearest neighbor search.
  *arXiv preprint arXiv:1509.05195*, 2015.
* Lovelace etÂ al. (2024)

  J.Â Lovelace, V.Â Kishore, Y.Â Chen, and K.Â Q. Weinberger.
  Diffusion guided language modeling.
  *ArXiv*, abs/2408.04220, 2024.
  URL <https://arxiv.org/pdf/2408.04220>.
* Lozhkov etÂ al. (2024)

  A.Â Lozhkov, L.Â BenÂ Allal, L.Â von Werra, and T.Â Wolf.
  Fineweb-edu, May 2024.
  URL <https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu>.
* Lu etÂ al. (2022)

  C.Â Lu, Y.Â Zhou, F.Â Bao, J.Â Chen, C.Â Li, and J.Â Zhu.
  Dpm-solver: A fast ode solver for diffusion probabilistic model
  sampling in around 10 steps.
  *Advances in Neural Information Processing Systems*,
  35:5775â€“5787, 2022.
* Marfurt and Henderson (2021)

  A.Â Marfurt and J.Â Henderson.
  Sentence-level planning for especially abstractive summarization.
  In *ACL*, pages 1â€“14, 2021.
  URL <https://aclanthology.org/2021.newsum-1.1.pdf>.
* Minixhofer etÂ al. (2023)

  B.Â Minixhofer, J.Â Pfeiffer, and I.Â VuliÄ‡.
  Whereâ€™s the point? self-supervised multilingual
  punctuation-agnostic sentence segmentation.
  In *Proceedings of the 61st Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)*, pages 7215â€“7235,
  Toronto, Canada, July 2023. Association for Computational Linguistics.
  URL <https://aclanthology.org/2023.acl-long.398>.
* Mohr etÂ al. (2024)

  I.Â Mohr, M.Â Krimmel, S.Â Sturua, M.Â K. Akram, A.Â Koukounas, M.Â GÃ¼nther,
  G.Â Mastrapas, V.Â Ravishankar, J.Â F. MartÃ­nez, F.Â Wang, Q.Â Liu, Z.Â Yu, J.Â Fu,
  S.Â Ognawala, S.Â Guzman, B.Â Wang, M.Â Werk, N.Â Wang, and H.Â Xiao.
  Multi-task contrastive learning for 8192-token bilingual text
  embeddings, 2024.
  URL <https://arxiv.org/abs/2402.17016>.
* Mostafazadeh etÂ al. (2016)

  N.Â Mostafazadeh, N.Â Chambers, X.Â He, D.Â Parikh, D.Â Batra, L.Â Vanderwende,
  P.Â Kohli, and J.Â Allen.
  A corpus and cloze evaluation for deeper understanding of commonsense
  stories.
  In *NAACL*, pages 839â€“849, June 2016.
  URL <https://aclanthology.org/N16-1098>.
* Narayan etÂ al. (2018)

  S.Â Narayan, S.Â B. Cohen, and M.Â Lapata.
  Donâ€™t give me the details, just the summary! topic-aware
  convolutional neural networks for extreme summarization.
  *arXiv preprint arXiv:1808.08745*, 2018.
* Ni etÂ al. (2021)

  J.Â Ni, G.Â H. Abrego, N.Â Constant, J.Â Ma, K.Â B. Hall, D.Â Cer, and Y.Â Yang.
  Sentence-t5: Scalable sentence encoders from pre-trained text-to-text
  models.
  *arXiv preprint arXiv:2108.08877*, 2021.
* Nichol and Dhariwal (2021)

  A.Â Q. Nichol and P.Â Dhariwal.
  Improved denoising diffusion probabilistic models.
  In *International conference on machine learning*, pages
  8162â€“8171. PMLR, 2021.
* Nichol etÂ al. (2022)

  A.Â Q. Nichol, P.Â Dhariwal, A.Â Ramesh, P.Â Shyam, P.Â Mishkin, B.Â Mcgrew,
  I.Â Sutskever, and M.Â Chen.
  GLIDE: Towards photorealistic image generation and editing with
  text-guided diffusion models.
  In K.Â Chaudhuri, S.Â Jegelka, L.Â Song, C.Â Szepesvari, G.Â Niu, and
  S.Â Sabato, editors, *Proceedings of the 39th International Conference on
  Machine Learning*, volume 162 of *Proceedings of Machine Learning
  Research*, pages 16784â€“16804. PMLR, 17â€“23 Jul 2022.
  URL <https://proceedings.mlr.press/v162/nichol22a.html>.
* Ning etÂ al. (2023)

  M.Â Ning, M.Â Li, J.Â Su, A.Â A. Salah, and I.Â O. Ertugrul.
  Elucidating the exposure bias in diffusion models.
  *arXiv preprint arXiv:2308.15321*, 2023.
* NLLB Team etÂ al. (2022)

  NLLB Team, M.Â R. Costa-jussÃ , J.Â Cross, O.Â Ã‡elebi, M.Â Elbayad, K.Â Heafield,
  K.Â Heffernan, E.Â Kalbassi, J.Â Lam, D.Â Licht, J.Â Maillard, A.Â Sun, S.Â Wang,
  G.Â Wenzek, A.Â Youngblood, B.Â Akula, L.Â Barrault, G.Â Mejia-Gonzalez,
  P.Â Hansanti, J.Â Hoffman, S.Â Jarrett, K.Â R. Sadagopan, D.Â Rowe, S.Â Spruit,
  C.Â Tran, P.Â Andrews, N.Â F. Ayan, S.Â Bhosale, S.Â Edunov, A.Â Fan, C.Â Gao,
  V.Â Goswami, F.Â GuzmÃ¡n, P.Â Koehn, A.Â Mourachko, C.Â Ropers, S.Â Saleem,
  H.Â Schwenk, and J.Â Wang.
  No language left behind: Scaling human-centered machine translation,
  2022.
  URL <https://arxiv.org/abs/2207.04672>.
* OpenAI (2024)

  OpenAI.
  GPT-4 technical report.
  *ArXiv*, abs/2303.08774, 2024.
  URL <https://arxiv.org/pdf/2303.08774>.
* Papineni etÂ al. (2002)

  K.Â Papineni, S.Â Roukos, T.Â Ward, and W.-J. Zhu.
  Bleu: a method for automatic evaluation of machine translation.
  In *Proceedings of the 40th Annual Meeting of the Association
  for Computational Linguistics*, pages 311â€“318, Philadelphia, Pennsylvania,
  USA, July 2002. Association for Computational Linguistics.
  [10.3115/1073083.1073135](https:/doi.org/10.3115/1073083.1073135).
  URL <https://aclanthology.org/P02-1040>.
* Parola etÂ al. (2023)

  A.Â Parola, J.Â M. Lin, A.Â Simonsen, V.Â Bliksted, Y.Â Zhou, H.Â Wang, L.Â Inoue,
  K.Â Koelkebeck, and R.Â Fusaroli.
  Speech disturbances in schizophrenia: Assessing cross-linguistic
  generalizability of nlp automated measures of coherence.
  *Schizophrenia Research*, 259:59â€“70, 2023.
* Peebles and Xie (2023)

  W.Â Peebles and S.Â Xie.
  Scalable diffusion models with transformers.
  In *Proceedings of the IEEE/CVF International Conference on
  Computer Vision*, pages 4195â€“4205, 2023.
* Perez etÂ al. (2018)

  E.Â Perez, F.Â Strub, H.Â DeÂ Vries, V.Â Dumoulin, and A.Â Courville.
  Film: Visual reasoning with a general conditioning layer.
  In *Proceedings of the AAAI conference on artificial
  intelligence*, 2018.
* Radford etÂ al. (2019)

  A.Â Radford, J.Â Wu, R.Â Child, D.Â Luan, D.Â Amodei, I.Â Sutskever, etÂ al.
  Language models are unsupervised multitask learners.
  *OpenAI blog*, 1(8):9, 2019.
* Raffel etÂ al. (2019)

  C.Â Raffel, N.Â Shazeer, A.Â Roberts, K.Â Lee, S.Â Narang, M.Â Matena, Y.Â Zhou,
  W.Â Li, and P.Â J. Liu.
  Exploring the limits of transfer learning with a unified text-to-text
  transformer.
  *arXiv e-prints*, 2019.
* Raffel etÂ al. (2020)

  C.Â Raffel, N.Â Shazeer, A.Â Roberts, K.Â Lee, S.Â Narang, M.Â Matena, Y.Â Zhou,
  W.Â Li, and P.Â J. Liu.
  Exploring the limits of transfer learning with a unified text-to-text
  transformer.
  *Journal of Machine Learning Research*, 21(140):1â€“67, 2020.
  URL <http://jmlr.org/papers/v21/20-074.html>.
* Reimers and Gurevych (2019)

  N.Â Reimers and I.Â Gurevych.
  Sentence-bert: Sentence embeddings using siamese Bert-networks.
  *arXiv preprint arXiv:1908.10084*, 2019.
* Rombach etÂ al. (2021)

  R.Â Rombach, A.Â Blattmann, D.Â Lorenz, P.Â Esser, and B.Â Ommer.
  High-resolution image synthesis with latent diffusion models.
  *CoRR*, abs/2112.10752, 2021.
  URL <https://arxiv.org/abs/2112.10752>.
* Rubenstein etÂ al. (2023)

  P.Â K. Rubenstein, C.Â Asawaroengchai, D.Â D. Nguyen, A.Â Bapna, Z.Â Borsos,
  F.Â deÂ ChaumontÂ Quitry, P.Â Chen, D.Â E. Badawy, W.Â Han, E.Â Kharitonov,
  H.Â Muckenhirn, D.Â Padfield, J.Â Qin, D.Â Rozenberg, T.Â N. Sainath,
  J.Â Schalkwyk, M.Â Sharifi, M.Â T. Ramanovich, M.Â Tagliasacchi, A.Â Tudor,
  M.Â Velimirovic, D.Â Vincent, J.Â Yu, Y.Â Wang, V.Â Zayats, N.Â Zeghidour,
  Y.Â Zhang, Z.Â Zhang, L.Â Zilka, and C.Â H. Frank.
  Audiopalm: A large language model that can speak and listen.
  *CoRR*, abs/2306.12925, 2023.
  URL <https://doi.org/10.48550/arXiv.2306.12925>.
* Salimans and Ho (2022)

  T.Â Salimans and J.Â Ho.
  Progressive distillation for fast sampling of diffusion models.
  *arXiv preprint arXiv:2202.00512*, 2022.
* Seamless Communication etÂ al. (2023a)

  Seamless Communication, L.Â Barrault, Y.-A. Chung, M.Â C. Meglioli, D.Â Dale,
  N.Â Dong, M.Â Duppenthaler, P.-A. Duquenne, B.Â Ellis, H.Â Elsahar, J.Â Haaheim,
  J.Â Hoffman, M.-J. Hwang, H.Â Inaguma, C.Â Klaiber, I.Â Kulikov, P.Â Li, D.Â Licht,
  J.Â Maillard, R.Â Mavlyutov, A.Â Rakotoarison, K.Â R. Sadagopan, A.Â Ramakrishnan,
  T.Â Tran, G.Â Wenzek, Y.Â Yang, E.Â Ye, I.Â Evtimov, P.Â Fernandez, C.Â Gao,
  P.Â Hansanti, E.Â Kalbassi, A.Â Kallet, A.Â Kozhevnikov, G.Â M. Gonzalez, R.Â S.
  Roman, C.Â Touret, C.Â Wong, C.Â Wood, B.Â Yu, P.Â Andrews, C.Â Balioglu, P.-J.
  Chen, M.Â R. Costa-jussÃ , M.Â Elbayad, H.Â Gong, F.Â GuzmÃ¡n, K.Â Heffernan,
  S.Â Jain, J.Â Kao, A.Â Lee, X.Â Ma, A.Â Mourachko, B.Â Peloquin, J.Â Pino,
  S.Â Popuri, C.Â Ropers, S.Â Saleem, H.Â Schwenk, A.Â Sun, P.Â Tomasello, C.Â Wang,
  J.Â Wang, S.Â Wang, and M.Â Williamson.
  Seamless: Multilingual expressive and streaming speech translation.
  *ArXiv*, abs/2312.05187, 2023a.
  URL <https://arxiv.org/abs/2312.05187>.
* Seamless Communication etÂ al. (2023b)

  Seamless Communication, L.Â Barrault, Y.-A. Chung, M.Â C. Meglioli, D.Â Dale,
  N.Â Dong, P.-A. Duquenne, H.Â Elsahar, H.Â Gong, K.Â Heffernan, J.Â Hoffman,
  C.Â Klaiber, P.Â Li, D.Â Licht, J.Â Maillard, A.Â Rakotoarison, K.Â R. Sadagopan,
  G.Â Wenzek, E.Â Ye, B.Â Akula, P.-J. Chen, N.Â E. Hachem, B.Â Ellis, G.Â M.
  Gonzalez, J.Â Haaheim, P.Â Hansanti, R.Â Howes, B.Â Huang, M.-J. Hwang,
  H.Â Inaguma, S.Â Jain, E.Â Kalbassi, A.Â Kallet, I.Â Kulikov, J.Â Lam, D.Â Li,
  X.Â Ma, R.Â Mavlyutov, B.Â Peloquin, M.Â Ramadan, A.Â Ramakrishnan, A.Â Sun,
  K.Â Tran, T.Â Tran, I.Â Tufanov, V.Â Vogeti, C.Â Wood, Y.Â Yang, B.Â Yu, P.Â Andrews,
  C.Â Balioglu, M.Â R. Costa-jussÃ , O.Â Celebi, M.Â Elbayad, C.Â Gao, F.Â GuzmÃ¡n,
  J.Â Kao, A.Â Lee, A.Â Mourachko, J.Â Pino, S.Â Popuri, C.Â Ropers, S.Â Saleem,
  H.Â Schwenk, P.Â Tomasello, C.Â Wang, J.Â Wang, and S.Â Wang.
  SeamlessM4T - massively multilingual & multimodal machine
  translation, 2023b.
  URL <https://arxiv.org/abs/2308.11596>.
* Shabalin etÂ al. (2024)

  A.Â Shabalin, V.Â Meshchaninov, E.Â Chimbulatov, V.Â Lapikov, R.Â Kim, G.Â Bartosh,
  D.Â Molchanov, S.Â Markov, and D.Â Vetrov.
  Tencdm: Understanding the properties of diffusion model in the space
  of language model encodings.
  *ArXiv*, abs/2402.19097, 2024.
  URL <https://arxiv.org/pdf/2402.19097>.
* Shazeer (2020)

  N.Â Shazeer.
  Glu variants improve transformer.
  *arXiv preprint arXiv:2002.05202*, 2020.
* Song etÂ al. (2020)

  J.Â Song, C.Â Meng, and S.Â Ermon.
  Denoising diffusion implicit models.
  *CoRR*, abs/2010.02502, 2020.
  URL <https://arxiv.org/abs/2010.02502>.
* Srivastava etÂ al. (2024)

  N.Â Srivastava, D.Â Kuchelev, T.Â M. Ngoli, K.Â Shetty, M.Â RÃ¶der,
  D.Â Moussallem, H.Â Zahera, and A.-C.Â N. Ngomo.
  Lolaâ€“an open-source massively multilingual large language model.
  *arXiv preprint arXiv:2409.11272*, 2024.
* Sturua etÂ al. (2024)

  S.Â Sturua, I.Â Mohr, M.Â K. Akram, M.Â GÃ¼nther, B.Â Wang, M.Â Krimmel, F.Â Wang,
  G.Â Mastrapas, A.Â Koukounas, N.Â Wang, and H.Â Xiao.
  jina-embeddings-v3: Multilingual embeddings with task lora, 2024.
  URL <https://arxiv.org/abs/2409.10173>.
* Su etÂ al. (2022)

  H.Â Su, W.Â Shi, J.Â Kasai, Y.Â Wang, Y.Â Hu, M.Â Ostendorf, W.-t. Yih, N.Â A. Smith,
  L.Â Zettlemoyer, and T.Â Yu.
  One embedder, any task: Instruction-finetuned text embeddings.
  *arXiv preprint arXiv:2212.09741*, 2022.
* Su etÂ al. (2024)

  J.Â Su, M.Â Ahmed, Y.Â Lu, S.Â Pan, W.Â Bo, and Y.Â Liu.
  Roformer: Enhanced transformer with rotary position embedding.
  *Neurocomputing*, 568:127063, 2024.
* Sun etÂ al. (2022)

  X.Â Sun, Z.Â Sun, Y.Â Meng, J.Â Li, and C.Â Fan.
  Summarize, outline, and elaborate: Long-text generation via
  hierarchical supervision from extractive summaries.
  In *Proceedings of the 29th International Conference on
  Computational Linguistics*, pages 6392â€“6402, Gyeongju, Republic of Korea,
  Oct. 2022. International Committee on Computational Linguistics.
  URL <https://aclanthology.org/2022.coling-1.556>.
* Team GLM etÂ al. (2024)

  Team GLM, A.Â Zeng, B.Â Xu, B.Â Wang, C.Â Zhang, D.Â Yin, D.Â Zhang, D.Â Rojas,
  G.Â Feng, H.Â Zhao, H.Â Lai, H.Â Yu, H.Â Wang, J.Â Sun, J.Â Zhang, J.Â Cheng, J.Â Gui,
  J.Â Tang, J.Â Zhang, J.Â Sun, J.Â Li, L.Â Zhao, L.Â Wu, L.Â Zhong, M.Â Liu, M.Â Huang,
  P.Â Zhang, Q.Â Zheng, R.Â Lu, S.Â Duan, S.Â Zhang, S.Â Cao, S.Â Yang, W.Â L. Tam,
  W.Â Zhao, X.Â Liu, X.Â Xia, X.Â Zhang, X.Â Gu, X.Â Lv, X.Â Liu, X.Â Liu, X.Â Yang,
  X.Â Song, X.Â Zhang, Y.Â An, Y.Â Xu, Y.Â Niu, Y.Â Yang, Y.Â Li, Y.Â Bai, Y.Â Dong,
  Z.Â Qi, Z.Â Wang, Z.Â Yang, Z.Â Du, Z.Â Hou, and Z.Â Wang.
  ChatGLM: A family of large language models from glm-130b to glm-4
  all tools.
  *ArXiv*, abs/2406.12793, 2024.
  URL <https://arxiv.org/pdf/2406.12793>.
* The Llama3 team (2024)

  The Llama3 team.
  The Llama 3 herd of models.
  *ArXiv*, abs/2407.21783, 2024.
  URL <https://arxiv.org/pdf/2407.21783>.
* Touvron etÂ al. (2023)

  H.Â Touvron, L.Â Martin, K.Â Stone, P.Â Albert, A.Â Almahairi, Y.Â Babaei,
  N.Â Bashlykov, S.Â Batra, P.Â Bhargava, S.Â Bhosale, D.Â Bikel, L.Â Blecher, C.Â C.
  Ferrer, M.Â Chen, G.Â Cucurull, D.Â Esiobu, J.Â Fernandes, J.Â Fu, W.Â Fu,
  B.Â Fuller, C.Â Gao, V.Â Goswami, N.Â Goyal, A.Â Hartshorn, S.Â Hosseini, R.Â Hou,
  H.Â Inan, M.Â Kardas, V.Â Kerkez, M.Â Khabsa, I.Â Kloumann, A.Â Korenev, P.Â S.
  Koura, M.-A. Lachaux, T.Â Lavril, J.Â Lee, D.Â Liskovich, Y.Â Lu, Y.Â Mao,
  X.Â Martinet, T.Â Mihaylov, P.Â Mishra, I.Â Molybog, Y.Â Nie, A.Â Poulton,
  J.Â Reizenstein, R.Â Rungta, K.Â Saladi, A.Â Schelten, R.Â Silva, E.Â M. Smith,
  R.Â Subramanian, X.Â E. Tan, B.Â Tang, R.Â Taylor, A.Â Williams, J.Â X. Kuan,
  P.Â Xu, Z.Â Yan, I.Â Zarov, Y.Â Zhang, A.Â Fan, M.Â Kambadur, S.Â Narang,
  A.Â Rodriguez, R.Â Stojnic, S.Â Edunov, and T.Â Scialom.
  Llama 2: Open foundation and fine-tuned chat models, 2023.
* Ubukata etÂ al. (2024)

  T.Â Ubukata, J.Â Li, and K.Â Tei.
  Diffusion model for planning: A systematic literature review.
  *ArXiv*, abs/2408.10266, 2024.
  URL <https://arxiv.org/pdf/2408.10266>.
* ÃœstÃ¼n etÂ al. (2024)

  A.Â ÃœstÃ¼n, V.Â Aryabumi, Z.-X. Yong, W.-Y. Ko, D.Â Dâ€™souza, G.Â Onilude,
  N.Â Bhandari, S.Â Singh, H.-L. Ooi, A.Â Kayid, etÂ al.
  Aya model: An instruction finetuned open-access multilingual language
  model.
  *arXiv preprint arXiv:2402.07827*, 2024.
* Wang etÂ al. (2023)

  C.Â Wang, S.Â Chen, Y.Â Wu, Z.Â Zhang, L.Â Zhou, S.Â Liu, Z.Â Chen, Y.Â Liu, H.Â Wang,
  J.Â Li, L.Â He, S.Â Zhao, and F.Â Wei.
  Neural codec language models are zero-shot text to speech
  synthesizers.
  *CoRR*, abs/2301.02111, 2023.
  URL <https://doi.org/10.48550/arXiv.2301.02111>.
* Wang etÂ al. (2024a)

  L.Â Wang, N.Â Yang, X.Â Huang, L.Â Yang, R.Â Majumder, and F.Â Wei.
  Improving text embeddings with large language models.
  In *ACL*, pages 11897â€“11916, 2024a.
  URL <https://aclanthology.org/2024.acl-long.642>.
* Wang etÂ al. (2024b)

  L.Â Wang, N.Â Yang, X.Â Huang, L.Â Yang, R.Â Majumder, and F.Â Wei.
  Multilingual e5 text embeddings: A technical report,
  2024b.
  URL <https://arxiv.org/abs/2402.05672>.
* Warstadt etÂ al. (2019)

  A.Â Warstadt, A.Â Singh, and S.Â R. Bowman.
  Neural network acceptability judgments.
  *Transactions of the Association for Computational Linguistics*,
  7:625â€“641, 2019.
  [10.1162/tacl\_a\_00290](https:/doi.org/10.1162/tacl_a_00290).
  URL <https://aclanthology.org/Q19-1040>.
* Welleck etÂ al. (2019)

  S.Â Welleck, I.Â Kulikov, S.Â Roller, E.Â Dinan, K.Â Cho, and J.Â Weston.
  Neural text generation with unlikelihood training.
  *arXiv preprint arXiv:1908.04319*, 2019.
* Yang etÂ al. (2019)

  Y.Â Yang, G.Â H. Abrego, S.Â Yuan, M.Â Guo, Q.Â Shen, D.Â Cer, Y.-H. Sung, B.Â Strope,
  and R.Â Kurzweil.
  Improving multilingual sentence embedding using bi-directional dual
  encoder with additive margin softmax.
  *arXiv preprint arXiv:1902.08564*, 2019.
* Ye etÂ al. (2024)

  J.Â Ye, J.Â Gao, S.Â Gong, L.Â Zheng, X.Â Jiang, Z.Â Li, and L.Â Kong.
  Beyond autoregression: Discrete diffusion for complex reasoning and
  planning.
  *ArXiv*, abs/2410.14157, 2024.
  URL <https://arxiv.org/pdf/2410.14157>.
* Yin etÂ al. (2024)

  Y.Â Yin, J.Â Ding, K.Â Song, and Y.Â Zhang.
  Semformer: Transformer language models with semantic planning.
  *ArXiv*, abs/2409.11143, 2024.
  URL <https://arxiv.org/pdf/2409.11143>.
* Zeghidour etÂ al. (2021)

  N.Â Zeghidour, A.Â Luebs, A.Â Omran, J.Â Skoglund, and M.Â Tagliasacchi.
  Soundstream: An end-to-end neural audio codec.
  *IEEE/ACM Transactions on Audio, Speech, and Language
  Processing*, 30:495â€“507, 2021.
* Zhang and Sennrich (2019)

  B.Â Zhang and R.Â Sennrich.
  Root mean square layer normalization.
  *Advances in Neural Information Processing Systems*, 32, 2019.
* Zhang etÂ al. (2024)

  X.Â Zhang, Y.Â Zhang, D.Â Long, W.Â Xie, Z.Â Dai, J.Â Tang, H.Â Lin, B.Â Yang, P.Â Xie,
  F.Â Huang, M.Â Zhang, W.Â Li, and M.Â Zhang.
  mgte: Generalized long-context text representation and reranking
  models for multilingual text retrieval, 2024.
  URL <https://arxiv.org/abs/2407.19669>.
* Zhang etÂ al. (2023)

  Y.Â Zhang, J.Â Gu, Z.Â Wu, S.Â Zhai, J.Â Susskind, and N.Â Jaitly.
  Planner: Generating diversified paragraph via latent language
  diffusion model.
  In *NeurIPS*, 2023.

## Appendix A Technical consideration for data preparation

Since our modeling approach uses a fixed encoder and a
fixed document segmentation method,
we decided to use pre-computed SONAR embeddings instead
of producing them on-the-fly for each training run. This allows for faster iteration on the same data mix, trading expensive GPU compute against storage capacity.

As we are storing sequences of SONAR embedding, which are fixed size tensors of 1024 floats, the storage requirements become more demanding than storing the raw text. For one terra bytes of raw text data we need to store between fifteen and twenty terra bytes of encoded data.
Overall, this trade-off in space vs compute reduces the GPU memory occupation and the compute load and lets use iterate faster.
Typically, with on single GPU we can produce around
300-400 SONAR sentence embeddings per second whereas
by loading precomputed data (potentially from remote storage)
we can load over 20 thousand embeddings per second per GPU (with around 15 CPU per GPU).

We store sequences of SONAR embeddings with 16 bits precision (FP16) in parquet datasets.
Embeddings remain aligned with the segmented texts and the
parquet binary format and library ecosystem is well suited for storing and loading efficiently such complex data structures. Parquet also lets us store extra data (such as quality metrics for each sentences) and enables non-trivial last mile data filtering and transformation.

For training the LCM, we processed around four billion documents, generating 310 billion sentences with an average of 27 tokens per sentences for 88 characters length on average; totaling a bit more than 889 terra-bytes of raw text.

## Appendix B Open Sourced Code

In the spirit of reproducibility, we release under an open source license the training, evaluation and data processing code for the LCM. This is available at <https://github.com/facebookresearch/large_concept_model>.

The training code is based on the Fairseq2 framework (Balioglu, [2023](#bib.bib11)) that allowed us to build and iterate over the different model architectures discussed above. While Fairseq2 shares the same name as the popular fairseq toolchain, its API architecture is different. It is not a monolithic toolchain but a set of modules that can be composed, this allows us to build different architectures side by side and easily share training components.

We also release our evaluation framework so the evaluation tasks reported in [3.1](#S3.SS1 "3.1 Evaluation Tasks and Data â€£ 3 Scaling the model to 7B â€£ Large Concept Models: Language Modeling in a Sentence Representation Space") and comparison between the LCM and other models can easily be reproduced. The evaluation framework provides a clear abstraction between *predictors*, *tasks* and *data loading*, which again, makes it modular and lets us describe a set of tasks to be evaluated. The evaluation framework can be run locally or distributed over a SLURM cluster to run evaluations at scale.

Finally, we release an updated version of stopes171717<https://github.com/facebookresearch/stopes> to simplify large scale data pre-processing on a SLURM cluster. This was used to run the sentence segmentation and SONAR encoding described in section [2.2](#S2.SS2 "2.2 Data preparation â€£ 2 Main Design Principles â€£ Large Concept Models: Language Modeling in a Sentence Representation Space"). The stopes data processing framework deals with scheduling and monitoring large number of jobs on SLURM or to run everything locally for small scale jobs. It provides an API compatible with ray.data181818<https://docs.ray.io/> that makes it easy to process large datasets in blocks and apply transform function over it. This makes our code reusable outside of a SLURM cluster as it can also be used with a ray.io cluster.

## Appendix C System prompt: Generation of Topic Descriptions

You are a topic description generator.
Your job is to read an extract of text and then generate a topic description.
The extract may be well formed or not.
The topic description you will write will be at most one sentence in length, and use as few words as possible.
However, it can not be generic and it can not contain any profanity.

Here is an example of an extract, an ideal topic description, and some examples of bad topic descriptions:

Example extract: â€œOne day, one neighborhood of the city was completely devasted. Glass windows were shattered, shops turned upside down, and many civilian killed. Superman instantly recognized the signature of one of his old enemies, Voltar, who he had barely beaten in the past. This was a message to him: "I challenge you! Come find me!""

An example of a good topic description: An old enemy of Supermanâ€™s, Voltar, appeared and challenged him.
  
An example of a bad topic description: Superman
  
An example of a bad topic description: Voltar
  
An example of a bad topic description:

## Appendix D User prompt: LLM As a Judge - Coherence

Below is a text extract. Your task is to analyze the extract and assign a coherence score between 0 and 5 inclusive, where:

0: The text is completely incoherent and lacks any logical connection.
  
1: The text has some minor connections, but overall it is disjointed and hard to follow.
  
2: The text has some coherence, but it is still difficult to understand due to unclear relationships between ideas.
  
3: The text is moderately coherent, with some clear connections between ideas, but may lack depth or clarity.
  
4: The text is highly coherent, with clear and logical connections between ideas, making it easy to follow.
  
5: The text is extremely coherent, with a clear and concise structure, making it effortless to understand.

You will provide a score ONLY. Do NOT also provide an explanation.

The extract: <extract>

After examining the extract, the coherence score between 0 and 5 inclusive is: