::: CCSXML
\<ccs2012\> \<concept\> \<concept_id\>10002951.10003317.10003338\</concept_id\> \<concept_desc\>Information systems Retrieval models and ranking\</concept_desc\> \<concept_significance\>500\</concept_significance\> \</concept\> \<concept\> \<concept_id\>10002951.10003317.10003347.10003356\</concept_id\> \<concept_desc\>Information systems Clustering and classification\</concept_desc\> \<concept_significance\>500\</concept_significance\> \</concept\> \</ccs2012\>
:::

# Introduction

Automated Essay Scoring (AES) aims to automatically assign a quality score to written essays. It comes in handy for teachers, alleviating the burden of correcting numerous essays and allowing them to concentrate on more crucial responsibilities. Furthermore, AES has the advantage of providing quick and consistent feedback to students, improving the learning process. AES research has primarily focused on holistic scoring [@ke2019automated], which provides a single score reflecting the overall quality of the essay. However, in terms of practicality and effectiveness, a single score falls short in guiding students on how to enhance their skills. Trait-based AES [@mathias-bhattacharyya-2020-neural] fills this gap by individually scoring *traits* of quality, e.g., organization, development, and relevance. Trait scoring enables students to gain insights into specific areas of improvement, empowering them to understand their weaknesses and enhance their writing proficiency.

In this work, we focus on scoring the *relevance* (or so-called *prompt adherence*) trait. The relevance trait, in particular, evaluates the extent to which the essay aligns with the given *task-prompt*.[^1] This trait is crucial as it gauges the student's ability to stay on topic, i.e., maintaining a clear and direct connection to the main subject throughout the writing. Several studies used the essence of prompt adherence as a means to aid in holistic scoring [@chen2018relevance]; however, there is a lack of specific scoring approaches for the relevance trait.

AES systems can be categorized into two types based on the training and inference settings. *Task-specific* AES focuses on a single writing task, where the system is trained exclusively on essays written for *one* specific task, then, during inference, it grades unseen essays written for the *same task*. This setup enables the AES system to learn the distinctive characteristics associated with a specific task, allowing for a more precise assessment of the essays written in response to that task. This type is predominant in the literature for both holistic and trait-based AES. The early research on task-specific AES used feature-based learning approaches [@persing-modeling; @{mathias2018asap++}], then, different neural-based approaches have been proposed [@dong2017attention; @mathias-bhattacharyya-2020-neural; @ijcai2020p536]. Subsequently, pre-trained language models have become dominant for developing AES systems [@yang-etal-2020-enhancing; @kumar2021many; @9530411; @jiang-etal]. Most of those works focused on holistic scoring, and less attention has been given to scoring individual traits.

Meanwhile, *cross-task* (also known as *cross-prompt*) AES systems are trained on essays written for multiple *source* writing tasks to grade essays written for *unseen target* tasks [@jiang-etal]. In this setting, the AES system leverages insights gained from various writing tasks to score essays from unseen tasks. It is commonly observed in real-world scenarios that limited data is available for target tasks [@jin-etal-2018-tdnn], which emphasizes the necessity for developing generalizable cross-task AES systems for grading essays for unseen tasks. Multiple research studies have proposed cross-task holistic scoring systems, e.g., [@jin-etal-2018-tdnn; @li2020sednn; @cao2020domain]. More recently, other approaches for cross-task AES systems have been proposed to score individual traits of the essay along with the holistic score, e.g., [@ridley2021automated; @do-etal-2023-prompt; @chen-li-2023-pmaes]. Nevertheless, all traits are handled the same, without employing a distinct approach for any specific trait, overlooking the fact that each trait focuses on a different dimension of the writing quality of the essay.

To that end, in this paper, we focus on scoring the relevance trait for both *task-specific* and *cross-task* scenarios. We propose a *novel* approach for *graded relevance* scoring (i.e., scoring into one of multiple relevance grades or levels) of written essays. It employs dense retrieval encoders to represent training essays in the embedding space. We then hypothesize that the dense representations of essays having the same relevance level form a cluster in that space such that the centroids of the clusters of different relevance levels will be separate enough to effectively represent their respective relevance levels. We hence use the simple $1NN$ classification model over those centroids to determine the relevance level of an unseen essay. As an example of effective unsupervised dense encoder, we leverage Contriever [@izacard2021towards], which is pre-trained with contrastive learning and has demonstrated comparable performance to supervised dense retrieval models.

In the context of graded relevance scoring of written essays, we address the following research questions:

- **RQ1**: Can the *pre-trained* Contriever model be effectively leveraged for the task? (**Out-of-the-box scenario**)

- **RQ2**: What is the impact of *fine-tuning* the Contriever model on the performance? (**Fine-tuning scenario**)

- **RQ3**: In case only *few* training essays are available for each relevance level, how would that affect the performance? (**Few-shot learning scenario**)

- **RQ4**: In case *no* training essays are available, can we effectively leverage previously-labeled essays from other tasks? (**Cross-task scenario**)

Our study yields promising results when scoring the relevance trait using the pre-trained Contriever. However, fine-tuning Contriever outperformed the state-of-the-art (SOTA) models. Furthermore, we propose an extension to our approach for the cross-task setup. We transform the approach to *task-independent* by "excluding" the information of the task-prompt from the essays. This simple trick resulted in a performance boost, achieving an on par performance with the SOTA model for that scenario.

In summary, our main contribution is four-fold:

- We propose a novel approach that employs dense retrieval for the general task of graded relevance scoring.

- Our approach establishes a new SOTA performance on scoring the relevance of essays in the task-specific setup.

- We analyze the performance of our approach in a more practical few-shot scenario, showing a significant saving of labeling cost while sacrificing only 10% of the effectiveness.

- We propose a simple but effective cross-task extension of our approach that made it on par with the SOTA performance.

The rest of the paper is organized as follows. Section [2](#sec:related-work){reference-type="ref" reference="sec:related-work"} discusses the work related to relevance trait scoring for task-specific and cross-task setups. A detailed description of our approach is provided in Section [3](#sec:methodology){reference-type="ref" reference="sec:methodology"}. Section [4](#sec:experimental_setup){reference-type="ref" reference="sec:experimental_setup"} presents the experimental setup. Section [5](#sec:experimental_evaluation){reference-type="ref" reference="sec:experimental_evaluation"} answers our research questions. Finally, we conclude in Section [6](#sec:conclusion){reference-type="ref" reference="sec:conclusion"}.

# Related Work {#sec:related-work}

In this section, we offer a comprehensive review of existing methods for task-specific and cross-task techniques for relevance scoring. We address the limitations of prior work and highlight the unique contributions and distinctions of our approach.

#### **Task-specific Scoring**

When it comes to AES, task-specific holistic scoring is predominantly emphasized, with less attention directed towards trait scoring. The early research concerning the scoring of the relevance trait focused on feature-based approaches. @persing-modeling pioneered the focused exploration of the relevance trait. They used a linear SVM regression model with a rich set of lexical and knowledge-based features to measure the relevance between the essays and the task-prompt. @{mathias2018asap++} used a common feature set with a Random Forest (RF) classifier for predicting the holistic and traits scores.

Others employed traditional retrieval approaches, such as TF-IDF and pseudo-relevance feedback (PRF), to measure the similarity between the essay and the task-prompt. @cummins2016unsupervised expanded the task-prompt, then the relevance score was computed as the cosine similarity between the TF-IDF representations of the essay and the expanded prompt. The expansion terms were selected based on the closest words to the prompt vector, constructed using random-indexing, CBoW, skip-gram, and PRF. Similarly, [@rei-cummins-2016-sentence] used TF-IDF, CBoW, and skip-thoughts models to measure the similarity between the prompt and each sentence in the essay. @chen2018relevance used the relevance of the essay to the prompt as a feature for holistic scoring. The essay and prompt representations were acquired through an attention-based RNN, with the relevance score being computed via element-wise multiplication between the essay and prompt vectors.

In light of the limited research on trait scoring, the objective of @mathias-bhattacharyya-2020-neural was to leverage various models originally intended for holistic scoring to score multiple essay traits, including relevance. Three approaches were used: a feature-based model with RF algorithm [@{mathias2018asap++}], a string kernel-base approach [@cozma-etal-2018-automated], and an attention-based neural model [@dong2017attention]. Most recently, @kumar2021many used a multi-task neural model to predict multiple writing trait scores in parallel (auxiliary tasks), and these scores were used by the network to predict the holistic score (primary task). They also tried to set one trait as a primary task and other traits along with the holistic score as auxiliary tasks.

Although there are some approaches that have targeted the relevance trait, the majority of those approaches rely on feature-based and traditional retrieval methods. Moreover, none of the recent approaches focused on the relevance trait in specific; rather, they offered a generalized model for all traits, ignoring the fact that each trait pertains to different aspects within the essay. To the best of our knowledge, this is the first work that leverages the recent advancement in the dense information retrieval domain for the task of AES for the relevance trait.

#### **Cross-task Scoring**

Cross-task AES aims to train a model using labeled essays from one or more source tasks and then apply the model to score essays from a target task. Similar to task-specific research, predominant studies on cross-task evaluation have focused on holistic scoring [@jin-etal-2018-tdnn; @li2020sednn; @ridley2020prompt]. The cross-task trait-based AES was introduced by @ridley2021automated, where all of the efforts previously focused solely on holistic scoring. Their approach is an extension of the work of @ridley2020prompt, which utilized part-of-speech embedding with a convolution network to generate the essay's representation. The architecture was modified by introducing shared low-level layers to learn common representation across tasks and high-level layers to capture task-specific information. ProTACT model [@do-etal-2023-prompt] also employed the idea of hierarchical representation, employing low-level layers for information sharing across traits and top layers for trait-specific information. Moreover, the task-prompt was utilized to acquire prompt-aware representation by applying essay-prompt attention. @chen-li-2023-pmaes proposed PMAES, a framework designed to improve cross-task representation through a prompt-mapping contrastive learning strategy. This approach involved the projection of source task essays onto target tasks, generating mapping representations specific to the target task. The objective was to minimize the distance between these mapping pairs, thereby aligning source and target tasks to achieve greater consistency in their representations.

Trait cross-task AES has received less attention compared to task-specific AES. Similar to the task-specific approaches, all of the existing solutions for cross-task provided a common framework for all the traits. Our cross-task approach is tailored specifically for the relevance trait. This is done by generating task-independent representations, which effectively map essays with similar relevance levels across different tasks closer together. One of the unique aspects of our approach is its adaptability for both task-specific and cross-task scenarios, requiring only minor modifications.

# Proposed Approach {#sec:methodology}

<figure id="fig:ts-approach" data-latex-placement="h">
<embed src="SIGIR_2024_Fig1.pdf" />
<figcaption>Training phase of our proposed approach for graded relevance.</figcaption>
</figure>

In this section, we present and discuss in detail our proposed approach for graded relevance scoring of essays in different scenarios. We address the task-specific scenario in Section [3.1](#sec:contriever-method){reference-type="ref" reference="sec:contriever-method"}, the fine-tuning scenario in Section [3.2](#sec:contriever-finetuning){reference-type="ref" reference="sec:contriever-finetuning"}, and the cross-task scenario in Section [3.3](#sec:contriever-crossprompt){reference-type="ref" reference="sec:contriever-crossprompt"}.

## Graded Relevance Scoring of Essays with Dense Retrieval {#sec:contriever-method}

Dense retrieval models are generally encoders that are trained to generate dense vector representations of documents (and queries) so that texts that are topically similar are close in the embedding space, while those that are topically distant are further apart in that space. This can then be employed to find documents that are relevant to a given query, in a typical retrieval scenario, by computing the similarity between the dense representations of the query on one side and documents on the other side.

Our approach builds on this idea to score the relevance trait of students' written essays given a task-prompt, but from a different angle. While the obvious approach is to use the prompt as a query and the test essays as documents in the above setup, several challenges arise. Notably, prompts often differ in length from essays, making the mapping of similarity scores to relevance levels harder. Furthermore, this approach may not capture the diverse writing proficiency levels exhibited in essays, particularly those written by school students. This variability adds complexity to the problem.

Our approach relies heavily on *two* main components: having examples of labeled (i.e., manually-graded or scored) essays from *each* relevance level, and utilizing a robust dense encoder. We hypothesize that the encoder (i.e., retrieval model) can effectively encode essays in a manner that positions those from the same relevance level in close proximity while keeping essays from different relevance levels distant. This yields a (somewhat) separate cluster of essays (in the embedding space) for each different relevance level. Therefore, *centroids* of those clusters can serve as reference points (or good representatives) of their corresponding relevance levels.

This constitutes the "training" phase of our approach. During inference (when we get unseen essays to score), we simply classify essays by assigning them the relevance level corresponding to the *nearest* centroid. In other words, our approach employs a *1-Nearest-Neighbor* (*1NN*) algorithm over the centroids corresponding to the different relevance levels.

This innovative setup reframes the problem; it treats the test essay (i.e., the one to be scored) as a query and the centroids as documents. Our goal is to identify the most "relevant" document (centroid) for the given query (test essay), effectively assigning a relevance score based on this proximity in the embedding space. This approach also transforms the problem into a multi-class classification scenario, where the classes represent the various relevance levels. It is worth mentioning that the model built here is a *task-specific* model, as it is trained on essays labeled for a given task-prompt. Figure [1](#fig:ts-approach){reference-type="ref" reference="fig:ts-approach"} illustrates the training phase of our approach.

Formally, we are given a task-prompt $p$ and a corresponding training set of essays $S_p$. Each training essay $s \in S_p$ is assigned a relevance level $R(s)$. We denote the set of all training essays that are assigned the relevance level $i$ as $S_p^i$, where $S_p^i = \{s \in S_p | R(s) = i\}$.

We first encode each training essay $s$ using the dense encoder $D$ to get its dense vector representation $\vec{e}_s$, where $\vec{e}_s = D(s)$.

Next, for each relevance level $i$, we compute its corresponding centroid $\vec{C}_i$ as the mean of the essay vectors of that level, as follows. $$\begin{equation}
    \vec{C}_i = \frac{1}{|S_p^i|} \sum_{s \in S_p^i}\vec{e}_s
\end{equation}$$ where $|S_p^i|$ is the number of training essays that are assigned the relevance level $i$.

Subsequently, for a given test essay $t$, we estimate its relevance level $R(t)$ based on its most similar relevance centroid, as follows. $$\begin{equation}
 \label{test_essay_eq}
  R(t) = \mathop{\mathrm{argmax}}_i \phi(\vec{e_t}, \vec{C}_i)
\end{equation}$$ where $\phi$ represents the similarity function used to measure the closeness of the test essay and the relevance centroids.

As mentioned earlier, our method relies heavily on the effectiveness of the encoder model used to represent the essays. Dense retrieval models are originally trained for relevance tasks, thus are perfect option to serve our purpose in scoring the *relevance* trait.

As there are several dense retrieval models in the literature, we opted to choose Contriever [@izacard2021towards], an unsupervised dense retrieval model trained with contrastive learning. There are several reasons why we believe Contriever fits our needs. Firstly, it is trained with *contrastive* learning focusing exclusively on distinguishing between similar and non-similar pairs of texts, which is highly needed in our problem. Moreover, it is pre-trained with a large amount of data, and being an unsupervised model makes it less susceptible to bias toward a specific topic or domain. Furthermore, it showed out-of-the-box strong performance in earlier retrieval tasks [@gao-etal-2023-precise].

Notice that the Contriever encoder is used thus far in its original *pre-trained* form. We therefore call this scenario the *out-of-the-box* scenario. We also denote the overall approach we introduced here by **pt**-*Contriever4GR*, the **pre-trained** Contriever for Graded Relevance, to distinguish it from the *fine-tuned* and *cross-task* models we introduce in the subsequent sections.

## Fine-tuning for Task-specific Scoring {#sec:contriever-finetuning}

A natural decision is to fine-tune pre-trained models for downstream tasks. Fine-tuning allows the model to learn more information about a specific task and, hence, potentially achieve better performance on that task. As fine-tuning dense retrieval models is confined to retrieval tasks, we propose a slightly different setup for fine-tuning the retrieval model for graded relevance.

In tasks involving retrieval with dense models, the training dataset comprises queries, relevant, and non-relevant documents. The primary objective is to train the model to decide whether a given document is relevant to a specific query. In our context, essays within the same relevance level are considered "relevant to each other," and essays with different relevance levels are considered "non-relevant to each other." Hence, our model needs to be optimized to differentiate between the different relevance levels.

In Contriever, the contrastive learning setup requires triplets within the training dataset. These triplets comprise anchor, positive, and negative examples. Positive instances are derived through random cropping from the anchor example, while negative examples are sampled from the batch using MoCo [@izacard2021towards]. In our scenario, the process is more straightforward. Positive examples consist of essays within the same relevance level as the anchor essay, while negative examples encompass essays within other relevance levels.

Contriever model was initially trained with InfoNCE loss, presented in Equation [\[equ:infonce\]](#equ:infonce){reference-type="ref" reference="equ:infonce"}. InfoNCE loss is a categorical cross-entropy loss over the similarity between the anchor and positive and the anchor and all negative examples [@oord2018representation], and it optimizes the negative log probability of classifying the positive sample correctly.

$$\begin{equation}
\label{equ:infonce}
    \mathcal{L}_\text{InfoNCE}(s_a , s^+, N) = -\log{\frac{e^{\phi(s_a, s^+)/\tau}}{e^{\phi(s_a, s^+)/\tau}+\sum_{s^- \in N}e^{\phi(s_a, s^-)/\tau}}}
\end{equation}$$ where $s_a$, $s^+$, $s^-$, and $N$ denote the encoded anchor, positive, negative, and set of negative essays, respectively, and $\tau$ is a temperature parameter.

Pairwise softmax cross-entropy (PSCE) loss is another loss function, used in fine-tuning another dense retrieval model [@khattab2020colbert], that serves the same objective. It incorporates the similarity between positive and negative examples with the anchor example. $$\begin{equation}
 \label{equ:loss}
    \mathcal{L}_\text{PSCE}(s_a , s^+, s^-)=-\log{\frac{e^{\phi({s_a , s^+})}}{e^{\phi({s_a , s^+})}+e^{\phi({s_a , s^-})}}}
\end{equation}$$ where $s_a$, $s^+$, and $s^-$ denote the encoded anchor, positive, and negative essays, respectively, constituting a training triplet.

This setup raises multiple alternatives in terms of how the negative training examples are sampled. For an anchor essay example $s_a$ of relevance level $R(s_a)$, we can sample negative examples $s^-$ from all the other relevance levels, i.e., $R(s_a) \neq R(s^-)$. We denote this as sampling from *All* levels. This exposes the model to training triplets from all different levels. We can also restrict the sampling to the relevance levels that are relatively far from the relevance level of the anchor, i.e., $R(s_a) \neq R(s^-)\pm 1$. We denote this as sampling from *Easy* levels, since they should be easy to distinguish. The rationale is that essays with closer scores might actually have the same score if rated by a different rater, potentially confusing the model. Alternatively, we can restrict the sampling to the relevance levels that are closest to the relevance level of the anchor, i.e., $R(s_a) = R(s^-)\pm 1$. We denote this as sampling from *Hard* levels, since they should be hard to distinguish. The rationale is that by mastering this, the model will also be better at distinguishing relevance levels with larger margins. In each case, the number of negative examples we sample from each selected level might vary.

As the Contriever encoder here is fine-tuned, we, hereafter, denote such model by **ft**-*Contriever4GR*, the **fine-tuned** Contriever for Graded Relevance.

## Cross-Task Scoring {#sec:contriever-crossprompt}

<figure id="fig:cross-prompt" data-latex-placement="h">
<embed src="SIGIR_2024_Fig2.pdf" />
<figcaption>Training phase of our proposed cross-task approach with task-independent representations.</figcaption>
</figure>

In the earlier scenarios, we assume there are labeled (i.e., scored or graded) essays for the given writing task, which are needed to build a task-specific model, whether it is pre-trained or fine-tuned. However, this is not always available. We might need to build a model with *no* labeled essays for the test task, i.e., a "zero-shot" model. In such a case, the only available labeled essays to learn from come from *other* different tasks than the test task, hence denoted as the *cross-task* scenario. In the cross-task setup, the training typically utilizes multiple *source* tasks, whereas testing is conducted on an *unseen target* task.

Applying the pre-trained or fine-tuned *Contriever4GR* approach directly to the cross-task scenario (by just training on the superset of all the training essays of the source tasks) would yield separate clusters (in the embedding space) of task-related essays rather than clusters of essays of same relevance level regardless of the task. Hence, the challenge of that scenario in our approach is how to "normalize" the representation of the essays in order to get "task-independent" representations.

Since the dense encoder represents texts in an embedding "semantic" space, it is tempting to make those representations *task-independent* by simply subtracting the task-prompt representation, disentangling the topic-specific semantic features from the relevance level features. Consequently, the cluster of each relevance level will contain the normalized task-independent essay embeddings from all the source tasks. Accordingly, the computed centroids are potentially task-independent. Similar to the earlier scenario, $1NN$ is used to classify the normalized essays for the target task.

Formally, we are given a set of source tasks $P$, each with a task-prompt $p_n$ and a corresponding set of labeled essays $S_{p_n}$. For a relevance level $i$, the corresponding centroid $\vec{C}_i$ is computed as:

$$\begin{equation}
    \vec{C}_i = \frac{1}{\sum_{p_n \in P}|S_{p_n}^i|} \sum_{p_n \in P} \sum_{s \in S_{p_n}^i} \vec{e}_s - \vec{p}_n
\end{equation}$$ The relevance level $R(t)$ for a test essay $t$ is then computed using Equation [\[test_essay_eq\]](#test_essay_eq){reference-type="ref" reference="test_essay_eq"} except that $\vec{e}_t$ is now normalized as $\vec{e}_t - \vec{p}_t$.

This approach scores the essay based on the aggregated information of the relevance levels from the source tasks. Nevertheless, an essential piece of information that can also be considered is the similarity with the target task-prompt. Consequently, we employed both the scores determined by the nearest centroid and the similarity with the target task-prompt to assign the final relevance score of the essay. For a test essay $t$ and a target task-prompt $p^{\prime}$ that has a maximum relevance score of $r_{\max}$, the similarity score $S(t)$ is normalized as follows:

$$\begin{equation}
    S(t) = \phi(\vec{e}_t,\vec{p}^{\prime}) * r_{\max}
\end{equation}$$

where $\phi$ is the similarity function with a \[0-1\] output range. Then, the final score $R^{*}(t)$ of the test essay $t$ is computed as the average between the two scores:

$$\begin{equation}
    R^{*}(t) = \frac{1}{2} (R(t) + S(t))
\end{equation}$$

The fine-tuning process for Contriever in the cross-task scenario follows the same setup for the task-specific scenario. The only difference lies in the essays' representations, which are made task-independent by subtracting the embedding vector of the task-prompt from the anchor, positive, and negative examples during the fine-tuning process.

Our general approach for the cross-task scenario is hereafter denoted by a prefix **ct** amended to the pre-trained or fine-tuned model names for Graded Relevance.

# Experimental Setup {#sec:experimental_setup}

In this section, we introduce the setup used to conduct our experiments. We first present the dataset we used for evaluation and the specific evaluation measure for the AES task. We next discuss the hyper-parameters we used for fine-tuning, before we list the baselines with which we compare our work for the task-specific and cross-task approaches.

[]{#sec:dataset label="sec:dataset"}

#### **Dataset**

We employed the commonly-used Automated Student's Assessment Prize (ASAP)[^2] and ASAP++ [@{mathias2018asap++}] datasets. ASAP dataset comprises 8 different tasks $T$, where each task has a set of written essays. The original ASAP dataset contains the holistic scores for all tasks and the trait scores only for T7 and T8. ASAP++ extends ASAP by scoring the essay traits for tasks T1-T6. To test our approach, we used the tasks that have annotations for the relevance trait, namely, T3, T4, T5, and T6. Table [\[tab:dataset\]](#tab:dataset){reference-type="ref" reference="tab:dataset"} summarizes the dataset we used in our experiment. Following previous work, we use 5-fold cross-validation with the same folds used by @taghipourng2016neural for task-specific models.

::: tblr
cells = c, hline1-2,6 = -, rowsep=1pt, Task & Relevance Levels & Ave. Length & Essays\
T3 & 0-3 & 100 & 1726\
T4 & 0-3 & 100 & 1772\
T5 & 0-4 & 125 & 1805\
T6 & 0-4 & 150 & 1800
:::

[]{#sec:measure label="sec:measure"}

#### **Evaluation Measure**

For evaluating our approach, we used Quadratic Weighted Kappa (QWK) [@cohen1968weighted], which is a widely used measure for AES. QWK measures the agreement between the scores of two raters, in our case, the human rater and the system. QWK is suitable for this task as it weighs the degree of disagreement between the raters, which is what we want as the scores are *ordered*.

[]{#sec:hyperparameters label="sec:hyperparameters"}

#### **Implementation and Hyper-parameters**

All of our experiments are carried out with the Pytorch library.[^3] For Contriever, we used the available checkpoint accessible on Hugging Face's model hub.[^4] For the hyper-parameter tuning, we employed a dynamic learning rate scheduler, ReduceLROnPlateau, which adjusts the learning rate by a factor of 0.5 upon observing a decrease in QWK on the validation set, with patience of 2 epochs. We used the AdamW optimizer with an initial learning rate of 1e-6 and a batch size of 16. Moreover, to avoid overfitting, we used an early stopping condition to stop the training when the QWK on the validation set shows no improvement for 20 epochs. Then, we used the best model on the validation set for testing.

[]{#sec:baselines label="sec:baselines"}

#### **Task-specific Baselines**

We employed the following baselines:

- *Feature-based*: @mathias-bhattacharyya-2020-neural utilized multiple models initially designed for holistic scoring. One of their baselines is a feature-based model described in [@{mathias2018asap++}].

- *Attention-based Neural Model* [@mathias-bhattacharyya-2020-neural]: This model, which is based the work of @dong2017attention, achieved the best results for the relevance trait. Hence, we use it as the SOTA baseline.

- *Multi-task Neural Model* [@kumar2021many]: This work developed a multi-task setup for holistic scoring by scoring the traits as sub-tasks. In a specific experiment, individual traits were set as the primary task for prediction, utilizing the other traits alongside the holistic score as sub-tasks. The reported results provided an average score across all tasks. As such, our comparison will be based on this averaged metric.

#### **Cross-task Baselines**

We employed the following baselines:

- *Vanilla Baseline*: To test the idea of removing topic information by subtracting the encoded task-prompt from the encoded essays, we developed a baseline model that uses the same approach for cross-task setup but without removing the topic information from the essays' representations. We denote this baseline as the "vanilla" cross-task approach **ct$^v$**-**pt**-*Contriever4GR*.

- *PMAES* [@chen-li-2023-pmaes]: This recent work developed a prompt-mapping contrastive learning strategy to capture the shared information between the source and target tasks. To score the traits, a different output layer is used for each trait.

- *ProTACT* [@do-etal-2023-prompt]: This work proposed a shared model to learn the prompt-aware essay representations. Subsequently, trait-specific layers were introduced on top of the shared layers to score the individual traits. ProTACT achieved the best performance for the relevance trait in the cross-task setup. Therefore, we use it as the SOTA baseline.

# Experimental Evaluation {#sec:experimental_evaluation}

In this section, we present and discuss the results of our experiments addressing the four research questions. Section [5.1](#sec:RQ1){reference-type="ref" reference="sec:RQ1"} illustrates the performance of the **pt**-*Contriever4GR* model comparing it with SOTA baselines. Section [5.2](#sec:RQ2){reference-type="ref" reference="sec:RQ2"} details the fine-tuning process and results. Section [5.3](#sec:RQ3){reference-type="ref" reference="sec:RQ3"} discusses the performance of **pt**-*Contriever4GR* and **ft**-*Contriever4GR* in few-shot settings. Finally, Section [5.4](#sec:RQ4){reference-type="ref" reference="sec:RQ4"} offers a comprehensive analysis of the cross-task scenario.

## Pre-trained Contriever (RQ1) {#sec:RQ1}

We first examine the performance of our pre-trained model **pt**-*Contriever4GR*, leveraging the Contriever model without any further training. Table [\[tab:pre-trained-results\]](#tab:pre-trained-results){reference-type="ref" reference="tab:pre-trained-results"} compares our model against the baseline models.

Remarkably, this simple and straightforward idea achieved an average QWK of 0.658 with an *out-of-the-box* Contriever model, which is quite effective for the AES task. It even performs consistently across tasks. Furthermore, it clearly outperforms the feature-based baseline, with only about 7-point lag behind the SOTA model. This performance showcases the versatility of the Contriever model while underscoring the potential of our proposed framework.

:::: table*
::: tblr
roweven = c, row1 = c, row3 = c, row5 = c, row9 = c, row11 = c, row13 = c, cell11 = c=5, cell16 = c=5, cell31 = r=2, cell51 = r=3, cell72 = c, cell73 = c, cell74 = c, cell75 = c, cell79 = c, cell710 = c, cell81 = r=2, cell101 = r=4, vline2 = 1, vline6,10 = 2-13, hline1-3,5,8,10,14 = -, rowsep=1pt, **Configuration** & & & & & ****Performance**** & & & &\
**Exp.** & **Similarity fn.** & **Loss fn.** & **Neg. Sampling** & **Neg. Samples/level** & **T3** & **T4** & **T5** & **T6** & **Ave.**\
**$A$** & Cosine & PSCE & All levels & 1 & 0.718 & 0.763 & 0.732 & 0.789 & 0.751\
& Euclidean & PSCE & All levels & 1 & 0.710 & 0.715 & 0.798 & 0.727 & 0.737\
**$B$** & Cosine & InfoNCE ($\tau$=0.1) & All levels & 1 & 0.658 & 0.693 & 0.710 & 0.668 & 0.683\
& Cosine & InfoNCE ($\tau$=0.2) & All levels & 1 & 0.648 & 0.686 & 0.711 & 0.672 & 0.679\
& Cosine & InfoNCE ($\tau$=0.3) & All levels & 1 & 0.662 & 0.687 & 0.699 & 0.667 & 0.679\
**$C$** & Cosine & PSCE & Easy levels & 1 & 0.675 & 0.727 & 0.788 & 0.724 & 0.728\
& Cosine & PSCE & Hard levels & 1 & 0.658 & 0.683 & 0.665 & 0.659 & 0.666\
**$D$** & Cosine & PSCE & All levels & 2 & 0.720 & 0.760 & 0.792 & 0.738 & 0.752\
& Cosine & PSCE & All levels & 3 & **0.726** & 0.766 & **0.801** & 0.737 & 0.758\
& Cosine & PSCE & All levels & 4 & 0.722 & **0.775** & 0.797 & 0.738 & 0.758\
& Cosine & PSCE & All levels & 5 & **0.726** & 0.774 & 0.739 & **0.801** & **0.760**
:::
::::

## Effect of Fine-tuning (RQ2) {#sec:RQ2}

With the encouraging performance of the pre-trained model shown in the previous experiment, we then turn to examining the performance of our model after fine-tuning.

For fine-tuning our model, we have 4 hyper-parameters: the similarity function, the loss function, the selection criteria of negative training samples, and the number of negative training samples per relevance level. As it is infeasible to tune those hyper-parameters together with a grid search, we tune one at a time while fixing the others in a sequential series of experiments $A$ through $D$. All experiments with different variations are conducted on the development sets. After tuning all parameters, we evaluate the best configuration on the test sets. Table [\[tab:val\]](#tab:val){reference-type="ref" reference="tab:val"} summarizes the performance of the tuning experiments on the development sets.

Firstly, we tested two similarity functions, cosine similarity and Euclidean distance, while using the PSCE loss function, selecting negative samples from all relevance levels, and choosing only 1 negative sample per relevance level. Cosine similarity is a metric frequently utilized for measuring the similarity between two vectors irrespective of their magnitudes. In contrast, Euclidean distance calculates the straight-line distance between two points, offering a different perspective on the similarity. The results of experiment $A$ show the superiority of the cosine similarity function, so we use it in the rest of the experiments.

For the next experiment $B$, we test the performance with the InfoNCE loss function, used for training Contriever [@izacard2021towards], with different temperature values. Surprisingly, although Contriever was pre-trained with InfoNCE loss, it performed less effectively compared to the PSCE loss for our task. This discrepancy may be closely tied to the nature of our problem reformulation. The later loss function has shown effectiveness for retrieval tasks, where the goal is to rank relevant documents higher than non-relevant ones. This aligns better with our problem setup. It is also worth noting that the use of InfoNCE in our setup might not have been optimal, where we have a limited number of negative samples per anchor essay. Accordingly, we continued our experiments with the PSCE loss.

We then examine, in experiment $C$, the effect of the other ways of selecting negative samples (Easy and Hard levels), which differ in what relevance levels to choose the samples from. The results revealed that using Easy negative samples exhibited better performance, as the Contriever model was better able to learn from them to distinguish between the different relevance grades; however, combining both Easy and Hard levels (which comprise the All levels option, shown in experiment $A$) was even better, indicating that they are complementary and both are needed to learn a better model, and highlighting the benefit of providing the model with contrastive examples from various levels.

The final experiment $D$ examines the effect of training with different numbers of negative samples per relevance level. As expected, the more negative samples we include, the better the performance. However, the performance was slightly improving from 0.751 (with one sample per level) to 0.76 (with 5 samples per level). While this points to the importance of increasing the size of our training set, increasing it further incurs further cost in terms of training time and computing resources.

Given the above findings, we settle on the following configuration for the final **ft**-*Contriever4GR* model: cosine similarity as the similarity function, PSCE as the loss function, selecting negative samples from all relevance levels, and drawing 5 negative samples per each level. Table [\[tab:fine-tuned-results\]](#tab:fine-tuned-results){reference-type="ref" reference="tab:fine-tuned-results"} shows the performance of **ft**-*Contriever4GR* on the *test* sets, and, again, we contrast it with the baselines. In fact, **ft**-*Contriever4GR* outperformed the SOTA model on 3 tasks out of 4, while improving the average score of all tasks with nearly 1 point, establishing a new SOTA for the problem of graded relevance scoring of written essays. Moreover, the results indicate that a simple use of a contrastively-learned dense retrieval model can match (and even outperform) the current SOTA models. These findings encourage utilizing dense retrieval models for downstream non-retrieval tasks, wherein a simple problem reformulation with 1NN can achieve impressive performance.

Our best experimented setup is constrained by allowing only up to 5 negative samples per relevance level per anchor essay. However, it is worth noting that we have not explored the optimal scenario of including all possible negative examples for each anchor essay. We argue that doing so could potentially achieve even greater performance improvements.

::: tblr
cells = c, hline1-2,5-6 = -, rowsep=1pt, **Model**& **T3**& **T4**& **T5**& **T6**& **Ave.**\
Feature-based [@mathias-bhattacharyya-2020-neural] & 0.575 & 0.636 & 0.639 & 0.581 & 0.608\
Multi-task NM [@kumar2021many] & - & - & - & - & 0.730\
Attn-based NM [@mathias-bhattacharyya-2020-neural] & 0.683 & 0.738 & **0.719** & 0.783 & 0.731\
**ft**-*Contriever4GR*& **0.704** & **0.766** & 0.707 & **0.785** & **0.740**
:::

<figure id="fig:visualization-TS" data-latex-placement="h">
<embed src="SIGIR_2024_Fig3.pdf" />
<figcaption>2D Visualization of the training set of each task before and after fine-tuning using UMAP.</figcaption>
</figure>

#### **Visualizing *Contriever4GR***

In Figure [3](#fig:visualization-TS){reference-type="ref" reference="fig:visualization-TS"}, we illustrate the impact of fine-tuning Contriever. These visualizations pertain to the training set of each task. Employing the 2D UMAP dimensional reduction method [@mcinnes2018umap], we plotted the encoded essays in a scatter plot for analysis. The figures at the top show how the pre-trained Contriever encodes the essays. Despite a somewhat mixed arrangement of essays with varying scores, distinct groups corresponding to each relevance level are somewhat discernible. However, after fine-tuning, there is a notable refinement, with a more clear ordinal hierarchy that emerges between each relevance level. Furthermore, it is observable that the fine-tuning process not only enhances score-level distinctions but also contributes to the compactness of each cluster of essays. This effect is particularly noticeable in tasks 4, 5, and 6, demonstrating the impact of the employed loss function.

## Few-shot Learning (RQ3) {#sec:RQ3}

![Performance of our models with the $k$-shot setup compared to the full-shot setup on the [test]{.underline} sets.](SIGIR_2024_Fig4.pdf){#fig:few-shots width="\\textwidth"}

In earlier scenarios, we assume many *labeled* essays from the same task are available (more than a thousand in ASAP++ dataset). While this is a common setup, it is indeed *impractical* in educational contexts and poses a considerable challenge, as it requires time-consuming manual grading for any new task. *Few-shot* learning alleviates this constraint by requiring only a few graded essays to effectively train the model, making our approach more feasible.

To that end, we study the performance of our pre-trained and fine-tuned models when trained within a $k$-shot setup, where $k$ is the number of labeled essays per relevance level that are available for training for a given task. We varied $k$ from 5 to 30 with a step of 5. As the number of shots increases, we augmented the initial set by randomly sampling additional 5 essays per relevance level, creating comparable subsets across experiments. We then test the trained models on the standard test set of the task. To improve robustness and mitigate the influence of chance, we repeated the entire process 5 times and reports the average performance over the 5 runs.

Figure [4](#fig:few-shots){reference-type="ref" reference="fig:few-shots"} illustrates the performance of both **pt**-*Contriever4GR* and **ft**-*Contriever4GR* with the few-shot setup compared to the full-shot setup (i.e., training with *all* labeled examples).

As anticipated, a general observation reveals that increasing $k$ positively correlates with improved performance, and the fine-tuned models generally outperform their pre-trained counterparts.

Most notably, the average performance of **ft**-*Contriever4GR* with 30 shots trails by approximately only 7.3 points compared to the full-shot setup; it is crucial to consider that 30 shots imply only 150 labeled essays (assuming 5 relevance levels), which constitutes less than 15% of the full training set that consists of 1000+ essays, reducing the labeling cost by more than 85% while sacrificing about 10% of the performance. This clearly has the potential to control the trade-off between practicality and performance; it indeed opens the door for future efforts toward narrowing the performance gap.

Interestingly, we also noticed that fine-tuning with 25 shots per relevance level was sufficient to reach a similar performance as the pre-trained model when trained with the full set. This highlights the power of the Contriever model when fine-tuned with few shots.

All in all, our models yielded promising performance in the few-shot scenario, though the rule of thumb of having more data for better performance remains. Nevertheless, our models showcase commendable overall performance even with a fraction of the original data, emphasizing the practicality of our approach.

## Cross-Task Scoring (RQ4) {#sec:RQ4}

::: {#crossprompt_results}
  **Model**                            **T3**   **T4**   **T5**   **T6**   **Ave.**
  ----------------------------------- -------- -------- -------- -------- ----------
  ProTACT [@do-etal-2023-prompt]         \-       \-       \-       \-      0.619
  PMAES [@chen-li-2023-pmaes]            \-       \-       \-       \-      0.584
  **ct$^v$**-**pt**-*Contriever4GR*    0.435    0.556    0.194    0.260     0.361
  **ct$^i$**-**pt**-*Contriever4GR*    0.541    0.551    0.526    0.441     0.515
  **ct$^i$**-**ft**-*Contriever4GR*    0.593    0.657    0.344    0.607     0.550
  **ct$^s$**-**pt**-*Contriever4GR*    0.379    0.494    0.381    0.315     0.392
  **ct$^s$**-**ft**-*Contriever4GR*    0.639    0.675    0.621    0.532     0.617

  : Performance of cross-task models on test sets.
:::

What if we do not have any labeled essays for the test task? Can we leverage labeled essays for other tasks to learn a model that can be used to score essays written for a *new* task for which we do not have any labeled essays? This is exactly the *cross-task* scenario.

For this scenario, the setup goes as follows. For each target (i.e., test) task in the dataset, the 3 other tasks (along with their labeled essays) are used for training. Due to the discrepancy of relevance levels across tasks (T3 and T4 range from 0 to 3, while T5 and T6 range from 0 to 4), centroids for relevance levels 0 to 3 are computed from all of the 3 training tasks, while the centroid for relevance level 4 is computed only from T5 and/or T6.

We experimented with 3 variants of *Contriever4GR* for the cross-task scenario. The first is the *vanilla* cross-task model, denoted by the prefix **ct$^v$**, which directly uses the centroids of the essays' vectors as done for the task-specific scenarios. The second is the *task-independent* version denoted by the prefix **ct$^i$**, in which we made the essay vectors (thus the centroids) task-independent by semantically "disentangling" their corresponding task-prompts. The third is an extended version of the second that is denoted by the prefix **ct$^s$**, in which we also involve the similarity with the target task-prompt when we score a given test essay. We tried the second and third variants with both **pt**-*Contriever4GR* and **ft**-*Contriever4GR* models. For fine-tuning, we used the same hyper-parameters used in the task-specific experiments but over only one epoch.

Table [1](#crossprompt_results){reference-type="ref" reference="crossprompt_results"} presents the performance of the variants of our model compared to the SOTA baselines for the cross-task setup. We can draw several observations. First, comparing the performance of the vanilla model with the performance of the task-independent models shows that the idea of excluding the task-prompt representation from the representation of the training essays was very effective, resulting in 15-point and 19-point average improvements in QWK with the pre-trained and fine-tuned models respectively over the vanilla model. Those noticeable improvements indicate that, with this simple trick, the per-task centroids of the same relevance level become indeed better aligned to represent the cluster of their relevance level independent of their corresponding tasks.

Furthermore, the incorporation of the similarity score between the essay and the target task-prompt contributed an additional improvement when applied to the fine-tuned model, achieving a comparable performance to the SOTA model and showing the effectiveness of our approach. This also emphasizes the need to fine-tune the Contriever model for our domain, which results in more distinct clusters for each relevance level. This, in turn, brings the task-prompt vector closer to the cluster of higher-quality essays, consequently improving the similarity score between the essays and their task-prompts.

# Conclusion and Future Work {#sec:conclusion}

In this study, we introduced a novel approach for graded relevance scoring of written essays that leverages dense retrieval, employing Contriever in particular as an example of unsupervised dense retrieval model. Out of the box, without any fine-tuning, our method yielded promising results, yet post-fine-tuning, it established a new SOTA performance for task-specific scenarios. Furthermore, we proposed a simple adjustment to our approach, eliminating the influence of the task-prompt to enable its adaptability for cross-task settings, achieving a performance that is on par with SOTA baselines. Moreover, we showed that our approach exhibited reasonable performance in more practical scenarios where only few essays are labeled for the target writing task. In particular, with only 30 graded essays per relevance level, we observed about 10% drop in performance while saving more than 85% of the manual labor cost.

However, it is important to acknowledge a limitation of our method, namely its assumption of discrete scores, which may not always hold true. Moreover, in cross-task scenarios, our method does not readily adapt to changes in the score range between source tasks and the target task.

This work opens doors for several future directions. One is to test the ability of other dense retrieval models in our task. We also recognize the need to refine the fine-tuning stage, particularly the loss function, e.g., treating negative examples differently. Finally, we plan to explore ways to better adapt to the score range difference across tasks, and to employ the dense representation of individual training examples in addition to their centroids in scoring.

::: acks
The work of Salam Albatarni was supported by GSRA grant# GSRA10-L-2-0521-23037, and the work of Sohaila Eltanbouly and Tamer Elsayed was made possible by NPRP grant# NPRP14S-0402-210127, both from the Qatar National Research Fund (a member of Qatar Foundation). The statements made herein are solely the responsibility of the authors.
:::

[^1]: The task-prompt refers to the specific instructions or guidelines provided to students to guide their essay writing on a particular *topic*.

[^2]: <https://www.kaggle.com/c/asap-aes>

[^3]: <https://pytorch.org/>

[^4]: <https://huggingface.co/facebook/contriever>
