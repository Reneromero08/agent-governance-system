# Introduction

::: {#fig:head}
+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Question: What is the angle of the Tower of Pisa?                                                                                                                                                                                                                                                               |
+:==========================+:====================================================================================================================================================================================================================================================================================+
| Passage   Retrieval       | Prior to restoration work performed between 1990 and 2001, the tower leaned at an angle of 5.5 degrees, but [the tower now leans at about 3.99 degrees]{.mark}. This means the top of the Leaning Tower of Pisa is displaced horizontally 3.9 meters (12 ft 10 in) from the center. |
+---------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Sentence   Retrieval      | Prior to restoration work performed between 1990 and 2001, the tower leaned at an angle of 5.5 degrees, but [the tower now leans at about 3.99 degrees]{.mark}.                                                                                                                     |
+---------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Proposition   Retrieval   | [The Leaning Tower of Pisa now leans at about 3.99 degrees.]{.mark}                                                                                                                                                                                                                 |
+---------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
:::

![image](figures/head.pdf){width="0.98\\columnwidth"}

[]{#fig:head label="fig:head"}

<figure id="fig:appraoch" data-latex-placement="t">
<embed src="figures/pipeline.pdf" style="width:99.0%" />
<figcaption> We discover that segmenting and indexing a retrieval corpus on the <em>proposition</em> level can be a simple yet effective strategy to increase dense retrievers’ generalization performance at inference time <em>(A, B)</em>. We empirically compare the retrieval and downstream open-domain QA task performance when dense retrievers work with Wikipedia indexed at the level of 100-word passages, sentences, or propositions <em>(C, D)</em>. </figcaption>
</figure>

Dense retrievers are a popular class of techniques for accessing external information sources for open-domain NLP tasks [@karpukhin-etal-2020-dense]. Before we use a *learned* dense retriever to retrieve from a corpus, an imperative design decision we have to make is the *retrieval unit* -- i.e. the granularity at which we segment and index the retrieval corpus for inference. In practice, the choice of retrieval units, e.g. documents, fixed-length passage chunks or sentences, etc, is usually pre-determined based on how the dense retrieval model is instantiated or trained [@lewis2020retrieval; @lee-etal-2021-learning; @santhanam-etal-2022-colbertv2; @ni-etal-2022-large].

In this paper, we investigate an overlooked research question with dense retrieval *inference* -- at what retrieval granularity should we segment and index the retrieval corpus? We aim to investigate this question in two aspects.

$\bullet$ First, we examine how the granularity of the index affects passage retrieval performance.

$\bullet$ Second, we investigate whether fine-grained units can replace passages in downstream QA tasks.

Based on our empirical experiments, we discover that selecting the *proper* retrieval granularity at inference time can be a simple yet effective strategy for improving dense retrievers' retrieval and downstream QA performance. We illustrate our intuition with an example of open-domain QA in [1](#fig:head){reference-type="ref+label" reference="fig:head"}. The example shows retrieved text by the same retriever at three different granularities. The *passage*, which represents a coarser retrieval unit with a longer context, is theoretically able to provide more relevant information for the question. However, a passage often includes extraneous details (e.g., restoration period and horizontal displacement in the example of [1](#fig:head){reference-type="ref+label" reference="fig:head"}) that could potentially distract both the retriever and the language model in downstream tasks [@shi2023large; @yu2023chain]. On the other hand, *sentence*-level indexing provides a finer-grained approach but does not entirely address the issue [@akkalyoncu-yilmaz-etal-2019-cross; @yang2020multilingual]. This is because sentences can still be complex and compounded, and they are often not self-contained, lacking necessary contextual information (e.g., in the example of [1](#fig:head){reference-type="ref+label" reference="fig:head"}, "the tower" is the coreference of "Pisa Tower") for judging the query-document relevance.

To address these shortcomings of typical retrieval units such as passages or sentences, we propose using *proposition* as a novel retrieval unit for dense retrieval. Propositions are defined as atomic expressions within text, where each encapsulates a distinct factoid and is presented in a concise, self-contained natural language format. We show an example proposition in [1](#fig:head){reference-type="ref+label" reference="fig:head"}. The proposition describes the information regarding the Tower of Pisa's current leaning angle in a self-contained way and precisely responds to what the question is querying. We provide a more detailed definition and description of *proposition* in [2](#sec:prop){reference-type="ref+label" reference="sec:prop"}. To validate the efficacy of using proposition as a retrieval unit for dense retrievers inference, we first process and index an English Wikipedia dump with all documents segmented into propositions, which we refer to as [FactoidWiki]{.smallcaps}.

We conduct experiments on five different open-domain QA datasets and empirically compare the performance of four dual-encoder retrievers when Wikipedia is indexed by passages, sentences, and our proposed propositions. Notably, our findings indicate that proposition-based retrieval outperforms sentence and passage-based retrieval, especially in terms of generalization, as discussed in [5](#sec:ir){reference-type="ref+label" reference="sec:ir"}. This suggests that propositions, being both compact and rich in context, enable dense retrievers to access precise information while maintaining adequate context. The average improvement over passage-based retrieval of Recall@20 is **+10.1** on unsupervised dense retrievers and **+2.7** on supervised retrievers, *even though these retrievers were directly trained on passage-level retrieval.* Furthermore, we observe a distinct advantage of proposition-based retrieval in downstream QA performance when using retrieval-augmented language models, as elaborated in  [6](#sec:qa){reference-type="ref+label" reference="sec:qa"}. Retrieval by finer-grained units inherently provides a higher density of question-relevant information. This finding implies using finer-grained units in the prompts achieves the same performance with a shorter input length, and hence, a faster inference time.

Our main contributions are:

- We provide a systemic study on how retrieval granularity impacts retrieval and downstream task performance. We observe that the retrieval units have a significant impact on performance.

- We introduce [FactoidWiki]{.smallcaps}, a processed English Wikipedia dump, where each page is segmented into multiple granularities: passages, sentences, and our proposed propositions.

- We propose retrieval by proposition as an alternative strategy, which achieves better retrieval and QA accuracy and generalization performance (with unsupervised retriever), compared to passage or sentence as retrieval unit.

# Proposition as a Retrieval Unit {#sec:prop}

The goal of our study is to understand how the granularity of a retrieval corpus influences the dense retrieval models' performance empirically. Aside from commonly-used retrieval units such as 100-word passages [@karpukhin-etal-2020-dense] or sentences, we propose using *proposition* as an alternative retrieval unit choice. Here, propositions represent atomic expressions of meanings in text [@min2023factscore] with three defining principles below.

1.  Each proposition should correspond to a distinct piece of meaning in text, where the composition of all propositions would represent the semantics of the entire text.

2.  A proposition should be *minimal*, i.e. it cannot be further split into separate propositions.

3.  A proposition should be *contextualized and self-contained* [@choi-etal-2021-decontextualization]. A proposition should include all the necessary context from the text (e.g. coreference) to interpret its meaning.

The use of proposition as a retrieval unit is inspired by a recent line of work [@min2023factscore; @kamoi2023wice; @chen2022propsegment; @chen2023subsentence], which finds success in representing and evaluating text semantics at the level of propositions. We demonstrate the concept of proposition and how a passage can be split into a set of propositions by an example on the left side of Figure [1](#fig:appraoch){reference-type="ref" reference="fig:appraoch"}. The passage contains three propositions, each of which corresponds to a distinct factoid about the *Leaning Tower of Pisa*: the angle before the restoration, the current angle, and the horizontal displacement.

Within each proposition, necessary context from the passage is incorporated so that the meaning of the proposition can be interpreted independently of the original text, e.g. the reference of *the tower* is resolved into its full mention, *the Leaning Tower of Pisa*, in the first proposition. We expect each proposition to describe exactly one atomic fact, and so our intuition is that propositions would suitably work as a retrieval unit for information-seeking questions.

# [FactoidWiki]{.smallcaps}: Proposition-Level Index and Retrieval for Wikipedia {#sec:atomwiki}

We empirically compare passages, sentences, and propositions as retrieval units on Wikipedia, a commonly-used retrieval source for knowledge-intensive NLP tasks [@petroni-etal-2021-kilt]. To allow a fair comparison across granularities, we process an English Wikipedia dump from 2021-10-13, as used by @bohnet2022attributed. We segment each document text into three different granularities: passages, sentences, and propositions. We include the details on passage- and sentence-level segmentation of the corpus in [9](#sec:corpus-details){reference-type="ref+label" reference="sec:corpus-details"}.

#### Parsing Passage to Propositions.

To segment the Wikipedia pages into propositions, we finetune a text generation model, which we refer to as the *Propositionizer*. The *Propositionizer* takes a passage as input and generates the list of propositions within the passage.

Following @chen2023subsentence, we train the *Propositionizer* with a two-step distillation process. We first prompt GPT-4 [@OpenAI2023GPT4TR] with an instruction containing the proposition definition and 1-shot demonstration. We include the details of the prompt in [7](#fig:prompt){reference-type="ref+label" reference="fig:prompt"}. We start with a set of 42k passages and use GPT-4 to generate the seed set of paragraph-to-proposition pairs. Next, we use the seed set to finetune a Flan-T5-large model [@chung2022scaling]. We refer to the processed corpus as [FactoidWiki]{.smallcaps}. The statistics of [FactoidWiki]{.smallcaps} are shown in [2](#tab:stats){reference-type="ref+label" reference="tab:stats"}.

::: {#tab:stats}
                    \# units     Avg. \# words
  -------------- -------------- ---------------
  Passages          41,393,528       58.5
  Sentences       114,219,127        21.0
  Propositions    256,885,003        11.2

  : Statistics of text units in the English Wikipedia.
:::

#### Quality Analysis.

We conduct a manual error analysis to understand the quality of propositions generated by GPT-4 and the Propositionizer. While there does not exist a fixed standard on deciding a ground truth set of propositions for a passage, we estimate the frequency of error cases where (1) a proposition is not fully supported by the passage, (2) a proposition can be further split into separate propositions, and (3) propositions are not self-contained, respectively ([3](#tab:prop-error){reference-type="ref+label" reference="tab:prop-error"}). On a random sample of 50 passages, we observe that almost all propositions generated by both models are faithful, while a small portion of the propositions are not stand-alone.

::: {#tab:prop-error}
                         GPT-4       Propositionizer
  ----------------- --------------- -----------------
  Not Faithful       0.7% (3/408)     1.3% (6/445)
  Not Minimal        2.9% (12/408)    2.0% (9/445)
  Not Stand-alone    4.9% (20/408)    3.1% (14/445)

  : Frequency of errors occurred in the generated propositions. Most generated propositions are faithful, while a small portion of them are not stand-alone.
:::

# Experimental Settings

To evaluate the impact of the three retrieval unit choices, we conduct experiments on five different open-domain QA datasets with [FactoidWiki]{.smallcaps}. With each dataset, we evaluate both passage retrieval and downstream QA performance when dense retrievers work with Wikipedia indexed in different granularities.

## Open-Domain QA Datasets

We experiment on five different open-domain QA datasets with Wikipedia as the retrieval source: Natural Questions  [NQ, @kwiatkowski2019natural], TriviaQA  [TQA, @joshi2017triviaqa], Web Questions  [WebQ, @berant2013semantic], SQuAD [@rajpurkar2016squad], and Entity Questions [EQ, @sciavolino2021simple].

## Dense Retrieval Models

We compare the performance of the four following supervised or unsupervised dense retriever models. Here, *supervised* models refer to ones that have used human-labeled query-passage pairs as supervision during training, and vice versa.

- **SimCSE** [@gao2021simcse] is a BERT-base [@devlin-etal-2019-bert] encoder trained on unlabeled sentences sampled randomly from Wikipedia. SimCSE can be transferred to use as an unsupervised retriever [@chen2023subsentence].

- **Contriever** [@izacard2022unsupervised] is an unsupervised retriever, instantiated with a BERT-base encoder. Contriever is contrastively trained by segment pairs constructed from unlabeled documents from Wikipedia and web crawl data.

- **DPR** [@karpukhin-etal-2020-dense] is a dual-encoder BERT-base model fine-tuned on passage retrieval tasks directly using the question-passage pair labels from NQ, TQA, WebQ and SQuAD.

- **GTR** [@ni-etal-2022-large] is a T5-base encoder [@raffel2020exploring] pretrained on online forum QA data, and fine-tuned with question-passage pair labels on MS MARCO [@tri2016msmarco] and NQ datasets.

## Passage Retrieval Evaluation

We evaluate the retrieval performance at the passage level when the corpus is indexed at the passage, sentence, or proposition level respectively. For sentence and proposition level retrieval, we follow the setting introduced in @lee-etal-2021-phrase, where the score of the passage is based on the maximum similarity score between the query and all sentences or propositions in a passage. In practice, we first retrieve a slightly larger number of text units, then map each unit to the source passage, and eventually return the top-$k$ unique passages. We use Passage Recall@$k$ as our evaluation metric, which is defined as the percentage of questions for which the correct answer is found within the top-$k$ retrieved passages.

To further understand how different retrieved passages affect the downstream QA. We use Fusion-in-Decoder [FiD, @izacard-grave-2021-leveraging] model to extract answers from retrieved passages. We use a T5-large sized FiD model trained on NQ dataset in our experiments. The exact match (EM) score computes the percentage of questions for which the predicted answer exactly matches the ground truth.

::: table*
:::

## Open-domain QA Evaluation on Retrieval-Augmented Language Models

Another aspect of the choice of granularity lies in what units should be used in the prompt for retrieval-augmented language models. For large language models, retrieval-augmented generation is achieved by prepending retrieved units to user instruction and taking them as the input for language models. We aim to understand the implications of using retrieved units of different granularity within the same computational budget at inference time. To fairly compare using different granularity in the prompts under the same computation budget, we set a token length limit for retrieved units.

For this reason, we follow an evaluation setup where the maximum number of retrieved tokens is capped at $l = 100$ or $500$, i.e. only the top $l$ tokens from passage, sentence, or proposition level retrieval are fed into the language model as input. We evaluate the percentage of questions for which the predicted answer exactly matches (EM) the ground truth. We denote our metric as EM @ $l$ tokens. We use LLaMA-2-7B [@touvron2023llama] in our evaluation. To ensure the model's output aligns with the format of each dataset, we employ in-context learning, incorporating **four-shot** demonstrations as illustrated in [8](#fig:llama-prompt){reference-type="ref+label" reference="fig:llama-prompt"}.

<figure id="fig:entity" data-latex-placement="h">
<embed src="figures/entity.pdf" style="width:75.0%" />
<figcaption>Document retrieval recall vs. the frequency of the target entity in each question from the <em>Entity Questions</em> dataset. The frequency of each entity (i.e. smaller value <span class="math inline">⇒</span> less common entities, and vice versa) is estimated by the frequency of the entity in its top-1000 passage retrieved by BM25. On queries with less common entities, we observe that retrieving by proposition shows a larger advantage over retrieval by proposition.</figcaption>
</figure>

::: table*
:::

# How Does Granularity Influence Passage Retrieval? {#sec:ir}

In this section, we report and discuss how indexing the corpus at various granularity influences the passage retrieval performance. Surprisingly, despite all of the dense retrieval models being trained on only passage-level documents, all the models demonstrate on-par or superior performance when the corpus is indexed at the proposition level. Our results suggest that indexing the corpus at the finer-grained units improves the cross-task generalization on passage retrieval.

## Passage Retrieval Performance {#ssec:retrieval-results}

We report our evaluation results in [\[tab:ir\]](#tab:ir){reference-type="ref+label" reference="tab:ir"}. We observe that retrieval by propositions outperforms retrieval by sentences or passages on most tasks for both unsupervised and supervised retrievers.

With all dense retrievers tested, proposition-level retrieval consistently outperforms sentence and passage-level retrieval on average across the five datasets. With the *unsupervised* retrievers, i.e. SimCSE and Contriever, we see an averaged Recall@5 improvement of $+12.0$ and $+9.3$ (35.0% and 22.5% relative improvement) on five datasets.

With the *supervised* retrievers, proposition-level retrieval still shows an advantage on average, yet the sizes of improvements are smaller. We hypothesize that this is due to these retrievers being trained on query-passage pairs. For instance, with DPR, which have been trained on NQ, TQA, WebQ, and SQuAD, we observe that proposition and sentence level retrieval perform slightly worse compared to passage level on three out of the four datasets, with the exception of SQuAD. As shown in [\[tab:ir\]](#tab:ir){reference-type="ref+label" reference="tab:ir"}, all supervised retrievers demonstrate comparable performance across three levels of retrieval granularity in NQ, TQA, and WebQ.

However, on datasets that the retriever model has *not* seen during training, we observe that retrieval by proposition demonstrates a clear advantage. For instance, most notably on SQuAD or EntityQuestions, we observe that proposition-based retrieval significantly outperforms the other two granularities. We see 25% Recall@5 relative improvement on EntityQuestions with relatively weak retrievers like DPR. Furthermore, the Recall@5 of retrieval by proposition on SQuAD improved most on GTR, with 16% relative improvements.

## Retrieval on Finer-grained Index $\Rightarrow$ Better Cross-Task Generalization {#ssec:generalization}

Our results show the advantage of retrieval on proposition-level index in cross-task generalization settings. We observe that on SQuAD and Entity Questions, retrieval on the proposition-level index brings more performance gain over the passage-level index and sentence-level index.

To better understand where the improvements can be attributed, we conduct an additional analysis on Entity Questions. As *Entity Questions* features questions targeting the properties of longer-tail entities, we study how the retrieval performance under three different granularities is affected by the *occurance* of the target entity in question, i.e. whether the entity appears frequently in Wikipedia or not. We estimate the frequency of each entity with the following method. Given the surface form of an entity, we use BM25 to retrieve the top 1000 relevant passages from Wikipedia. We use the number of occurrences of the entity in its relevant passages as an estimate of its frequency. With the 20,000 test queries, around 25% of the target entities have an frequency value of less or equal to 3.

[2](#fig:entity){reference-type="ref+label" reference="fig:entity"} shows the passage retrieval performance vs. the frequency of the target entity in each question. Across all four dense retrievers, we observe that retrieving by proposition shows a much larger advantage over retrieving by passages with questions targeting less common entities. As the frequency of entities increases, the performance gap decreases. Our findings indicate that the performance gain from retrieval by proposition can mostly be attributed to queries for long-tailed information. This echoes our observation that retrieval on proposition-level index improves the cross-task generalization performance of dense retrievers.

## Higher Passage Recall $\Rightarrow$ Higher Downstream QA Accuracy

To further understand whether the passage retrieval on a finer-grained index achieves higher downstream QA performance, we extract the answer from the retrieved passage by a QA reader, Fusion-in-decoder. The results are shown in [\[tab:ir-fid\]](#tab:ir-fid){reference-type="ref+label" reference="tab:ir-fid"}.

Retrieval by proposition-level index achieves the highest average exact match (EM) on all four retriever models. Apart from limited exceptions, the proposition-level index achieves the highest EM for most retrieval tasks and on most datasets. We observe that the trend of downstream QA performance is highly consistent with passage retrieval recall, suggesting higher passage recall implies better downstream QA performance.

::: table*
[]{#tab:qa-llama label="tab:qa-llama"}
:::

<figure id="fig:position" data-latex-placement="t">
<embed src="figures/position_gtr.pdf" style="width:99.0%" />
<figcaption>Recall of the gold answer in the retrieved text limited to first <span class="math inline"><em>k</em></span> words for the GTR retriever. Finer-grained retrieval has a higher recall across all numbers of words.</figcaption>
</figure>

# How Does Granularity Influence Retrieval-Augmented LMs? {#sec:qa}

In this section, we study how the choice of different granularity used in the prompts affects the retrieval-augmented generation across open-domain QA tasks. To fairly compare different granularity with the same computation budget, we limit the number of retrieved tokens for input to the language model at $l=100$ or $500$ tokens. Our results suggest that retrieval by finer-grained units enables a higher density of question-related information in the prompts, leading to better performance.

## Open-domain QA Performance {#ssec:odqa-results}

[\[tab:qa-llama\]](#tab:qa-llama){reference-type="ref+label" reference="tab:qa-llama"} shows the evaluation results with LLaMA-2-7B as the language model. Across different retrievers, we observe higher QA performance in terms of the EM@$l$ metric on average when using propositions as the retrieval unit.

Using propositions rather than passages in the prompts, the four dense retrievers---SimCSE, ConRetriever, DPR, and GTR---improve by +4.1, +3.2, +2.7, and +2.8 in the EM@500 score. The improvements for using sentences over passages for the four retrieval models are +2.4, +2.1, +2, and +1.6, respectively. It is interesting to note that in the LLaMA-2-7B model, the QA accuracy on TQA and WebQ is not sensitive to retrieval type. The highest improvements over the closed-book setting are only +4.9 and +3.2, achieved by GTR with propositions. Nevertheless, we observe that using sentences and propositions in the prompts results in higher performance than using passages for all retrieval models on these two datasets. The results suggest that using finer-grained units in the prompts is beneficial to retrieval-augmented generation.

## Finer-grained Granularity $\Rightarrow$ Higher Density of Question-Related Information {#sec:density}

Intuitively, compared to sentences or passages as retrieval units, the advantage of propositions is that the retrieved propositions have a higher density of relevant information to the query. With finer-grained retrieval units, the correct answer to the query would more likely appear in the top-$l$ retrieved words by a dense retriever.

We illustrate this phenomenon by an analysis shown in [3](#fig:position){reference-type="ref+label" reference="fig:position"}. Here, we investigate the position at which the ground truth answer appears in the top-$l$ retrieved words. Specifically, we calculate the recall of the gold answer within the initial $l$ retrieved words with GTR working with Wikipedia indexed in three different granularities.

We show the results in [3](#fig:position){reference-type="ref+label" reference="fig:position"} and [6](#fig:position-all){reference-type="ref" reference="fig:position-all"} with $l$ ranging from 0 to 500 across all five datasets. For a fixed word retrieval budget, proposition retrieval shows a higher success rate than sentence and passage retrieval methods. The largest improvement of proposition retrieval over passage retrieval occurs within the range of 100-200 words, which corresponds to roughly 10 propositions, 5 sentences, or 2 passages. As word count increases, the recall rate of the three granularities converges, encompassing all relevant information.

# Related Work {#sec:related}

Recent works on dense retrievers typically adopt a dual-encoder architecture [@yih-etal-2011-learning; @reimers-gurevych-2019-sentence; @karpukhin-etal-2020-dense; @ni-etal-2022-large]. With dual-encoders, each query and document is encoded into a low-dimensional feature vector respectively, and their relevance is measured by a non-parametric similarity function between the embedding vectors [@mussmann2016learning]. Due to the limited expressivity from the similarity function, dual encoder models often generalize poorly to new tasks with scarce training data [@thakur2021beir]. Previous studies use techniques such as data augmentation [@wang-etal-2022-gpl; @yu2022generate; @izacard2022unsupervised; @gao-callan-2022-unsupervised; @lin2023train; @dai2023promptagator], continual pre-training [@chang2020pre; @sachan-etal-2021-end; @oguz-etal-2022-domain], task-aware training [@xin-etal-2022-zero; @cheng-etal-2023-task], hybrid sparse-dense retrieval [@luan-etal-2021-sparse; @chen-etal-2022-salient], or mixed strategy retrieval [@ma-etal-2022-open-domain; @ma-etal-2023-chain] and so on to improve cross-task generalization performance of dense retrievers.

The motivation of our work echoes in part with multi-vector retrieval, e.g. ColBERT [@khattab2020colbert], DensePhrase [@lee-etal-2021-learning; @lee-etal-2021-phrase], ME-BERT [@luan-etal-2021-sparse], and MVR [@zhang-etal-2022-multi], where the retrieval model learns to encode a candidate retrieval unit into multiple vectors to increase model expressivity and improve retrieval granularity [@seo-etal-2019-real; @humeau2019poly]. Our work instead focuses on the setting where we do not update the dense retriever model or its parameters. We show that indexing the retrieval corpus by different granularity can be a simple and orthogonal strategy for improving the generalization of dense retrievers at inference time.

In line with generating retrieval units from the original corpus, @sarthi2024raptor propose using generative summaries as additional retrieval units alongside the original text, enhancing queries with document-level understanding. In contrast, our work generates propositions to improve queries related to long-tailed entities. These approaches are complementary, as they address different aspects of retrieval enhancement.

The use of propositions as a unit of text representation dates back to the Pyramid method in summarization evaluation [@nenkova-passonneau-2004-evaluating], where a model-generated summary is evaluated by each proposition. Proposition extraction from text has been a long-standing task, with earlier formulations focusing on a structured representation of propositions [@etzioni2008open; @gildea-jurafsky-2000-automatic]. More recent studies have found success in extracting free-text propositions via few-shot prompting with LLMs [@min2023factscore; @kamoi2023wice], or fine-tuning compact-sized models [@chen2023subsentence].

*Retrieve-then-read*, or more broadly retrieval augmented generation, has recently emerged as a popular paradigm for open-domain question answering [@lewis2021retrievalaugmented; @jiang2023active; @asai2023selfrag]. While earlier works provide up to the top 100 retrieved passages for the downstream reader [@izacard-grave-2021-leveraging; @kedia-etal-2022-fie], the amount of context allowed is significantly reduced when using recent large language models [@touvron2023llama; @yu2023chain], due to the limited context window length and inability to reason over long context [@liu2023lost]. Recent efforts try to improve the quality of the reader context by filtering or compressing the retrieved documents [@wang2023learning; @xu2023recomp]. Our work offers a new perspective by changing the retrieval granularity, in order to achiev greater information density with a fixed context length.

# Conclusion

This paper studies how the choice of granularity for indexing a corpus, as well as the granularity used in the prompts, influences retrieval and downstream QA performance. Our results show that retrieval by propositions outperforms passage-level and sentence-level retrieval on passage retrieval and downstream QA across five open-domain QA datasets. Our analysis shows that indexing a corpus with finer-grained units enhances the cross-task generalization of dense retrievers and increases the density of question-related information in the prompts. We hope that [FactoidWiki]{.smallcaps} and our findings will facilitate future research on information retrieval and retrieval-augmented generation.

# Limitations {#limitations .unnumbered}

The scope of our current study on the granularity of retrieval corpus has the following limitations. (1) *Retrieval Corpus* -- Our study only focuses on Wikipedia as the retrieval corpus, due to the fact that most open-domain QA datasets adopt Wikipedia as the retrieval corpus. (2) *Types of dense retrievers evaluated* -- In the current version of the paper, we evaluate 6 types of popular dense retrievers, most of which follow the bi- or dual-encoder architecture. In future versions, we will include and discuss results on a broader range of dense retrievers. (3) *Language* -- Our current study is limited to English Wikipedia only. We leave the exploration on other languages to future work.

# Ethical Considerations {#ethical-considerations .unnumbered}

This article follows the ACL Code of Ethics. Our work is a foundational research on information retrieval. To the best of our knowledge, we do not find obvious risks related to malicious harmful effects, environmental impact, fairness considerations, or privacy considerations.

# Acknowledgements {#acknowledgements .unnumbered}

The authors sincerely appreciate anonymous reviewers for helpful discussions and comments. The authors would like to thank Xuanyu Ben Zhou, Ruixin Hong, Ning Dai, and Linfeng Shen for valuable feedback on the project. Xinran Zhao is supported by the ONR Award N000142312840.

# Retrieval Corpus Processing {#sec:corpus-details}

The English Wikipedia dump used in this study, released by [@bohnet2022attributed], was selected because it has been filtered to remove figures, tables, and lists, and is organized into paragraphs. The dump dates back to October 13, 2021. We have segmented Wikipedia into three retrieval units for this study: 100-word passage chunks, sentences, and propositions. Paragraphs are divided into 100-word passage chunks using a greedy method. We divide only at the end of sentences to ensure each passage chunk contains complete sentences. As we process the paragraph, we add sentences one by one. If including the next sentence causes the passage chunk to exceed 100 words, we start a new passage chunk with that sentence. However, if the final passage chunk is shorter than 50 words, we merge it with the previous one to avoid overly small segments. Each passage is further segmented into sentences using the widely used Python SpaCy [^3] model. Additionally, each passage is decomposed into propositions by our *Propositionizer* model. Decomposing the entire Wikipedia corpus requires approximately 500 GPU hours on NVIDIA P100 GPUs using the default implementation in the transformers[^4] package. We decomposed 6 million pages into 41 million passages, 114 million sentences, and 257 million propositions. On average, a passage contains 6.3 propositions, and a sentence contains 2.3 propositions.

# Training the Propositionizer

We generated a list of propositions from a given paragraph using GPT-4 with a prompt, as shown in Figure [7](#fig:prompt){reference-type="ref" reference="fig:prompt"}. After filtering, 42,857 pairs were used to fine-tune a Flan-T5-Large model. We named the model Propositionizer. The AdamW optimizer was used with a batch size of 64, learning rate of 1e-4, weight decay of 1e-4, and 3 epochs.

To compare the proposition generation performance of different models, we set up a development set and an evaluation metric. The development set contains an additional 1,000 pairs collected by GPT-4 using the same approach as the training set. We evaluated the quality of the predicted propositions by the F1 score of two sets of propositions. Motivated by the F1 score of two sets of tokens in BertScore, we designed the F1 score for two sets of propositions. Let $P=\{p_1, ..., p_n\}$ denote the set of labeled propositions and $\hat{P}=\{\hat{p}_1, ..., \hat{p}m\}$ the set of predicted propositions. We use $\mathrm{sim}(p_i, \hat{p}j)$ to represent the similarity between two propositions. Theoretically, any text similarity metric can be used. We chose BertScore [@Zhang2020BERTScore] with roberta-large [@liu2020roberta] configuration as our $\mathrm{sim}$ function since we wanted our metric to reflect the semantic difference between propositions. We define $$\begin{equation*}
\begin{aligned}
\mathrm{Recall} &= \frac{1}{|P|} \sum_{p_i \in P} \max_{\hat{p}_j \in \hat{P}} \mathrm{sim}(p_i, \hat{p}_j) \\
\mathrm{Precision} &= \frac{1}{|\hat{P}|} \sum_{\hat{p}_j \in \hat{P}} \max_{p_i \in P} \mathrm{sim}(p_i, \hat{p}_j) \\
\mathrm{F1} &= 2 \cdot \frac{\mathrm{Precision} \cdot \mathrm{Recall}}{\mathrm{Precision} + \mathrm{Recall}}
\end{aligned}
\end{equation*}$$ Here is a figurative explanation of the F1 score: $\mathrm{Recall}$ represents the percentage of propositions in the labeled set that are similar to those in the generated set, $\mathrm{Precision}$ represents the percentage of propositions in the generated set that are similar to the labeled set, and $\mathrm{F1}$ is the harmonic mean of $\mathrm{Recall}$ and $\mathrm{Precision}$. $\mathrm{F1}$ is 1 if the two sets are exactly the same, and 0 if any two propositions are semantically different.

We conducted a comparative analysis of base-size and large-size Flan-T5 models, which were trained using varying amounts of data (shown in [4](#fig:prop-f1){reference-type="ref+label" reference="fig:prop-f1"}). Our findings suggest that larger models, coupled with extensive training data, yield better results. The *Propositionizer* presented in this paper attained an F1 score of 0.822. Upon manually reviewing the generated propositions, we found them to be satisfactory.

<figure id="fig:prop-f1" data-latex-placement="h">
<embed src="figures/f1_vs_num_samples.pdf" />
<figcaption>Performance of proposition-level decomposition by models with different sizes and number of training data.</figcaption>
</figure>

# Quality Analysis of Generated Propositions

We collected propositions generated from 50 randomly selected passages. There are 408 and 445 propositions generated by GPT-4 and Propositionizer, respectively. The propositions and passages were provided to an expert without knowing which model generated each proposition. The expert annotated three scores from different perspectives for each proposition: (1) whether the proposition is fully supported by the passage, (2) whether the proposition is minimal and cannot be further split into separate propositions, and (3) whether the proposition is self-contained. The scores range from 1 to 3, where 1 means \"no,\" 2 means \"maybe,\" and 3 means \"yes.\" We report the number of cases where the annotation was \"no.\" The detailed instructions are provided in [\[tab:quality-anno\]](#tab:quality-anno){reference-type="ref+label" reference="tab:quality-anno"}.

# Offline Indexing

We used the `pyserini` and `faiss` packages to encode retrieval units into embeddings. We exploited multiple GPUs to encode each text unit in groups of 1M units with a batch size of 64. After preprocessing the embeddings, we used an exact search for the inner product (`faiss.IndexFlatIP`) in all experiments. The plain index of [FactoidWiki]{.smallcaps}is approximately 768GB in size. To reduce memory pressure, the embeddings are split into 8 shards. An approximate nearest neighbor search is conducted per shard before aggregating all results.

Although the number of propositions is six times that of passages, using efficient indexing techniques can enable sub-linear search times relative to the total count of vectors. Moreover, utilizing GPU parallelism and distributed indexes significantly decreases the online search time. As a result, with proper implementation, we can make proposition retrieval a practically viable and efficient option.

# Retriever Models and QA Models

We used and packages for the model implementation. We used the following checkpoints released on HuggingFace: SimCSE (), Contriever (), DPR (, ), GTR ().

We use T5-large size Fusion-in-decoder model () released by the authors in <https://github.com/facebookresearch/FiD>. We use HuggingFace checkpoint () for LLaMA-2-7B.

# Additional Results

In Section [5.2](#ssec:generalization){reference-type="ref" reference="ssec:generalization"}, we demonstrated the advantage of retrieval by proposition over retrieval by sentence, particularly as the population of the entity decreases in EQ. We used the occurrence in the top-1000 paragraphs retrieved by BM25 as a proxy for frequency, rather than counting the number of hyperlinks to the entity used in [@sciavolino2021simple]. Therefore, the trend in the performance versus frequency plot shows some differences ([5](#fig:entity-all){reference-type="ref+label" reference="fig:entity-all"}) between our results and those in [@sciavolino2021simple]. For example, some entities are ambiguous (e.g., *1992*, a TV series). In such cases, the occurrence of the surface form of the entity is large. Simultaneously, questions related to ambiguous entities are challenging to answer, leading to lower recall.

In Section [6.2](#sec:density){reference-type="ref" reference="sec:density"}, we discussed the recall of answers in the retrieved text with respect to the context length. We further illustrate the performance trends of six dense retrievers, as detailed in [6](#fig:position-all){reference-type="ref+label" reference="fig:position-all"}. The results indicate that the recall rate of propositions consistently outperforms that of sentences and passages. Our findings lead to the conclusion that question-related density is greater in proposition units compared to sentences and passages.

# Error Case Study

To understand the source of errors from each type of retrieval granularity, we present and discuss four typical examples of mistakes in [\[tab:case\]](#tab:case){reference-type="ref+label" reference="tab:case"} and [\[tab:case-app\]](#tab:case-app){reference-type="ref+label" reference="tab:case-app"}. With each example, we show the question and its corresponding top-1 retrieved text unit by the GTR retriever across the three granularities.

We observe that with passage-level retrieval, the ambiguity of an entity or its references presents a challenge for dense retrievers, which echoes findings from [@min-etal-2020-ambigqa]. For instance, in example Q1, the question asks for "*Super Bowl 50*", but the retrieved passage and sentence refers to "*Super Bowl 5*". In Example Q2, passage retrieval fails to identify the part referring to the correct "*atomic number*". Instead, the top-1 retrieved passage mentions "*atomic number*" in a different and irrelevant context to the question. Retrieval by sentences can also have a similar problem as retrieval by passages like Example Q1. Also, retrieval by sentences faces another challenge of lacking context. In Example Q3 (shown in [\[tab:case-app\]](#tab:case-app){reference-type="ref+label" reference="tab:case-app"}), sentence-based retrieval fails as the correct sentence in the retrieved passage uses "*it*" to refer to the pericardial sac.

Retrieval by propositions tackles the aforementioned problems by ensuring each retrieval unit contains one piece of fact only and necessary context is incorporated in the propositions. However, proposition-based retrieval faces challenges with questions that involve multi-hop reasoning over long-range textual analysis. In Example Q4 (shown in [\[tab:case-app\]](#tab:case-app){reference-type="ref+label" reference="tab:case-app"}), the retrieved passage separately describes the actor's name and the character they portray. There is not a single proposition that entails both the question and the answer.

::: table*
[]{#tab:case label="tab:case"}
:::

::: table*
[]{#tab:case-app label="tab:case-app"}
:::

<figure id="fig:entity-all" data-latex-placement="h">
<p><embed src="figures/entity_birth.pdf" style="width:75.0%" /><br />
(a) Where was [X] born?</p>
<p><embed src="figures/entity_creator.pdf" style="width:75.0%" /><br />
(b) Who was [X] created by?</p>
<figcaption>Document retrieval recall vs. the frequency of the target entity in each question from the <em>Entity Questions</em> dataset. We display the performance of two relations. </figcaption>
</figure>

<figure id="fig:position-all" data-latex-placement="h">
<p><embed src="figures/position_simcse.pdf" style="width:99.0%" /> <embed src="figures/position_contriever.pdf" style="width:99.0%" /> <embed src="figures/position_dpr.pdf" style="width:99.0%" /> <embed src="figures/position_gtr.pdf" style="width:99.0%" /></p>
<figcaption>Recall of the gold answer in the retrieved text limited to first <span class="math inline"><em>k</em></span> words. Finer-grained retrieval has a higher recall across all numbers of words.</figcaption>
</figure>

::: table*
  **Is the proposition fully supported by the passage?** **No** : The information provided relates to the proposition, but there are some gaps or inconsistencies that prevent full support. **Maybe** : The information provided supports the proposition adequately, covering most aspects well; however, minor details or implications might not be fully explored or clarified. **Yes**: The information provided clearly and comprehensively addresses all aspects of the proposition, leaving no relevant details unexplained or ambiguous.
  -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  **Should the given propositions be further split into separate propositions?** **No**: The proposition has a compound structure that could be separated into distinct propositions. **Maybe**: The proposition is mostly straightforward with a single main idea and perhaps a minor additional detail. Splitting might enhance clarity but is not strictly necessary. **Yes**: The proposition is already concise and does not contain a compound structure. Splitting it into separate propositions would likely reduce clarity.
  **Is the given proposition self-contained?** **No**: The proposition contains pronouns, terms, or references whose full names or meanings are not in the proposition. **Maybe**: The proposition is almost entirely self-contained, with only a few minor terms that might be ambiguous without additional context. **Yes**: The proposition is a self-contained claim without any ambiguities, fully understandable on its own.
:::

<figure id="fig:prompt" data-latex-placement="t">
<div class="prompt">
<p><span>Passage <span class="math inline">⇒</span> Propositions</span> Decompose the "Content" into clear and simple propositions, ensuring they are interpretable out of context.</p>
<ol>
<li><p>Split compound sentence into simple sentences. Maintain the original phrasing from the input whenever possible.</p></li>
<li><p>For any named entity that is accompanied by additional descriptive information, separate this information into its own distinct proposition.</p></li>
<li><p>Decontextualize the proposition by adding necessary modifier to nouns or entire sentences and replacing pronouns (e.g., "it", "he", "she", "they", "this", "that") with the full name of the entities they refer to.</p></li>
<li><p>Present the results as a list of strings, formatted in JSON.</p></li>
</ol>
<p><strong>Input</strong>: Title: Ēostre. Section: Theories and interpretations, Connection to Easter Hares. Content: The earliest evidence for the Easter Hare (Osterhase) was recorded in south-west Germany in 1678 by the professor of medicine Georg Franck von Franckenau, but it remained unknown in other parts of Germany until the 18th century. Scholar Richard Sermon writes that "hares were frequently seen in gardens in spring, and thus may have served as a convenient explanation for the origin of the colored eggs hidden there for children. Alternatively, there is a European tradition that hares laid eggs, since a hare’s scratch or form and a lapwing’s nest look very similar, and both occur on grassland and are first seen in the spring. In the nineteenth century the influence of Easter cards, toys, and books was to make the Easter Hare/Rabbit popular throughout Europe. German immigrants then exported the custom to Britain and America where it evolved into the Easter Bunny."</p>
<p><strong>Output</strong>: [ "The earliest evidence for the Easter Hare was recorded in south-west Germany in 1678 by Georg Franck von Franckenau.", "Georg Franck von Franckenau was a professor of medicine.", "The evidence for the Easter Hare remained unknown in other parts of Germany until the 18th century.", "Richard Sermon was a scholar.", "Richard Sermon writes a hypothesis about the possible explanation for the connection between hares and the tradition during Easter", "Hares were frequently seen in gardens in spring.", "Hares may have served as a convenient explanation for the origin of the colored eggs hidden in gardens for children.", "There is a European tradition that hares laid eggs.", "A hare’s scratch or form and a lapwing’s nest look very similar.", "Both hares and lapwing’s nests occur on grassland and are first seen in the spring.", "In the nineteenth century the influence of Easter cards, toys, and books was to make the Easter Hare/Rabbit popular throughout Europe.", "German immigrants exported the custom of the Easter Hare/Rabbit to Britain and America.", "The custom of the Easter Hare/Rabbit evolved into the Easter Bunny in Britain and America." ]<br />
<strong>Input</strong>: &lt;<em>a new passage</em>&gt;</p>
<p><strong>Output</strong>:</p>
</div>
<figcaption>Prompt for generating propositions from a passage using GPT-4.</figcaption>
</figure>

<figure id="fig:llama-prompt" data-latex-placement="t">
<div class="prompt">
<p><span>Open-domain QA for LLaMA-2-7B</span> ... [demonstrations] ...<br />
<strong>Refer to the passages below and answer the following question with just a few words.</strong></p>
<p><strong>Title</strong>: 1972 in spaceflight. <strong>Passage</strong>: In 1972, humanity’s last crewed mission to the Moon of the 20th century was Apollo 17.</p>
<p><strong>Title</strong>: 1970s. <strong>Passage</strong>: Apollo 17 Astronaut Gene Cernan becomes the last man on the Moon on December 13, 1972.</p>
<p><strong>Title</strong>: List of Apollo missions</p>
<p><strong>Refer to the context above and answer the following question with just a few words.</strong></p>
<p>Question: when was the last time anyone was on the moon</p>
<p>The answer is</p>
</div>
<figcaption>Prompt for retrieval-augmented generation of open-domain QA for the LLaMA-2-7B model.</figcaption>
</figure>

[^1]: <https://github.com/chentong0/factoid-wiki>

[^2]: $\ \ $Work was done during internship at Tencent AI Lab, Bellevue.

[^3]: <https://spacy.io/>

[^4]: <https://huggingface.co/docs/transformers/en/index>
