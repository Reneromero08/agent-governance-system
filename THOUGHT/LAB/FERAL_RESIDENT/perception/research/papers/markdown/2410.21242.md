# Introduction

<figure id="fig:rede" data-latex-placement="t!">
<img src="figures/rede_figure.png" />
<figcaption>An illustration of the ReDE-RF approach. </figcaption>
</figure>

Information Retrieval (IR) aims to identify relevant documents from a large collection of text given a user's information needs. With recent advancements in transformer-based language models, dense retrieval techniques [@karpukhin-etal-2020-dense; @xiong2020approximate; @qu2020rocketqa] -- which map queries and documents to a shared semantic embedding space that captures relevance patterns -- have demonstrated significant success compared to traditional retrieval approaches based on exact-term matching, such as BM25 [@robertson2009probabilistic]. Despite great performance, it remains difficult to build dense retrieval systems in settings that do not have large amounts of dedicated training data [@thakur2021beir].

Recent work has explored improving unsupervised dense retrieval systems, such as Contriever [@izacard2021unsupervised], by leveraging Large Language Models (LLMs) to enrich the query embedding for nearest neighbor search. HyDE [@gao-etal-2023-precise], for example, prompts an LLM to generate hypothetical documents that are used to search for the closest *real* documents. This casts dense retrieval as a document similarity task, which aligns well with pre-training techniques of unsupervised dense retrieval methods [@gao-etal-2023-precise; @izacard2021unsupervised]. While HyDE demonstrates strong zero-shot[^1] performance, it is highly reliant on the LLMs' parametric knowledge, which can be a barrier in deployment for out-of-domain corpora settings (e.g., proprietary documents). A potential solution could be to leverage top-retrieved documents as context when prompting the LLM to generate hypothetical documents [@shen2024retrieval]. However, this increases search latency due to the longer input context. Additionally, even with better prompt context, hypothetical documents generated by LLMs remain susceptible to common issues such as overlooking or ignoring the provided information [@zhou-etal-2023-context; @shi2023trusting; @liu2024lost; @simhi2024constructing].

To address these challenges, we propose re-framing the task as *relevance estimation* rather than *hypothetical document generation*. Drawing inspiration from relevance feedback, we introduce **Re**al **D**ocument **E**mbeddings from **R**elevance **F**eedback (ReDE-RF). ReDE-RF first retrieves an initial set of documents from a fully unsupervised hybrid sparse-dense retrieval system and prompts an LLM to mark the returned documents as relevant or non-relevant. Then, given the set of relevant documents, ReDE-RF fetches the document embeddings -- which are precomputed offline -- from the dense index and generates an updated query vector. When updating the query representation with LLM relevance feedback, the new representation is based strictly on *real* documents from the corpus; the LLM does not generate any content that is used to refine the query representation. Importantly, we also note that ReDE-RF's goal is similar to that of @gao-etal-2023-precise: we want to develop a full zero-shot dense retrieval pipeline that requires no relevance supervision.

The core motivation behind our approach is simple: if we can easily access top retrieved documents, we do not need to exclusively rely on LLMs to generate hypothetical documents. First, employing an LLM to generate a hypothetical document for every query is inefficient and introduces unnecessary latency costs. Second, we argue that the task of generating a hypothetical document is highly complex and requires the LLM to (1) already memorize the domain-specific knowledge relevant to the query and (2) replicate the structure of a relevant document. In contrast, knowing what is relevant is a much simpler task. Furthermore, when making use of real documents, we guarantee that the content used to refine the query representation is inherently grounded in the corpus, enabling our method to more seamlessly generalize across different domains.

We empirically evaluate ReDE-RF on a wide range of retrieval tasks. Our findings reveal that for low-resource tasks, ReDE-RF surpasses zero-shot dense retrieval methods that use LLMs for hypothetical document generation by up to 6% when LLMs are prompted to generate a hypothetical document with top-retrieved documents as context and 14% when prompted without. Furthermore, ReDE-RF reduces retrieval latency by as much as 7.5-11.2$\times$ compared to hypothetical document generation with top-retrieved documents as context and 4.4$\times$ without.

Our contributions are summarized as follows:

- We propose ReDE-RF, a method that enhances query embeddings for unsupervised dense retrieval systems while addressing key challenges associated with approaches that rely entirely on hypothetical document generation.

- We comprehensively evaluate ReDE-RF on a variety of search tasks and show that ReDE-RF improves upon state-of-the-art zero-shot dense retrieval approaches in low-resource domains while also improving latency.

- We demonstrate an approach for *distilling* ReDE-RF to a smaller, more efficient unsupervised dense retriever, DistillReDE. DistillReDE is able to make a 33% improvement on Contriever while not requiring any update to the Contriever document index or relying on LLMs at inference time.

# Methodology

In this section, we first provide a brief overview of HyDE, a method that performs zero-shot dense retrieval through the generation of *hypothetical* documents. We then describe how we leverage LLMs to perform relevance feedback, a critical component that allows us overcome the challenges of hypothetical documents. Lastly, we describe how we use relevance feedback outputs to update our query representation.

## Preliminaries: HyDE

The main challenge in zero-shot dense retrieval is learning query and document embedding functions that capture relevance when human-annotated relevance scores are not available. HyDE [@gao-etal-2023-precise] seeks to overcome this by re-casting the task as a document-document similarity task.

Given a query, $q$, HyDE first samples $N$ hypothetical documents $\{\hat{d}_1, \dots, \hat{d}_N\}$ from a generative LLM -- denoted by $\textnormal{LLM}_{\textnormal{DocGen}}$ -- via zero-shot prompting: $$\begin{equation}
    \hat{d}_{i}=\textnormal{LLM}_{\textnormal{DocGen}}(q), \quad 1 \leq i \leq N
\end{equation}$$ where $\textnormal{LLM}_{\textnormal{DocGen}}(q)$ denotes the stochastic output of $\textnormal{LLM}_{\textnormal{DocGen}}$ given $q$. One could optionally provide the top-*k* documents $D = \{d_{1}, d_{2}, \dots, d_{k}\}$ from an unsupervised retrieval system (e.g., BM25) as context for $\textnormal{LLM}_{\textnormal{DocGen}}$: $$\begin{equation}
    \hat{d}_{i}=\textnormal{LLM}_{\textnormal{DocGen}}(D, q), \quad 1 \leq i \leq N
\end{equation}$$

We refer to this as $_\text{PRF}$[^2].

Subsequently, the hypothetical documents, $\{\hat{d}_1, \dots, \hat{d}_N\}$, are encoded by an unsupervised contrastive encoder, $f$, and averaged to generate an updated query embedding, $\hat{v}_{q_{\textnormal{HyDE}}}$. When generating $\hat{v}_{q_{\textnormal{HyDE}}}$, the original query is also considered. More formally, $$\begin{equation}
    \hat{v}_{q_{\textnormal{HyDE}}} = \frac{1}{N + 1}\ \left ( f(q) + \sum_{i=1}^{N} {f(\hat{d}_i)} \right )
    \label{eq:HyDE}
\end{equation}$$ $\hat{v}_{q_{\textnormal{HyDE}}}$ is then searched against the corpus embeddings to retrieve the most similar *real* documents.

Through this two-step process, zero-shot dense retrieval moves from directly modeling query-document similarity to modeling document-document similarity. Without the need for relevance supervision, HyDE is able to out-perform state-of-the-art unsupervised dense retrievers.

## ReDE-RF

Similar to HyDE, ReDE-RF models zero-shot dense retrieval as a document similarity task. Unlike HyDE, the LLM is leveraged for relevance *feedback* rather than document *generation*. ReDE-RF has two key components: (1) relevance feedback with LLMs and (2) updating the query representation. These two components are described in this subsection and illustrated in Figure [1](#fig:rede){reference-type="ref" reference="fig:rede"}.

### Relevance Feedback with LLMs {#sec:rel_feed}

Given a query, we first retrieve the top-*k* documents, $D$, from an unsupervised retrieval system. Subsequently, we employ zero-shot prompting to score the relevance of given document, $d_{i}$, to the query. Based on the prompt, a generative LLM, denoted by $\textnormal{LLM}_{\textnormal{Rel-Judge}}$, returns a list of the $k^*$ documents classified as relevant $D_{r} = \{d_{r_{1}}, d_{r_{2}}, \dots, d_{r_{k^{*}}}\}$, where $r_i \in \{1,2,\hdots,k^{*}\}$ for $1 \leq i \leq k^*$.

<figure id="fig:prompt" data-latex-placement="t">
<img src="figures/relevance_feedback_prompt.png" style="width:100.0%" />
<figcaption> Prompt for relevance feedback, which is a modified version of the prompt used in <span class="citation" data-cites="upadhyay2024llms"></span>. {} denotes the placeholder for the corresponding text.</figcaption>
</figure>

With the recent success of using LLMs for patching up missing relevance judgements, we use a modified version of the prompt from @upadhyay2024llms, which is shown in Figure [2](#fig:prompt){reference-type="ref" reference="fig:prompt"}.

### Updating the Query Representation {#sec:query_rep}

Given the list of documents, $D_{r}$, that $\textnormal{LLM}_{\textnormal{Rel-Judge}}$ deems relevant, we follow Equation [\[eq:HyDE\]](#eq:HyDE){reference-type="ref" reference="eq:HyDE"} to update the query embedding. One key difference between ReDE-RF and HyDE is that $f(d_{r_{i}})$ -- where $1 \leq {i} \leq k^{*}$ -- already exists as it is the embedding of a real document that was pre-computed offline. As such, we denote $C_{E}[d_{r_{i}}]$ as the action of retrieving the embedding for a specified document, $d_{r_{i}}$, from the set of corpus embeddings $C_{E}$. Thus, to update our query:

$$\begin{equation}
 \hat{v}_{q_{\textnormal{ReDE}}} =  \frac{1}{k^* + 1}\ \left ( f(q) + \sum_{i=1}^{k^*} {C_{E}[d_{r_{i}}]} \right )
\end{equation}$$

If no relevant documents are found in the top-$k$, i.e., $D_{r} = \emptyset$, a simple option could be to default to the unsupervised contrastive encoder, and just return $f(q)$. However, in these cases we also argue defaulting to hypothetical document generation can be a viable option as it would only hurt latency for difficult queries that the initial retrieval struggles with. We compare the trade-off between effectiveness and efficiency of these two options in Section [3.2](#sec:main_results_benchmarks){reference-type="ref" reference="sec:main_results_benchmarks"} and [3.3](#sec:main_results_latency){reference-type="ref" reference="sec:main_results_latency"}.

# Experiments

::: table*
:::

## Setup {#sec:setup}

**Implementation** ReDE-RF requires an instruction-tuned LLM and a dense retriever. For the instruction-tuned LLM (i.e., $\textnormal{LLM}_{\textnormal{Rel-Judge}}$) we leverage Mistral-7B-Instruct-v0.2 [@jiang2023mistral] and for dense retrieval we use the unsupervised Contriever [@izacard2021unsupervised]. When prompting $\textnormal{LLM}_{\textnormal{Rel-Judge}}$ for relevance feedback, we truncate the input document to at most 128 tokens and generate a relevance score by applying a softmax on the logits of the "1" and "0" tokens as shown in @nogueira-etal-2020-document. Only documents that $\textnormal{LLM}_{\textnormal{Rel-Judge}}$ scores as '1' are used for updating the query representation. In cases in which $D_{r} = \emptyset$, we consider two defaults: Contriever and $_\text{PRF}$.

To generate an initial document set for $\textnormal{LLM}_{\textnormal{Rel-Judge}}$, we retrieve the top-20 documents from a hybrid, sparse-dense, retrieval model (BM25 + Contriever)[^3]. Retrieval experiments were performed with Pyserini [@lin2021pyserini] and LLM implementations in HuggingFace [@wolf2019huggingface].

**Datasets** In our experiments, we evaluate on two web search datasets: TREC DL19 [@craswell2020overview] and TREC DL20 [@craswell2021overview]. We also evaluate on seven low-resource retrieval datasets from BEIR [@thakur2021beir]. The tasks include news retrieval (TREC-News, Robust04), financial question answering (FiQA), entity retrieval (DBpedia), biomedical IR (TREC-Covid, NFCorpus), and fact checking (SciFact). For metrics, we report NDCG@10, the offical metric for the TREC and BEIR datasets.

**Baselines** We first compare ReDE-RF to unsupervised retrievers that do not leverage LLMs: BM25, Contriever, and a hybrid retrieval model (BM25 + Contriever). We also include a pseudo-relevance feedback (PRF) baseline which averages all top-$k$ initially retrieved documents to update the query representation: $\textnormal{Contriever}_{\textnormal{AvgPRF}}$ [@li2022pseudo]. $\textnormal{Contriever}_{\textnormal{AvgPRF}}$ is equivalent to ReDE-RF if all top-$k$ retrieved documents are considered relevant. For $\textnormal{Contriever}_{\textnormal{AvgPRF}}$, the initially retrieved documents are from the hybrid retrieval model (BM25 + Contriever).

We then compare ReDE-RF to methods that use LLMs for zero-shot dense retrieval and require no training. Our main point of comparison for ReDE-RF is HyDE and $_\text{PRF}$. For $_\text{PRF}$ we prompt the LLM using the top-20 initially retrieved documents from the same hybrid retrieval system as ReDE-RF. We also compare ReDE-RF to the dense retrieval version of PromptReps [@zhuang2024promptreps], which generates query and document representations by prompting an LLM to generate a single token that describes the text.

Lastly, we compare against dense retrieval systems that have been fine-tuned with supervised data: DPR [@karpukhin-etal-2020-dense], ANCE [@xiong2021approximate], and $\textnormal{Contriever}^{\textnormal{FT}}$, a fine-tuned version of Contriever.

## Results on Benchmarks {#sec:main_results_benchmarks}

Table [\[tab:BEIR\]](#tab:BEIR){reference-type="ref" reference="tab:BEIR"} presents the evaluation results on the TREC and BEIR datasets and reveals several insights:

\(1\) ReDE-RF outperforms $\textnormal{Contriever}_{\textnormal{AvgPRF}}$, which uses all initially retrieved documents to enhance the query embeddings. This exemplifies that simply leveraging the top-$k$ retrieved documents is not sufficient, and demonstrates the value of leveraging an LLM to filter out non-relevant documents.

\(2\) Comparing ReDE-RF to HyDE, we find that using real documents for zero-shot dense retrieval consistently outperforms hypothetical documents based solely on LLM knowledge (i.e., without top documents as context).

\(3\) When incorporating corpus text as a guide for HyDE (i.e., $_\text{PRF}$), the performance gap between ReDE-RF and HyDE decreases. However, ReDE-RF still provides substantial improvements in low-resource domains (6.0% when defaulting to $_\text{PRF}$ and 4.6% when defaulting to Contriever). For high-resource domains --- DL19 and DL20 --- $_\text{PRF}$ yields better results, which we hypothesize is due to the advantages of combining the LLM's parametric knowledge with corpus knowledge in domains the LLM is well-versed.

\(4\) As the performance of ReDE-RF (Default: $_\text{PRF}$) is equivalent to $_\text{PRF}$ for queries that ReDE-RF defaults, the performance gains of ReDE-RF in low-resource domains can be attributed to the benefits of doing nearest-neighbor search in the *real* document embedding space -- that $\textnormal{LLM}_{\textnormal{Rel-Judge}}$ deemed relevant -- versus the *hypothetical* document space generated by $\textnormal{LLM}_{\textnormal{DocGen}}$

\(5\) While ReDE-RF remains competitive with fine-tuned dense retrieval systems (DPR, ANCE, and $\textnormal{Contriever}^{\textnormal{FT}}$) in high-resource domains, in low-resource domains, ReDE-RF outperforms DPR and ANCE on all but one dataset, and surpasses $\textnormal{Contriever}^{\textnormal{FT}}$ on four of the seven low-resource datasets.

## Comparing Latencies {#sec:main_results_latency}

In Figure [3](#fig:latency_main){reference-type="ref" reference="fig:latency_main"}, we empirically compare the average query latency for $_\text{PRF}$, HyDE and ReDE-RF. We also include $_\text{PRF}$ with 10 initially retrieved documents -- $_\text{PRF}$ (10 Docs) -- as an additional comparison[^4]. All experiments were run on one A100 GPU and measure the time from input query to retrieval of results.

Comparing the latencies across systems, we find that ReDE-RF (Default: Contriever) consistently reduces latency compared to HyDE and $_\text{PRF}$. Specifically, on average, ReDE-RF (Default: Contriever) is 3.8$\times$ faster than HyDE and 6.7 to 9.7$\times$ faster than $_\text{PRF}$, depending on whether 10 or 20 documents are used as context. This finding is true even when ReDE-RF defaults to $_\text{PRF}$, improving latency by 2.4$\times$ compared to HyDE and and 4.1 to 5.9$\times$ compared to $_\text{PRF}$. These results confirm our hypothesis that leveraging hypothetical document generation for every query introduces unnecessary latency costs and that it is possible to improve performance while also improving efficiency.

For ReDE-RF, Table [\[tab:BEIR\]](#tab:BEIR){reference-type="ref" reference="tab:BEIR"} showed that defaulting to $_\text{PRF}$ can provide a performance boost as compared to defaulting to Contriever. However, these improvements come with a higher latency. Ideally, ReDE-RF can achieve the performance of ReDE-RF (Default: $_\text{PRF}$) while fully removing the need for generating hypothetical documents. We investigate this in Section [5](#sec:distill_rede){reference-type="ref" reference="sec:distill_rede"}.

<figure id="fig:latency_main" data-latex-placement="t">
<img src="figures/latency_main_results.png" style="width:100.0%" />
<figcaption>Latency per query for <span class="math inline"><sub>PRF</sub></span>, HyDE and ReDE-RF. Speedup is relative to slowest method (<span class="math inline"><sub>PRF</sub></span>).</figcaption>
</figure>

# Ablation Study on ReDE-RF

There are many design decisions that one can make when implementing ReDE-RF. In this section, we study the effects of these different choices on ReDE-RF's performance. As some approaches may default more frequently than others, the result will be affected by how strong the chosen default is. Thus, in the ablation study, we choose no default: return no results for the query if $k^* = 0$ (yielding an NDCG@10 of 0) to limit our study solely to the relevance feedback portion of ReDE-RF. We refer to this as ReDE-RF (No Default.)

[]{#sec:Initial_Retrieval_Abalation label="sec:Initial_Retrieval_Abalation"}

**Effect of Initial Retrieval Method** How does the initial retriever effect ReDE-RF accuracy? The results for this experiment are in Table [\[tab:initial_ret\]](#tab:initial_ret){reference-type="ref" reference="tab:initial_ret"}. Feeding documents using hybrid retrieval consistently improves results compared to only sparse or only dense retrieval. As ReDE-RF is highly dependent on the initial retrieval, these results suggest that leveraging multiple unsupervised retrievers can improve performance.

**Impact of $k^*$** In Table [1](#tab:retrieved_res){reference-type="ref" reference="tab:retrieved_res"} we study how the number of relevant documents ($k^*$ as defined in [2.2.1](#sec:rel_feed){reference-type="ref" reference="sec:rel_feed"}) used to update the query representation influences the accuracy of ReDE-RF. Accuracy generally increases up to and stabilizes around $k^* = 10$. We hypothesize this may be due to the increased number of potential false positives returned as the number of relevant documents increases, which may push ReDE-RF's query embedding further away from the true positives.

::: {#tab:retrieved_res}
                 **DL20**   **News**   **DBpedia**  
  ------------- ---------- ---------- ------------- --
  *Max $k^*$*                                       
  1                54.2       35.8        31.5      
  5                58.5       40.8        35.0      
  10               60.0       41.1        36.1      
  20               59.0       41.2        35.4      

  : Impact of the number of documents used to update the ReDE-RF (No Default) query embedding.
:::

**Effect of Relevance Feedback Model** In Table [\[tab:model_impact\]](#tab:model_impact){reference-type="ref" reference="tab:model_impact"} we investigate ReDE-RF with different LLMs. We generally find similar trends across model families: as the model size increases, performance improves. Generally, smaller models are competitive with their larger counterparts, demonstrating the potential for using a smaller LLM for relevance feedback at the benefit of faster inference times.

**Prompt Variations** In Table [\[tab:prompt_variations\]](#tab:prompt_variations){reference-type="ref" reference="tab:prompt_variations"} we study how different prompts impact ReDE-RF's retrieval effectiveness. The results show that, on average, performance across prompts is similar. The only prompt that performs worse is RG-YN, which asks *\"For the following query and document, judge whether they are relevant. Output "Yes" or "No"*. We hypothesize that the drop in performance stems from the prompt not giving a clear definition of what relevance is (which the other prompts do more clearly, e.g., *\"the passage answers the query\"*). To test this, we augment the prompt with the description relevance from Figure [2](#fig:prompt){reference-type="ref" reference="fig:prompt"} (RG-YN$*$). The results show that this simple augmentation improves performance of RG-YN by 4.6%, on average. This finding hints that when creating prompts for ReDE-RF it is important the prompt includes a clear definition of what should be classified as relevant versus not.

# Can we Distill ReDE-RF? {#sec:distill_rede}

As noted in Section [3.3](#sec:main_results_latency){reference-type="ref" reference="sec:main_results_latency"}, ReDE-RF improves latency per query as compared to HyDE and $_\text{PRF}$. However, if ReDE-RF is implemented with $_\text{PRF}$ as its default, it still occasionally needs to default to hypothetical document generation if no relevant documents are found, thus making it costly for certain queries. In this section, we explore if we can improve the latency of ReDE-RF without trading off accuracy. With this in mind, we aim to answer two questions: 1) Can we *distill* ReDE-RF's performance to Contriever (DistillReDE)? 2) Can using DistillReDE in tandem with ReDE-RF remove the need for defaulting to $_\text{PRF}$ while matching the performance of ReDE-RF (Default: $_\text{PRF}$)?

<figure id="fig:distill_rede" data-latex-placement="t!">
<img src="figures/distill_rede.png" style="width:100.0%" />
<figcaption>Comparison of DistillReDE to Contriever and ReDE-RF (Default: <span class="math inline"><sub>PRF</sub></span>).</figcaption>
</figure>

**Distilling ReDE-RF** We aim to explore whether ReDE-RF can be distilled to a student Contriever model, DistillReDE. Since ReDE-RF's embeddings are an average of the Contriever document embeddings, one advantage is that the student model can be trained without the need to re-index the corpus. To generate the training set, we first run ReDE-RF offline using LLM-generated synthetic queries and treat the corresponding ReDE-RF embeddings as the target representation. For training, we follow the framework from @pimpalkhute2024softqe: we optimize a combination of MSE loss and contrastive loss with in-batch random negatives. See Appendix [14](#sec: distill_rede_training){reference-type="ref" reference="sec: distill_rede_training"} for training details.

The results, shown in Figure [4](#fig:distill_rede){reference-type="ref" reference="fig:distill_rede"}, indicate that DistillReDE can achieve significant improvements on Contriever, narrowing its performance gap with ReDE-RF (Default: $_\text{PRF}$) while removing the need for LLMs at inference time.

**ReDE-RF with DistillReDE** We next explore the possible advantages of leveraging DistillReDE as a drop-in replacement for Contriever in the ReDE-RF system. Table [\[tab:distill_rede_w_rede\]](#tab:distill_rede_w_rede){reference-type="ref" reference="tab:distill_rede_w_rede"} shows the results of this experiment. When fully leveraging DistillReDE as the initial retriever and ReDE-RF default (row 3), performance is very competitive compared to ReDE-RF when defaulting to $_\text{PRF}$ (row 2). When performing initial retrieval using a hybrid (BM25 + DistillReDE) initial retriever (row 4), performance improves compared to row 3 and increases slightly over row 2, while being significantly less costly (as we remove any need for hypothetical document generation). These results demonstrate that with a simple offline training scheme, ReDE-RF can match the performance of ReDE-RF (Default: $_\text{PRF}$) while fully removing the need for hypothetical document generation.

::: table*
  **Default**          **Init. Retrieval**   **Covid**   **NFCorpus**   **FiQA**   **Robust04**   **DBPedia**
  ------------------- --------------------- ----------- -------------- ---------- -------------- -------------
  Contriever                 Hybrid            65.6          34.8         28.2         49.8          37.0
  HyDE$_\text{PRF}$          Hybrid            65.6          35.5         29.3       **51.7**        37.6
  DistillReDE              DistillReDE         63.8          35.6         30.2         47.9          37.8
  DistillReDE              Hybrid$^*$        **66.3**      **35.8**     **30.9**       49.2        **38.4**
:::

# ReDE-RF vs. Pointwise Reranking

The $\textnormal{LLM}_{\textnormal{Rel-Judge}}$ component of ReDE-RF (discussed in [2.2.1](#sec:rel_feed){reference-type="ref" reference="sec:rel_feed"}) is similar to LLM-based pointwise re-rankers [@zhuang2024setwise]. In this subsection, we ask: What benefits do we achieve by feeding relevant documents to improve the query representation -- as described in [2.2.2](#sec:query_rep){reference-type="ref" reference="sec:query_rep"} -- versus simply re-ordering the initial retrieval based on the logits from $\textnormal{LLM}_{\textnormal{Rel-Judge}}$? To answer this, we focus on comparing pointwise re-ranking to ReDE-RF (Default: $_\text{PRF}$) in equal settings: Both systems have access to the top-20 passages from a hybrid (BM25 + Contriever) retriever and employ the same prompt as shown in Figure [2](#fig:prompt){reference-type="ref" reference="fig:prompt"}. Note, for the rest of this section we refer to ReDE-RF (Default: $_\text{PRF}$) as ReDE-RF.

In Table [\[tab:rede_v_reranking\]](#tab:rede_v_reranking){reference-type="ref" reference="tab:rede_v_reranking"} we present the results of this experiment across three backbone LLMs. Based on the results, we can make the following observations: (1) When comparing NDCG@10, ReDE-RF and pointwise re-ranking are generally on par -- outside of Llama-3.1-8B-I on DL19. (2) ReDE-RF consistently outperforms pointwise re-ranking in terms of NDCG@20 by large amounts. This demonstrates that ReDE-RF's improvements extend beyond the top-ranked results and is not confined to the initial retrieval. (3) Besides the evaluation on the TREC News dataset -- where it appears pointwise re-ranking is not well calibrated -- re-ranking and ReDE-RF are generally *complementary*. ReDE-RF + PR outperforms Hybrid + PR eight out of nine times (six out of six if excluding TREC News) and outperforms ReDE-RF five out of nine times (five out of six if excluding TREC News).

These results exemplify the difference in the roles of passage re-ranking and ReDE-RF. While passage re-ranking primarily enhances the ordering of the top-$k$ passages, ReDE-RF improves the overall quality of candidates from the first-stage retrieval.

# Related Work

**Query Expansion with LLMs** GAR [@mao-etal-2021-generation] was among the first methods to demonstrate the effectiveness of LLMs for query expansion by training an LLM to expand queries through the generation of relevant contexts, such as the target answer or answer sentence. Recent work has looked into leveraging LLMs to generate query expansions via zero or few-shot prompting. This has been explored in contexts where the LLM generates hypothetical documents that can be used to augment the query [@gao-etal-2023-precise; @wang-etal-2023-query2doc; @jagerman2023query; @lei-etal-2024-corpus; @mackie2023generative; @shen2024retrieval]. While some of these works generate hypothetical texts given *real* documents (e.g., @jagerman2023query [@lei-etal-2024-corpus; @shen2024retrieval]) they still rely on LLM-generated content to augment the query.

Other work has looked into improving the outputs of LLM query expansions through query re-ranking [@chuang-etal-2023-expand] or further training an LLM with preferences from the target retrieval systems [@yoon2024ask].

**Zero-Shot Dense Retrieval** With advancements in deep learning, IR systems moved away from representations based on exact-term matching to dense vector representations generated from transformer language models [@lin2022pretrained], such as BERT [@devlin-etal-2019-bert]. However, learning these representations typically requires large, labeled datasets. As such, researchers have looked into methods for learning dense representations without manually labeled data through techniques such as using synthetic data [@izacard2021unsupervised; @wang2023improving; @sachan2023questions; @lee2024gecko; @dai2022promptagator], addressing architecture limitations to improve zero-shot decoder-LLM embeddings [@springer2024repetition], or leveraging LLMs to generate outputs that can be used to improve representations for zero-shot dense retrieval [@gao-etal-2023-precise; @zhuang2024promptreps].

# Conclusion

We introduce ReDE-RF, a zero-shot dense retrieval method that addresses key challenges associated with approaches that rely entirely on hypothetical document generation. Through extensive experiments, we show that ReDE-RF improves upon state-of-the-art zero-shot dense retrieval approaches in low-resource domains, while also lowering latency compared to techniques that rely only on hypothetical document generation. Further analysis shows that ReDE-RF can be easily distilled to a smaller, more efficient unsupervised dense retriever, DistillReDE, removing any reliance on LLMs at inference time. In summary, ReDE-RF presents an approach that achieves the benefits of casting zero-shot dense retrieval as a document similarity task while being more efficient and domain-agnostic.

# Limitations {#limitations .unnumbered}

A limitation of ReDE-RF is its reliance on retrieved results from first-stage retrievers. If an initial retriever provides a poor set of results, performance gains will not be as apparent as no relevant documents can be used to update the query embedding. This in turn makes ReDE-RF equivalent to Contriever or HyDE, depending on what default the user leverages. How to make ReDE-RF less reliant on retrieved results from unsupervised first-stage retrievers is a question worth exploring in future work. Another simple improvement could be leveraging a rules-based approach that keeps assessing retrieved documents if none of the top results are deemed relevant. We do note that this would likely increase latency.

Another limitation is that while ReDE-RF seeks to minimize reliance on LLM-generated outputs, it does still depend on an LLM to be accurate in its relevance feedback. If the LLM provides inaccurate relevance assessments during the relevance feedback stage, it can further harm the query representation. Lastly, while ReDE-RF improves latency compared to previous approaches based on hypothetical document generation, the latency is still slower than approaches that do not rely on LLMs at inference time. However, we demonstrated the potential for doing offline training as a way to mitigate this. Additionally, as LLMs advance, we may see improvements in the efficiency of LLM inference. Techniques such as flash-attention [@dao2022flashattention], can also significantly decrease the inference time of LLMs.

# Ethics Statement {#ethics-statement .unnumbered}

While the ultimate goal of our work is to minimize reliance on LLM generated output, we do recognize that our system does still rely on LLMs, which means that there is a risk that the LLM can produce biased, harmful, or offensive output. To mitigate this, we limit our LLM to only generate one token, which we hope can eliminate this risk. Additionally, our dense retrieval system is based on pre-trained language models which can potentially produce retrieval results that contain human biases.

Our research solely uses publicly available datasets, and no personal information is collected. All datasets and models are used in accordance with its intended use and licenses. Our method is designed to improve the performance of information retrieval systems in settings in which there exists no data and proprietary LLMs may not be available. We hope this can enable the deployment of our system in similar settings.

# Acknowledgements {#acknowledgements .unnumbered}

We sincerely thank Charlie Dagli, John Holodnak, and Daniel Gwon for their discussion and help in this project. Research was sponsored by the Department of the Air Force Artificial Intelligence Accelerator and was accomplished under Cooperative Agreement Number FA8750-19-2-1000. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Department of the Air Force or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein.

::: table*
:::

::: table*
:::

# Dataset Details

We show the number of test queries for each dataset used to evaluate ReDE-RF in Table [2](#tab:dataset_details){reference-type="ref" reference="tab:dataset_details"}.

::: {#tab:dataset_details}
  Dataset       \# Queries
  ------------ ------------
  TREC DL19         43
  TREC DL20         54
  TREC-News         57
  TREC-Covid        50
  FiQA             648
  SciFact          300
  DBPedia          400
  NFCorpus         323
  Robust04         249

  : Dataset Details
:::

The above datasets have the following licenses.

- TREC DL19 and DL20 are under "MIT License" for non-commercial research purposes.

- TREC News and Robust04 are under Copyright.

- DBPedia is under CC BY-SA 3.0 license.

- SciFact is under CC BY-NC 2.0 license.

- TREC Covid is provided under Dataset License Agreement

- NFCorpus and FiQA do not report the dataset license as per @thakur2021beir.

The BEIR dataset is under the Apache 2.0 License.

# Model Details

- $\texttt{Mistral-7B-Instruct-v.02}$: A 7B parameter model that is instruction fine-tuned. Huggingface ID: $\texttt{mistralai/Mistral-7B-Instruct-v0.2}$

- $\texttt{contriever}$: Based on $\texttt{bert-base-uncased}$ which has 110M parameters. HuggingFace ID: $\texttt{facebook/contriever}$.

- $\texttt{Mixtral-8x7B-Instruct-v0.1}$: A 8x7B parameter model (47B total parameters) that is instruction fine-tuned. Huggingface ID: $\texttt{mistralai/Mixtral-8x7B-Instruct-v0.1}$

- $\texttt{gemma-2-2b-it}$: A 2B parameter model that is instruction fine-tuned. Huggingface ID: $\texttt{google/gemma-2-2b-it}$

- $\texttt{gemma-2-9b-it}$: A 9B parameter model that is instruction fine-tuned. Huggingface ID: $\texttt{google/gemma-2-9b-it}$

- $\texttt{Llama-3.2-3B-Instruct}$: A 3B parameter model that is instruction fine-tuned. Huggingface ID: $\texttt{meta-llama/Llama-3.2-3B-Instruct}$

- $\texttt{Llama-3.1-8B-Instruct}$: A 8B parameter model that is instruction fine-tuned. Huggingface ID: $\texttt{meta-llama/Llama-3.1-8B-Instruct}$

The above models have the following licenses.

- $\texttt{Mistral-7B-Instruct-v0.2}$ is under the Apache 2.0 License.

- $\texttt{Mixtral-8x7B-Instruct-v0.1}$ is under the Apache 2.0 License.

- $\texttt{contriever}$ is under CC BY-NC 4.0 license.

- $\texttt{gemma-2-2b-it}$ is under the Apache 2.0 License.

- $\texttt{gemma-2-9b-it}$ is under the Apache 2.0 License.

- $\texttt{Llama-3.2-3B-Instruct}$ is under the Llama 2 Community License Agreement

- $\texttt{Llama-3.1-8B-Instruct}$ is under the Llama 2 Community License Agreement

We also leverage Pyserini [@lin2021pyserini] which is under the Apache 2.0 License.

# Results Across Multiple Runs {#sec:standard_deviations}

Due to the randomness of sampling hypothetical documents from an LLM, we run HyDE, $_\text{PRF}$, and ReDE-RF (Default: $_\text{PRF}$) three times. In Table [\[tab:standard_deviation_across_runs\]](#tab:standard_deviation_across_runs){reference-type="ref" reference="tab:standard_deviation_across_runs"} we report the mean and standard deviation of NDCG@10 across the runs for all TREC and BEIR datasets.

# HyDE and $_\text{PRF}$ Implementation {#sec:hyde_implementation}

## Generation Details

To re-implement HyDE and $_\text{PRF}$ with Mistral-7B-Instruct we follow the same parameters that were mentioned in the original paper [@gao-etal-2023-precise] and in the provided [codebase](https://github.com/texttron/hyde/blob/main/src/hyde/generator.py). In particular, we sample eight hypothetical documents from Mistral-7B-Instruct with temperature of 0.7 and allow up to 512 maximum new generation tokens per hypothetical document.

## $_\text{PRF}$ with less in-context documents {#sec:hyde_prf_less_docs}

We explore how less initially retrieved results impact performance of $_\text{PRF}$. For results, see Table [\[tab:hyde_prf_diff_init_retrieval\]](#tab:hyde_prf_diff_init_retrieval){reference-type="ref" reference="tab:hyde_prf_diff_init_retrieval"}.

## Prompts

For HyDE [@gao-etal-2023-precise], we leverage the same prompts from the original implementation. For $_\text{PRF}$, we follow the format of the HyDE prompts and provide context following the format in Q2D/PRF from @jagerman2023query.

### HyDE Prompts

**TREC DL19 and DL20**

::: tcolorbox
Please write a passage to answer the question.\
Question: {}\
Passage:
:::

**SciFact**

::: tcolorbox
Please write a scientific paper passage to support/refute the claim.\
Claim: {}\
Passage:
:::

**TREC Covid and NFCorpus**

::: tcolorbox
Please write a scientific paper passage to answer the question.\
Question: {}\
Passage:
:::

**FIQA**

::: tcolorbox
Please write a financial article passage to answer the question.\
Question: {}\
Passage:
:::

**DBPedia**

::: tcolorbox
Please write a passage to answer the question.\
Question: {}\
Passage:
:::

**TREC News and Robust04**

::: tcolorbox
Please write a news passage about the topic.\
Topic: {}\
Passage:
:::

### $_\text{PRF}$ Prompts

**TREC DL19 and DL20**

::: tcolorbox
Please write a passage to answer the question based on the context:

Context:\
{}\
Question: {}\
Passage:
:::

**SciFact**

::: tcolorbox
Please write a scientific paper passage to support/refute the claim based on the context:

Context:\
{}\
Claim: {}\
Passage:
:::

**TREC Covid and NFCorpus**

::: tcolorbox
Please write a scientific paper passage to answer the question based on the context:

Context:\
{}\
Question: {}\
Passage:
:::

**FIQA**

::: tcolorbox
Please write a financial article passage to answer the question based on the context:

Context:\
{}\
Question: {}\
Passage:
:::

**DBPedia**

::: tcolorbox
Please write a passage to answer the question based on the context:

Context:\
{}\
Question: {}\
Passage:
:::

**TREC News and Robust04**

::: tcolorbox
Please write a news passage about the topic based on the context:

Context:\
{}\
Topic: {}\
Passage:
:::

# Hybrid Retrieval Implementation Details {#sec: hybrid_retrieval_desc}

We use the hybrid retrieval implementation from Pyserini, which scores the document by a weighted average of the sparse retrieval and dense retrieval scores. We implement using the default parameters. See [here](https://github.com/castorini/pyserini/blob/master/pyserini/search/hybrid/_searcher.py) for more details.

# DistillReDE Training Details {#sec: distill_rede_training}

To train DistillReDE, we first need synthetic queries given our corpus. We leverage the filtered set of 10K synthetic queries provided by @jeronymo2023inpars which were generated using GPT-J [@wang2021gpt]. Then, we run ReDE-RF on these synthetic queries and store the ReDE-RF embedding for each query. If ReDE-RF finds no relevant documents for a given query, we remove that query from our training set.

To train DistillReDE, following @pimpalkhute2024softqe, we optimize the following objective:

$$\begin{equation}
\mathcal{L}_{\text{DistillReDE}} = 0.5 \mathcal{L}_{\text{MSE}}(\hat{v}_{q_{\textnormal{ReDE}}}, f(q)) + 
    0.5\mathcal{L}_{\text{Cont}}
\end{equation}$$

where $\mathcal{L}_{\text{Cont}}$ is a contrastive objective [@karpukhin-etal-2020-dense]:

$$\begin{eqnarray}
&& \scalebox{1}{$
    L(q_i, \hat{v}_{q_{\textnormal{ReDE}, {i}}}^+,
    \hat{v}_{q_{\textnormal{ReDE}, {i,1}}}^-, \dots, 
    \hat{v}_{q_{\textnormal{ReDE}, {i,n}}}^-)
    $} \\
&=& \scalebox{1}{$
    -\log \frac{ e^{\mathrm{sim}(q_i, \hat{v}_{q_{\textnormal{ReDE}, {i}}}^+)} }
    { e^{\mathrm{sim}(q_i,  \hat{v}_{q_{\textnormal{ReDE}, {i}}}^+)} 
    + \sum\limits_{j=1}^n{e^{\mathrm{sim}(q_i, 
    \hat{v}_{q_{\textnormal{ReDE}, {i,j}}}^-)}} }
    $} \nonumber
\end{eqnarray}$$

where given a query, $q$, we have one positive ReDE-RF embedding $\hat{v}_{q_{\textnormal{ReDE}, {i}}}^+$ and $n$ negative ReDE-RF embeddings $(\hat{v}_{q_{\textnormal{ReDE}, {i,1}}}^-, \cdots, \hat{v}_{q_{\textnormal{ReDE}, {i,n}}}^-)$.

We train with in-batch negatives using a training batch size of 256 and use the Adam optimizer [@diederik2014adam] with a learning rate of 5e-5.

# ReDE-RF Prompt Varations {#sec:rede_prompt_variations}

Below are the prompts studied in the prompt variations subsection of Section [\[sec:Initial_Retrieval_Abalation\]](#sec:Initial_Retrieval_Abalation){reference-type="ref" reference="sec:Initial_Retrieval_Abalation"}.

**pointwise.yes_no [@zhuang2024setwise]**

::: tcolorbox
Passage: {}\
Query: {}\
Does the passage answer the query? Answer 'Yes' or 'No'.
:::

**RG-YN [@zhuang2024beyond]**

::: tcolorbox
For the following query and document, judge whether they are relevant. Output "Yes" or "No".\
\
Query: {}\
Document: {}\
Output:
:::

**RG-YN$*$**

::: tcolorbox
For the following query and document, judge whether they are relevant. Output "Yes" if the passage is dedicated to the query and contains the exact answer and output \"No\" if the passage has nothing to do with the query.\
\
Query: {}\
Document: {}\
Output:
:::

**@thomas2024large**

::: tcolorbox
You are a search quality rater evaluating the relevance of web pages. Given a query and a web page, you must provide a score on an integer scale of 0 to 1 with the following meanings:\
\
1 = highly relevant, very helpful for this query\
0 = not relevant, should never be shown for this query\
\
Assume that you are writing a report on the subject of the topic. If the web page is primarily about the topic, or contains vital information about the topic, mark it 1. Otherwise, mark it 0.\
Passage: {}\
Query: {}\
Score:
:::

[^1]: In this paper, zero-shot and unsupervised are used without distinction.

[^2]: We note that this shares similarity with LameR [@shen2024retrieval], but LameR is built on BM25 and has slight implementation differences.

[^3]: We provide implementation details in Appendix [13](#sec: hybrid_retrieval_desc){reference-type="ref" reference="sec: hybrid_retrieval_desc"}

[^4]: NDCG@10 results can be found in Appendix [12.2](#sec:hyde_prf_less_docs){reference-type="ref" reference="sec:hyde_prf_less_docs"}
