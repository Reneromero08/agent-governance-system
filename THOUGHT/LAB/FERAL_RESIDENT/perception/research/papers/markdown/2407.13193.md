**Reference Format:**\
. . [^1]

# Introduction

Large language models (LLMs) [@llama; @gemma; @mistral; @gpt4; @23iclr-glm] have achieved significant advancements in recent years and have become the cornerstone of various applications in the field of natural language processing (NLP). These LLMs are typically pre-trained on a large amount of natural language corpus and then fine-tuned on the specific downstream tasks' datasets. Recent works [@19emnlp-lm-as-kb; @22arxiv-lmkb-survey; @22nips-rome; @24arxiv-lmkb-scale] demonstrate the success of LLMs can be explained by the fact that LLMs act as knowledge bases, which refers to implicitly storing the learned knowledge in the parameters as internal memory and generating responses by retrieving answers from memory. To store more knowledge for better generation performance, existing works generally enlarge the memory capacity by increasing the volume of parameters [@22iclr-scaling-law; @gpt3; @20arxiv-scaling-law; @22arxiv-scaling-law].

Although existing LLMs have shown great power, several challenges still hinder the development of LLMs. One of the most prominent challenges is the hallucination problem [@23cs-hallucination-survey; @23acl-hallucination-mt; @23acl-hallucination-dialogue], which refers to the tendency of LLMs to generate responses that are coherent and fluent but factually incorrect. Another big challenge is the knowledge update issue. To update the knowledge stored in the LLMs' internal memory [@22nips-rome; @23arxiv-editing-survey; @24arxiv-editing-survey], it is necessary to retrain/fine-tune LLMs with new data, which is a costly process. Another challenge for general LLMs is lacking domain-specific expertise [@23emnlp-huatuogpt; @23nature-med-palm; @23arxiv-med-palm2; @24arxiv-saulm]. Training a domain-specific LLM demands considerable manpower for dataset collection.

To address these challenges, recent works [@20neurips-rag; @22icml-retro; @20icml-realm] have proposed leveraging an external knowledge database to augment LLMs, known as Retrieval-Augmented Generation (RAG). By supplying LLMs with retrieved relevant factual information, the hallucination problem can be alleviated to some extent. Besides, the knowledge update issue can also be addressed by updating the external knowledge database, which can augment LLMs with up-to-date knowledge. RAG can also convert a general LLM into a domain-specific LLM by constructing and utilizing a domain-specific knowledge database. Therefore, RAG plays an important role in augmenting the functionality of LLMs, making them more accurate, knowledgeable, and reliable in a wide range of applications.

<figure id="fig:overview" data-latex-placement="t">
<embed src="figures/fig1-overview.pdf" style="width:80.0%" />
<figcaption>The overview of retrieval-augmented generation for natural language processing. The inputs as queries are fed into both the retriever for retrieval knowledge and the generator for outputs. There are three kinds of retrieval fusions, including query-based fusion, logits-based fusion, and latent fusion.</figcaption>
</figure>

**Contributions:** This paper reviews all techniques involved in RAG for natural language processing. Although there are several survey papers for RAG [@22arxiv-rag-survey; @23arxiv-survey; @24arxiv-survey-ecust; @24arxiv-survey-pku; @24arxiv-rag-benchmark], our survey still has some key insights,

1.  This paper systematically introduces each component of RAG, including details about the retriever from building to querying and techniques of the retrieval fusions with tutorial codes.

2.  This paper exhibits different RAG training strategies, including RAG with/without datastore update.

3.  This paper further discusses RAG evaluation and benchmarking, as well as the applications of RAG on downstream NLP tasks and practical NLP scenarios.

4.  This paper finally identifies promising future directions for exploring and the main challenges for addressing.

The remainder of this paper is organized as follows. Section [2](#sec:para){reference-type="ref" reference="sec:para"} gives an overview of RAG. Section [3](#sec:retriever){reference-type="ref" reference="sec:retriever"} and Section [4](#sec:fusion){reference-type="ref" reference="sec:fusion"} comprehensively introduce all technical details used in retrievers and retrieval fusions. Section [6](#sec:training){reference-type="ref" reference="sec:training"} presents how to train the RAG with/without new knowledge. Section [8](#sec:task){reference-type="ref" reference="sec:task"} presents the techniques used in representative NLP tasks. Section [9](#sec:app){reference-type="ref" reference="sec:app"} shows the applications of RAG in practical NLP scenarios. Section [10](#sec:future){reference-type="ref" reference="sec:future"} discusses the future directions of RAG. Section [11](#sec:con){reference-type="ref" reference="sec:con"} makes a final conclusion of this paper.

# Overview of Retrieval-Augmented Generation {#sec:para}

This section gives an overview of Retrieval-Augmented Generation (RAG) for NLP. An RAG system utilizes the external knowledge base $\boldsymbol D$ to enhance the generation system. Taking external documents as an example, $\boldsymbol D$ consists of external documents, each of which contains a set of chunks $c_i \in \boldsymbol{C_i}$. These chunks are transformed into vector embedding using an embedding model. When inputting a query $q$, which is embedded as a vector, then the retriever in the RAG system retrieves top-$k$ chunks $R_q=\{r_1, r_2, \dots, r_k\}$ most relevant to the query $q$. The RAG system can use different fusion methods to fuse the retrieved chunks, which is discussed in Section [4](#sec:fusion){reference-type="ref" reference="sec:fusion"}. The overall process is formulated as follows: $$\begin{equation}
    \texttt{Retriever}(q, D) \rightarrow R_q
\end{equation}$$ $$\begin{equation}
     \texttt{Generator} (\texttt{Retrieval Fusion} (q, R_q)) \rightarrow answer
\end{equation}$$ As shown in Figure [1](#fig:overview){reference-type="ref" reference="fig:overview"}, RAG typically consists of three modules, the retriever, the generator, and retrieval fusions.

**Retriever** module usually comprises three components: an encoder for encoding inputs into embeddings, an efficient indexing that supports approximate nearest neighbor search, and a datastore for storing external knowledge in the form of key-value pairs. The main challenge in the retriever module is *finding the optimal trade-off between retrieval efficiency and retrieval quality*. The retrieval efficiency refers to how fast the relevant information can be obtained, which involves accelerating encoding, efficient indexing, batch querying in the datastore, etc. The retrieval quality refers to how relevant the information can be retrieved, which involves chunk representation learning, advanced approximate nearest neighbor search algorithms, etc.

**Retrieval Fusions** aims to leverage the retrieved information to augment the generation. These fusion techniques can be categorized into three major types: query-based fusion, latent fusion, and logits-based fusion. The query-based fusion augments inputs with retrievals before feeding them into the generators. The logits-based fusion focuses on the output logits of generators and fuses the retrievals logits for more robust logits. The latent fusion refers to introducing retrieval representations into the latent representations of generators, thus implicitly improving the models' performance.

**Generator** modules can be classified into two branches of generators: default generators and retrieval-augmented (RA) generators. The default generators include most pre-trained/fine-tuned large language models, such as GPT-series models [@gpt1; @gpt2; @gpt3; @gpt4], Mistral models [@mistral], and Gemini-series models [@gemini-1; @gemma; @gemini-1.5]. The RA generators refer to the pre-trained/fine-tuned generators that consist of modules for fusing retrievals, such as RETRO [@22icml-retro; @24icml-instructretro] and Enc-Dec [@22neurips-encoder-decoder]. Those generators generate responses or make predictions.

The workflow of RAG involves three steps: 1) retrieving the relevant information from external databases based on given inputs; 2) fusing the retrieved information with inputs or intermediate states based on the fusion techniques; 3) making predictions by generators based on the input and corresponding retrievals.

# Retriever {#sec:retriever}

<figure id="fig:retriever" data-latex-placement="t">
<embed src="figures/fig2-retriever.pdf" style="width:100.0%" />
<figcaption>Two stages of using the retriever.</figcaption>
</figure>

Figure [2](#fig:retriever){reference-type="ref" reference="fig:retriever"} shows the two stages for using the retriever, which involves first building the retriever and then querying the retriever. The following sections will introduce details about each stage.

## Building the Retriever

This section will explain how to build a retriever using a large natural language corpus. As shown in Figure [2](#fig:retriever){reference-type="ref" reference="fig:retriever"} (a), the process involves three steps: chunking corpus, encoding chunks, and building the vector database. Specifically, building the vector database includes building the ANN index and storing the data with key-value pairs.

### Chunking Corpus

Chunking techniques generally refer to dividing large documents into small text chunks [@16acl-sentence-chunking; @17acl-chunk; @20acl-rec-chunk; @22icml-retro; @22arxiv-sedr], which is an indispensable key step in the process of building the retriever. The intuitions behind chunking techniques are, (1) The texts or embeddings used for the indexing should be semantically independent, containing one core idea for models to encode. Short texts are more likely to be ambiguous, for example, the word "apple" can refer to a fruit or a company. (2) Encoding a long sequence document would result in considerable resource overheads when using existing transformer-based models, while processing shorter text chunks can significantly accelerate the encoding process and save memory costs. Therefore, the main challenge of the chunking techniques is to find the best chunking size to make a better trade-off between text semantics and encoding efficiency.

To solve the above challenge, three key points need to be considered when determining the chunking size:

::: compactenum
**Task's preference.** Different tasks may benefit from different kinds of retrieval chunks. For example, question-answer tasks may prefer short phrases, while summarization tasks may prefer long documents.

**Encoder's preference.** Different encoder models have varying encoding capabilities on texts with different lengths. For example, models in the sentence-transformer [@sen-trans] behave better on a single sentence, while the text-embedding-ada-002 [@text-emb-ada] is good at longer texts.

**Query's preference.** The length of the user's queries should be aligned with the chunking size, which implicitly aligns the amount of contextual information in chunks with that in queries, thus improving the relevance between queries and retrievals. For example, a retrieval database built on short phrases may be useless for queries with long documents.
:::

Overall, there is no golden rule for determining the chunking size, and it depends on the specific RAG scenarios.

There are basically three types of chunking techniques, including **the chunking with fixed length**, **the semantic chunking**, and **the content-based chunking**. Chunking with fixed length is the simplest way to split documents sequentially using a length hyperparameter. The semantic chunking cuts documents based on semantics, such as the period character or the newline character that represents the end of the sentence. Existing state-of-the-art natural language processing toolkits, such as NLTK [@nltk] and spaCy [@spacy], have provided convenient sentence-cutting methods. The content-based chunking segments documents according to the unique structural characteristics. For example, electronic medical records can be easily segmented based on the sections, or programming codes can be segmented based on function blocks.

### Encoding Chunks {#sec:encoding}

Encoding refers to numericalizing textual chunks as vector representations (embeddings). These embeddings generally capture the semantics of the chunks, enabling the retriever to perform similarity searches based on content relevance rather than just keyword matching.

According to the sparsity of the embeddings, there are two kinds of encoding methods, i.e., **sparse encoding** and **dense encoding**. The sparse encoding represents text by creating high-dimensional vectors where most elements are zero. The basic sparse encoding is one-hot encoding [@one-hot], which represents a word with a high-dimensional vector as large as the vocabulary table size but only marks the value corresponding to the presence of the word as one. The embeddings produced by such encodings are called the one-hot vector. Other common sparse encodings include:

::: compactenum
**Bag of Words (BoW)** [@bow]. This encoding improves one-hot encoding by replacing the zero-one counting with the frequency counting. However, BoW ignores the syntax and word order in the documents and focuses on statistical information, thus only expressing limited semantics.

**Term Frequency-Inverse Document Frequency (TF-IDF)** [@tf-idf]. This encoding not only counts the occurrence (frequency) of each word but also adjusts these counts based on how common the word is across all documents (inverse document frequency). TF-IDF helps emphasize words that are more descriptive of the document's content.

**BM25** [@bm25] is a probabilistic ranking algorithm used in information retrieval to estimate the relevance of documents to a search query by balancing term frequency, inverse document frequency, and document length normalization, ensuring robust scoring even for long or short documents. BM25 focuses on lexical matches and is computationally efficient, making it a cornerstone of traditional search engines.
:::

Sparse encoding is an efficient way to encode textual chunks. However, such encoding methods may not capture deeper semantic meanings well.

The dense encoding generates vectors where each dimension can capture a range of semantic features, and most elements are non-zero floating points. The dense embeddings are generally produced by (deep) neural network models,

::: compactenum
**BERT [@19naacl-bert] and Variants.** Bidirectional Encoder Representation from Transformers (BERT) is a typical pre-trained transformer model, generating dense semantic embeddings that capture the contextual information. Other BERT variants, such as RoBERTa [@19arxiv-roberta], DistilBERT [@19arxiv-distilbert], and ELECTRA [@20iclr-electra], further improve the semantic representations with advanced learning techniques.

**Siamese Encoders.** This is a type of neural network designed to learn the similarity between inputs, which is usually trained with contrastive learning. Existing state-of-the-art siamese encoders are DPR [@20emnlp-dpr], SimCSE [@21emnlp-simcse], Contriever [@22_tmlr_contriever].

**LLM-based Encoders.** This type of encoder benefits from the powerful representation capability of LLMs. LLMs, which contain billions of parameters and are pre-trained on vast amounts of data covering a wide range of topics, have advanced semantic language understanding capabilities. Typical LLM-based encoders are text-embedding-ada-002 [@text-emb-ada], bge-embedding [@bge-embedding], mxbai-embedding [@mxbai-embedding], MedCPT [@23bib_medcpt].
:::

Compared to sparse encoding, dense encoding leverages deep neural networks, especially transformers [@17nips-attention], to capture broader linguistic and semantic information. Dense encodings are widely used in most representation scenarios. Moreover, some works also utilized hybrid methods to encode text for leveraging both lexical and semantic information [@22naacl_drboost; @23acl_elife].

### Building the Index {#building index}

Indexing in the vector database aims to accelerate the search process for data similar to high-dimensional query embedding. Unlike common indexing in databases, indexing in the vector database mainly focuses on supporting efficient approximate nearest neighbor (ANN) search [@21tbd-faiss; @24arxiv-faiss; @20icml-scann] rather than transaction operations like insertion, deletion, and update. The key challenge of indexing is making a good trade-off between search quality and search efficiency. To solve the challenge, there are various specific optimizations in both algorithmic aspects and systematic aspects to be explored, including choices of similarity metrics, dimension reduction (DR) on embeddings, advanced ANN indexing, system-level optimizations, hardware-aware optimization, and so on. Due to the page limits, this section discusses the optimizations that significantly affect the search quality and efficiency.

**Choice of Similarity Metrics.** The similarity metrics are the basic components in the retriever, which measures the degree of relevance between query embeddings and chunk embeddings. The similarity metrics would affect the search quality. Typical similarity metrics include cosine similarity, Euclidean similarity, and Manhattan distance.

**Dimension Reduction on Embeddings.** Reducing the dimensionality of embeddings can improve search efficiency but at the risk of harming the semantic representations. The basic but effective dimension reduction (DR) is the principal component analysis (PCA). The PCA is a simple statistical technique that transforms the original data into a new coordinate system while retaining the most important features. Another popular and advanced dimension reduction is locality-sensitive hashing (LSH). LSH significantly reduces the dimensionality by mapping the data into buckets but preserves the similarity of the original input data. The intuition behind LSH is that the nearest neighbors will be mapped into the same buckets. Unlike LSH, product quantization (PQ) [@11tpami-pq] is another popular and effective DR technique for ANN search. The core idea of the PQ is to divide the high-dimensional space into smaller, independently quantized subspaces. Each subspace creates a codebook of different quantized integers to form the representative and compact vectors. The above techniques enable efficient storage and fast approximate search but may lose semantic information. Recent work [@23emnlp-autocompressor] proposed a new technique named AutoCompressor that reduces the dimension of embeddings by compressing the original context into semantically shorter embeddings.

**Advanced ANN Indexing.** ANN Indexing generally refers to the methods or structures used to organize and manage data so that the approximate-nearest-neighbor search process is optimized for retrieval quality and retrieval efficiency. This paper will introduce several advanced ANN indexing techniques.

1.  **The InVerted File system with Product Quantization (IVFPQ)** [@24arxiv-faiss] is a simple but effective indexing framework that combines two powerful techniques to enable an efficient and scalable ANN search process. The main idea of IVFPQ is first to cluster the data for coarse-grained partition and then to compress the data within each cluster into sub-vectors for fine-grained quantization. The coarse-grained clustering (the IVF component) significantly reduces the search space, while the fine-grained quantization (the PQ component) ensures a high retrieval performance.

2.  **The Hierarchical Navigable Small World (HNSW)** [@hnsw] uses a hierarchical graph structure to perform ANN search in high-dimensional spaces efficiently. Specifically, HNSW treats high-dimensional vectors as nodes and connects them with their nearest neighbors. The multi-layer graph structure is determined probabilistically to ensure fewer nodes at higher layers for efficient search.

3.  **Tree-based Indexing** aims to organize high-dimensional vectors in tree-liked structures, such as KD-Trees [@19kdd-kd-tree], Ball Trees [@23icde-ball-tree] and VP-Trees [@15prl-vp-tree]. Typical tree-based indexing is Approximate Nearest Neighbors Oh Yeah (Annoy) [@annoy], which uses a forest of trees built based on random projections to separate the vector space into multiple hyperplanes for efficient ANN search.

### Building the Datastore with Key-Value Pairs

The datastore used in the vector database is a specialized database that stores and manages data as a collection of key-value pairs, where keys are the unique identifiers of high-dimensional embeddings and values are the domain-specific knowledge. Since the amount of the data stored in the datastore may be quite large, the storage engine, such as LMDB [@lmdb] or RocksDB [@rocksdb], should be capable of efficient retrieval and data persistence. The key point in the datastore for ANN search is what should be used to be stored as values. For example, for question-answer tasks, when adding retrievals to prompts, the naive but effective way is to store the question embedding as the key and question-answer pairs as the value. This can help the generation process as retrievals are used as demonstrations for models. Recent works have proposed various state-of-the-art vector databases including the indexing and the datastore, such as Milvus [@21sigmod-milvus; @22vldb-manu], FAISS [@24arxiv-faiss; @21tbd-faiss], LlamaIndex [@22github-llamaindex], etc.

:::: algorithm
::: algorithmic
A natural language corpus $D=\{d_1,\ldots, d_n\}$ for building the knowledge database, an encoder $\mathcal{E}$ for encoding chunks.\
The index $\mathcal{I}$ and the key-value store $\mathcal{S}$. $\mathcal{K}=\{\}, \mathcal{V}=\{\}$; $c^1_i, \ldots, c^m_i = Chunk(d_i)$; $e^j_i = \mathcal{E}(c^j_i)$; Add $e^j_i$ into $\mathcal{K}$ and $c^j_i+c^{j+1}_i$ into $\mathcal{V}$; The $\mathcal{K}$ and $\mathcal{V}$ persist in the storage (e.g., SSD) if necessary; Build the index $\mathcal{I}$ with $\mathcal{K}$; Store $\mathcal{K}$ and $\mathcal{V}$ into the key-value store $\mathcal{S}$; $\mathcal{I}$ and $\mathcal{S}$;
:::
::::

### Code Demonstrations

Algorithm [\[alg:build\]](#alg:build){reference-type="ref" reference="alg:build"} shows detailed steps to build the retriever. Lines 2-8 present the chunking and the encoding process for a natural language corpus containing multiple documents. In line 6, algorithm [\[alg:build\]](#alg:build){reference-type="ref" reference="alg:build"} takes the concatenation of the current chunk and the next chunk as the value. Notably, the choice of value can vary for different tasks. Another practical issue is that the memory cost of all keys and values may exceed the memory capacity of the server in the practical scenario. Thus, it is recommended that the keys and values persist in the storage if necessary.

## Querying the Retriever

This section will explain how to query the pre-built retriever, which basically includes three steps as shown in Figure [2](#fig:retriever){reference-type="ref" reference="fig:retriever"}(b): encoding queries, ANN search, and post-processing.

### Encoding Queries and ANN Search

To align with the pre-built embedding space, the retriever uses the same encoder to encode queries during the querying stage. The ANN search leverages the pre-built indexing and datastore to find similar data via ANN searching algorithms and then retrieves the corresponding values.

**Searching the index** refers to searching the pre-built index, finding the top-k nearest neighbors, and returning the unique identifiers of k nearest neighbors. The nearest neighbor search process depends on indexing algorithms or structures. Taking IVFPQ as an example, the search process first compares the query embedding with cluster embeddings and selects several candidate clusters for further search. Then, within each cluster, the search process performs the same product quantization on the query embedding and finds the top-k nearest neighbors based on the distance. Finally, the search process merges all nearest neighbor candidates and re-orders all candidates for the final top-k nearest neighbors.

**Retrieving values from datastore** fetches the corresponding values based on the nearest key identifiers.

:::: algorithm
::: algorithmic
A query input $q$, an encoder $\mathcal{E}$ for encoding chunks, the index $\mathcal{I}$, the key-value store $\mathcal{S}$, the parameter $k$.\
Top-$k$ nearest neighbor knowledge. $e=\mathcal{E}(q)$; $\{idx_1, \ldots, idx_k\} = \mathcal{I}.Search(e, k)$; $\{v_1, \ldots, v_k\}=\mathcal{S}.Fetch(\{idx_1, \ldots, idx_k\})$; $\{v_{j_1}, \ldots, v_{j_k}\}=PostProcess(\{v_1, \ldots, v_k\})$ $\{v_{j_1}, \ldots, v_{j_k}\}$;
:::
::::

### Post-Processing

The post-processing involves a set of techniques after the initial retrieval step. These techniques aim to refine, enhance, or adapt the retrievals based on the specific task objectives. This section will list some typical post-processing techniques.

**Reranking** aims to reorder the retrieved knowledge based on task-specific objectives. The intuition is that the knowledge is retrieved based on task-agnostic metrics, such as Euclidean distance. Existing reranking methods [@23acl-ear; @20acl-retrieve-edit-rerank; @22arxiv-ialm; @23arxiv-refreshllms] mostly design different architectures or strategies to reorder the retrieved knowledge.

### Code Demonstrations

After building the retriever, this section demonstrates the detailed steps of querying the retriever to obtain the top-$k$ nearest neighbor knowledge in algorithm [\[alg:query\]](#alg:query){reference-type="ref" reference="alg:query"}, including encoding the query (line 1), performing the approximate nearest neighbor search (line 2), and fetching the knowledge for fusion (line 3). These three steps depend on the specific APIs of encoders, indexing, and datastore. After obtaining the top-$k$ retrievals, optimizations for post-processing are applied (line 4).

# Retrieval Fusions {#sec:fusion}

<figure id="fig:refusion_med" data-latex-placement="t">

<figcaption>The categories of fusion methods in RAG.</figcaption>
</figure>

Retrieval fusions refer to how to leverage the retrieved knowledge to improve generators' performance. Basically, there are three types of retrieval fusions: query-based fusions, logits-based fusions, and latent fusions. Figure [3](#fig:refusion_med){reference-type="ref" reference="fig:refusion_med"} shows the detailed categorization of fusions and representative works of each retrieval fusion in RAG.

:::: algorithm
::: algorithmic
A query input $q$, top-$k$ nearest neighbor knowledge $\{v_1, \ldots, v_k\}$, an encoder $\mathcal{E}_f$ and a decoder $\mathcal{D}_f$ for feature concatenation, the generator $\mathcal{G}$ for text concatenation.\
Generated response $y$. $x=v_1 \oplus \ldots \oplus v_k \oplus q$; $y=\mathcal{G}(x)$; $e_q=\mathcal{E}_f(q), e_{v_j}=\mathcal{E}_f(v_j), j \in \{1, \ldots, k\}$; $e_x=e_q \oplus e_{v_1} \oplus \ldots \oplus e_{v_k}$; $y=\mathcal{D}_f(e_x)$ $y$;
:::
::::

## Query-based Fusion

The simplest and most direct fusion technique is query-based fusion, which integrates the retrieved information with input queries to generate responses. The query-based fusion can be further categorized into two sub-classes according to the type of concatenated information, i.e., text concatenation and feature concatenation.

Text concatenation involves performing query-based fusion with raw texts, making it particularly suitable for contemporary LLMs like GPT-4. These models function as black-box systems with limited interaction capabilities, typically offering only API access to users. Existing works [@20icml-realm; @20neurips-rag; @23arxiv-ralm] directly concatenate the input with the top-$k$ retrieved sentences/documents to form the query for generators. To better use the in-context learning capability of LLMs, some works [@20acl-templated-retrieval; @22acl-reina; @23acl-udr; @23arxiv-refreshllms] design effective prompt templates to integrate retrieved information and inputs. To address the issue of lengthy inputs after concatenating retrievals, recent studies [@23arxiv-dil; @24iclr-recomp; @23arxiv-leancontext; @23arxiv-filco; @23emnlp-tcrallm] have introduced methods for assigning importance weights to elements within the retrieved knowledge base and filtering out less relevant contexts based on these weights.

The feature concatenation involves merging the encoded retrievals with the input features. A simple yet effective approach is FID [@21eacl-fid], which first encodes the retrieved passages into sparse or dense representations and then takes the concatenated features as the input for a generator. The state-of-the-art performance of the FID demonstrates the efficacy of feature concatenation. The follow-up works [@21neurips-emdr2; @23acl-pgra; @23icml-lumen; @23jmlr-atlas; @23acl-recap] further improve the FID by jointly tuning the retriever and the encoder, which can enhance the retrieved knowledge's representations. Besides, Chen et al. [@22neurips-retroprompt] concatenate the representations of related knowledge as demonstrations for prompt learning, yielding better generalization.

Algorithm [\[alg:query-fusion\]](#alg:query-fusion){reference-type="ref" reference="alg:query-fusion"} presents how to leverage query-based fusions to fuse retrieved knowledge. For those using text concatenation [@20icml-realm; @23arxiv-ralm], algorithm [\[alg:query-fusion\]](#alg:query-fusion){reference-type="ref" reference="alg:query-fusion"} first concatenates the retrieved texts and inputs (line 2), then feeds the concatenated input into the generator. Notably, since there is a limit to the maximum input length of existing language models, concatenating too many retrievals would result in a truncation of the concatenated input, which may cut the given input. Therefore, designing the prompt template is the key step for this branch of work. For those using feature concatenation [@21eacl-fid; @23acl-pgra], algorithm [\[alg:query-fusion\]](#alg:query-fusion){reference-type="ref" reference="alg:query-fusion"} first leverages an encoder to obtain the feature (line 5), then concatenates the feature of input and retrievals (line 6), finally passes the concatenated feature into a decoder model (line 7). This branch of work generally incurs high memory costs due to the long sequence length.

:::: algorithm
::: algorithmic
A query input $q$, top-$k$ nearest neighbor knowledge $\{v_1, \ldots, v_k\}$, the generator $\mathcal{G}$.\
Generated response $y$. $y_q=\mathcal{G}(q)$; $y_{v_j}=\mathcal{G}(v_j)$ $y=\lambda \sum_j y_{v_j} + (1-\lambda) y_q$; $\lambda_t=Calibrate(y_q, y_{v_1}, \ldots, y_{v_k})$ $y=\lambda_t \sum_j y_{v_j} + (1-\lambda_t) y_q$; $y$;
:::
::::

## Logits-based Fusion

The logits-based fusion refers to incorporating the retrieved knowledge into the output layers. Basically, retrieved knowledge would be fed into the same model to obtain the logits for enhancing or calibrating the predictions. Therefore, logits-based fusion can be categorized into two branches, i.e., ensemble-based fusion and calibration-based fusion.

Ensemble-based fusion treats the logits from the retrieved knowledge as part of an ensemble of predictions. Such ensemble-based fusion can significantly improve the generalization and robustness of the model [@23emnlp-eara; @20iclr-knn-lm; @21iclr-knn-mt]. One notable work of ensemble-based fusion is kNN-LM [@20iclr-knn-lm], which aggregates the logits of the top-$k$ nearest neighbors' targets and then interpolates the final predictions. Similar to kNN-LM, Khandelwal et al. [@21iclr-knn-mt] propose kNN-MT to enhance the machine translation using retrievals' logits, which is also followed by a branch of works [@21acl-ijcnlp-adaptive-knn-mt; @23arxiv-knn-adapter].

Different from ensemble-based fusion, calibration-based fusion uses the logits from the retrieved knowledge as a form of calibration for the model's predictions. Specifically, Jiang et al. [@22emnlp-robust-knn-mt] propose a confidence-enhanced kNN-MT that refines the kNN distribution and interpolation weights with the neural machine translation confidence. Li et al. [@23emnlp-source-context] propose to leverage the source context to calibrate the retrieval-augmented neural machine translation.

Algorithm [\[alg:logits-fusion\]](#alg:logits-fusion){reference-type="ref" reference="alg:logits-fusion"} demonstrates the detailed steps of using the logits-based fusion to integrate the retrieved knowledge. This branch of work first treats retrievals as similar data to augment the model (lines 2-4). For ensemble, algorithm [\[alg:logits-fusion\]](#alg:logits-fusion){reference-type="ref" reference="alg:logits-fusion"} leverages a hyperparameter to fuse the retrieval logits and the output logits (line 6). For calibration, algorithm [\[alg:logits-fusion\]](#alg:logits-fusion){reference-type="ref" reference="alg:logits-fusion"} dynamically determines the parameter based on the retrieval logits and the output logits (line 8). Then, algorithm [\[alg:logits-fusion\]](#alg:logits-fusion){reference-type="ref" reference="alg:logits-fusion"} performs the same fusion with the computed parameter (line 9).

:::: algorithm
::: algorithmic
A query input $q$, top-$k$ nearest neighbors $\{v_1, \ldots, v_k\}$, the encoder $\mathcal{E}$, the generator $\mathcal{G}$ containing $l$ pairs of modules $\{(\mathcal{M}^A_1, \mathcal{M}^F_1), \ldots\}$, where $\mathcal{M}^A_i$ and $\mathcal{M}^F_i$ are the attention module and the FFN module at layer $i$, $\mathcal{M}^C_i$ is the cross-attention module used in attention-based latent fusions.\
Generated response $y$. $h^F_0 = q$; $h^A_i=\mathcal{M}^A_i(h^F_{i-1})$; $e_{v_1}, \ldots, e_{v_k}=\mathcal{E}(v_1, \ldots, v_k, h^A_i)$ $h^R_i=\mathcal{M}^C_i(h^A_i, e_{v_1}, \ldots, e_{v_k})$; $h^F_i=\mathcal{M}^F_i(h^R_i)$ $y=LM\_HEAD(h^F_l)$ $e_{v_1}, \ldots, e_{v_k}=\mathcal{E}(v_1, \ldots, v_k)$ $h^F_0 = q$; $h^A_i=\mathcal{M}^A_i(h^F_{i-1})$; $h^R_i=h^A_i+\frac{1}{k}\sum_j w_j e_{v_j}$ $h^F_i=\mathcal{M}^F_i(h^R_i)$ $y=LM\_HEAD(h^F_l)$ $y$;
:::
::::

## Latent Fusion {#sec:latent}

The latent fusion investigates merging the retrieved knowledge into the hidden states of generators for a better generation. Based on the introduction method, latent fusion can be further classified into two categories: attention-based and weighted-addition.

One notable contribution of attention-based fusion is the Retrieval-Enhanced Transformer (RETRO) [@22icml-retro]. RETRO represents a pioneering effort in pre-training retrieval-based LLMs, introducing a new cross-attention module to integrate retrieved knowledge directly into the model's hidden states. A significant finding from this work is demonstrating a scaling law for the retrieval database, where RETRO, with a 2 trillion token database, attains performance comparable to that of major models like GPT-3 and Jurassic-1, albeit with 25 times fewer parameters. Customizing the transformer model in RETRO highlights the potential of pre-trained, retrieval-enhanced architectures in improving the efficiency and scalability of LLMs.

In addition to RETRO, other studies [@21acl-monolingual-mem; @22iclr-mem-transformer; @22iclr-tome; @22neurips-encoder-decoder; @23iclr-retmol] have contributed to the field by leveraging new attention modules to introduce external knowledge. Typically, Li et al. [@22neurips-encoder-decoder] have extended the RETRO model by decoupling the context encoding from the model inference. Wu et al. [@22iclr-mem-transformer], Wang et al. [@23neurips-longmem] store the hidden attention keys and values into external memory and retrieve the knowledge from the memory using an attention mechanism.

Due to the high complexity of the attention mechanism, another branch of work adopts lightweight (weighted) additions to introduce retrieved knowledge. Fevry et al. [@20emnlp-eae] propose the EAE model that retrieves top-$k$ related entities' embeddings from a learnable external memory and adds entities' embeddings to the hidden states of the model. Wu et al. [@24iclr-refusion] propose ReFusion, which explores various learnable reranking schemes to first re-weight the retrieved knowledge's embeddings, then use weighted addition to incorporate them into the hidden states of the model. Those approaches signify a growing trend towards models that dynamically select and integrate relevant knowledge, paving the way for more sophisticated and nuanced language generation and understanding.

Algorithm [\[alg:latent-fusion\]](#alg:latent-fusion){reference-type="ref" reference="alg:latent-fusion"} shows the steps of using latent fusion to introduce the retrieved knowledge into the hidden states of the generator. For attention-based latent fusion, algorithm [\[alg:latent-fusion\]](#alg:latent-fusion){reference-type="ref" reference="alg:latent-fusion"} first encodes the retrievals with the output states of the attention module (line 5), then uses a cross-attention module to fuse the retrieval features into the hidden state (line 6). Different from attention-based latent fusion, weighted-addition-based latent fusion adopts a more lightweight way to incorporate retrieved knowledge (lines 10-19). Algorithm [\[alg:latent-fusion\]](#alg:latent-fusion){reference-type="ref" reference="alg:latent-fusion"} first encodes the retrievals before feeding them into the generator (line 11), which can be done offline and directly stored as values in the datastore. Then, algorithm [\[alg:latent-fusion\]](#alg:latent-fusion){reference-type="ref" reference="alg:latent-fusion"} learns a set of weights to add the retrieval features on the hidden states of generators (line 15).

## Comparison of Different Fusion Methods

Each fusion method exhibits distinct advantages and limitations. The query-based fusion method is a simple and straightforward approach that preserves the raw information from retrieved information. However, it significantly increases the input sequence length, resulting in higher computational costs. Additionally, this kind of method suffers from limited contextual integration capabilities and scalability, as it struggles to effectively combine retrieved information with the input context.

The logits-based fusion method combines the output logits (or probabilities) of the base LLM with those from a separate model (or shared LLM) that processes the retrieved information. This approach is computationally efficient, as it can process retrievals and inputs in batches, avoiding increasing the input sequence length. It decouples the retrieval and generation processes, enabling independent improvements to each component. Nevertheless, this kind of method is limited by its shallow contextual integration, as fusion occurs only at the output level. Consequently, it may underperform in complex reasoning tasks that require fine-grained interaction between the input and retrieved information.

In contrast, the latent fusion method integrates retrieved information directly into the hidden states (or intermediate representations) of the LLM. This approach is both scalable and efficient, as it avoids expanding the input sequence length. Moreover, it facilitates deep, context-aware integration of retrieved information and supports fine-grained control over how the information is fused. However, the latent fusion method necessitates significant architectural modifications and careful training to achieve optimal performance. In summary, the choice of fusion method depends on the specific application, available resources, and the desired trade-off between simplicity and performance. Each method offers unique strengths and weaknesses, making them suitable for different use cases and operational constraints.

# Generators {#sec:gen}

This section introduces representative generators, which are generally pre-trained on large datasets. Existing generators are mostly large language models that adopt or modify the transformer-based architecture [@17nips-attention]. For example, DeepSeek [@guo2025deepseek], Llama-series models [@llama; @llama2], GPT-series models [@gpt1; @gpt2; @gpt3; @gpt4], and Gemini-series models [@gemini-1; @gemini-1.5; @gemma] remove all encoder modules, retaining only the decoder module, which includes an attention module and a feed-forward network module. Other advanced techniques, such as root mean square layer normalization [@19nips-rmsnorm], rotary position embedding [@24neurocomputing-rope], and group query attention mechanisms [@23emnlp-gqa], have been incorporated into the design of existing large language models to enhance their performance.

Existing generators can be generally categorized into two groups, close-sourced LLMs [@gpt3; @gpt4] and open-sourced LLMs [@guo2025deepseek; @llama; @llama2]. The formers can only adopt query-based fusions to introduce external knowledge. While the latter can adapt to all kinds of fusions. Especially generators using the latent fusion typically would train the generators from scratch. Those generators integrate novel modules designed for retrieved information fusion, such as cross-attention-based module [@22icml-retro; @22iclr-mem-transformer; @22neurips-encoder-decoder]. Their training paradigm combines pre-training on massive textual datasets with integrating external retrieved knowledge to establish robust information grounding. As detailed in Section [4.3](#sec:latent){reference-type="ref" reference="sec:latent"}, those generators enable dynamic, context-aware retrieved information incorporation while further improving the LLM's generative capabilities.

# RAG Training and Datastore Update {#sec:training}

<figure id="fig:update" data-latex-placement="t">
<embed src="figures/fig3-update.pdf" style="width:80.0%" />
<figcaption>Different RAG training strategies with/without datastore update.</figcaption>
</figure>

This section introduces RAG training, which can be categorized into two main classes: **RAG without datastore update** and **RAG with datastore update**. The former refers to the case where only trainable parameters in each module of RAG would be updated, and the knowledge in the datastore would remain the same during the training stage. The latter refers to the case where the knowledge in the datastore would be updated, then each module's parameters in RAG would be updated in a similar way as the former case.

## RAG without Datastore Update

The goal of training RAG without datastore update is to update the knowledge stored in the short-term memory of generators based on the existing knowledge datastore. As shown in Figure [4](#fig:update){reference-type="ref" reference="fig:update"} (a)-(c), there are three training cases, i.e., training the retriever, training the generator, and jointly training the retriever and generator.

### Training retriever.

Considering the case of no datastore update, training the retriever generally refers to training the retriever encoder and rebuilding the indexing. Since sparse encodings rely on statistical methods without parameters, training the encoder pertains only to dense encoding methods. Different training methods may have different goals, such as improving the semantic representations, accelerating the encoding process, or learning the domain-specific representations. The first two goals are often achieved by replacing the original encoder with a more powerful or tiny encoder, such as DistilBERT [@19arxiv-distilbert], or TinyBERT [@20emnlp-tinybert]. The last requires training the original encoder on the domain-specific corpus. REPLUG [@23arix-replug] updated the retriever by minimizing KL divergence between retrieval and LM scores, and asynchronously refreshing the datastore index. After training the retriever encoder, the vector database's embeddings that serve as keys will also change. Thus, all indexes should be rebuilt with new embeddings. Besides, if the encoder remains unchanged, the indexing can be updated using new ANN searching algorithms or re-tuning the hyperparameters. After the retriever is trained, it can be directly incorporated into the RAG without updating the generator.

### Training generator.

Training the generator involves updating its parameters or those in the retrieval fusion modules. Since the generator is generally an LLM, training the LLM is a resource- and time-consuming process. Fortunately, several parameter-efficient fine-tuning techniques, such as LoRA [@22iclr-lora], are proposed to address the fine-tuning problem of LLMs. Although the parameters in the retrieval fusion modules are less than those in the generator, only fine-tuning those parameters may encounter some training problems, such as low convergence and overfitting. Jointly tuning the parameters in the generator and the retrieval fusion modules is a better way to train the generator and the retrieval fusion modules if there are sufficient and powerful resources.

### Jointly training the retriever and generator.

Apart from independently training the retriever and the generator, jointly training the retriever and the generator can be another good choice for better performance on downstream tasks. The primary challenges involve computational complexity from large-scale retrieval and marginalization over latent documents, alongside training instability due to potential retrieval collapse and interdependent retriever-generator feedback loops. Additionally, sparse gradients from discrete retrieval steps and approximation errors from fixed top-$k$ document selection limit effective end-to-end optimization. Typically, complex indexes, such as FAISS [@24arxiv-faiss], are not a suitable choice during the fine-tuning stage.

Existing works generally leverage the complex indexes to pre-select a small subset of nearest neighbors as candidates, then choose the final top-$k$ nearest neighbors by performing the matrix-multiplication operations. REALM [@20icml-realm] and RAG [@20neurips-rag] both unify retriever and language modeling by treating retrieved documents as latent variables and optimizing end-to-end via gradient descent. They both used Maximum Inner Product Search (MIPS). RAG [@20neurips-rag] only updated the encoder in the retriever, but REALM [@20icml-realm] also asynchronous refreshed the indexing. Atlas [@23jmlr-atlas] jointly pre-trained the retriever and generator, using different loss functions (such as attention distillation and perplexity distillation) to optimize the retriever. Besides, it also asynchronous refreshed the indexing. Experimental results in these works demonstrate that joint training is an end-to-end optimization that can lead to better coordination between the retriever and the generator and improve the contextual understanding of the generator.

## RAG with Datastore Update

As shown in Figure [4](#fig:update){reference-type="ref" reference="fig:update"} (d), the scenario involves two stages: updating the knowledge database, then training the retriever and the generator. There are three cases for updating the knowledge database, i.e., updating with trainable embeddings, updating with new values, and updating with new corpus. In the first case, values generally are trainable embeddings and are simultaneously/asynchronously updated with parameters in the RAG [@22neurips-retroprompt]. The last two cases usually refer to updating the knowledge database with up-to-date information. Taking question-answer corpus as an example, updating with new values refers to updating the answer to existing questions, while updating with new corpus refers to adding new question-answer pairs. To update the value of existing keys requires first querying the existing key-value pairs and then performing in-place updates. For a new corpus, the datastore first needs to perform insertion operations, then rebuilds or updates the indexes as new keys are added. After updating the datastore, training the retriever and the generator is similar to RAG without datastore updates. However, this training step is not always necessary, thanks to the in-context learning capability of LLMs.

# RAG Evaluation and Benchmark {#sec:eval}

Retrieval-Augmented Generation (RAG) systems integrate the capabilities of large language models (LLMs) with external knowledge retrieval to advance text generation tasks in natural language processing (NLP). Evaluating RAG systems requires a dual focus on retrieval quality and generation performance to ensure the system effectively leverages external knowledge and produces accurate, contextually appropriate outputs. Recent research has introduced domain-specific benchmarks and comprehensive evaluation frameworks to address these challenges.

For retrieval quality, Pipitone et al. [@24arxiv_legal_ragbench] proposed a benchmark tailored for RAG systems in the legal domain, employing key retrieval metrics such as Precision@K and Recall@K to assess the relevance and ranking of retrieved documents. Adlakha et al. [@24tacl_rag_eval] expanded the evaluation scope by incorporating correctness, faithfulness, and human evaluation to assess retrieved information, alongside novel token-overlap metrics for finer-grained analysis.

On the generation side, Hui et al. [@24nips_ragbench] introduced a benchmark suite for RAG systems in real-world document analysis, utilizing metrics such as accuracy, F1 score, and exact match score to evaluate answer quality. Similarly, Xiong et al. [@24acl_medical_ragbench] developed a medical RAG benchmark to systematically compare the performance of medical RAG systems, emphasizing domain-specific evaluation criteria.

Recent advancements have further enriched the evaluation landscape by proposing more systematic and comprehensive methodologies. Chen et al. [@24aaai_ragbench] identified four fundamental RAG abilities---noise robustness, negative rejection, information integration, and counterfactual robustness---as key evaluation dimensions. Saad-Falcon et al. [@24naacl_ares] introduced ARES, an automated RAG evaluation system, which assesses RAG systems using context relevance, answer faithfulness, and answer relevance. Building on this, Friel et al. [@25arxiv_ragbench] extended the evaluation framework by incorporating additional metrics such as context utilization and answer completeness, providing a more holistic assessment of RAG system performance. These efforts collectively highlight the growing emphasis on rigorous, domain-specific, and multi-dimensional evaluation methods to ensure RAG systems meet the demands of diverse applications while maintaining high retrieval and generation quality standards.

# Tasks {#sec:task}

This section lists several classical tasks in the NLP domain and introduces advanced RAG techniques used to solve these tasks.

## Language Modeling

Language modeling is the task that requires the prediction of the probability distribution of the next word or character given a sequence of words or characters, which is also named the next-token prediction task. Language modeling has become the fundamental task for pre-training large language models, which can measure the models' generation capability using the perplexity metric. The formal definition is as follows: given such a sequence of tokens $x_1, \ldots, x_n$ called *Prefix*, the language modeling task aims to model its probability via next-token prediction, $$\begin{equation}
p(x_1, \ldots, x_n)=p(x_1)\cdot\prod^n_{i=2} p(x_i|x_1, \ldots, x_{i-1}),
\end{equation}$$ where the conditional probabilities $p(x_i|x_1, \ldots, x_{i-1})$ are modeled by a parameterized language model.

Recent works mainly leverage RAG further to improve language modeling capability in the pre-training stage. A branch of works [@22icml-retro; @22neurips-encoder-decoder; @22iclr-mem-transformer; @23neurips-longmem] modifies the architecture of generators by adding a new cross-attention module in each transformer block for introducing retrieval knowledge. The intuition of those works is that given the similar *Prefix*es and their next tokens (retrieving stage), the pre-trained model can calibrate the model's prediction using the cross-attention module to capture the pattern between the next token and prefix (model forwarding stage). Zhong et al. [@22emnlp-trime] propose to augment the language model with three types of retrieval memories/databases (local memory, long-term memory, and external memory) and optimize the next-token probability distribution with nearest neighbors retrieved from the memories/databases. Another branch of works [@20iclr-knn-lm; @23arxiv-knn-adapter; @23icml-knn-lm-why; @23arxiv-ralm; @20icml-realm] focuses on augmenting the inputs or outputs of generators with retrievals. Guu et al. [@20icml-realm] and Ram et al. [@23arxiv-ralm] concatenate the retrieved knowledge with inputs and feed the retrieval-augmented inputs into the generators. Other works [@20iclr-knn-lm; @23arxiv-knn-adapter; @23icml-knn-lm-why] fuse the logits of inputs as well as retrievals at the final output layer and generate the final probability distribution based on the interpolated results. Those works believe that the concatenated/fused retrievals can provide useful context information on inputs/outputs to improve models' robustness during the pre-training stage. Besides, Doostmohammadi et al. [@23acl-retro-bm25] focus on pre-training models with a semantic retriever (BM25) and achieve a better language modeling performance.

## Machine Translation

Machine translation (MT) leverages computational linguistics algorithms to translate text or speech from one language to another automatically. The goal of MT is to produce an accurate and fluent translation, preserving the meaning of the original text while adhering to the grammatical and stylistic norms of the target language. MT systems have evolved from rule-based machine translation (RBMT) to statistical machine translation (SMT) and, more recently, to neural machine translation (NMT). In particular, NMT methods have significantly improved translation quality by leveraging deep learning techniques, which thus will be the focus of this section.

RAG techniques can further enhance MT by incorporating external knowledge into the translation process. The simplest way is to concatenate the similar translation examples into the inputs or fuse the logits of similar translation examples at the output layer. For example, some works [@22acl-reina; @23neurips-selfmem] retrieve similar translations according to the source text and concatenate corresponding target texts or pairs of source and target texts as examples into inputs. Other works [@20acl-retrieve-edit-rerank; @21iclr-knn-mt; @21acl-ijcnlp-adaptive-knn-mt] feed the retrieved source text into the models and obtain the logits of the next target tokens, then aggregate all logits to generate the final predictions. Moreover, Jiang et al. [@22emnlp-robust-knn-mt] and Li et al. [@23emnlp-source-context] use the logits of retrieved examples to calibrate the aggregated logits, improving the robustness of the generation. Another branch of works [@23acl-ink; @22emnlp-trime] injects external knowledge into the objective function during the training stage, refining the representation space with similar translations. Besides, Cai et al. [@21acl-monolingual-mem] encode similar translations and store them as the translation memory, then introduce the knowledge from memory with a cross-attention module. Instead of improving the performance, a branch of work focuses on accelerating the generation efficiency on MT tasks, such as searching from a pre-built subset [@22acl-fast-knn-mt; @23acl-subset-knn-mt] or a dynamic datastore [@23iclr-sk-mt], searching by chunks [@22emnlp-chunk-knn-mt].

## Text Summarization

Text summarization is the process of condensing a larger text document into a shorter version, preserving key information and the overall message. This task can be broadly categorized into two types: extractive summarization, which involves selecting and compiling parts of the original text, and abstractive summarization, which entails rewriting the essence of the text in a new, concise form. The goal is to produce a coherent and fluent summary that encapsulates the most critical information from the source material.

RAG techniques can significantly enhance text summarization tasks by leveraging external knowledge and similar documents to inform the summarization process. [@22acl-reina; @22acl-retrieval-bio; @23acl-udr; @23neurips-selfmem] simply concatenates the retrieved similar summaries into inputs to generate summarizations. Instead of concatenating texts, other works fuse features at the intermediate layers by cross-attention [@23neurips-unlimiformer], or at the output layers by logits ensemble [@20acl-retrieve-edit-rerank]. Besides, Jiang et al. [@23emnlp-flare] argue that retrieving for every generation may not always be the best choice and propose to retrieve external knowledge during the generation process adaptively.

## Question Answering

Question Answering (QA) is a fundamental task in NLP that involves building systems capable of automatically answering human questions in natural language. QA systems can be broadly classified into two categories: open-domain, where the system answers questions about virtually anything, and closed-domain, focusing on a specific area of knowledge. The primary challenge in QA is understanding the question's intent and retrieving accurate, relevant information from a vast collection of data to provide a concise answer. Due to the page limits, this paper only discusses the works of open-domain QA systems.

RAG techniques combine information retrieval with model-based generation, which is highly suitable for QA systems. In particular, open-domain QA systems usually first require searching for knowledge from the Internet or large-scale databases, then generate the corresponding answers according to the retrieved knowledge. Naturally, given similar questions and corresponding answers as demonstrations which are concatenated into inputs [@22acl-reina; @23acl-udr; @23arxiv-raven], generators in RAG can learn the pattern between questions and answers and infer what answers should be. For some specific QA tasks where a set of reference documents is given, retrievers in RAG would retrieve the relevant documents for concatenation, and then generators in RAG would read the context then generate the final answers via the self-attention mechanism [@20icml-realm; @23acl-ur-qa; @23arxiv-ralm; @23arxiv-self-rag], which is similar to solving a reading comprehension problem. Besides, Fabbri et al. [@20acl-templated-retrieval] focus on designing effective templates for re-organizing the concatenated contexts. Baek et al. [@23arxiv-kaping] leverage the knowledge graph to retrieve the related facts for the input questions, then feed their concatenation and inputs into the generators. Instead of directly concatenating texts, another branch of works focuses on joining the retrieval embeddings with input embeddings for the encoder-decoder models [@21eacl-fid; @21neurips-emdr2; @23icml-lumen; @23jmlr-atlas].

Some works incorporate the external knowledge in the hidden states or the final logits of generators. For the fusion in the hidden states, the key is what kind of knowledge representation would be injected, such as entities [@20emnlp-eae; @22iclr-tome], chunks [@22icml-retro; @{23arxiv-retrio++}], documents [@23arxiv-pluglm]. For the fusion in the logits, most works combine the logits of retrievals and inputs by ensemble techniques [@23arix-replug; @20icml-realm; @20neurips-rag; @23acl-demrag].

Instead of designing different knowledge fusions for QA systems, existing works also improve QA systems with RAG from other aspects. Some works [@19acl-coupling-retrieval; @22acl-rgf; @22neurips-recross] use retrieved question-answering pairs as extra training data. Some works optimize the retriever module, e.g., improving the keys' representation when building the retriever database [@23acl-dist-over-vocab], replacing the indexing with a pre-trained ranking model [@23acl-aar], or enabling retrieving phrases with two queries [@23acl-npm]. Other works focus on accelerating the generation efficiency of RAG. Jong et al. [@22arxiv-fido] propose the layer-sparse cross-attention to speed up the decoding. Some works [@23arxiv-self-rag; @23emnlp-flare; @23emnlp-skr] observe that the retrievals may not always provide useful information during the generation process and learn to determine when to retrieve. Moreover, Sun et al. [@24iclr-tog] combine the RAG with agents to iteratively reason the final results.

## Information Extraction

Information Extraction (IE) is a critical task in NLP to automatically extract structured information from unstructured and semi-structured text sources. This task encompasses several sub-tasks, including Named Entity Recognition (NER), Entity Linking (EL), Coreference Resolution (CR), Relation Extraction (RE), etc. The goal is to identify and classify key elements from text and understand the relationships between them, thereby converting textual data into a structured format amenable to analysis and interpretation.

With RAG techniques, addressing IE tasks can be significantly improved in terms of not only performance but also interpretability. In NER tasks, Wang et al. [@21acl-cl-kl] first retrieve similar sentences and then concatenate the ranked retrievals for better semantic representations. Ren et al. [@23acl-retrieve-sample] show that naive RAG may not address Event Argument Extraction (EAE) tasks. Thus, they adopt a sampling-based method to guarantee the same distribution of event labels between retrievals and inputs then concatenate retrieval texts into inputs for better performance in EAE tasks. Table augmentation is also a challenging task, which requires extracting information from tables. Glass et al. [@23acl-rata] propose to extract information in a retrieval-augmented manner.

## Text Classification

Text classification tasks are common in NLP applications. Sentiment analysis, a prominent text classification task in NLP, entails identifying and categorizing the emotional tone conveyed in a text. For example, given a sentence of "I love to watch movies", the analysis models should determine whether it has a positive attitude or a negative attitude. The attitude in sentiment analysis can range from positive to negative or can be neutral, nuanced, and even mixed. The sentiment analysis task is crucial for understanding consumer feedback, monitoring brand reputation, and gaining insights into public opinion on various issues.

RAG techniques can significantly enhance sentiment analysis with different external knowledge fusion strategies. Li et al. [@23acl-udr] concatenate the retrieved options and corresponding prompt-based labels with input options. Other works [@22neurips-retroprompt; @23acl-pgra] concatenate the retrieval embeddings with input embeddings before feeding them into the decoder. Some works fuse the retrieval features into the hidden states of generators via cross-attention [@23arxiv-pluglm; @23neurips-longmem] or ranking-based addition [@24iclr-refusion]. Besides, other works focus on fusing the logits of retrievals with the output logit using ensemble techniques [@23acl-reaugkd; @23emnlp-eml]. Except for knowledge fusions, Min et al. [@23acl-npm] enable locating knowledge in phrases more accurately via two queries.

## Dialogue Systems

Dialogue systems, also known as conversational agents or chatbots, are designed to simulate conversation with human users, either in text or speech form. These systems can be categorized into two main types: task-oriented systems [@ra_pdg], which assist users in completing specific tasks such as booking tickets or ordering food, and open-domain systems, which aim to carry on a general conversation on a wide range of topics [@ra_hc]. The core challenge in developing effective dialogue systems lies in understanding user intent, maintaining context, and generating coherent, relevant responses.

Existing works improve the dialogue system with RAG mostly via the query-based fusions. Some works [@21acl-mamd; @23acl-refpydst; @23neurips-selfmem] concatenate the retrieved history conversations with current inputs. Other works [@21tacl-kif; @23acl-recap; @23neurips-selfmem] first leverage an encoder to encode the history responses, then feed the concatenated embeddings into a decoder to generate new responses.

# Applications {#sec:app}

## LLM-based Autonomous Agents

LLM-based autonomous agents are intelligent software systems that leverage the power of LLMs to perform tasks without the need for continuous human intervention [@li2024personalllmagentsinsights; @xi2023risepotentiallargelanguage; @Wang2024]. These agents use LLMs as a brain or controller [@huang2024understandingplanningllmagents], and extend their abilities through multimodal perception [@xie2024largemultimodalagentssurvey], tool utilization [@NEURIPS2023_d842425e] and external memory [@packer2024memgptllmsoperatingsystems]. Especially, external long-term memory for agents functions as the knowledge datastore in RAG, which provides agents with the capability to incorporate external knowledge over extended periods. Therefore, applying RAG would be beneficial to access a broader range of information, improving agents' decision-making and problem-solving abilities [@zhang2024llmmastermindsurveystrategic]. This section explores how LLM-based agents can leverage RAG from two perspectives.

**Using RAG to Retrieve from External Memory.** LLM-based agents can utilize RAG to access and retrieve information from their own external memory [@hatalis2023memory; @zhang2024surveymemorymechanismlarge; @mei2024aiosllmagentoperating]. This external memory serves as a knowledge base that the agent can draw upon to enhance its understanding and decision-making. When faced with a query or a task, the agent can use RAG to retrieve relevant information from this memory, which is then integrated into the generation process of the LLM. This allows the agent to produce responses or solutions that are informed by a wider range of knowledge, leading to more accurate and contextually relevant outcomes.

The ability to tap into a vast external memory enables the agent to continuously learn and adapt based on new information, making it more effective over time. **Using Tools to Search the Web and RAG for Up-to-Date Information.** In addition to retrieving information from its own memory, an LLM-based agent can use tools to search the web for the most current information [@NEURIPS2023_d842425e]. This capability is particularly useful for tasks that require up-to-date knowledge, such as news summarization, market analysis, or responding to rapidly evolving situations. Once the agent retrieves the latest information from the web, it can use RAG to integrate this data into its generation process. By combining the LLM's natural language understanding with real-time data from the web, the agent can generate responses that are not only contextually relevant but also reflect the latest developments. This approach enhances the agent's ability to provide accurate and timely information, improving its effectiveness in dynamic environments.

In both cases, RAG plays a crucial role in augmenting the capabilities of LLM-based agents by enabling them to access and leverage a wider range of information, whether it's from their own external memory or from real-time sources on the web. This leads to more informed decision-making and enhances the overall performance of the agents.

## Frameworks

Frameworks like Langchain [@langchain] and LLaMAindex [@22github-llamaindex] pose a significant impact on enhancing the practical implementation of RAG. Langchain and LLaMAindex exemplify the integration of sophisticated retrieval mechanisms with generative models, facilitating the seamless incorporation of external data into the language generation process. This section will introduce these two representative RAG frameworks in details.

Langchain is a framework designed to augment the capabilities of language models by integrating them with external knowledge sources and databases. It acts as a middleware that facilitates the interaction between language models and various data retrieval systems, enabling a more informed and accurate generation of responses. The core functionality of Langchain involves orchestrating the flow of information from external databases into the generative process of language models, enhancing their ability to leverage context and specific knowledge in their responses. This integration plays a crucial role in enabling language models to perform tasks that require access to up-to-date or detailed information that is not contained within the model's initial training data.

LLaMAindex is a specialized data framework that focuses on organizing and indexing vast amounts of data to improve the retrieval capabilities of language models. This framework supports efficient querying mechanisms, allowing language models to quickly access relevant information from a structured repository. LLaMAindex is designed to be highly scalable and can handle diverse data types, from text documents to structured databases. The indexed data supports a wide range of applications, from simple fact retrieval to complex analytical tasks, making it an indispensable tool for enhancing the information retrieval phase in language models.

Both Langchain and LLaMAindex are deeply connected to the concept of RAG. Langchain enhances RAG by providing a structured way for language models to interact with external databases and knowledge sources during the generation process. On the other hand, LLaMAindex serves as a powerful backend for RAG systems by ensuring that the retrieval process is both fast and relevant. Together, Langchain and LLaMAindex enhance the capabilities of RAG by ensuring that the language models are not only generating text based on their internal knowledge but are also capable of pulling in external data to provide responses that are contextually enriched and informationally robust.

# Discussion and Future Direction {#sec:future}

Despite the success of the RAG for natural language processing, some challenges should be considered. This paper highlights these challenges to inspire future research and provides possible future research directions in RAG for NLP.

## Retrieval Quality

The retrieval quality largely determines the performance of the generator. Park et al. [@24tacl_retriever_eval] investigated imperfect retrieval's effect and demonstrated that unanswerability or contradiction of a document set, which frequently leads to hallucinations. However, Ren et al. [@24coling_fk_rag] demonstrated that the improvement brought about by the increase in the number of supporting documents is not entirely due to the increase in recall. To this end, using various evaluation methods referred to in Section  [7](#sec:eval){reference-type="ref" reference="sec:eval"} for retrieval information will be more beneficial to model generation. The retrieval quality involves the following four key factors that must be designed. The first consideration is **determining the optimal key** to use in the vector database. This process typically involves subjective decision-making and requires human effort to design effectively. The naive idea is to choose inputs for the given tasks, treating each task as a QA problem.

The second is **the choice of embedding model**. After determining the key, the next step is leveraging embedding models to convert text into vector representations. Models such as BERT [@19naacl-bert], RoBERTa [@19arxiv-roberta], or domain-specific embeddings can be crucial to determine how well nuances and contextual meanings are captured. Adapting the embedding model to suit specific data types or queries better can significantly enhance retrieval quality. This requires training the model on domain-specific corpora, including the types of queries and documents the system will encounter.

Thirdly, **designing effective similarity metrics** is also crucial to improve retrieval quality. The goal of similarity metrics is to measure the relevance between the query and the retrieved information. Some classical similarity metrics, such as cosine similarity or Euclidean distance, used for ranking in the recommender system can also be used in RAG [@ir_similarity]. Apart from these metrics, some works explored more complex similarity metrics, such as optimal transport distance [@23nips-ra-mil], to obtain a task-specific similarity.

Finally, **approximate nearest neighbor (ANN) searching** is also a key step in determining what knowledge should be returned as nearest neighbors. Advanced ANN searching aims to accelerate the retrieval efficiency at the cost of sacrificing the retrieval quality. Choosing a suitable ANN algorithm, such as product quantization [@11tpami-pq] or HNSW [@hnsw], requires a good trade-off between retrieval efficiency and retrieval quality. All of these factors collectively contribute to the retrieval quality of the retriever.

## RAG Efficiency

RAG efficiency is crucial for downstream NLP applications, which limits the volume of data that can be retrieved. There are two simple ways to guarantee RAG efficiency without new algorithms, i.e., reducing the volume of data or adding more powerful computing and memory resources. However, the former may impact the retrieval quality, while the latter requires more resource cost.

RAG efficiency encompasses the efficiency of the retriever and the efficiency of retrieval fusions. Retriever efficiency refers to the time cost of retrieving relevant information, which can be divided into three parts, i.e., encoding time, ANN searching time, and data fetching time of the datastore. It is unnecessary to jointly optimize all three components as the bottleneck would vary from different database sizes. For smaller retrieval databases, such as those with fewer than 1 million entries, the encoding phase is often the primary bottleneck, as the vector database can be all stored in the memory. Several topics, such as model quantization [@21icml-i-bert; @21acl-binarybert], distillation [@20emnlp-tinybert; @23aaai-skdbert], or model pruning [@21tacl-compress-bert], are used to accelerate the encoding.

In contrast, for larger databases, the time cost of searching in the index and fetching data from the datastore becomes the major bottleneck, as the searching is over a considerable amount of data, and the fetching involves I/O overheads. In this case, efficient ANN searching algorithms [@21tbd-faiss; @24arxiv-faiss; @20icml-scann] and system-level optimizations [@24arxiv-ragcache; @24arxiv-piprag] are the main focus.

Retrieval fusion efficiency, which aims to enhance the inference efficiency when integrating retrievals, is worth to be optimized for improving the RAG efficiency. For example, the computational overhead of query-based fusion is often non-negligible due to the long sequence length. Some works, such as Fid-light [@23sigir-fid-light] and ReFusion [@24iclr-refusion], mainly target reducing the computations while integrating the retrieved information.

## Discussion for Query-based Fusion in RAG

Two main query-based fusion RAG concerns must be discussed. The first is the performance comparison towards long-context LLMs. Hui et al. [@24nips_ragbench] demonstrated that RAG system and long-context LLMs show similar performance for free-form or knowledge-based tasks (e.g., paper-based and wiki-based Q&A). However, in tasks requiring numerical reasoning (e.g., financial Q&A), long-context LLMs underperform compared to RAG, as the verbose content in long contexts can obscure key facts and hinder precise reasoning. Additionally, RAG tends to outperform long-context mechanisms in smaller LLMs, which struggle to process large volumes of data effectively. The second concern is improving the query-based fusion method, which is prevalent in modern RAG systems. Recent works used prompt engineering strategies [@20neurips-rag; @23acl-pgra], query refinement techniques [@24arxiv_crag], calibration-based strategies [@23emnlp-flare; @23arxiv-self-rag], and iterative RAG [@23emnlp_iterag] to improve the query-based fusion RAG. Among these strategies, prompt engineering and calibration offer improved results with minimal overhead, while iterative retrieval-augmented generation (RAG) and query refinement prioritize higher computational costs in exchange for potential accuracy gains. Prompt engineering and query refinement depend on manual tuning, whereas calibration and iterative RAG are more automation-friendly but require high-quality, robust data. Simpler strategies, such as prompting, are easier to deploy but lack flexibility, while more complex methods like iterative RAG provide greater adaptability at the expense of stability. These approaches can be combined (e.g., refined queries with iterative loops and calibrated prompts), though such synergies must carefully balance the compounded drawbacks, such as increased latency and complexity.

## Choices of Fusions

This paper introduces three kinds of retrieval fusions, where each fusion is worth further exploring. Query-based fusions concatenate the texts or embeddings of retrieved knowledge with inputs. These methods have better interpretability and are easy to apply even only when the API of LLMs is provided. However, concatenation leads to a long sequence of inputs, thus resulting in a large computational overhead in the attention and truncation of inputs. Some works [@24iclr-refusion; @23arxiv-leancontext] aim to improve efficiency when integrating retrievals, while others [@23neurips-unlimiformer; @23neurips-longmem] focus on improving the efficiency when increasing the model input length.

Conversely, latent-based fusions amalgamate information at a deeper, more abstract level, which may capture more nuanced relationships between the retrieved information and the query. However, these fusions significantly lack interpretability and often require pre-training or fine-tuning to adjust the retrieval embeddings or reweight the retrievals. Therefore, enhancing the interpretability of such latent-based fusions is also worth exploring in the future.

Logits-based fusions incorporate information at the decision level, thereby offering a potentially more flexible and robust integration of data from various sources. Nonetheless, these fusions may oversimplify the fusion process, diminishing the richness of the retrieved information by reducing them to logit values. Meanwhile, such fusions require performing all inference of retrievals, which is also a time-consuming process.

Apart from applying one kind of fusion in practical applications, combining different fusions is also worth exploring for better performance. These fusion methods are not mutually exclusive, as they focus on augmenting the different stages of generators, i.e., inputs, hidden states, and outputs. Besides, during the generation, when to fuse retrieved knowledge is also a significant problem worthy of further exploration [@23acl-trust-lm].

## RAG Training

As introduced in Section [6](#sec:training){reference-type="ref" reference="sec:training"}, RAG training includes two branches of works, RAG with/without datastore update. For RAG without a datastore update, the main challenge is how to jointly optimize all parameters in RAG. This may involve new loss functions with multiple objectives, new optimizations for efficient tuning parameters in the retriever and generator, or other training strategies.

For RAG with datastore update, one challenge is how to align the retrieval representations with the generator's representations. Although the time cost of the update operation in datastore cannot be ignored, some works [@22neurips-retroprompt] reduce the update frequency by asynchronously updating, thus achieving the alignment of knowledge representation and model's representation. Another challenge is when to retrain/fine-tune the generator in RAG when a new corpus is added. Due to the in-context learning capability of existing LLM-based generators and high training overhead, retraining/fine-tuning the generator or directly inferring the generator becomes a challenging choice for different scenarios. Recently, some efficient training strategies [@22iclr-lora; @23nips-qlora] have been proposed to accelerate the fine-tuning process, which can be taken into consideration.

## Cross-Modality Retrieval

Retrieving cross-modality information in NLP tasks can greatly enhance the quality and richness of the representations, leading to improved performance. First, cross-modality information, such as combining text with images, videos, or audio, provides a richer context to the content [@hu2023multimodal]. For instance, when language is ambiguous, accompanying images can clarify meanings difficult to convey through text alone. Second, different modalities can contribute various types of information that are not accessible from a single source. For example, visual data can provide spatial, color, and action cues, while textual data can offer detailed descriptions, emotions, or abstract concepts. Combining these can lead to a more comprehensive understanding of the data. Moreover, Models trained on multi-modal data typically exhibit increased robustness and generalizability [@multirobust]. These models are adept at associating information across diverse inputs, mitigating overfitting to the peculiarities of a single modality. This attribute is particularly valuable in real-world applications of NLP, such as in autonomous vehicles, where systems must interpret textual information from signs or dialogues and sensory data from the surrounding environment to make informed decisions. Furthermore, multi-modal data can resolve ambiguities that cannot be resolved within a single modality. For example, the phrase \"bank\" can refer to either a financial institution or the side of a river, and visual context can help disambiguate this. Last, human communication is inherently multi-modal, incorporating elements such as gestures, facial expressions, and tone of voice. Systems capable of processing multiple modes of communication can interact with humans in a manner that is both more natural and intuitive. In conclusion, integrating cross-modality information in RAG for NLP tasks not only enhances the richness and quality of data representations but also significantly improves the systems' comprehension, interaction capabilities, and adaptability to diverse applications.

# Conclusion {#sec:con}

In this survey, we delve into the development of RAG within the field of natural language processing. First, this paper introduces the components of RAG and their functionalities. Subsequently, this paper elaborates on each step involved in retriever, discussing the diverse techniques. Furthermore, this paper categorizes the retrieval fusions, evaluating the strengths and weaknesses inherent in each retrieval fusion technique. Besides, this paper discusses the RAG training, including RAG with/without datastore update. Then, this paper presents RAG evaluation and benchmarking, and explores how RAG can be adapted for various NLP tasks and provides practical applications of RAG in real-world scenarios. Conclusively, this paper identifies ongoing challenges and suggests directions for future research to foster advancements in this evolving area.

[^1]: \*Corresponding author.\
