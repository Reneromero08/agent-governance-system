# Text Embedding Inversion Attacks on Multilingual Language Models

Yiyi Chen Heather Lent Johannes Bjerva
  
Department of Computer Science, Aalborg University, Denmark
  
{yiyic, hcle, jbjerva}@cs.aau.dk

###### Abstract

Representing textual information as real-numbered embeddings has become the norm in NLP.
Moreover, with the rise of public interest in large language models (LLMs), Embeddings as a Service (EaaS) has rapidly gained traction as a business model.
This is not without outstanding security risks, as previous research has demonstrated that sensitive data can be reconstructed from embeddings, even without knowledge of the underlying model that generated them.
However, such work is limited by its sole focus on English, leaving all other languages vulnerable to attacks by malicious actors.
To this end, this work investigates LLM security from the perspective of multilingual embedding inversion.
Concretely, we define the problem of black-box multilingual and cross-lingual inversion attacks, with special attention to a cross-domain scenario.
Our findings reveal that multilingual models are potentially more vulnerable to inversion attacks than their monolingual counterparts.
This stems from the reduced data requirements for achieving comparable inversion performance
in settings where the underlying language is not known a-priori.
To our knowledge, this work is the first to delve into multilinguality within the context of inversion attacks, and our findings highlight the need for further investigation and enhanced defenses in the area of NLP Security.

## 1 Introduction

Industrial applications of Natural Language Processing (NLP) typically utilize Large Language Models (LLMs) and frequently rely on vector databases via frameworks such as Embeddings as a Service (EaaS).
In this context, rather than storing data as strings, high quality sentence embeddings are stored in a remote database instead.
This allows end-users to efficiently search across these condensed representations, which are seemingly impervious to privacy breaches.
However, while such EaaS workflows have previously been assumed to be secure, recent work has demonstrated that access to the embeddings is no more safe than raw text, as models can learn to decode these embeddings Song and Raghunathan ([2020](#bib.bib31)); Morris et¬†al. ([2023](#bib.bib20)); Zhou et¬†al. ([2023](#bib.bib39)).
As such, there is a substantial threat to privacy if malicious actors are able to eavesdrop on communication channels between EaaS providors and customers, and access the embeddings in the process.

Decoding the content of these embeddings can be done via inversion attacks.
After gaining access to embeddings and the black-box embedder via the EaaS API, the malicious actor can train an external model, which approximates the inversion function that reconstructs the text from the embeddings.
Previous work has proven has demonstrated that an exact match for data recreation can be obtained in specific settings, albeit with the limitation of assuming monolingual English models and embeddings (Morris et¬†al., [2023](#bib.bib20)).

In a real-world scenario however, an eavesdropper may not necessarily know the language of the text encoded within the embedding.
For instance, a Spanish EaaS provider might host its data in Germany, for a French-speaking company.
Thus in this work we investigate three research questions:
(i) To what extent are inversion attacks feasible in a multilingual setting?;
(ii) Are attacks feasible and effective when the language is unknown a-priori?;
(iii) Does cross-lingual transfer allow information to be leaked across the languages included in a multilingual model?

#### Contributions

In this work, we define the problem of black-box multilingual and cross-lingual inversion attacks, with special attention to a cross-domain scenario.
While previous research has succeeded in reconstruction of tokens with bag-of-words approach¬†(Song and Raghunathan, [2020](#bib.bib31)) and sequences with informative words¬†(Li et¬†al., [2023](#bib.bib16)),¬†Morris et¬†al. ([2023](#bib.bib20)) has proven the potential and effectiveness of embedding inversion to the extent of exact text reconstruction in English.
We approach multilingual inversion by extending the methodology introduced by Morris et¬†al. ([2023](#bib.bib20)) to a multilingual setting over English, French, Spanish, and German.

In this study, we are thus the first to investigate multilingual inversion attacks and potential of exact textual reconstruction in a multilingual setting, in particular when the language of a target embedding is unknown.
Concretely, we experiment using a state-of-the-art multilingual black-box encoder, where the trained multilingual inversion model reconstructs texts in certain languages outperforming their monolingual counterpart without extra steps of corrections.
Furthermore, we conduct experiments on cross-lingual and cross-domain text reconstruction, and propose a straightforward Ad hoc Translation method to counteract the shortcomings of the current standard of string-matching metrics in this specific setting.

Finally, we open-source all of our trained inversion models due to the computational cost of their training.111The trained inversion models are available at: <https://huggingface.co/yiyic>
While open sourcing our model comes with such risk as providing models to attackers, the underlying attack mechanism presented in this paper is already established, together with a defense in (Morris et¬†al., [2023](#bib.bib20)).
We believe these models will be useful to the research community, allowing for the development of multilingual defense mechanisms, without needing to spend resource on training the models we present.

## 2 Related Work

Models are well known to memorize training data, and are therefore susceptible to leaking private information Shokri et¬†al. ([2016](#bib.bib30)); Carlini et¬†al. ([2018](#bib.bib3)); Nasr et¬†al. ([2019](#bib.bib23)).
As such, there is increased research interest in exploring this vulnerability to inversion attacks from the perspective of cyber-security, simulating attacks against models to recreate sensitive training data.
Work in this direction has been conducted across various domains of machine learning, such as computational genetics Fredrikson et¬†al. ([2014](#bib.bib9)), computer vision Fredrikson et¬†al. ([2015](#bib.bib8)), and more recently NLP Song and Raghunathan ([2020](#bib.bib31)).
Generally, such works at the intersection of machine learning and cyber-security (e.g., on inversion attacks or adversarial attacks) make assumptions about the imagined attacker‚Äôs levels of access to the victim model.
White-box scenarios assume attacker access to the full model Wallace et¬†al. ([2019](#bib.bib34)); Tsymboi et¬†al. ([2023](#bib.bib33)), resulting in many possible attack surfaces.
Previous works in NLP have shown that it is possible to retrieve sensitive training data by attacking models directly Fredrikson et¬†al. ([2014](#bib.bib9), [2015](#bib.bib8)), attacking gradients Zhu et¬†al. ([2019](#bib.bib40)); Deng et¬†al. ([2021](#bib.bib7)), as well as through leveraging leaked hidden states Li et¬†al. ([2022](#bib.bib15)).
Meanwhile, black-box attacks assume an attacker has no knowledge of the underlying model itself, and can only interact with models at the most abstracted level (e.g., provide input and register output through an API). For example, Carlini et¬†al. ([2020](#bib.bib4)) are able to extract sensitive training data (e.g., names and phone numbers) from GPT-2 Radford et¬†al. ([2019](#bib.bib27)), by first generating data from the model and then using membership inference attacks to filter utterances likely to be part of the original training data.

In the case of embedding inversion attacks, whereby an imagined attacker aims to recreate the text encoded by the distributed representation, Song and Raghunathan ([2020](#bib.bib31)) first demonstrated that 50%‚Äì70% percent of tokens could be recovered from an embedding. Since then, the success of subsequent attacks has only improved, with newer approaches now able to retrieve entire sentences of encoded text H√∂hmann et¬†al. ([2021](#bib.bib11)); Hayet et¬†al. ([2022](#bib.bib10)); Morris et¬†al. ([2023](#bib.bib20)); Li et¬†al. ([2023](#bib.bib16)). Meanwhile, the development of counter-measures to embedding inversion attacks is an area of ongoing investigation. For example,
Zhou et¬†al. ([2023](#bib.bib39)) propose a defense method which makes randomly perturbed embeddings after an initial clustering step, such that the embeddings are still semantically meaningful and useful for downstream tasks, while remaining resistant against inversion attacks.
Parameter-efficient fine-tuning has also been found to protect models against white-box (via gradients) inversion attacks in the setting of federated learning Zhang et¬†al. ([2023](#bib.bib38)). Beyond direct defenses against inversion attacks, other methods for producing more secure embeddings have made use of existing encryption methods Huang et¬†al. ([2020](#bib.bib12)); Xie and Hong ([2021](#bib.bib36)) as well as differential privacy Lyu et¬†al. ([2020](#bib.bib18)).
However, until privacy can be guaranteed for embeddings, inversion attacks will continue to pose a threat, and thus require continued investigation.

Finally, to our knowledge, previous works in embedding inversion are all conducted in a monolingual setting over English Song and Raghunathan ([2020](#bib.bib31)); Lyu et¬†al. ([2020](#bib.bib18)); Hayet et¬†al. ([2022](#bib.bib10)); Parikh et¬†al. ([2022](#bib.bib24)); Kim et¬†al. ([2022](#bib.bib13)); Morris et¬†al. ([2023](#bib.bib20)); Zhou et¬†al. ([2023](#bib.bib39)); Li et¬†al. ([2023](#bib.bib16)).
This is a significant shortcoming, as it risks leaving defences for non-English languages unexplored, with implications for LLM Security for all languages.
For instance, this could lead to implementations of LLMs in non-English being considerably less secure than their English counterparts.

![Refer to caption](/html/2401.12192/assets/x1.png)

Figure 1: Overview of the method, Vec2Text¬†(Morris et¬†al., [2023](#bib.bib20)) plus Ad hoc Translation. The texts are examples of crosslingual text reconstruction evaluation. English text is evaluated on the inversion model trained on German texts. Assuming access to a target embedding eùëíe (blue), and query access to the embedder œïitalic-œï\phi (blue model) via EaaS API, the inversion model œàùúì\psi (orange) aims to iteratively generate hypothesis e^^ùëí\hat{e} (pink) to reach the target. During cross-lingual evaluation on English text with inversion model trained on German data, the generated text y^^ùë¶\hat{y} is in German, and translated to English (AdTrans(y^^ùë¶\hat{y})), to be compared with the input yùë¶y. Example input is from test data in MTG-EN.

## 3 Methodology

Text embeddings can be generated through the encoding of text using a language model, or through dedicated text vectorization techniques like Word2Vec Mikolov et¬†al. ([2013](#bib.bib19)) or GLoVE Pennington et¬†al. ([2014](#bib.bib25)).
As EaaS typically deals with embeddings of sentences or phrases, this requires us to explore this relatively more challenging setting, as opposed to word-level embeddings.
In this work, we consider a black-box embedding inversion attack scenario.
To exemplify such attacks, envision malicious actors eavesdropping on communication channels between EaaS providers and customers, ultimately gaining the access to the embeddings during the process.
While previous work has assumed that this entire process can be assumed to take place in English, we here consider the considerably more difficult setting where the underlying language is unknown.
That is, we specifically aim at multilingual embedding inversion.

### 3.1 Black-box Embedding Inversion

To formalize the attack scenario, assume that a sensitive text sequence xùë•x and a black-box encoder œïitalic-œï\phi are given.
The embedding inversion attack is then defined as using an external attacker model œàùúì\psi to recover textual input xùë•x from the embedding obtained via œï‚Äã(x)italic-œïùë•\phi(x).
However, the architecture and parameters of œïitalic-œï\phi are both inaccessible, we can solely access œïitalic-œï\phi via an EaaS API.
The attacker model œàùúì\psi is built to learn the inverse mapping œï‚àí1superscriptitalic-œï1\phi^{-1}, which we formulate as an approximation problem¬†(Li et¬†al., [2023](#bib.bib16)):

|  |  |  |  |
| --- | --- | --- | --- |
|  | œà‚Äã(œï‚Äã(x))‚âàœï‚àí1‚Äã(œï‚Äã(x))=xùúìitalic-œïùë•superscriptitalic-œï1italic-œïùë•ùë•\psi(\phi(x))\approx\phi^{-1}(\phi(x))=x |  | (1) |

Text generation models have proven to be effective in generating contextually coherent texts (Shen et¬†al., [2019](#bib.bib29)), they have a widerange of applications, such as machine translation, summarization and dialogue systems¬†(Moslem et¬†al., [2022](#bib.bib21); Sun et¬†al., [2022](#bib.bib32)). In our work, we approach the inversion attacks in the context of text generation. In this case, a generation model œàùúì\psi determines how much information can be encoded and decoded, and down the line, how well text can be constructed.
For example, if œàùúì\psi is solely pre-trained on Latin script, then Sanskrit or Cyrillic data cannot be encoded or decoded.
Hence, it is not feasible to reconstruct text in unknown scripts, and it is unexplored whether text in unknown languages can be reconstructed. In this study, we investigate text reconstruction in unknown languages but in the same script.
More specifically, how well a generation model can generalize across languages in the same script is also a determinant factor for inversion attacks.

Moreover, to recover the text from œï‚Äã(x)italic-œïùë•\phi(x), it is implicit that the data space to which xùë•x belongs is
¬†unknown.
In practice, to build an attacker model based on the eavesdropped embeddings, a training dataset Dùê∑D is used so that the attacker directly learns œàùúì\psi from pairs (œï‚Äã(y),y)italic-œïùë¶ùë¶(\phi(y),y), where y‚ààDùë¶ùê∑y\in D. Moreover, Dùê∑D has a strong impact on inversion performance. In reality, the inversion attacks are essentially cross-domain problem, since Dùê∑D most likely do not represent the data space of xùë•x.

#### Multilingual Inversion Attacks

Compared to monolingual embedding inversion, to investigate the potential and effects of multilingual inversion attacks, the complexity of experimentation scales up rapidly, as each language space of œàùúì\psi, œïitalic-œï\phi, xùë•x and Dùê∑D plays a vital role. For example, defining the investigated languages as a set L={l1,l2,‚Ä¶‚Äãln}ùêøsubscriptùëô1subscriptùëô2‚Ä¶subscriptùëôùëõL=\{l\_{1},l\_{2},\ldots l\_{n}\}. The scale of training attacker models multiplies by languages and other controlled parameters, such as maximal sequence length for text generation (cf. Section¬†[4](#S4 "4 Experimental Setup ‚Ä£ Text Embedding Inversion Attacks on Multilingual Language Models")). Moreover, the complexity of an exhaustive multilingual cross-evaluation has a complexity of O‚Äã(|L|)ùëÇùêøO(|L|), since each monolingual and multilingual model should be evaluated on all languages.

We investigate the potential of multilingual embedding inversion under the assumption that we can directly send almost unlimited queries to the black-box œïitalic-œï\phi, and obtain embeddings œï‚Äã(y)italic-œïùë¶\phi(y) for y‚ààDùë¶ùê∑y\in D. Following the same approximation approach from¬†Morris et¬†al. ([2023](#bib.bib20)), assuming that e=œï‚Äã(y)ùëíitalic-œïùë¶e=\phi(y), the search for text y^^ùë¶\hat{y} with the embeddings that closest to the target embedding eùëíe under œïitalic-œï\phi is optimized by the following formula, utilizing Cosine similarity to measure the semantic similarity in the embedding space:

|  |  |  |  |
| --- | --- | --- | --- |
|  | y^=a‚Äãr‚Äãg‚Äãmaxy‚Å°cos‚Å°(œï‚Äã(y),e)^ùë¶ùëéùëüùëîsubscriptùë¶italic-œïùë¶ùëí\hat{y}=arg\max\_{y}\cos(\phi(y),e) |  | (2) |

Specifically, as shown in Fig.¬†[1](#S2.F1 "Figure 1 ‚Ä£ 2 Related Work ‚Ä£ Text Embedding Inversion Attacks on Multilingual Language Models"), following ¬†(Morris et¬†al., [2023](#bib.bib20)), the inversion model training and inference is conditioned on the previous output, that at correction step t+1ùë°1t+1, the model takes the concatenation of previous output y^(t)superscript^ùë¶ùë°\hat{y}^{(t)} and hypothesis embedding e^(t)superscript^ùëíùë°\hat{e}^{(t)}, plus target embedding eùëíe.

We assume that the attacker (1) has access to the black-box œïitalic-œï\phi via EaaS API and also (2) has the knowledge of the language script of the input text for the target embeddings. Then multilingual embedding inversion attack is composed of the following steps: (a) build an attacker model œàùúì\psi, based on a text generation model, pre-trained on the same language scripts, ideally the same language; (b) [Attack Model Training] train œàùúì\psi by iteratively querying the black-box embedding model œïitalic-œï\phi with text y‚ààDùë¶ùê∑y\in D, and resulting in y^^ùë¶\hat{y} optimized with Eq.¬†[2](#S3.E2 "In Multilingual Inversion Attacks ‚Ä£ 3.1 Black-box Embedding Inversion ‚Ä£ 3 Methodology ‚Ä£ Text Embedding Inversion Attacks on Multilingual Language Models") (correction step 1); (c) [Inference] having tested sound generalizability of œàùúì\psi, the embeddings œï‚Äã(x)italic-œïùë•\phi(x) can be inverted, and reconstruct text xùë•x, and further steps of optimizations (correction steps >> 1) with Eq.¬†[2](#S3.E2 "In Multilingual Inversion Attacks ‚Ä£ 3.1 Black-box Embedding Inversion ‚Ä£ 3 Methodology ‚Ä£ Text Embedding Inversion Attacks on Multilingual Language Models") and implement beam search at the sequence level where a new generation is taken only when it is closer to the target embeddings compared to the previous step.

#### Ad hoc Translation

Without prior knowledge of the language lxsubscriptùëôùë•l\_{x} of target text, the language lysubscriptùëôùë¶l\_{y} of the training dataset Dùê∑D can be different from lxsubscriptùëôùë•l\_{x}, which may result in the trained inverter model decoding texts only in lysubscriptùëôùë¶l\_{y}.
However, the generated text, albeit in lysubscriptùëôùë¶l\_{y}, can convey the same information as the target text in lxsubscriptùëôùë•l\_{x}.
To investigate this aspect, as shown in Fig.¬†[1](#S2.F1 "Figure 1 ‚Ä£ 2 Related Work ‚Ä£ Text Embedding Inversion Attacks on Multilingual Language Models"), we propose a post-inversion strategy, i.e., Ad hoc Translation (AdTrans), where the generated text is translated from lysubscriptùëôùë¶l\_{y} in lxsubscriptùëôùë•l\_{x}, further the translated text is evaluated against the target text, to verify whether the inverted text in lysubscriptùëôùë¶l\_{y} leak information of the target text in lxsubscriptùëôùë•l\_{x} (cf. Section¬†[5.3](#S5.SS3 "5.3 Cross-lingual Text Reconstruction ‚Ä£ 5 Results and Analysis ‚Ä£ Text Embedding Inversion Attacks on Multilingual Language Models")).

Previous research has solely focused on inverting embeddings for English texts, taking for granted the knowledge of language of the input text from target embedding.
Our study expands on this line of research by expanding the language space of each essential components, without assuming prior knowledge of the languages.

## 4 Experimental Setup

#### Multilingual Embeddings

We leverage T5-base (Raffel et¬†al., [2023](#bib.bib28)) as our generation model, following¬†Morris et¬†al. ([2023](#bib.bib20)).
We train the multilingual inversion models œàùúì\psi on a state-of-the-art multilingual encoder œïitalic-œï\phi: multilingual-e5-base (ME5-base)222transformers: intfloat/multilingual-e5-base¬†(Wang et¬†al., [2022](#bib.bib35)), a pre-trained transformer based on XLM-R based¬†(Conneau et¬†al., [2020](#bib.bib6)), via weakly-supervised contrastive pre-training on a mixture of multilingual datasets. The model is chosen as it is one of the best performing multilingual models on the MTEB text embeddings benchmark¬†(Muennighoff et¬†al., [2023](#bib.bib22)).
Furthermore, we also reproduce the results from¬†(Morris et¬†al., [2023](#bib.bib20)) by training inversion models on GTR-base using English datasets, as our baselines.

#### Datasets

Previous research¬†(Morris et¬†al., [2023](#bib.bib20)) trains text inversion models on natural questions and question-answer pairs, such as MSMarco¬†(Bajaj et¬†al., [2018](#bib.bib1)) and Natural Questions (NQ)¬†(Kwiatkowski et¬†al., [2019](#bib.bib14)) datasets.
While these datasets are large, they are limited to English.
Thus for our experiments, we train and evaluate the multilingual inversion models on the MTG dataset, a benchmark suite specific for multilingual text generation training and evaluation¬†(Chen et¬†al., [2022](#bib.bib5)), with parallel examples across all languages.
MTG is curated from different domains, including news, daily life and Wikipedia.
In order to ensure the validity of our experiments, and test generalizability, we exclude the data curated from Wikipedia, since this domain data is used to train both the T5-base and ME5-base models.
For each language, this results in 123k passages to be used as training data. Passages refer to paragraphs or sections of a document. We obtain 3-5M sentences in each language for training data in MTG using NLTK¬†(Bird and Loper, [2004](#bib.bib2)) sentence tokenization.
This is considerably fewer samples as compared to ¬†(Morris et¬†al., [2023](#bib.bib20)), in which the GTR-base model is trained on 5M passages from NQ.333The models truncate texts into 32 tokens and 64 tokens, to evaluate how sequence length affects the performance of embeddings inversion. Each passage in NQ is significantly longer than 32 and 64 tokens. To obtain more training data samples from MTG dataset, we implement NLTK sentence tokenization on MTG dataset, which results in about 3-5M sentences for each language. 
Meanwhile, we train and evaluate on data in English, French, German and Spanish, noted as MTG-EN, MTG-FR, MTG-DE, and MTG-ES, respectively.
We also compose a 5M-sentence multilingual dataset including 1.2M sentences from each language, noted as MTG-MULTI.

#### Evaluation Metrics

We measure model performance using two types of metrics, to compare with the results from previous research¬†(Morris et¬†al., [2023](#bib.bib20)).
First, for evaluating text reconstruction, word-match metrics are used: BLEU score¬†(Post, [2018](#bib.bib26)), where n-gram similarities between the true and reconstructed text are measured; ROUGE score¬†(Lin, [2004](#bib.bib17)), where recall of overlapping words of reconstructed text is reported; Token F1, the multi-class F1 scores between the set of predicted tokens and the set of true tokens, considering each word as a class; Exact-match, the percentage of reconstructed texts matching perfectly the true texts.
Additionally, the cosine similarity between the true embedding and the embedding of the reconstructed text in the embedding space of trained œïitalic-œï\phi.
Such metrics fall short in terms of evaluating whether the semantic content, e.g., specific private information, is recovered. The limitation is particularly evident in cross-lingual settings, for example, where the generated German text conveys similar meaning as the input English text, a nuance that word-match metrics fail to capture (cf. Fig ¬†[1](#S2.F1 "Figure 1 ‚Ä£ 2 Related Work ‚Ä£ Text Embedding Inversion Attacks on Multilingual Language Models"))

#### Experiments

Following the setup from ¬†(Morris et¬†al., [2023](#bib.bib20)), there are two stages of model training for embedding inversion:
(1) Base inversion model, learning text distributions given embeddings, (2) Vec2Text corrector model, initialized with the trained Base model and training using Eq.¬†[2](#S3.E2 "In Multilingual Inversion Attacks ‚Ä£ 3.1 Black-box Embedding Inversion ‚Ä£ 3 Methodology ‚Ä£ Text Embedding Inversion Attacks on Multilingual Language Models").
To evaluate the potential of multilingual and cross-lingual embedding inversion attacks, we train Base models and Vec2Text models for each language and MTG-MULTI, and evaluate extensively in multilingual settings.
In comparison with previous research, we train and evaluate English inversion models on NQ and MTG-EN.

The Adam optimizer with a learning rate of 0.0010.0010.001 with 625 warmup steps is used.
We train each base model and corrector model for 100 epochs each.
We use a batch size of 512 for inversion models and 256 for corrector models trained on data with 32 tokens, while the batch sizes are halved for models trained on data truncated to 64 tokens, accordingly.
All models are trained on 4 AMD MI250 GPUs with distributed training.444Distributed Training with Accelerate: <https://huggingface.co/docs/transformers/accelerate>
Under these circumstances, training our slowest model takes about 8 days.

|  | #Tokens | | #Pred Tok. | | BLEU | | ROUGE | | TF1 | | Exact | | COS | |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
|  | GTR | ME5 | GTR | ME5 | GTR | ME5 | GTR | ME5 | GTR | ME5 | GTR | ME5 | GTR | ME5 |
| Base (0 Steps) | 32 | 32 | 32 | 32 | 0.2718 | 0.2877 | 0.6286 | 0.6368 | 63.74 | 65.9 | 0.4 | 0.4 | 0.8793 | 0.9738 |
| Vec2Text (1 Step) | 32 | 31 | 32 | 32 | 0.4862 | 0.4792 | 0.7839 | 0.7703 | 78.44 | 78.35 | 8 | 4.8 | 0.9210 | 0.9588 |
| (20 Steps) | 32 | 32 | 32 | 32 | 0.8330 | 0.7447 | 0.9512 | 0.8957 | 95.11 | 90.3 | 58 | 21.8 | 0.9862 | 0.9920 |
| (20 Steps) | 32 | 32 | 32 | 32 | 0.8431 | 0.7503 | 0.9549 | 0.8976 | 95.6 | 90.56 | 58.4 | 21.8 | 0.9862 | 0.9920 |
| (50 Steps + 4 sbeam) | 32 | 32 | 32 | 32 | 0.9018 | 0.7887 | 0.9726 | 0.9111 | 97.15 | 91.55 | 74.4 | 32.6 | 0.9853 | 0.9902 |
| (50 Steps + 8 sbeam) | 32 | 32 | 32 | 32 | 0.9244 | 0.8086 | 0.9776 | 0.9189 | 97.78 | 92.42 | 82 | 35 | 0.9921 | 0.9926 |
| (100 Steps) | 32 | 32 | 32 | 32 | 0.9245 | 0.8082 | 0.9775 | 0.9183 | 97.79 | 92.37 | 82 | 35 | 0.9921 | 0.9926 |
| (100 Steps + 4 sbeam) | 32 | 32 | 32 | 32 | 0.9017 | 0.7882 | 0.9725 | 0.9111 | 97.15 | 91.53 | 74.4 | 32.8 | 0.9824 | 0.9902 |
| (100 Steps + 8 sbeam) | 32 | 32 | 32 | 32 | 0.9245 | 0.8082 | 0.9775 | 0.9183 | 97.79 | 92.37 | 82 | 35 | 0.9921 | 0.9926 |

Table 1: Evaluation of English Text Reconstruction. The best performances for each model reached in the earliest stages are in bold. The underlined results are where ME5-base model outperforms GTR-base model.

## 5 Results and Analysis

### 5.1 Monolingual English Text Reconstruction

#### In-Domain

To have a proof of concept, we replicate the experiment from¬†Morris et¬†al. ([2023](#bib.bib20)), by training inversion models using GTR-base and ME5-base as embedders on the NQ dataset.
The Base and Vec2Text model with 1 correction step trained on ME5-base has a performance on par with GTR-base.
Moreover, the text embeddings trained on ME5-base are more closer in embedding space than embeddings trained on GTR-base, i.e., with higher cosine similarities.
However, with more steps of correction and beam search, the performance is boosted to 0.92440.92440.9244 in BELU score with 82%percent8282\% exact match for GTR-base model, while the best performance for ME5-base is 0.80860.80860.8086 in BLEU score with 35%percent3535\% exact match.
The performance difference could be due to the fact that the GTR-base is t5-based model, the same structure as the generation model œàùúì\psi.
However, utilizing ME5-base sets up a more realistic attack scenario of black-box embedding inversion, as the structure of the embedder œïitalic-œï\phi is unknown.

|  | NQ‚Üí‚Üí\rightarrowMTG-EN | MTG-EN‚Üí‚Üí\rightarrowNQ | MTG-MULTI‚Üí‚Üí\rightarrowNQ |
| --- | --- | --- | --- |
| GTR |  |  |  |
| Base | 0.0581 (0.7334) | - | - |
| Vec2Text | 0.3623 (0.9767) |  |  |
| ME5 |  |  |  |
| Base | 0.0589 (0.9272) | 0.0715 (0.9511) | 0.0671 (0.9553) |
| Vec2Text | 0.2119 (0.9440) | 0.1535 (0.9669) | 0.1079 (0.9708) |

Table 2: Cross-Domain English Text Reconstruction Evaluation, BLEU scores and COS are reported.
Horizontal comparison on ME5-Base models, and vertically on two embedders trained on the same NQ dataset. The Vec2Text models are evaluated by 50 steps of correction with sequence beam search width 8. The arrow ‚Üí‚Üí\rightarrow indicates the cross-domain evaluation direction. For example, NQ‚Üí‚Üí\rightarrow MTG-EN indicates that the model is trained on NQ and evaluated on MTG-EN.

#### Cross-Domain

To evaluate the performance of embedding inversion attacks on out-of-domain dataset in English, the models trained on NQ and MTG-EN datasets are cross-evaluated on both datasets, respectively, as shown in Table¬†[2](#S5.T2 "Table 2 ‚Ä£ In-Domain ‚Ä£ 5.1 Monolingual English Text Reconstruction ‚Ä£ 5 Results and Analysis ‚Ä£ Text Embedding Inversion Attacks on Multilingual Language Models"). The results on MTG-EN are similar in BLEU scores for both Base models trained on GTR-Base and ME5-Base, while GTR model outperforms ME5 by more than 0.150.150.15 in BLEU scores, and the cosine similarity of reconstructed and true text embeddings are boosted by over 0.240.240.24 . In comparison, the cosine similarity for ME5 models are not much varied and constantly high (‚â•0.92absent0.92\geq 0.92) across stages of evaluations and across domains. From the observations of both in-domain and out-of-domain English text reconstruction, with solely training the first stage of inversion model, the multilingual embeddings model yields better word-matching performances and the embeddings are closer in the embedding space. However, the adapted approximation approach Eq.¬†[2](#S3.E2 "In Multilingual Inversion Attacks ‚Ä£ 3.1 Black-box Embedding Inversion ‚Ä£ 3 Methodology ‚Ä£ Text Embedding Inversion Attacks on Multilingual Language Models") boosts performance more on monolingual embedding model.

### 5.2 Multilingual Text Reconstruction

![Refer to caption](/html/2401.12192/assets/x2.png)

Figure 2: BLEU score vs. Runtime by Evaluation for Inversion Models in English, French, German and Spanish.

|  | #Tokens | | #Pred Tok. | | BLEU | | ROUGE | | TF1 | | Exact | | COS | |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
|  | mono | multi | mono | multi | mono | multi | mono | multi | mono | multi | mono | multi | mono | multi |
| MTG-EN |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| Base (0 Steps) | 32 | 32 | 31.94 | 31.95 | 0.1157 | 0.1079 | 0.4598 | 0.4439 | 44.97 | 43.71 | 0 | 0 | 0.9381 | 0.9215 |
| Vec2Text (1 Step) | 32 | 32 | 31.95 | 31.96 | 0.183 | 0.1338 | 0.5874 | 0.4895 | 56.37 | 48.22 | 0.4 | 0.2 | 0.9236 | 0.8637 |
| (20 Steps) | 32 | 32 | 31.99 | 31.98 | 0.4148 | 0.2372 | 0.7905 | 0.6253 | 75.15 | 59.74 | 8.8 | 3 | 0.9441 | 0.8433 |
| (50 Steps) | 32 | 32 | 31.99 | 31.97 | 0.4305 | 0.2527 | 0.802 | 0.6414 | 76.29 | 61.39 | 9.4 | 3.2 | 0.9464 | 0.9296 |
| (50 Steps + 4 sbeam) | 32 | 32 | 31.99 | 31.98 | 0.4587 | 0.2989 | 0.827 | 0.6817 | 78.24 | 65.27 | 10.8 | 5 | 0.9372 | 0.9487 |
| (50 Steps + 8 sbeam) | 32 | 32 | 31.98 | 31.98 | 0.4849 | 0.3204 | 0.8351 | 0.6938 | 79.16 | 66.67 | 12 | 7.4 | 0.9277 | 0.9303 |
| (100 Steps) | 32 | - | 31.98 | - | 0.4853 | - | 0.8351 | - | 79.12 | - | 12 | - | 0.9277 | - |
| (100 Steps + 4 sbeam) | 32 | - | 31.99 | - | 0.459 | - | 0.8271 | - | 78.24 | - | 10.8 | - | 0.9372 | - |
| (100 Steps + 8 sbeam) | 32 | - | 31.98 | - | 0.4853 | - | 0.8351 | - | 79.12 | - | 12 | - | 0.9277 | - |
| MTG-FR |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| Base [0 Steps] | 32 | 32 | 32 | 32 | 0.1864 | 0.1981 | 0.5286 | 0.552 | 52.93 | 55.68 | 0 | 0.2 | 0.9408 | 0.9511 |
| Vec2Text (1 Step) | 32 | 32 | 32 | 31.98 | 0.291 | 0.2832 | 0.6358 | 0.6308 | 63.36 | 63.1 | 2.6 | 2 | 0.9655 | 0.9271 |
| (20 Steps) | 32 | 32 | 31.98 | 32 | 0.6239 | 0.5878 | 0.8412 | 0.8132 | 83.48 | 81.02 | 36 | 32 | 0.9752 | 0.9492 |
| (50 Steps) | 32 | 32 | 31.98 | 32 | 0.6404 | 0.6075 | 0.8518 | 0.8301 | 84.51 | 82.49 | 36.8 | 33 | 0.9754 | 0.9252 |
| (50 Steps + 4 sbeam) | 32 | 32 | 32 | 32 | 0.7196 | 0.6872 | 0.8829 | 0.867 | 87.91 | 86.22 | 50.4 | 45.2 | 0.9643 | 0.942 |
| (50 Steps + 8 sbeam) | 32 | 32 | 32 | 32 | 0.7454 | 0.73 | 0.8912 | 0.8938 | 88.83 | 88.84 | 54.4 | 49.6 | 0.9757 | 0.942 |
| (100 Steps) | 32 | - | 32 | - | 0.7444 | - | 0.891 | - | 88.77 | - | 54.4 | - | 0.9757 | - |
| (100 Steps + 4 sbeam ) | 32 | - | 32 | - | 0.7193 | - | 0.8826 | - | 87.89 | - | 50.4 | - | 0.9643 | - |
| (100 Steps + 8 sbeam) | 32 | - | 32 | - | 0.7444 | - | 0.891 | - | 88.77 | - | 54.4 | - | 0.9757 | - |
| MTG-DE |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| Base (0 Steps) | 32 | 32 | 32 | 31.98 | 0.133 | 0.137 | 0.4313 | 0.4524 | 44.6 | 46.14 | 0 | 0 | 0.9599 | 0.9642 |
| Vec2Text (1 step) | 32 | 32 | 31.93 | 31.98 | 0.22 | 0.1808 | 0.5555 | 0.5195 | 56 | 52.07 | 1.2 | 0.2 | 0.9699 | 0.9516 |
| (20 Steps) | 32 | 32 | 31.95 | 32 | 0.566 | 0.4137 | 0.8095 | 0.7041 | 79.84 | 69.81 | 30.2 | 16.6 | 0.9573 | 0.9232 |
| (50 Steps) | 32 | 32 | 31.95 | 32 | 0.5736 | 0.4359 | 0.8233 | 0.7228 | 81.4 | 71.54 | 30.4 | 17.4 | 0.9687 | 0.9278 |
| (50 Steps + 4 sbeam) | 32 | 32 | 31.98 | 31.98 | 0.6579 | 0.5248 | 0.8584 | 0.767 | 84.56 | 75.75 | 42.4 | 28.2 | 0.9778 | 0.9321 |
| (50 Steps + 8 sbeam) | 32 | 32 | 32 | 32 | 0.695 | 0.5408 | 0.878 | 0.7757 | 86.46 | 76.44 | 47.4 | 29.6 | 0.9671 | 0.9646 |
| (100 Steps) | 32 | - | 32 | - | 0.6955 | - | 0.878 | - | 86.47 | - | 47.4 | - | 0.9791 | - |
| (100 Steps + 4 sbeam) | 32 | - | 31.98 | - | 0.6561 | - | 0.8573 | - | 84.46 | - | 42.2 | - | 0.9778 | - |
| (100 Steps + 8 sbeam) | 32 | - | 32 | - | 0.6955 | - | 0.878 | - | 86.47 | - | 47.4 | - | 0.9791 | - |
| MTG-ES |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| Base (0 steps) | 32 | 32 | 31.95 | 32 | 0.2321 | 0.2709 | 0.5515 | 0.6054 | 56.75 | 62.07 | 1.6 | 1.8 | 0.938 | 0.9501 |
| Vec2Text (1 step) | 32 | 32 | 32 | 32 | 0.3518 | 0.3692 | 0.6621 | 0.6804 | 67.76 | 68.92 | 8 | 9.6 | 0.9549 | 0.9423 |
| (20 Steps) | 32 | 32 | 32 | 32 | 0.6661 | 0.6443 | 0.8559 | 0.8461 | 85.78 | 84.73 | 44.8 | 38.4 | 0.9632 | 0.9563 |
| (50 Steps) | 32 | 32 | 32 | 32 | 0.6785 | 0.6593 | 0.8661 | 0.8525 | 86.67 | 85.46 | 45.4 | 38.8 | 0.9697 | 0.9582 |
| (50 Steps + 4 sbeam) | 32 | 32 | 32 | 32 | 0.7729 | 0.7452 | 0.9041 | 0.8945 | 90.47 | 89.23 | 60.8 | 53.6 | 0.9697 | 0.9515 |
| (50 Steps + 8 sbeam) | 32 | 32 | 32 | 32 | 0.8002 | 0.7772 | 0.9134 | 0.9072 | 91.54 | 90.44 | 65 | 56.8 | 0.9579 | 0.987 |
| (100 Steps) | 32 | - | 32 | - | 0.7996 | - | 0.9121 | - | 91.43 | - | 65 | - | 0.9579 | - |
| (100 Steps + 4 sbeam) | 32 | - | 32 | - | 0.7748 | - | 0.9052 | - | 90.56 | - | 60.8 | - | 0.9697 | - |
| (100 Steps + 8 sbeam) | 32 | - | 32 | - | 0.7996 | - | 0.9121 | - | 91.43 | - | 65 | - | 0.9579 | - |

Table 3:  mono represents the evaluation of Text Reconstruction in multiple languages, with the models trained and evaluated on MTG datasets with tokens length 32 in English, French, German and Spanish, respectively. multi represents the evaluation of multilingual text reconstruction, with models trained on MTG-MULTI and evaluated on MTG datasets with tokens length 32 in English, French, German and Spanish, respectively. The best results reached in the earliest stage for each language across metrics are in bold. The results where multi outperforms mono is underlined.

To explore the potential of multilingual embedding inversion, we train ME5-base embedder on MTG datasets in English, German and French, Spanish, noted as me5\_mtg-en, me5\_mtg-fr, me5\_mtg-de and me5\_mtg-es, respectively, and the composed multilingual dataset of all four languages, noted as me5\_mtg-multi, and tested on each language for both experimental settings. The results are shown in Table¬†[3](#S5.T3 "Table 3 ‚Ä£ 5.2 Multilingual Text Reconstruction ‚Ä£ 5 Results and Analysis ‚Ä£ Text Embedding Inversion Attacks on Multilingual Language Models").

#### Monolingual Text Reconstruction in Multiple Languages

For monolingual models, we evaluate on Base and Vec2Text models with correction steps of 1, 20, 50, 100 combined with 4 and 8 sequence beam search width (sbeam).
We can observe that the BLEU score for each language peaks either by 50 steps correction with 8 sbeam or 100 steps.
This evaluation is expensive in terms of time and computation.
In order to search for the optimal runtime and performance trade-off, Fig.¬†[2](#S5.F2 "Figure 2 ‚Ä£ 5.2 Multilingual Text Reconstruction ‚Ä£ 5 Results and Analysis ‚Ä£ Text Embedding Inversion Attacks on Multilingual Language Models") shows BLEU scores at each step and the lines represent the trend for runtime for the monolingual models.
The best trade-off points are at the correction step of 50 with 8 sbeam for all the models, while 100 steps takes more than double the time achieving similar performance.
Until correction step 50 with 8 sbeam, performance increases steadily, and the trend is generally aligned with cosine similarity.
As a result, we evaluate the subsequent models until correction step 50 with 8 sbeam.

Moreover, Spanish models outperform the others in terms of the word-match metrics across correction steps, achieving 0.80020.80020.8002 in BLEU score with 65%percent6565\% of exact match. Despite having a larger volume of data compared to other languages, the English model unexpectedly performs the worst across various metrics, as illustrated by the training data distribution in Fig.¬†[3](#A1.F3 "Figure 3 ‚Ä£ Appendix A Training Data Distribution ‚Ä£ Text Embedding Inversion Attacks on Multilingual Language Models"). However, as shown in Appendix¬†[B](#A2 "Appendix B The Effect of Translationese on Test Data ‚Ä£ Text Embedding Inversion Attacks on Multilingual Language Models"), the evaluation of round-trip translated English test data indicates no evidence of translationese effect.

#### Multilingual Text Reconstruction Without prior Knowledge of Language

To evaluate the potential of multilingual text inversion without prior knowledge of the language in which a target text is written, we train inversion models on MTG-MULTI dataset.
As shown in Table¬†[3](#S5.T3 "Table 3 ‚Ä£ 5.2 Multilingual Text Reconstruction ‚Ä£ 5 Results and Analysis ‚Ä£ Text Embedding Inversion Attacks on Multilingual Language Models"), me5\_mtg-multi Base model outperforms (underlined) or has on par performance with monolingual Base models across languages. Overall, the performance does not deteriorate in proportion of the monolingual training data volume, i.e., for MTG-MULTI, each language has a quarter of the data volume compared to its monolingual counterpart. Rather, the performances are comparable, especially for French and Spanish, with 0.01540.01540.0154 and 0.0230.0230.023 differences, respectively. For Spanish, me5\_mtg-multi performs slightly better in word-match metrics than me5\_mtg-es also for Vec2Text model by 1 step correction. Across languages, the cosine similarities of the multilingual model are constantly higher than their monolingual counterparts, with more correction steps, they worsened for French, i.e., 0.9420.9420.942 compared to 0.95110.95110.9511.

Additionally, to evaluate the out-of-domain performance, me5\_mtg-multi is tested on NQ dataset.
As shown in Table¬†[2](#S5.T2 "Table 2 ‚Ä£ In-Domain ‚Ä£ 5.1 Monolingual English Text Reconstruction ‚Ä£ 5 Results and Analysis ‚Ä£ Text Embedding Inversion Attacks on Multilingual Language Models"), for Base models, ME5-Base trained on multilingual data has better performance than trained on NQ dataset, while it fall short compared to its monolingual counterpart.
Its Vec2Text model under-performs compared to others.

These phenomena indicate that (i) the high monolingual data volume is not the determining factor on training a high-performing Base model and Vec2Text models without the extra correction steps in both monolingual and multilingual settings, (ii) the multilingual model training renders closer embeddings of reconstructed and target texts in the embedding space, and (iii) the optimization approach utilizing cosine similarity is not as effective for multilingual models compared to monolingual models.

|  |  |  |  |
| --- | --- | --- | --- |
|  | MTG-FR | MTG-DE | MTG-ES |
| GTR-Base |  |  |  |
| Base | 0.0439 (0.7581) | 0.0322 (0.7052) | 0.0474 (0.7134) |
| Vec2Text | 0.0994(0.8833) | 0.0575 (0.8138) | 0.1012 (0.9020) |
| AdTrans | 0.0928 (‚Üì‚Üì\downarrow-6.59) | 0.0518 (‚Üì‚Üì\downarrow-9.82) | 0.0995 (‚Üì‚Üì\downarrow-1.67) |
| ME5-Base |  |  |  |
| Base | 0.0313 (0.9513) | 0.0273 (0.9298) | 0.0364 (0.9293) |
| Vec2Text | 0.0449 (0.9487) | 0.0299 (0.9107) | 0.0392 (0.8963) |
| AdTrans | 0.1017 (‚Üë‚Üë\uparrow126.58) | 0.0895 (‚Üë‚Üë\uparrow128.11) | 0.0469 (‚Üë‚Üë\uparrow56.57) |

(a) Cross-lingual cross-domain evaluation with monolingual models trained on NQ.

| ‚Üí‚Üí\rightarrowNQ | me5\_mtg-fr | me5\_mtg-de | me5\_mtg-es |
| --- | --- | --- | --- |
| Base | 0.0180 (0.9399) | 0.0182 (0.9016) | 0.0141 (0.9178) |
| Vec2Text | 0.0207 (0.9467) | 0.0231 (0.9248) | 0.0181 (0.9253) |
| AdTrans | 0.0418 (‚Üë‚Üë\uparrow103.48) | 0.0508 (‚Üë‚Üë\uparrow119.9) | 0.0356 (‚Üë‚Üë\uparrow91.28) |

(b) Cross-lingual cross-domain evaluation on NQ with monolingual models trained on MTG datasets.

|  | MTG-EN | MTG-FR | MTG-DE | MTG-ES |
| --- | --- | --- | --- | --- |
| me5\_mtg-en |  |  |  |  |
| Base | - | 0.032 (0.9132) | 0.0371 (0.8945) | 0.031 (0.9068) |
| Vec2Text | - | 0.0462 (0.9421) | 0.0561 (0.9474) | 0.0433 (0.911) |
| AdTrans | - | 0.124 (‚Üë‚Üë\uparrow168.08) | 0.0672 (‚Üë‚Üë\uparrow19.75) | 0.1238 (‚Üë‚Üë\uparrow185.79) |
| me5\_mtg-fr |  |  |  |  |
| Base | 0.033 (0.9176) | - | 0.0297 (0.9038) | 0.0452 (0.9206) |
| Vec2Text | 0.0536 (0.9235) | - | 0.0426 (0.9431) | 0.0594 (0.9241) |
| AdTrans | 0.0725 (‚Üë‚Üë\uparrow37.71) | - | 0.0635 (‚Üë‚Üë\uparrow49.47) | 0.137 (‚Üë‚Üë\uparrow126.79) |
| me5\_mtg-de |  |  |  |  |
| Base | 0.0399 (0.8902) | 0.0296 (0.9082)) | - | 0.0273 (0.9224) |
| Vec2Text | 0.0813 (0.9223) | 0.0454 (0.9223) | - | 0.0461 (0.9163) |
| AdTrans | 0.0961 (‚Üë‚Üë\uparrow18.19) | 0.1037 (‚Üë‚Üë\uparrow128.62) | - | 0.1101 (‚Üë‚Üë\uparrow138.91) |
| me5\_mtg-es |  |  |  |  |
| Base | 0.0331 (0.9186) | 0.0396 (0.9035) | 0.0267 (0.8958) | - |
| Vec2Text | 0.0471 (0.9223) | 0.0513 (0.8699) | 0.0397 (0.9460) | - |
| AdTrans | 0.0591 (‚Üë‚Üë\uparrow25.51) | 0.0957 (‚Üë‚Üë\uparrow86.56) | 0.0556 (‚Üë‚Üë\uparrow39.89) | - |

(c) Cross-lingual evaluation on monolingual inversion models trained and test on MTG datasets.

Table 4: Cross-lingual evaluation using BLEU score and Cosine Similarity (in the brackets) for Base and Vec2Text models by correction steps of 50 with 8 sbeam. The BLEU scores and their percentage growth (in the brackets) compared with BLEU scores on Vec2Text models are reported for AdTrans strategy for each model. The up-arrows ‚Üë‚Üë\uparrow indicate performance gain while the down-arrows ‚Üì‚Üì\downarrow indicate performance loss. The result with the highest BLEU score with each evaluated model on each dataset is in bold.

### 5.3 Cross-lingual Text Reconstruction

Cross-lingual text reconstruction is a specific case of multilingual text reconstruction without prior knowledge of languages and also a more realistic scenario, where the embedder œïitalic-œï\phi is trained on a different language with regarding to the target text language. To investigate the potential of this scenario, we conduct cross-lingual evaluation on all the monolingual models we trained on NQ and MTG datasets, the results are reported in Table¬†[4](#S5.T4 "Table 4 ‚Ä£ Multilingual Text Reconstruction Without prior Knowledge of Language ‚Ä£ 5.2 Multilingual Text Reconstruction ‚Ä£ 5 Results and Analysis ‚Ä£ Text Embedding Inversion Attacks on Multilingual Language Models"), with regard to the type of models.

We observe that, ME5-Base models trained on both NQ and MTG-datasets, have a tendency to decode texts, for example x^^ùë•\hat{x}, in the language of training data, e.g., lysubscriptùëôùë¶l\_{y}, given the target text xùë•x which is in a different language, e.g., lxsubscriptùëôùë•l\_{x}. However, x^^ùë•\hat{x} could expose the same information just in a different language, then the current word-match metrics are not able to capture this phenomenon. Nonetheless, the privacy leakage still exists.

For example, evaluating me5\_mtg-de model on MTG-EN dataset,  Report: Trump einmal fragte damals FBI Director Andrew Mccabe w√§hrend seiner 2016-vote is generated given the target text Report: Trump once asked then-acting FBI director Andrew Mccabe about his 2016 vote. The generated text mistook ‚Äúw√§hrend‚Äù (during) for ‚Äúabout‚Äù, otherwise, the generated text is close in meaning with the target English text. The information leakage would not be properly captured with the current metrics evaluated on the German texts. We use AdTrans strategy with EasyNMT¬†555<https://github.com/UKPLab/EasyNMT>, the generated German text is translated in English, i.e., Report: Trump once asked FBI director Andrew Mccabe during his 2016-vote, in which case information is missing such as ‚Äúthen-acting‚Äù, but BLEU score is up to 0.5 for this individual example. We implement this strategy for all cross-lingual evaluation. As shown in Table¬†[2](#S5.T2 "Table 2 ‚Ä£ In-Domain ‚Ä£ 5.1 Monolingual English Text Reconstruction ‚Ä£ 5 Results and Analysis ‚Ä£ Text Embedding Inversion Attacks on Multilingual Language Models"), other than for GTR-Base model, the performances are lifted across monolingual and multilingual models for each language.

### 5.4 Cross-lingual Defense Mechanism

As the underlying architecture is based on Morris et¬†al. ([2023](#bib.bib20)), we point the reader to their paper for a defence mechanism.

## 6 Conclusion

While all previous work on embedding inversion attacks has focused solely on English, we present the first work on multilingual embedding inversion.
By defining the problem of black-box multilingual and cross-lingual inversion attacks, we lay the foundation for future work in this direction.
As one of our core findings is the fact that multilingual models, in some circumstances, are more vulnerable than monolingual English models, we hope that this work inspires a multilingual approach to LLM security and NLP security as a whole.

## Limitations

A core limitation of this work is the computationally intense experiments, requiring in the area of 20,000 GPU computing hours.
While expanding this research direction to more languages will further increase this expense, we advocate for ensuring that languages other than English are not left behind in terms of NLP security.

## Ethics Statement

This work explores attacks on multilingual embedding models.
Our intent with this research is to shed light on the vulnerabilities of languages other than English, aiming to encourage the community to include such languages in NLP security work.
While there is potential for misuse by malicious actors, as with many works in NLP security, we attempt to mitigate harm by including a brief pointer to a countermeasure to the attack in the paper.
Moreover, the language models examined in this paper are open-source models, and thus this work does not constitute an imminent threat to embedding-as-a-service providers, who are likely using private models.
Moreover, we do not experiment with truly sensitive data, ensuring that no real-world harm is caused by the work carried out in this paper.

## Acknowledgements

All authors of this paper are funded by the Carlsberg Foundation, under the Semper Ardens: Accelerate programme (project nr.¬†CF21-0454).
We are furthermore grateful to the support of the AAU AI Cloud, and to DeiC for allocating us computing resources on the LUMI cluster (project nr.¬†DeiC-AAU-S5-412301).
We thank Sighvatur Sveinn Davidsson for setting us up with this access, and for his diligence in assisting with the problems in the experimental infrastructure, in addition to the LUMI user support for their very prompt answers and competence, especially Jing Gong.
We further thank Esther Ploeger for her assistance in testing translationese effect for the under-performance of multilingual inversion model in English and Marcell Richard¬†Fekete for his insightful input in proof-reading the paper.

## References

* Bajaj et¬†al. (2018)

  Payal Bajaj, Daniel Campos, Nick Craswell, Li¬†Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. 2018.
  [Ms marco: A human generated machine reading comprehension dataset](http://arxiv.org/abs/1611.09268).
* Bird and Loper (2004)

  Steven Bird and Edward Loper. 2004.
  [NLTK: The natural language toolkit](https://aclanthology.org/P04-3031).
  In *Proceedings of the ACL Interactive Poster and Demonstration Sessions*, pages 214‚Äì217, Barcelona, Spain. Association for Computational Linguistics.
* Carlini et¬†al. (2018)

  Nicholas Carlini, Chang Liu, Jernej Kos, √ölfar Erlingsson, and Dawn Song. 2018.
  [The secret sharer: Measuring unintended neural network memorization & extracting secrets](http://arxiv.org/abs/1802.08232).
  *CoRR*, abs/1802.08232.
* Carlini et¬†al. (2020)

  Nicholas Carlini, Florian Tram√®r, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom¬†B. Brown, Dawn Song, √ölfar Erlingsson, Alina Oprea, and Colin Raffel. 2020.
  [Extracting training data from large language models](http://arxiv.org/abs/2012.07805).
  *CoRR*, abs/2012.07805.
* Chen et¬†al. (2022)

  Yiran Chen, Zhenqiao Song, Xianze Wu, Danqing Wang, Jingjing Xu, Jiaze Chen, Hao Zhou, and Lei Li. 2022.
  [MTG: A benchmark suite for multilingual text generation](https://doi.org/10.18653/v1/2022.findings-naacl.192).
  In *Findings of the Association for Computational Linguistics: NAACL 2022*, pages 2508‚Äì2527, Seattle, United States. Association for Computational Linguistics.
* Conneau et¬†al. (2020)

  Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm√°n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020.
  [Unsupervised cross-lingual representation learning at scale](http://arxiv.org/abs/1911.02116).
* Deng et¬†al. (2021)

  Jieren Deng, Yijue Wang, Ji¬†Li, Chenghong Wang, Chao Shang, Hang Liu, Sanguthevar Rajasekaran, and Caiwen Ding. 2021.
  [TAG: Gradient attack on transformer-based language models](https://doi.org/10.18653/v1/2021.findings-emnlp.305).
  In *Findings of the Association for Computational Linguistics: EMNLP 2021*, pages 3600‚Äì3610, Punta Cana, Dominican Republic. Association for Computational Linguistics.
* Fredrikson et¬†al. (2015)

  Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. 2015.
  [Model inversion attacks that exploit confidence information and basic countermeasures](https://doi.org/10.1145/2810103.2813677).
  In *Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security*, CCS ‚Äô15, page 1322‚Äì1333, New York, NY, USA. Association for Computing Machinery.
* Fredrikson et¬†al. (2014)

  Matthew Fredrikson, Eric Lantz, Somesh Jha, Simon Lin, David Page, and Thomas Ristenpart. 2014.
  Privacy in pharmacogenetics: An end-to-end case study of personalized warfarin dosing.
  In *Proceedings of the 23rd USENIX Conference on Security Symposium*, SEC‚Äô14, page 17‚Äì32, USA. USENIX Association.
* Hayet et¬†al. (2022)

  Ishrak Hayet, Zijun Yao, and Bo¬†Luo. 2022.
  [Invernet: An inversion attack framework to infer fine-tuning datasets through word embeddings](https://doi.org/10.18653/v1/2022.findings-emnlp.368).
  In *Findings of the Association for Computational Linguistics: EMNLP 2022*, pages 5009‚Äì5018, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.
* H√∂hmann et¬†al. (2021)

  Johannes H√∂hmann, Achim Rettinger, and Kai Kugler. 2021.
  [Invbert: Text reconstruction from contextualized embeddings used for derived text formats of literary works](http://arxiv.org/abs/2109.10104).
  *CoRR*, abs/2109.10104.
* Huang et¬†al. (2020)

  Yangsibo Huang, Zhao Song, Danqi Chen, Kai Li, and Sanjeev Arora. 2020.
  [TextHide: Tackling data privacy in language understanding tasks](https://doi.org/10.18653/v1/2020.findings-emnlp.123).
  In *Findings of the Association for Computational Linguistics: EMNLP 2020*, pages 1368‚Äì1382, Online. Association for Computational Linguistics.
* Kim et¬†al. (2022)

  Donggyu Kim, Garam Lee, and Sungwoo Oh. 2022.
  [Toward privacy-preserving text embedding similarity with homomorphic encryption](https://doi.org/10.18653/v1/2022.finnlp-1.4).
  In *Proceedings of the Fourth Workshop on Financial Technology and Natural Language Processing (FinNLP)*, pages 25‚Äì36, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.
* Kwiatkowski et¬†al. (2019)

  Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew¬†M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.
  [Natural questions: A benchmark for question answering research](https://doi.org/10.1162/tacl_a_00276).
  *Transactions of the Association for Computational Linguistics*, 7:452‚Äì466.
* Li et¬†al. (2022)

  Haoran Li, Yangqiu Song, and Lixin Fan. 2022.
  [You don‚Äôt know my favorite color: Preventing dialogue representations from revealing speakers‚Äô private personas](https://doi.org/10.18653/v1/2022.naacl-main.429).
  In *Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies*, pages 5858‚Äì5870, Seattle, United States. Association for Computational Linguistics.
* Li et¬†al. (2023)

  Haoran Li, Mingshi Xu, and Yangqiu Song. 2023.
  [Sentence embedding leaks more information than you expect: Generative embedding inversion attack to recover the whole sentence](https://doi.org/10.18653/v1/2023.findings-acl.881).
  In *Findings of the Association for Computational Linguistics: ACL 2023*, pages 14022‚Äì14040, Toronto, Canada. Association for Computational Linguistics.
* Lin (2004)

  Chin-Yew Lin. 2004.
  [ROUGE: A package for automatic evaluation of summaries](https://aclanthology.org/W04-1013).
  In *Text Summarization Branches Out*, pages 74‚Äì81, Barcelona, Spain. Association for Computational Linguistics.
* Lyu et¬†al. (2020)

  L.¬†Lyu, Xuanli He, and Yitong Li. 2020.
  [Differentially private representation for nlp: Formal guarantee and an empirical study on privacy and fairness](https://api.semanticscholar.org/CorpusID:222134003).
  *ArXiv*, abs/2010.01285.
* Mikolov et¬†al. (2013)

  Tomas Mikolov, Kai Chen, Gregory¬†S. Corrado, and Jeffrey Dean. 2013.
  [Efficient estimation of word representations in vector space](https://api.semanticscholar.org/CorpusID:5959482).
  In *International Conference on Learning Representations*.
* Morris et¬†al. (2023)

  John Morris, Volodymyr Kuleshov, Vitaly Shmatikov, and Alexander Rush. 2023.
  [Text embeddings reveal (almost) as much as text](https://doi.org/10.18653/v1/2023.emnlp-main.765).
  In *Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing*, pages 12448‚Äì12460, Singapore. Association for Computational Linguistics.
* Moslem et¬†al. (2022)

  Yasmin Moslem, Rejwanul Haque, John Kelleher, and Andy Way. 2022.
  [Domain-specific text generation for machine translation](https://aclanthology.org/2022.amta-research.2).
  In *Proceedings of the 15th biennial conference of the Association for Machine Translation in the Americas (Volume 1: Research Track)*, pages 14‚Äì30, Orlando, USA. Association for Machine Translation in the Americas.
* Muennighoff et¬†al. (2023)

  Niklas Muennighoff, Nouamane Tazi, Lo√Øc Magne, and Nils Reimers. 2023.
  [Mteb: Massive text embedding benchmark](http://arxiv.org/abs/2210.07316).
* Nasr et¬†al. (2019)

  Milad Nasr, Reza Shokri, and Amir Houmansadr. 2019.
  [Comprehensive privacy analysis of deep learning: Passive and active white-box inference attacks against centralized and federated learning](https://doi.org/10.1109/sp.2019.00065).
  In *2019 IEEE Symposium on Security and Privacy (SP)*. IEEE.
* Parikh et¬†al. (2022)

  Rahil Parikh, Christophe Dupuy, and Rahul Gupta. 2022.
  [Canary extraction in natural language understanding models](https://doi.org/10.18653/v1/2022.acl-short.61).
  In *Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)*, pages 552‚Äì560, Dublin, Ireland. Association for Computational Linguistics.
* Pennington et¬†al. (2014)

  Jeffrey Pennington, Richard Socher, and Christopher¬†D. Manning. 2014.
  [Glove: Global vectors for word representation](https://api.semanticscholar.org/CorpusID:1957433).
  In *Conference on Empirical Methods in Natural Language Processing*.
* Post (2018)

  Matt Post. 2018.
  [A call for clarity in reporting BLEU scores](https://doi.org/10.18653/v1/W18-6319).
  In *Proceedings of the Third Conference on Machine Translation: Research Papers*, pages 186‚Äì191, Brussels, Belgium. Association for Computational Linguistics.
* Radford et¬†al. (2019)

  Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019.
  [Language models are unsupervised multitask learners](https://api.semanticscholar.org/CorpusID:160025533).
* Raffel et¬†al. (2023)

  Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter¬†J. Liu. 2023.
  [Exploring the limits of transfer learning with a unified text-to-text transformer](http://arxiv.org/abs/1910.10683).
* Shen et¬†al. (2019)

  Dinghan Shen, Asli Celikyilmaz, Yizhe Zhang, Liqun Chen, Xin Wang, Jianfeng Gao, and Lawrence Carin. 2019.
  Towards generating long and coherent text with multi-level latent variable models.
  *arXiv preprint arXiv:1902.00154*.
* Shokri et¬†al. (2016)

  Reza Shokri, Marco Stronati, and Vitaly Shmatikov. 2016.
  [Membership inference attacks against machine learning models](http://arxiv.org/abs/1610.05820).
  *CoRR*, abs/1610.05820.
* Song and Raghunathan (2020)

  Congzheng Song and Ananth Raghunathan. 2020.
  [Information leakage in embedding models](https://doi.org/10.1145/3372297.3417270).
  In *Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security*, CCS ‚Äô20, page 377‚Äì390, New York, NY, USA. Association for Computing Machinery.
* Sun et¬†al. (2022)

  Xiaofei Sun, Zijun Sun, Yuxian Meng, Jiwei Li, and Chun Fan. 2022.
  [Summarize, outline, and elaborate: Long-text generation via hierarchical supervision from extractive summaries](https://aclanthology.org/2022.coling-1.556).
  In *Proceedings of the 29th International Conference on Computational Linguistics*, pages 6392‚Äì6402, Gyeongju, Republic of Korea. International Committee on Computational Linguistics.
* Tsymboi et¬†al. (2023)

  Olga Tsymboi, Danil Malaev, Andrei Petrovskii, and Ivan Oseledets. 2023.
  [Layerwise universal adversarial attack on NLP models](https://doi.org/10.18653/v1/2023.findings-acl.10).
  In *Findings of the Association for Computational Linguistics: ACL 2023*, pages 129‚Äì143, Toronto, Canada. Association for Computational Linguistics.
* Wallace et¬†al. (2019)

  Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. 2019.
  [Universal adversarial triggers for attacking and analyzing NLP](https://doi.org/10.18653/v1/D19-1221).
  In *Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)*, pages 2153‚Äì2162, Hong Kong, China. Association for Computational Linguistics.
* Wang et¬†al. (2022)

  Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2022.
  [Text embeddings by weakly-supervised contrastive pre-training](http://arxiv.org/abs/2212.03533).
* Xie and Hong (2021)

  Shangyu Xie and Yuan Hong. 2021.
  [Reconstruction attack on instance encoding for language understanding](https://doi.org/10.18653/v1/2021.emnlp-main.154).
  In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing*, pages 2038‚Äì2044, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.
* Zhang and Toral (2019)

  Mike Zhang and Antonio Toral. 2019.
  [The effect of translationese in machine translation test sets](https://doi.org/10.18653/v1/W19-5208).
  In *Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers)*, pages 73‚Äì81, Florence, Italy. Association for Computational Linguistics.
* Zhang et¬†al. (2023)

  Zhuo Zhang, Yuanhang Yang, Yong Dai, Qifan Wang, Yue Yu, Lizhen Qu, and Zenglin Xu. 2023.
  [FedPETuning: When federated learning meets the parameter-efficient tuning methods of pre-trained language models](https://doi.org/10.18653/v1/2023.findings-acl.632).
  In *Findings of the Association for Computational Linguistics: ACL 2023*, pages 9963‚Äì9977, Toronto, Canada. Association for Computational Linguistics.
* Zhou et¬†al. (2023)

  Xin Zhou, Yi¬†Lu, Ruotian Ma, Tao Gui, Yuran Wang, Yong Ding, Yibo Zhang, Qi¬†Zhang, and Xuanjing Huang. 2023.
  [TextObfuscator: Making pre-trained language model a privacy protector via obfuscating word representations](https://doi.org/10.18653/v1/2023.findings-acl.337).
  In *Findings of the Association for Computational Linguistics: ACL 2023*, pages 5459‚Äì5473, Toronto, Canada. Association for Computational Linguistics.
* Zhu et¬†al. (2019)

  Ligeng Zhu, Zhijian Liu, and Song Han. 2019.
  [Deep leakage from gradients](https://proceedings.neurips.cc/paper_files/paper/2019/file/60a6c4002cc7b29142def8871531281a-Paper.pdf).
  In *Advances in Neural Information Processing Systems*, volume¬†32. Curran Associates, Inc.

## Appendix A Training Data Distribution

![Refer to caption](/html/2401.12192/assets/x3.png)

Figure 3: The Distribution of the training data for models with the maximal token length of 32.

In MTG datasets, English texts are curated from various sources, while texts in German, Spanish and French are machine translated and manually validated. The languages have diverse morphologies, resulting in different lengths of sentences and the number of sentences after sentence tokenization across languages. NQ dataset is included to replicate results from previous work¬†(Morris et¬†al., [2023](#bib.bib20)), and evaluate cross-domain and cross-lingual performance of text reconstruction task. NQ has huge amount of data for English only, and no tokenization has been implemented on the included Wikipedia passages, hence all the training data from NQ has 32 tokens.

## Appendix B The Effect of Translationese on Test Data

The effect of translationese in machine translation has been extensively studied, and there is a clear evidence that the use of translationese in test sets results in inflated human evaluation scores for MT systems ¬†(Zhang and Toral, [2019](#bib.bib37)). To investigate whether our multilingual inversion model‚Äôs sub-par performance in English is due to the characteristics of translationese in other languages, we implement round trip translation on MTG-EN test data using Spanish as the pivot language with EasyNMT, the translation path is thus English ‚Üí‚Üí\rightarrow Spanish ‚Üí‚Üí\rightarrow English. Then the evaluation of the multilingual inversion model is done on the round-trip translated English test set, the result is shown as in Table¬†[5](#A2.T5 "Table 5 ‚Ä£ Appendix B The Effect of Translationese on Test Data ‚Ä£ Text Embedding Inversion Attacks on Multilingual Language Models"). Compared to evaluation on MTG-EN test set, as shown in Table¬†[3](#S5.T3 "Table 3 ‚Ä£ 5.2 Multilingual Text Reconstruction ‚Ä£ 5 Results and Analysis ‚Ä£ Text Embedding Inversion Attacks on Multilingual Language Models"), the performance of translated English test set is about 0.3 worse at each stage of corrections.
The hypothesis of the translationese effect on the difference of the performances can therefore be rejected.

|  | #Tokens | #Pred Tok. | BLEU | ROUGE | TF1 | EXACT | COS |
| --- | --- | --- | --- | --- | --- | --- | --- |
| Vec2Text (1 Step) | 29.59 | 30.98 | 0.1003 | 0.4754 | 41.28 | 0.0 | 0.9046 |
| (20 Steps) | 29.59 | 30.95 | 0.1448 | 0.5514 | 47.80 | 0.2 | 0.9130 |
| (50 Steps) | 29.59 | 30.98 | 0.1511 | 0.5601 | 48.56 | 0.2 | 0.9261 |
| (50 Steps + 4 sbeam ) | 29.59 | 30.88 | 0.1756 | 0.6181 | 52.64 | 0.2 | 0.9461 |
| (50 Steps + 8sbeam) | 29.59 | 30.96 | 0.1742 | 0.6128 | 52.44 | 0.4 | 0.9185 |

Table 5: Evaluation of multilingual inversion model on round-trip translated MTG-EN test dataset.

## Appendix C Text Construction on Tokens Length 64

|  | #Tokens | #Pred Tokens | BLEU | ROUGE | TF1 | Exact | COS |
| --- | --- | --- | --- | --- | --- | --- | --- |
| English |  |  |  |  |  |  |  |
| Vec2Text (1 Step) | 37.78 | 43.73 | 0.1813 | 0.5933 | 57.28 | 0.8 | 87.94 |
| (20 Steps) | 37.78 | 41.32 | 0.3848 | 0.7838 | 74.23 | 10.0 | 88.75 |
| (50 Steps) | 37.78 | 40.97 | 0.3927 | 0.7974 | 75.40 | 10.2 | 92.70 |
| (50 Steps + 4 sbeam) | 37.78 | 40.67 | 0.4523 | 0.8168 | 77.31 | 14.6 | 89.18 |
| (50 Steps + 8 sbeam) | 37.78 | 40.19 | 0.4729 | 0.8334 | 78.62 | 16.6 | 91.09 |
| French |  |  |  |  |  |  |  |
| Vec2Text (1 Step) | 51.61 | 57.23 | 0.2645 | 0.6358 | 64.03 | 0.8 | 95.07 |
| (20 Steps) | 51.61 | 53.25 | 0.5825 | 0.8310 | 83.01 | 26.6 | 96.54 |
| (50 Steps) | 51.61 | 52.60 | 0.5958 | 0.8399 | 83.69 | 26.8 | 96.26 |
| (50 Steps + 4 sbeam) | 51.61 | 52.62 | 0.6461 | 0.8611 | 86.03 | 37.8 | 97.26 |
| (50 Steps + 8 sbeam) | 51.61 | 52.54 | 0.6680 | 0.8674 | 86.44 | 41.8 | 93.83 |
| German |  |  |  |  |  |  |  |
| Vec2Text(1 Step) | 49.75 | 56.09 | 0.1965 | 0.5458 | 55.19 | 0.2 | 97.43 |
| (20 Steps) | 49.75 | 52.62 | 0.4611 | 0.7610 | 75.30 | 15.6 | 93.98 |
| (50 Steps) | 49.75 | 52.76 | 0.4661 | 0.7669 | 75.86 | 15.8 | 95.72 |
| (50 Steps + 4 sbeam) | 49.75 | 51.91 | 0.5278 | 0.7960 | 78.93 | 25.6 | 92.98 |
| (50 Steps + 8 sbeam) | 49.75 | 51.82 | 0.5573 | 0.8087 | 80.21 | 30.8 | 94.97 |
| Spanish |  |  |  |  |  |  |  |
| Vec2Text(1 Step) | 62.66 | 62 | 0.2603 | 0.6416 | 65.78 | 0.4 | 97.57 |
| (20 Steps) | 62.66 | 62.23 | 0.5607 | 0.8353 | 83.70 | 17.4 | 98.28 |
| (50 Steps) | 62.66 | 62.09 | 0.5673 | 0.8437 | 84.46 | 17.4 | 97.01 |
| (50 Steps + 4 sbeam) | 62.66 | 61.95 | 0.6427 | 0.8678 | 87.01 | 29.2 | 95.39 |
| (50 Steps + 8 sbeam) | 62.66 | 61.76 | 0.6557 | 0.8773 | 87.85 | 32.8 | 97.36 |

Table 6: The evaluation of Text Reconstruction in multiple languages, with the models trained and evaluated on MTG datasets with maximal token length 64 in English, French, German and Spanish, respectively.

## Appendix D Cross-lingual Evaluation using AdTrans

![Refer to caption](/html/2401.12192/assets/x4.png)

Figure 4: Cross-lingual evaluation between reconstructed texts and translated reconstructed texts.