**Disclaimer**

This work is still in progress, we aim to present preliminary results for learned sparse retrievers using LLMs.

# Introduction

Information Retrieval (IR) has been greatly influenced by traditional systems like BM25, which use lexical matching and inverted indices for efficient retrieval. These methods have been the backbone of search engines for years. However, Pre-trained Language Models (PLMs) like BERT [@devlin-etal-2019-bert] have brought a major shift towards contextualized semantic matching. While bag-of-words (BOW) models remain strong baselines [@critical-neural-ir], they suffer from the long-standing vocabulary mismatch problem, where relevant documents might not contain terms that appear in the query. Thus, there have been attempts to substitute standard Bag of Words approaches with neural rankers.

In modern IR, dense representations combined with Approximate Nearest Neighbors (ANN) search has become the standard for first-stage ranking to reduce inference and maintain semantic representation similarity. These models have improved their in-domain performance thanks to better training pipelines. Despite these improvements, the generalization ability of these models has been questioned, especially with the BEIR benchmark [@beir] showing that traditional systems like BM25 can outperform neural retrievers in various IR tasks and in some cases only show incremental gains. Moreover, dense models have also not been able to explicitly model term matching.

Recent advances in the "lexical representation space" through learned sparse retrieval (LSR) have shown encouraging results for fast sparse inference [@spladev2; @splade; @spladev3]. This approach aims to combine the strengths of lexical bag-of-words and vector embedding algorithms by learning sparse representations that can be integrated with inverted indexing techniques. These methods often involve term weighting and expansion, using the benefits of explicit lexical matching and the efficiency of inverted indices. LSR methods have shown good generalization capabilities, both in inference latency and IR performance.

Despite their potential, sparse retrieval models have not yet been fully used to their full potential. We investigate LSR systems using LLMs and demonstrate substantial performance improvements over existing SPLADE based models and its variants. Additionally, we examine the effects of LSR training on interpretability, offering insights into the future of learned sparse retrieval systems.

![Learned Sparse Retrieval: Training vs Inference](splade.drawio.pdf){#fig:enter-label width="\\linewidth"}

# Related Work

We review the literature on different retrieval paradigms including embedding based dense models with an Approximate Nearest Neighbor (ANN) search and more recent attempts to do learned sparse retrievals which aims to learn semantic expansion of contextual synonyms to do better retrieval and retain interpretability. We also touch upon the latest research advances that attempts to utilize causal only decoder models to learn better dense representations, which as we will see in our work, is also suitable for learned sparse retrievers.

## Dense Representation Learning

Dense retrieval has become a prevalent method for applying PLM retrievers in Information Retrieval (IR) where the models typically rely on the `[CLS]` token representation. Various training strategies have been proposed to enhance learned representations including but not limited to distillation [@distillation1; @distillation2; @distillation3; @distillation4], hard negative mining [@hnm1; @hnm2; @hnm3; @hnm4], pre-training [@pretraining1; @pretraining2] and also all of them combined. Models such as DPR [@dpr] and ANCE [@hnm2] have been recently evaluated on generalization tasks, as demonstrated in the BEIR benchmark [@beir]. Colbert [@colbert] which is a dense representation model utilizes a late-interaction mechanism that allows fine-grained term-level matching at the expense of higher indexing cost and latency.

## Sparse Representation Learning

Sparse representations based on PLM have garnered increasing interest due to their inherent advantages from lexical models. These approaches generally involve a term-importance component and/or a term-expansion mechanism. Various designs have been proposed to address these aspects. COIL [@coil] learns term-level dense representations for a contextualized lexical match, while uniCOIL [@unicoil] simplifies this by learning a single weight per term, extending methods like DeepCT [@deepct] and DeepImpact [@deepimpact]. SPLADE [@splade], on the other hand, directly learns high-dimensional sparse representations capable of joint expansion and re-weighting through the Masked Language Modeling head of the PLM and sparse regularization. Recent Splade versions like [@hardnegatives] and [@spladev3] show the importance of mining stronger hard negatives and utilization of distillation losses to boost in-domain and out-of-domain retriever performance. There have also been works to combine both sparse and dense representations for better retrieval [@sparseembed] while maintaining linear time complexity.

## Echo embeddings

Since LLMs are decoder-only causal language models, they are inherently weak at contextual representation due to unidirectional attention [@llm2vec]. To mitigate this @echoembeddings proposed to use repetition to overcome this bias. In this idea, they propose to use two occurrences of the same text which is passed to the language model, and take a mean representation of only the second occurrence. This way, all the tokens from the second occurrence can already be seen in the whole context, leading to better representations. We utilize echo embeddings instead of an expensive retraining of the model for bidirectional attention.

# Splade and Methodology

In this section, we first describe in detail SPLADE [@splade], alongside how to leverage diverse data without any hard negative mining and how to utilize causal LLMs for SPLADE training.

## SPLADE

**Model.** SPLADE generates interpretable sparse representations by utilizing the BERT Wordpiece vocabulary for token expansion and the MLM head for assigning term weights. This way the model performs implicit vocabulary expansion without any need for providing manual expansion terms. The interpretability of SPLADE is maintained due to the tied weight matrices of the embedding and the MLM layer. For a query or document $t$, let $w_{i,j}$ denote the resulting importance of the $j$-th vocabulary token, for the input token $i$. Text representations are obtained by pooling such importance predictors over the input sequence, after a log-saturation effect. We thus consider by default the following formulation: $$w_j = \max_{\substack{i \in t}} \log \left( 1 + \text{ReLU}(w_{ij}) \right)$$ and the ranking score $s(q, d)$ is given by the dot product between $q$ and $d$ representations.

**Training.** Given a query $q$, a positive document $d^+$, and additional in-batch negatives $d^-_j$ (which are documents from other queries within the same batch), the model is trained through a combined approach. This involves optimizing a contrastive InfoNCE loss [@infonce], as commonly employed in various prior works on learning first-stage rankers, alongside FLOPS regularization [@flops]. The regularization is directly applied to the representations in the vocabulary space to achieve the desired sparsity factor in the indices during inference: $$\mathcal{L} = \mathcal{L}_{\text{InfoNCE}} + \lambda_q \mathcal{L}^q_{\text{FLOPS}} + \lambda_d \mathcal{L}^d_{\text{FLOPS}}$$

Since we rely only on in-batch negatives $d^-_j$, we reduce the cost of any additional hard negative mining as compared to previous SPLADE versions [@splade; @spladev3; @hardnegatives].

::: table*
:::

## Scaling SPLADE for Causal decoder LLMs

Learned Sparse Retriever models maintain their interpretability by expanding the input sequence into a set of related tokens, which leads to better retrieval. This interpretability is ensured by the tied weight matrices of the embedding layer and the language modeling head [@weight-tying]. Models like BERT [@devlin-etal-2019-bert] are pre-trained with tied weight matrices, and thus no intervention is required during SPLADE training apart from keeping them tied. Untying the weights would lead the language modeling layer to be equivalent to a linear layer, which does not correspond to any token for every logit. Many LLMs today are trained with untied weight matrices [@llama2; @mistral], which is bad for SPLADE training since the latent space is not bound to generate interpretable logits. Freezing the embedding and the output layer can still lead to uninterpretable logits and defeat the purpose of sparse retrieval systems. Another approach could be to reinitialize the language modeling head layer with the embedding layer and then retrain on a causal LM objective with tied weights. However, this is expensive and can also lead to loss of information. We thus use LoRA finetuning [@lora] to avoid this altogether. This way, we also reduce the number of parameters to be trained while maintaining interpretability. Another problem of causal LLMs is the weak representation of past context due to unidirectional attention. We utilize Echo embeddings [@echoembeddings] to improve the representation of the LLM. This effectively doubles the sequence length during training and inference. We refer to this model as Echo-Mistral SPLADE in our experiments.

# Experiments and Results

In this section, we discuss our experimental setup for all the proposed methods and discuss their potential advantages and limitations through empirical results.

## Training

All the reference models are trained on the MS MARCO passage ranking dataset, which contains 8.8M passages and 500k training queries with shallow annotations. These models have been trained in multiple stages with each stage introducing more and more hard negatives and better margin scores for distillation. Instead, we rely on the same training mechanism with much more diverse training data. Specifically, we use the Sentence-transformers embedding data (15.5M subset) which has been previously used for sentence similarity tasks. Instead of mining hard negatives and calculating distillation scores which requires a lot of excess compute and effort. We rely only on in-batch negatives for training, with a batch size of 512. We use the same number of steps (150k) as other splade variants with echo embeddings during training and inference for our Echo-Mistral SPLADE model.

We initialized the smaller SPLADE models with the BERT-base checkpoint and the LLM with Mistralv3[^1]. Models are trained with the ADAM optimizer, using a learning rate of 2e-5 with linear scheduling and a warmup of 6000 steps. We consider a maximum length of 256 for input sequences. To mitigate the contribution of the regularizer at the early stages of training, we follow [@flops] and use a scheduler for $\lambda$, quadratically increasing $\lambda$ at each training iteration, until a given step (50k in our case), from which it remains constant. For training the larger Echo-Mistral-7B SPLADE model we use QLoRA finetuning [@qlora] with $\alpha=8$, $rank=16$, and a dropout value of 0.1 over the projection layers. Models are trained using PyTorch [@pytorch] and HuggingFace transformers [@huggingface], on 4 NVIDIA DGX A100 GPUs with 80GB memory.

## Sentence Transformers embedding data

Sparse retrievers have been trained only on individual datasets like the MSMarco passage ranking dataset which has 8.8M passages and 500k training queries. MSMarco is a high-quality retrieval dataset that is generated from Bing search logs. Specifically, it contains anonymized data derived from real user queries submitted to the Bing search engine. Instead of MSMarco, we rely on a large-scale sentence embedding dataset called Sentence-Transformer[^2] that has shown improvements over BEIR benchmark for many DPR models [@sbert; @senttrans1; @senttrans2; @senttrans3; @senttrans4; @llm2vec]. Specifically, we take the 15.5M sample subset and set their sampling probabilities proportional to their dataset size. We list all the dataset names in Appendix [6](#app: dataset){reference-type="ref" reference="app: dataset"}. We use this dataset for Echo-Mistral-SPLADE training without any hard negative mining or distillation.

## Evaluation

We report nDCG@10 for comparing zero-shot performance of our models on the BEIR benchmark [@beir]. We rely on the subset of 13 readily available datasets to compare with other baselines, thus we do not consider CQADupstack, BioASQ, Signal-1M, TREC-NEWS, and Robust04 BEIR datasets for evaluation.

## Results and Discussion

We compare our model with the most prelevant learned sparse models from SPLADE family and its subsequent variations and extensions like Elserv2 [@hardnegatives; @splade; @spladev3; @elastic_elser_2024] and also with ColBERT[@colbert] which is a state-of-the-art dense baseline. Our results can be summarized in Table [\[tab:results\]](#tab:results){reference-type="ref" reference="tab:results"}, we can see that scaling with echo embeddings work significantly well over the same dataset, achieving state-of-the-art results for sparse retrieval on BEIR-13 without any need for complicated training and hard negative mining.

For a fair comparison with similar-sized LLM-based dense retriever models, we also compare our results with LLM2Vec on Mistral7B in a supervised setting after MNTP and SimCSE pretraining [@llm2vec] and Echo Embeddings on Mistral7B with pooled representation of the second occurrence [@echoembeddings]. Firstly, we do not use any of the BEIR train sets while training our LSR model so all the numbers reported are purely zero shot as compared to LLM2Vec and Echo Embeddings which use train sets of NQ, Fever, and HotpotQA during their training. In Table [\[tab:my-table\]](#tab:my-table){reference-type="ref" reference="tab:my-table"} we can see that a similar-sized Echo-Mistral SPLADE model achieves competitive performance to dense models. If we exclude in-domain performance for all models, we see Echo-Mistral SPLADE surpassing the performance of Echo Embeddings but lagging behind LLM2Vec which shows performance gains due to unsupervised pretraining. This is a strong indicator of how well LSR systems can perform in practice.

# Conclusion

In this paper, we present Echo-Mistral SPLADE, which uses Mistral-7B LLM with Echo embeddings as the backbone for learned sparse retrieval training. Echo-Mistral-SPLADE achieves state-of-the-art performance on BEIR benchmark outperforming existing sparse retrieval models including SPLADE and its variants. We also compare with a similar-sized dense model and show that Echo-Mistral-SPLADE can outperform dense models in out-of-domain setting, which can be further improved with unsupervised pretraining. We believe such diverse data and the model scale is the leading reason for improvement but further exploration like joint distillation and hard negatives can further improve this performance. With this, we aim to motivate the use of LSR systems in practice due to their low inference latency and interpretability over dense retriever systems.

# Dataset Details {#app: dataset}

In this section we list down the dataset details and sampling probabilities during training. The probabilities are calculated by normalizing the size of each dataset and we exclude any dataset that conflicts with our evaluation on zero-shot out-of-domain BEIR retrieval datasets [@beir].

[^1]: <https://huggingface.co/mistralai/Mistral-7B-v0.3>

[^2]: <https://huggingface.co/collections/sentence-transformers/embedding-model-datasets-6644d7a3673a511914aa7552>
