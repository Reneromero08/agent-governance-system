# Introduction

<figure id="fig:radar_plot" data-latex-placement="t!">
<embed src="figs/files/radar_plot_new.pdf" style="width:49.0%" />
<figcaption> Paired t-test statistics comparing retriever scores between document pairs (<span class="math inline"><em>D</em><sub>1</sub></span> vs. <span class="math inline"><em>D</em><sub>2</sub></span> <span class="math inline"> ≈ <em>M</em>(<em>Q</em><em>u</em><em>e</em><em>r</em><em>y</em>, <em>D</em><sub>1</sub>) − <em>M</em>(<em>Q</em><em>u</em><em>e</em><em>r</em><em>y</em>, <em>D</em><sub>2</sub>)</span> where <span class="math inline"><em>M</em></span> is the retrieval score of the model). Document pairs are designed for controlled experiments shown in Table <a href="#tab:examples" data-reference-type="ref" data-reference="tab:examples">[tab:examples]</a>. Positive values indicate a retriever’s preference for the more biased document in each bias scenario. The significance of the answer’s existence is often less than the significance of the distracting signals. </figcaption>
</figure>

Retrieval-based language models have demonstrated strong performance on a range of knowledge-intensive NLP tasks [@RAGLewis2020; @asai-etal-2023-retrieval; @gao2024retrievalaugmentedgenerationlargelanguage]. At the core of these models is a retriever that identifies relevant context to ground the generated output. Dense retrieval methods such as Contriever [@izacard2021contriever]---where passages or documents are stored as learned embeddings---are especially appealing for their scalability across large knowledge bases and handling lexical gaps [@ni2022large; @ScalingShao2024], compared to alternatives like BM25 [@INR-019-BM25] or ColBERT[@10.1145/3397271.3401075-colbert]. Despite their widespread use, relatively little is understood about how these dense models encode and organize information, leaving key questions about their robustness against adversarial attacks unanswered.

Existing evaluations often focus on downstream task performance, as seen in benchmarks like BEIR [@thakur2021beir], without probing the underlying behaviors of retrievers. Some studies have analyzed specific issues in information retrieval (IR) models, such as position bias [@coelho-etal-2024-dwell] or lexical overlap [@ram-etal-2023-token].

::: table*
+---+---+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|   |   | **Document 1 (Higher Query Document Similarity Score)** - $\boldsymbol{D_1}$                                                                                                                                                                                                                                                                                                                                                     | **Document 2 (Lower Query Document Similarity Score)** - $\boldsymbol{D_2}$                                                                                                                                                                                                                                                                                                                 |
+:==+:==+:=================================================================================================================================================================================================================================================================================================================================================================================================================================+:============================================================================================================================================================================================================================================================================================================================================================================================+
|   |   | **Query:** What is the sister city of *[Leonessa]{.mark}* ?\                                                                                                                                                                                                                                                                                                                                                                     | **Query:** What is the sister city of *[Leonessa]{.mark}* ?\                                                                                                                                                                                                                                                                                                                                |
|   |   | **Document:** *[Leonessa]{.mark}* [is twinned with the French town of]{.mark} *[Gonesse]{.mark}* [.]{.mark} \                                                                                                                                                                                                                                                                                                                    | **Document:** *[Leonessa]{.mark}* is a town and comune in the far northeastern part of the Province of Rieti in the Lazio region of central Italy .\                                                                                                                                                                                                                                        |
|   |   | Its population in 2008 was around 2,700 . Situated in a small plain at the foot of \.....                                                                                                                                                                                                                                                                                                                                        | Its population in 2008 was around 2,700 . Situated in a small plain at the foot of \.....                                                                                                                                                                                                                                                                                                   |
+---+   +----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|   |   | **Query:** Which country is *[Wonyong Sung]{.mark}* a citizen of?\                                                                                                                                                                                                                                                                                                                                                               | **Query:** Which country is *[Wonyong Sung]{.mark}* a citizen of?\                                                                                                                                                                                                                                                                                                                          |
|   |   | **Document:** *[Wonyong Sung]{.mark}* [( born 1950s ) ,]{.mark} *[South Korean]{.mark}* [professor of electronic engineering]{.mark} Won - yong is a Korean masculine given name \..... People with this name include : Kang Won - yong ( 1917 -- 2006 ) \..... , South Korean swimmer                                                                                                                                           | **Document:** Won - yong is a Korean masculine given name \..... People with this name include : \.... Jung Won - yong ( born 1992 ) , South Korean swimmer *[Wonyong Sung]{.mark}* [( born 1950s ) ,]{.mark} *[South Korean]{.mark}* [professor of electronic engineering]{.mark}                                                                                                          |
+---+   +----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|   |   | **Query:** When was *[Seyhun]{.mark}* born?\                                                                                                                                                                                                                                                                                                                                                                                     | **Query:** When was *[Seyhun]{.mark}* born?\                                                                                                                                                                                                                                                                                                                                                |
|   |   | **Document:** *[Seyhun]{.mark}* [, (]{.mark} *[August 22 , 1920]{.mark}* [-- May 26 , 2014 ) was an Iranian architect , sculptor , painter , scholar and professor .]{.mark} He studied fine arts at \.....                                                                                                                                                                                                                      | **Document:** *[Houshang Seyhoun]{.mark}* [, (]{.mark} *[August 22 , 1920]{.mark}* [-- May 26 , 2014 ) was an Iranian architect , sculptor , painter , scholar and professor .]{.mark} He studied fine arts at \.....                                                                                                                                                                       |
+---+   +----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|   |   | **Query:** What series is *[Lost Verizon]{.mark}* part of?\                                                                                                                                                                                                                                                                                                                                                                      | **Query:** What series is *[Lost Verizon]{.mark}* part of?\                                                                                                                                                                                                                                                                                                                                 |
|   |   | **Document:** [\"]{.mark} *[Lost Verizon]{.mark}* [\" is the second episode of]{.mark} *[The Simpsons]{.mark}* [' twentieth season .]{.mark}                                                                                                                                                                                                                                                                                     | **Document:** [\"]{.mark} *[Lost Verizon]{.mark}* [\" is the second episode of]{.mark} *[The Simpsons]{.mark}* [' twentieth season .]{.mark} It first aired on the Fox network in the United States on October 5 , 2008 . Bart becomes jealous of his friends and their cell phones . Working at a golf course , Bart takes the cell phone of Denis Leary \.....                            |
+---+---+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|   |   | **Query:** Where was *[James Paul Maher]{.mark}* born?\                                                                                                                                                                                                                                                                                                                                                                          | **Query:** Where was *[James Paul Maher]{.mark}* born?\                                                                                                                                                                                                                                                                                                                                     |
|   |   | **Document:** [Born in]{.mark} *[Brooklyn , New York]{.mark}* [,]{.mark} *[Maher]{.mark}* [graduated from St. Patrick 's Academy in Brooklyn .]{.mark} *[James Paul Maher]{.mark}* ( November 3 , 1865 -- July 31 , 1946 ) was a U.S. Representative from New York . *[Maher]{.mark}* was elected as a Democrat to the Sixty - second and to the four succeeding Congresses ( March 4 , 1911 -- March 4 , 1921 ) .               | **Document:** [Born in]{.mark} *[Brooklyn , New York]{.mark}* [,]{.mark} *[Maher]{.mark}* [graduated from St. Patrick 's Academy in Brooklyn .]{.mark} Apprenticed to the hatter 's trade , he moved to Danbury , Connecticut in 1887 and was employed as a journeyman hatter . He became treasurer of the United Hatters of North America in 1897 .                                        |
+---+   +----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|   |   | **Query:** Who is the publisher of *[Assassin 's Creed Unity]{.mark}* ?\                                                                                                                                                                                                                                                                                                                                                         | **Query:** Who is the publisher of *[Assassin 's Creed Unity]{.mark}* ?\                                                                                                                                                                                                                                                                                                                    |
|   |   | **Document:** \" *[Assassin 's Creed Unity]{.mark}* \" \" *[Assassin 's Creed Unity]{.mark}* \" *[Assassin 's Creed Unity]{.mark}* received mixed reviews upon its release .                                                                                                                                                                                                                                                     | **Document:** Isa is a town and Local Government Area in the state of Sokoto in Nigeria . It shares borders with \..... *[Assassin 's Creed Unity]{.mark}* [is an action - adventure video game developed by Ubisoft Montreal and published by]{.mark} *[Ubisoft]{.mark}* . Isa is a town and Local Government Area in the state of Sokoto in Nigeria . It shares borders with \.....       |
+---+   +----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|   |   |                                                                                                                                                                                                                                                                                                                                                                                                                                  |                                                                                                                                                                                                                                                                                                                                                                                             |
+---+---+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
:::

In this work, we study multiple biases' impact on retrievers---both individually and in combination---for the first time. To enable fine-grained control over document structure and factual positioning, we repurpose a document-level relation extraction dataset (Re-DocRED [@tan-etal-2022-revisiting]).

We first investigate biases individually, identifying tendencies such as an **over-prioritization of document beginnings, document brevity, repetition of matching entities, and literal matches at the expense of ignoring answer presence.** Our statistical approach, illustrated in Figure [1](#fig:radar_plot){reference-type="ref" reference="fig:radar_plot"}, allows for comparative analysis across different biases. Additionally, we explore the interplay between these biases and propose an adversarial benchmark that combines multiple vulnerabilities.

We further study combining multiple biases and reveal concerning patterns in current retriever architectures. **When exposed to multiple interacting biases, even top-performing models exhibit dramatic degradation**, selecting the answer-containing document over the foil document---filled with biases---less than 10% of the time. Moreover, we demonstrate that **these biases can be exploited to manipulate Retrieval-Augmented Generation (RAG)**, causing retrievers to favor misleading or adversarially constructed documents, which misguide LLMs into using incorrect information and ultimately degrade its performance.

# Related Work

#### Benchmarking in Information Retrieval

Popular benchmarks like BEIR [@thakur2021beir; @guo-etal-2021-multireqa; @petroni-etal-2021-kilt; @muennighoff-etal-2023-mteb] have played a crucial role in evaluating retrieval models across diverse datasets and tasks. In addition to general IR benchmarks, domain-specific benchmarks such as COIR [@li2024coircomprehensivebenchmarkcode] for code retrieval and LitSearch [@ajith-etal-2024-litsearch] for scientific literature search address retrieval challenges in specialized domains. While these benchmarks have advanced the evaluation of IR models, they primarily focus on downstream performance rather than conducting systematic analyses of biases inherent in retrieval systems.

#### Information Retrieval Model Analysis

Prior work in information retrieval has explored various dimensions of retrieval performance, including positional biases [@coelho-etal-2024-dwell]. Studies have also examined how dense retrievers exhibit biases towards common entities and struggle with OOD scenarios [@sciavolino-etal-2021-simple]. Furthermore, analysis by projecting representations to the vocabulary space has shown that supervised dense retrievers tend to learn relying heavily on lexical overlap during training [@ram-etal-2023-token]. Similarly, @behnamghader-etal-2023-retriever has indicated that Dense Passage Retrieval (DPR) models often fail to retrieve statements requiring reasoning beyond surface-level similarity. Furthermore, neural IR models have been shown to exhibit over-penalization for extra information, where adding a relevant sentence to a document can unexpectedly decrease its ranking [@Usuha-etal-2024-Over-penalization-for-Extra-Information]. Additionally, @reichman-heck-2024-dense takes a mechanistic approach to analyze the impact of DPR fine-tuning, showing that while fine-tuned models gain better access to pre-trained knowledge, their retrieval capabilities remain constrained by the pre-existing knowledge in their base models. Further, @macavaney-etal-2022-abnirml provides a framework for analyzing neural IR models, identifying key biases and sensitivities in these models.

#### Adversarial Attacks in Information Retrieval

Numerous studies have explored various dimensions of robustness in information retrieval, including aspects related to adversarial robustness [@10.1145/3626772.3661380]. Adversarial perturbations, for instance, have been shown to significantly degrade BERT-based rankers' performance, revealing their brittleness to subtle modifications [@10.1145/3539813.3545122]. Existing retrieval attack methods primarily encompass corpus poisoning [@10646819; @zhong-etal-2023-poisoning], backdoor attacks [@long2024whispersgrammarsinjectingcovert], and encoding attacks [@10.1145/3607199.3607220].

While previous work has analyzed some retrieval biases, most studies focus on task-specific supervised models and a single aspect in isolation. Our work provides a comprehensive comparative analysis of popular retrieval models across multiple dimensions of vulnerability. We systematically investigate how these biases interact and affect the retrieval capabilities of dense retrievers. By repurposing a relation extraction dataset, we gain precise control over factual information in documents, enabling a rigorous evaluation of retrieval robustness. This multi-dimensional approach provides a nuanced understanding of the strengths and weaknesses of dense retrievers.

# Experiments

<figure id="fig:decompx_heatmap" data-latex-placement="t!">
<img src="figs/files/Main_Example.png" style="width:96.0%" />
<figcaption> Visualization of the contribution of each query and document token to the final retrieval score using DecompX. Literal Bias reflects the model’s preference for exact word matches, such as failing to match "esteban goemz" with "estevao gomes." Position Bias indicates a preference for entities earlier in the document receiving more attention. Repetition Bias shows that repeating an entity multiple times increases its score. Lastly, Answer Importance demonstrates that the query’s answer entity receives less attention compared to head entity matches. </figcaption>
</figure>

## A Framework for Identifying Biases in Retrievers

To gain fine-grained control over the facts present in a document, we take a novel approach by repurposing a relation extraction dataset that provides relation-level fact granularity. This enables a structured analysis of retrieval biases by explicitly linking queries to individual factual statements.

One such dataset is DocRED [@yao-etal-2019-docred], a relation extraction dataset constructed from Wikipedia and Wikidata. DocRED consists of human-annotated triplets ( *[head entity]{.mark}* , relation, *[tail entity]{.mark}* )---for example, (Albert Einstein, educated at, University of Zurich). However, DocRED suffers from a significant false negative issue, as many valid relations are missing from the annotations. To address this, we use Re-DocRED [@tan-etal-2022-revisiting], a re-annotated version of DocRED that recovers missing facts, leading to more complete and reliable annotations.

To construct a retrieval dataset from Re-DocRED, we map each relation to a query template (Templates are in Table [\[tab:query_templates\]](#tab:query_templates){reference-type="ref" reference="tab:query_templates"}). For example, for the relation \"educated at,\" we use the template \"Where was {Head Entity} educated?\" This transformation allows us to systematically examine how retrievers handle different types of factual queries.

::: {#tab:models_performance_nq}
  Model                  pooling      nDCG@10   Recall@10
  ---------------------- --------- ---------- -----------
  Dragon RoBERTa         cls         **0.55**    **0.75**
  Dragon+                cls             0.54        0.74
  COCO-DR Base MSMARCO   cls             0.50        0.71
  Contriever MSMARCO     avg             0.50        0.71
  RetroMAE MSMARCO FT    cls             0.48        0.68
  Contriever             avg             0.25        0.41

  :  Models' performance on NQ dataset with test set queries and 2,681,468 corpus size.
:::

The answers to these queries are the tail entities found in the evidence sentences provided by the dataset. For our analysis, we ensure that each query has a single evidence sentence ($S_{ev}$) within the original document ($S_{ev} \in D_{orig}$) that contains both the head and tail entities. This constraint makes the sentence self-contained, allowing for precise control over the document structure in subsequent sections. We also introduce the notation $S^{+h}_{-t}$ for sentences in $D_{orig}$ that contain the head entity but not the tail entity, and $S^{-h}_{-t}$ for sentences that do not contain either entity. In each of the following sections, we will use this notation to construct a pair of document sets, $D_1$ and $D_2$, enabling a systematic investigation of retrieval score variations and potential biases. As a result, for each of our six analysis settings, we compile 250 queries, each with a single corresponding gold document, based on the test and validation sets of Re-DocRED.

## Models Performance & Bias Discovery

First, we evaluate several dense retrievers on the NQ dataset [@kwiatkowski-etal-2019-natural], comparing their performance using nDCG@10 and Recall@10 metrics. Table [1](#tab:models_performance_nq){reference-type="ref" reference="tab:models_performance_nq"} shows that Dragon models lead in performance, and the significant improvement of fine-tuned Contriever (Contriever MSMARCO) over its unsupervised counterpart highlights the importance of supervision and task-specific adaptation. Models also differ in pooling mechanisms, with Contriever using average pooling and others using CLS pooling. For details, refer to the appendix [5.2](#sec:models_performance){reference-type="ref" reference="sec:models_performance"}.

In our preliminary analysis, we utilized DecompX [@modarressi-etal-2023-decompx; @modarressi-etal-2022-globenc], a method that decomposes the representations of encoder-based models such as BERT into their constituent token-based representations. By applying DecompX to the embeddings generated by dense retrievers, we obtain decomposed representations for both the query and the document. Instead of using the original embeddings, we compute the similarity score via a dot product of the decomposed vectors. This approach enables us to visualize the contribution of each query and document token to the final similarity score as a heatmap (Figure [2](#fig:decompx_heatmap){reference-type="ref" reference="fig:decompx_heatmap"}), revealing biases in token-level interactions.

In our preliminary error analysis of 60 retrieval failure examples, we identified potential biases and limitations in the models (Table [7](#tab:preliminary_analysis){reference-type="ref" reference="tab:preliminary_analysis"}). Figure [2](#fig:decompx_heatmap){reference-type="ref" reference="fig:decompx_heatmap"} highlights some of these biases, such as Literal Bias, where the term \"esteban gomez\" fails to match \"estevao gomez,\" reflecting a preference for exact matching. In subsequent sections, we design experiments and perform statistical tests to evaluate these observed biases.

## Bias Types in Dense Retrieval {#sec:bias_types}

The following experiments are meticulously designed to control for all other factors and biases, isolating the specific bias under evaluation.

### Answer Importance

An effective retrieval model should accurately identify the query's intent. It should retrieve relevant documents that address the query, rather than just matching entities. To assess whether dense retrieval models truly recognize the presence of answers or merely focus on entity matching, we developed a controlled experimental framework. Our experimental design contrasts two carefully constructed document types. **1. Document with Evidence**: Contains a leading evidence sentence with both the head entity and the tail entity (answer). **2. Document without Evidence $\boldsymbol{D_2}$:** Contains a leading sentence with only the head entity but no tail. $$\begin{equation}
\small
\begin{aligned}
    \boldsymbol{D_1}& \coloneq \boldsymbol{S_{ev}}+\sum_{S^{-h}_{-t} \in D_{orig}} S^{-h}_{-t} \\
     \boldsymbol{D_2}& \coloneq S^{+h}_{-t} + \sum_{S^{-h}_{-t} \in D_{orig}} S^{-h}_{-t}
\end{aligned}
% \vspace{-1.0ex}
\end{equation}$$ Here, $S^{+h}_{-t}$ is another sentence from $D_{orig}$ that replaces the original evidence $S_{ev}$ while containing the head entity but not containing the tail entity to isolate the impact of answer presence. The remainder of both documents consists of neutral sentences $S^{-h}_{-t} \in D_{orig}$, carefully filtered to exclude any sentences containing similar head relations or tail entities. This ensures the answer information appears exclusively in the leading sentence of the evidence document. We strategically positioned the key sentences at the beginning of both documents to mitigate potential position bias effects, which we analyze in subsequent sections. An example of this setup is presented in Table [\[tab:examples\]](#tab:examples){reference-type="ref" reference="tab:examples"} (Answer Impact).

<figure id="fig:tail_ttest" data-latex-placement="t!">
<embed src="figs/files/tail_ttest.pdf" style="width:48.0%" />
<figcaption> Paired t-test statistics comparing dot product similarity between the first sentence containing both head and tail (Answer) entities versus only the head entity, with 95% CI error bars. Higher values indicate recognition of the answer’s importance. </figcaption>
</figure>

To quantify the models' ability to distinguish between these document types, we employ **Paired t-Test**[^2] to analyze the difference in similarity scores. The t-test statistic (t) is calculated as:

$$\begin{equation}
    t = \frac{\bar{d}}{SE(d)} = \frac{\text{Average Difference}}{\text{Standard Error}}
\end{equation}$$

where $\bar{d}=mean\left(M(Q, D_1) - M(Q, D_2)\right)$ is the mean difference between paired observations[^3], and $SE(d)$ is the standard error of these differences[^4]. A positive t-statistic indicates higher scores for $D_1$ documents, while negative values suggest a preference for $D_2$ documents. In this scenario, positive values are desirable as they indicate the model prefers $D_1$, which contains the answer, over $D_2$, which does not.

As shown in Figure [3](#fig:tail_ttest){reference-type="ref" reference="fig:tail_ttest"}, our analysis reveals variations across models. Dragon+ and Dragon-RoBERTa demonstrate superior tail recognition, achieving the highest positive t-statistics. In contrast, Contriever exhibits poor performance, yielding negative t-statistics that indicate a failure to properly distinguish answer-containing passages.

The vanilla Contriever's underwhelming performance can be attributed to its unsupervised training methodology, which differs from models trained on MS MARCO [@bajaj2018msmarcohumangenerated]. While MS MARCO provides supervised training with explicit query-passage relevance labels, Contriever employs unsupervised contrastive learning. It generates positive pairs through data augmentation from document segments and derives negative examples implicitly via in-batch sampling from other texts. This training approach, while efficient for general text representation, appears insufficient for developing the fine-grained discrimination needed to understand query intent in retrieval tasks.

### Position Bias

<figure id="fig:position_evidence_dot" data-latex-placement="t!">
<embed src="figs/files/moving_evidence_ttest.pdf" style="width:49.0%" />
<figcaption> Paired t-test statistics comparing the effect of moving the evidence sentence position within the document to keeping it in the first position. Negative values indicate a bias towards the beginning of the document. </figcaption>
</figure>

Position bias refers to the preference of retrieval models for information located in specific positions within a document, typically favoring content at the beginning over content appearing later. This bias is problematic as it may lead to the underrepresentation of relevant information that is positioned deeper within documents, thus reducing the overall retrieval quality and fairness.

Our analysis reveals a strong positional bias in dense retrievers, with models consistently prioritizing information at the beginning of documents. As shown in Figure [4](#fig:position_evidence_dot){reference-type="ref" reference="fig:position_evidence_dot"}, we conducted paired t-tests comparing retrieval scores when the evidence sentence is placed at different positions to scores when it is placed at the document's beginning ($M(Q, D_i)-M(Q, D_1)$).

$$\begin{equation}
\small
\begin{aligned}
    \boldsymbol{D_1}& \coloneq \boldsymbol{S_{ev}}+{}^{1}S^{-h}_{-t}+{}^{2}S^{-h}_{-t}+{}^{3}S^{-h}_{-t}+...+{}^{n}S^{-h}_{-t} \\
     \boldsymbol{D_2}& \coloneq {}^{1}S^{-h}_{-t}+ \boldsymbol{S_{ev}} + {}^{2}S^{-h}_{-t} + {}^{3}S^{-h}_{-t} + ... + {}^{n}S^{-h}_{-t}\\
     \boldsymbol{D_3} & \coloneq {}^{1}S^{-h}_{-t} + {}^{2}S^{-h}_{-t} + \boldsymbol{S_{ev}} + {}^{3}S^{-h}_{-t} + ... + {}^{n}S^{-h}_{-t}
\end{aligned}
% \vspace{-1.0ex}
\end{equation}$$

To ensure fairness, the examples were curated so that the remaining content was free of any evidence or head entity ($S^{-h}_{-t}$) like the last section. This design ensured that the evidence's position was the sole factor under evaluation. The consistently negative t-statistics across models in Figure [4](#fig:position_evidence_dot){reference-type="ref" reference="fig:position_evidence_dot"} confirm a strong bias favoring content at document beginnings.[^5] This bias is most pronounced in Dragon-RoBERTa and Contriever-MSMARCO, which show the most negative t-statistics, indicating severe degradation in recognizing evidence further into the document. While Dragon+ and RetroMAE perform better, their negative t-statistics still confirm position bias in these models.

These findings align with recent research by @coelho-etal-2024-dwell, who demonstrated that positional biases emerge during the contrastive pre-training phase and worsened through fine-tuning on MS MARCO dataset with T5 [@raffel2020exploring-t5] and RepLLaMA [@rankllama] models. This can significantly impact retrieval performance when relevant information appears later in documents.

### Literal Bias

Retrievers should ideally recognize semantic equivalence across different surface forms of the same entity. For instance, a robust model should understand that \"Gomes\" and \"Gomez\" refer to the same person, or that \"US\" and \"United States\" represent the same entity. However, our analysis reveals that current models exhibit a strong bias toward exact literal matches rather than semantic matching.

::: {#tab:literal_bias}
+:--------:+:--------:+:---------:+:---------:+-------:+-------:+
|          |          |           | Model     |        |        |
+----------+----------+-----------+-----------+--------+--------+
| MSMARCO  |          |           |           |        |        |
+----------+----------+-----------+-----------+--------+--------+
|          |          |           |           |        |        |
+----------+----------+-----------+-----------+--------+--------+
|          |          |           |           |        |        |
+----------+----------+-----------+-----------+--------+--------+
| **long** | **long** | **long**  | **short** | +21.05 | +21.04 |
|          |          +-----------+-----------+--------+--------+
|          |          | **short** | **long**  | +22.04 | +13.40 |
|          |          |           +-----------+--------+--------+
|          |          |           | **short** | +4.62  | +9.04  |
+----------+----------+-----------+-----------+--------+--------+
|          |          | **short** | **long**  | +14.37 | +16.62 |
+----------+----------+-----------+-----------+--------+--------+

:  Paired t-test statistics (p-values \< 0.05) comparing retrieval scores between exact name matches ($Q1$-$D1$) and variant name pairs ($Q2$-$D2$). Positive statistics indicate a preference for exact literal matches over semantically equivalent name variants (e.g., "US"-"US" over "US"-"United States"). (All models in Table [\[tab:literal_bias_complete\]](#tab:literal_bias_complete){reference-type="ref" reference="tab:literal_bias_complete"}.)
:::

In our dataset, each head entity can be represented by multiple alternative names. To investigate literal bias, we created different combinations of query and document by replacing all head entities with the shortest or longest name variants as illustrated in Table [\[tab:examples\]](#tab:examples){reference-type="ref" reference="tab:examples"} (Literal Bias). For example, an entity might be represented as \"NYC\" (shortest) or \"New York City\" (longest), allowing us to test how the model performs when matching different combinations of these representations.

Table [2](#tab:literal_bias){reference-type="ref" reference="tab:literal_bias"} presents the paired t-test statistics comparing different combinations of name selections in queries and documents. The results consistently show positive statistics when Query 1 and Document 1 contain similar name representations. For our subsequent analysis of bias interplay, we specifically examine the comparison between two scenarios (Figure [6](#fig:literal_ttest){reference-type="ref" reference="fig:literal_ttest"}): one where both query and document use the shortest name variant (short-short) versus cases where the query uses the short name but the document contains the long name variant (short-long). This corresponds to +14.37 and +16.62 in Table [2](#tab:literal_bias){reference-type="ref" reference="tab:literal_bias"} for Contriever and Dragon+, respectively.[^6]

### Brevity Bias

Brevity bias refers to the tendency to favor concise text, such as a single evidence sentence, over longer documents that include the same evidence alongside additional context. This bias is problematic because retrievers may favor a shorter, non-relevant document over a relevant one. We will discuss this potential hazard further in §[3.5](#Sec:RAG){reference-type="ref" reference="Sec:RAG"}.

Here, we performed paired t-tests to compare the similarity scores of queries with two sets of documents: **(1) *Single Evidence***, consisting of only the evidence sentence, and **(2) *Evidence+Document***, consisting of the evidence sentence followed by the rest of the document. The examples are carefully selected to ensure the evidence sentence includes both the head and tail entity and the rest of the document contains no repetition of the head entity or additional evidence.

$$\begin{equation}
\small
\begin{aligned}
    \boldsymbol{D_1}& \coloneq \boldsymbol{S_{ev}} \\
     \boldsymbol{D_2}& \coloneq \boldsymbol{S_{ev}} + \sum_{S^{-h}_{-t} \in D_{orig}} S^{-h}_{-t}
\end{aligned}
% \vspace{-1.0ex}
\end{equation}$$

Figure [1](#fig:radar_plot){reference-type="ref" reference="fig:radar_plot"} and [9](#fig:brevity_ttest){reference-type="ref" reference="fig:brevity_ttest"}, illustrate the paired t-test statistics, where significant positive values indicate a strong bias toward brevity, as models assign higher scores to concise texts ($D_1$) than to longer ones with the same evidence ($D_2$). This behavior likely stems from the way dense passage retrievers compress document representations. Most retrievers use either a mean-pooling strategy or a \[CLS\] token-based method. Both methods struggle with integrating useful evidence into the representation when unrelated content is present, leading to a "pollution effect." As a result, the additional context in longer documents dilutes the importance of the evidence, causing retrievers to favor concise input.

### Repetition Bias

Repetition bias refers to the tendency of retrieval models to prioritize documents or passages with repetitive content, particularly repeated mentions of head entities present in the query. This bias is problematic as it may skew retrieval results toward redundant or verbose documents, undermining the goal of surfacing concise and diverse information.

<figure id="fig:repetition_bias_heatmap_contriever" data-latex-placement="t!">
<embed src="figs/files/repetition_heatmap_Contriever_MSMARCO.pdf" style="width:48.0%" />
<figcaption> The average retrieval score of Contriever MSMARCO increases with head entity repetitions but decreases with document length (all models in Figure <a href="#fig:repetition_bias_heatmap" data-reference-type="ref" data-reference="fig:repetition_bias_heatmap">10</a>). </figcaption>
</figure>

To analyze repetition bias, we conducted an experiment evaluating the average retrieval dot product score of the models for samples with varying document lengths and head entity repetitions (Figure [5](#fig:repetition_bias_heatmap_contriever){reference-type="ref" reference="fig:repetition_bias_heatmap_contriever"} and [10](#fig:repetition_bias_heatmap){reference-type="ref" reference="fig:repetition_bias_heatmap"}). A key concern is that longer documents naturally have a higher chance of lexical overlap with the query, as they may contain more repeated mentions of the head entity. This makes it difficult to disentangle the effects of document length from the number of entity repetitions. Therefore, we structure our analysis to separately examine these two factors. Our findings (Figure [5](#fig:repetition_bias_heatmap_contriever){reference-type="ref" reference="fig:repetition_bias_heatmap_contriever"}) reveal that the retrieval score increases with the number of head entity mentions, indicating a preference for documents with repeated entities. Conversely, the retrieval score decreases as document length increases, suggesting that longer documents are penalized despite potential relevance. Figure [10](#fig:repetition_bias_heatmap){reference-type="ref" reference="fig:repetition_bias_heatmap"} in the appendix generalizes these observations across all models. This experiment highlights the trade-off between repetition and document length, emphasizing the need for retrieval systems to balance these factors to mitigate bias.

We further explored this phenomenon through the results shown in Figures [1](#fig:radar_plot){reference-type="ref" reference="fig:radar_plot"} and [8](#fig:repetition_ttest){reference-type="ref" reference="fig:repetition_ttest"}. Here, we performed paired t-tests to compare the dot product similarity scores of queries with two sets of documents: **(1) *More Heads***, comprising an evidence sentence and two sentences containing head mentions but no tails, and **(2) *Fewer Heads***, comprising an evidence sentence and two sentences without head or tail mentions from the document (Table [\[tab:examples\]](#tab:examples){reference-type="ref" reference="tab:examples"}).

$$\begin{equation}
\small
\begin{aligned}
    \boldsymbol{D_1}& \coloneq \boldsymbol{S_{ev}}+S^{+h}_{-t}+S^{+h}_{-t} \\
     \boldsymbol{D_2}& \coloneq \boldsymbol{S_{ev}}+S^{-h}_{-t}+S^{-h}_{-t}
\end{aligned}
% \vspace{-1.0ex}
\end{equation}$$

Positive paired t-test values indicate higher similarity for sentences with more head mentions (Figure [8](#fig:repetition_ttest){reference-type="ref" reference="fig:repetition_ttest"}). The results strongly suggest that the model favors sentences with repeated heads, confirming the presence of repetition bias.

## Interplay Between Bias Types {#sec:interplay}

To understand how different biases interact and amplify retrieval model weaknesses, we conduct a systematic analysis using a controlled 250-sample dataset across all experiments. This consistent sample size ensures comparability of paired t-test statistics across bias types and provides a robust basis for evaluating their interplay.

As illustrated in Figure [1](#fig:radar_plot){reference-type="ref" reference="fig:radar_plot"}, the paired t-test results reveal that brevity bias, literal bias, and position bias are the most problematic for dense retrievers. In contrast, repetition bias, while still detrimental, exhibits a relatively lower impact, suggesting that models are slightly more robust against this type of bias. Answer importance demonstrates an acceptable distinction between evidence-containing and no-evidence documents. However, the scores are not as strong as one would expect from models designed for accurate answer retrieval, highlighting the need for further improvement in this area.

::: {#tab:foil_acc}
  ---------------------- -------------------------------- -------- ---------
  Model                              Accuracy                      
  Statistic                          p-value                       
  Contriever              [0.4%]{style="color: DarkRed"}   -34.58   \< 0.01
  RetroMAE MSMARCO FT     [0.4%]{style="color: DarkRed"}   -41.49   \< 0.01
  Contriever MSMARCO      [0.8%]{style="color: DarkRed"}   -42.25   \< 0.01
  Dragon RoBERTa          [0.8%]{style="color: DarkRed"}   -36.53   \< 0.01
  Dragon+                 [1.2%]{style="color: DarkRed"}   -40.94   \< 0.01
  COCO-DR Base MSMARCO    [2.4%]{style="color: DarkRed"}   -32.92   \< 0.01
  ColBERT (v2)            [7.6%]{style="color: DarkRed"}   -20.96   \< 0.01
  ReasonIR-8B             [8.0%]{style="color: DarkRed"}   -36.92   \< 0.01
  ---------------------- -------------------------------- -------- ---------

  :  The accuracy and paired t-test comparing a foil document $D_1$ (exploiting biases but lacking the answer) to a second document $D_2$ with evidence embedded in unrelated sentences. Accuracy is the proportion of 250 example pairs where $M(Q,D_2) > M(Q,D_1)$. All retrieval models perform extremely poorly ([\<10%]{style="color: DarkRed"} accuracy), highlighting their inability to distinguish biased distractors from genuine evidence. (Extra models explained in §[5.1](#sec:more_models){reference-type="ref" reference="sec:more_models"})
:::

To further investigate the compounded effects of multiple biases, we conducted another experiment that combines several bias types into a single challenging setup. In this experiment, we created two document types. **1) Foil Document with Multiple Biases:** This document contains multiple biases, such as repetition and position biases. It includes two repeated mentions of the head entity in the opening sentence, followed by a sentence that mentions the head but not the tail (answer). So it does not include the evidence. **2) Evidence Document with Unrelated Content:** This document includes four unrelated sentences from another document, followed by the evidence sentence with both the head and tail entities. The document ends with the same four unrelated sentences. An example is shown in Table [\[tab:examples\]](#tab:examples){reference-type="ref" reference="tab:examples"} (Foil vs. Evide.).[^7]

$$\begin{equation}
\small
\begin{aligned}
    \boldsymbol{D_1}& \coloneq 2\times h+S^{+h}_{-t} \\
     \boldsymbol{D_2}& \coloneq 4\times \tilde{S}^{-h}_{-t}+\boldsymbol{S_{ev}}+4\times \tilde{S}^{-h}_{-t}
\end{aligned}
% \vspace{-1.0ex}
\end{equation}$$

Table [3](#tab:foil_acc){reference-type="ref" reference="tab:foil_acc"} presents the accuracy (proportion of times the model prefers $D_2$ over $D_1$), paired t-test statistics, and p-values. The results are striking: all models exhibit extremely poor performance, with accuracy dropping below 10%. The paired t-test statistics are highly negative across all models, indicating a consistent preference for foil documents over the correct evidence-containing ones. This outcome highlights the severity of bias interplay and its detrimental impact on model reliability. Furthermore, a sufficient number of biased documents can potentially cause the model to select all top-k documents from only biased results.

## Impact on RAG {#Sec:RAG}

::: {#tab:rag}
  --------------------- --------------------------------- ------- ------- -----------
  []{.smallcaps}                     Poison                Foil     No     Evidence
  [Model]{.smallcaps}                  Doc                  Doc     Doc       Doc
  gpt-4o-mini            [32.0%]{style="color: DarkRed"}   44.0%   52.0%   **88.0%**
  gpt-4o                 [30.8%]{style="color: DarkRed"}   62.8%   64.8%   **93.6%**
  --------------------- --------------------------------- ------- ------- -----------

  :  RAG accuracy when using different document versions as references. The poisoned document, preferred by retrievers 100% of the time (Table [\[tab:poison_acc\]](#tab:poison_acc){reference-type="ref" reference="tab:poison_acc"}), results in worse performance than providing no document, highlighting the impact of retriever biases on RAG.
:::

To assess the impact of the identified vulnerabilities on RAG systems, we use GPT-4o models [@openai2024gpt4ocard] and provide them with different versions of the reference document for a given query. Additionally, we construct a poisoned document by modifying the foil document from §[3.4](#sec:interplay){reference-type="ref" reference="sec:interplay"}, introducing a poisoned evidence sentence (Table [\[tab:examples_poison\]](#tab:examples_poison){reference-type="ref" reference="tab:examples_poison"}). Specifically, we generate this sentence using GPT-4o by replacing the tail entity with a contextually plausible but entirely incorrect entity. This approach ensures that the poisoned document both exploits the previously discussed retrieval biases and contains an incorrect answer to the query.[^8]

$$\begin{equation}
\small
\begin{aligned}
    \boldsymbol{D_1}& \coloneq 2\times h+S^{+h}_{-t} + S^{+h}_{+PoisonTail}\\
     \boldsymbol{D_2}& \coloneq 4\times \tilde{S}^{-h}_{-t}+\boldsymbol{S_{ev}}+4\times \tilde{S}^{-h}_{-t}
\end{aligned}
% \vspace{-1.0ex}
\end{equation}$$

Table [4](#tab:rag){reference-type="ref" reference="tab:rag"} reports the RAG accuracy,[^9] showing that, as expected, providing the evidence document enables the LLM to achieve high accuracy. However, since retrievers prefer the foil document from §[3.4](#sec:interplay){reference-type="ref" reference="sec:interplay"}, which lacks evidence, LLM performance drops to levels near[^10] the no-document condition. This preference is concerning, as it allows biases to be exploited, making certain documents more likely to be retrieved despite embedding incorrect information. This is evident with the poisoned document, which degrades performance even worse than presenting no document by introducing false facts. In summary, **retriever biases can mislead RAG systems by providing poisoned or non-informative documents**, ultimately harming performance.

# Conclusions

In this work, we introduced a comprehensive framework for analyzing biases in dense retrieval models. By leveraging a relation extraction dataset (Re-DocRED), we constructed a diverse set of controlled experiments to isolate and evaluate specific biases, including literal, position, repetition, and brevity biases as well as the answer's importance.

Our findings reveal that retrieval models often prioritize superficial patterns, such as exact string matches, repetitive content, or information positioned early in documents, over deeper semantic understanding and the existence of the answer. Moreover, when multiple biases combine, retriever performance deteriorates dramatically.

Furthermore, our analysis shows that retriever biases can undermine RAG's reliability by favoring poisoned or non-informative documents over evidence-containing ones, leading to degraded performance of LLMs. These findings underscore the need for dense retrieval models that are robust to biases and capable of prioritizing semantic relevance.

# Limitations {#limitations .unnumbered}

#### Quality of the Relation Extraction Dataset

Our framework relies on a relation extraction dataset, making both annotation accuracy (precision) and completeness (recall) critical. We use Re-DocRED, which addresses annotation issues in DocRED, but it may still contain imperfections that introduce minor noise into our experiments. To mitigate this, we employ statistical tests and report error margins and p-values to ensure the robustness of our findings.

#### Limitations of RAG Evaluation by LLMs

In our RAG experiments, we utilized GPT-4o models and carefully designed prompts (Table [\[tab:rag_prompts\]](#tab:rag_prompts){reference-type="ref" reference="tab:rag_prompts"}) to poison documents, generate answers using RAG, and evaluate the results against gold-standard answers. Although GPT-4o is one of the most advanced models available, it is not infallible and may introduce some variance in the RAG results and evaluations. Nevertheless, we believe the observed trends and findings remain valid given the model's high performance and the consistency of our experimental setup.

# Acknowledgements {#acknowledgements .unnumbered}

This research is partly supported by a National Science Foundation CAREER award #2339766 and an Amazon AGI Research Award. We thank Jia-Chen Gu for valuable discussions and feedback. We also appreciate the insights from our peers, and we are grateful to the anonymous reviewers for their constructive comments.

# Appendix {#sec:appendix}

## More Models {#sec:more_models}

Our study focuses on dense retrievers that generate a single embedding per document, ensuring efficiency. In contrast, newer models like ColBERT [@10.1145/3397271.3401075-colbert] prioritize accuracy at the cost of efficiency. Specifically, ColBERT employs late interaction, requiring per-token embeddings for both queries and documents, which increases computational and storage demands, especially problematic for long documents. Despite these trade-offs, we evaluated ColBERT and ReasonIR-8B [@shao2025reasonirtrainingretrieversreasoning] on the Foil dataset, and their performance results are in Table [3](#tab:foil_acc){reference-type="ref" reference="tab:foil_acc"}. Despite their higher computational cost, these models still fail on the Foil dataset, with under 9% correct document preferences.

## Models Downstream Performance {#sec:models_performance}

We evaluate several dense retrievers on the Natural Questions (NQ) dataset [@kwiatkowski-etal-2019-natural], comparing their performance using standard retrieval metrics: nDCG@10 and Recall@10[^11]. The models differ in training objectives, datasets, and pooling mechanisms, offering a comprehensive view of their retrieval capabilities in our experimental setup. Table [1](#tab:models_performance_nq){reference-type="ref" reference="tab:models_performance_nq"} (and [6](#tab:models_performance){reference-type="ref" reference="tab:models_performance"}) summarizes the results.

Dragon RoBERTa and Dragon+ [@lin-etal-2023-train] demonstrate the highest performances due to diverse data augmentations and multiple supervision sources, which progressively enhance their generalization.[^12]

COCO-DR [@yu-etal-2022-coco] adopts continuous contrastive learning and implicit distributionally robust optimization (DRO) to address distribution shifts in dense retrieval tasks. It exhibits moderate performance, scoring lower than Dragon models.

Contriever [@izacard2021contriever] uses unsupervised contrastive learning but performs poorly without fine-tuning (nDCG@10: 0.25). Fine-tuning on MSMARCO significantly improves its performance (nDCG@10: 0.50), underscoring the importance of fine-tuning for robust retrieval.

RetroMAE [@xiao-etal-2022-retromae], which introduces a retrieval-oriented pre-training paradigm based on Masked Auto-Encoder (MAE), featuring innovative designs like asymmetric masking, achieves slightly lower performance (nDCG@10: 0.48) compared to fine-tuned Contriever.

::: {#tab:foil_acc_more}
  ---------------------- -------------------------------- -------- ---------
  Model                              Accuracy                      
  Statistic                          p-value                       
  Contriever              [0.4%]{style="color: DarkRed"}   -34.58   \< 0.01
  RetroMAE MSMARCO FT     [0.4%]{style="color: DarkRed"}   -41.49   \< 0.01
  Contriever MSMARCO      [0.8%]{style="color: DarkRed"}   -42.25   \< 0.01
  Dragon RoBERTa          [0.8%]{style="color: DarkRed"}   -36.53   \< 0.01
  Dragon+                 [1.2%]{style="color: DarkRed"}   -40.94   \< 0.01
  COCO-DR Base MSMARCO    [2.4%]{style="color: DarkRed"}   -32.92   \< 0.01
  ColBERT (v2)            [7.6%]{style="color: DarkRed"}   -20.96   \< 0.01
  ReasonIR-8B             [8.0%]{style="color: DarkRed"}   -36.92   \< 0.01
  ---------------------- -------------------------------- -------- ---------

  :  The accuracy and paired t-test comparing a foil document (exploiting biases but lacking the answer) to a second document with evidence embedded in unrelated sentences. All retrieval models perform extremely poorly, highlighting their inability to distinguish biased distractors from genuine evidence.
:::

::: tabular
lc Model & Citation\
[facebook/dragon-plus-query-encoder](https://huggingface.co/facebook/dragon-plus-query-encoder) & @lin-etal-2023-train\
[facebook/dragon-plus-context-encoder](https://huggingface.co/facebook/dragon-plus-context-encoder) &\
[facebook/dragon-roberta-query-encoder](https://huggingface.co/facebook/dragon-roberta-query-encoder) & @lin-etal-2023-train\
[facebook/dragon-roberta-context-encoder](https://huggingface.co/facebook/dragon-roberta-context-encoder) &\
[facebook/contriever-msmarco](https://huggingface.co/facebook/contriever-msmarco) & @izacard2021contriever\
[facebook/contriever](https://huggingface.co/facebook/contriever) & @izacard2021contriever\
[OpenMatch/cocodr-base-msmarco](https://huggingface.co/OpenMatch/cocodr-base-msmarco) & @yu-etal-2022-coco\
[Shitao/RetroMAE_MSMARCO_finetune](https://huggingface.co/Shitao/RetroMAE_MSMARCO_finetune) & @xiao-etal-2022-retromae\

[colbert-ir/colbertv2.0](https://huggingface.co/colbert-ir/colbertv2.0) & @10.1145/3397271.3401075-colbert\
[reasonir/ReasonIR-8B](https://huggingface.co/reasonir/ReasonIR-8B) & @shao2025reasonirtrainingretrieversreasoning\

gpt-4o-mini-2024-07-18 & @openai2024gpt4ocard\
gpt-4o-2024-08-06 & @openai2024gpt4ocard\
:::

::: {#tab:models_performance}
  Model                  Pooling      nDCG@10   Recall@10
  ---------------------- --------- ---------- -----------
  Dragon+                cls         **0.55**    **0.63**
  Dragon RoBERTa         cls             0.53        0.59
  Contriever MSMARCO     avg             0.52        0.59
  Contriever             avg             0.50        0.59
  RetroMAE MSMARCO FT    cls             0.49        0.55
  COCO-DR Base MSMARCO   cls             0.48        0.53

  :  Models' performance on our refined redocred dataset with 7170 queries and 105925 corpus size.
:::

::: {#tab:preliminary_analysis}
  Issue              Count   Percentage
  ---------------- ------- ------------
  Long Document         33          55%
  Missing Answer        19          32%
  Literal Bias          11          18%
  Repetition             6          10%
  Numbers                2           3%
  Position Bias          2           3%

  :  Preliminary findings from our annotation of 60 retrieval errors based on DecompX
:::

The models also differ in their pooling mechanisms. Contriever uses average pooling, where token representations are averaged to form a dense vector for retrieval. In contrast, the other models use CLS pooling, where the representation of the \[CLS\] token is taken as the sentence embedding.

In summary, Dragon models lead in performance, and the significant improvement of fine-tuned Contriever over its unsupervised counterpart highlights the importance of supervision and task-specific adaptation in dense retrieval.

## Position Bias: First vs. Last {#sec:position_bias_first_last}

Further evidence is provided in Figure [7](#fig:initial_final_evidence_ttest){reference-type="ref" reference="fig:initial_final_evidence_ttest"}, where we compared two document variants: **1. Beginning-Evidence Document $\boldsymbol{D_1}$:** The evidence sentence is positioned at the start of the document. **2. End-Evidence Document $\boldsymbol{D_2}$:** The same evidence sentence is positioned at the end of the document.

$$\begin{equation}
\small
\begin{aligned}
    \boldsymbol{D_1}& \coloneq \boldsymbol{S_{ev}}+\sum_{S^{-h}_{-t} \in D_{orig}} S^{-h}_{-t} \\
     \boldsymbol{D_2}& \coloneq \sum_{S^{-h}_{-t} \in D_{orig}} S^{-h}_{-t} + \boldsymbol{S_{ev}}
\end{aligned}
% \vspace{-1.0ex}
\end{equation}$$

An example of the document pairs (Position Bias) is shown in Table [\[tab:examples\]](#tab:examples){reference-type="ref" reference="tab:examples"}. The resulting t-statistics (Figure [1](#fig:radar_plot){reference-type="ref" reference="fig:radar_plot"} and [7](#fig:initial_final_evidence_ttest){reference-type="ref" reference="fig:initial_final_evidence_ttest"}), where higher positive values indicate a stronger preference for evidence at the beginning ($D_1$) over the end ($D_2$), provide another clear metric of positional bias. These results serve as a foundation for our subsequent analysis in the interplay between biases section.

::: table*
  **Relation ID**   **Relation Name**                                  **Query Template**
  ----------------- -------------------------------------------------- ------------------------------------------------------------------------
  P131              located in the administrative territorial entity   Which administrative territorial entity is \<head_entity\> located in?
  P577              publication date                                   When was \<head_entity\> published?
  P17               country                                            Which country is \<head_entity\> associated with?
  P264              record label                                       Which record label is \<head_entity\> associated with?
  P571              inception                                          When was \<head_entity\> founded?
  P361              part of                                            What is \<head_entity\> a part of?
  P800              notable work                                       What is a notable work of \<head_entity\>?
  P569              date of birth                                      When was \<head_entity\> born?
  P159              headquarters location                              Where is the headquarters of \<head_entity\> located?
  P527              has part                                           What are the components of \<head_entity\>?
  P123              publisher                                          Who is the publisher of \<head_entity\>?
  P175              performer                                          Who performed \<head_entity\>?
  P449              original network                                   What is the original network of \<head_entity\>?
  P706              located on terrain feature                         Where is \<head_entity\> located on a terrain feature?
  P580              start time                                         When did \<head_entity\> start?
  P740              location of formation                              Where was \<head_entity\> formed?
  P27               country of citizenship                             Which country is \<head_entity\> a citizen of?
  P403              mouth of the watercourse                           What is the mouth of the watercourse of \<head_entity\>?
  P570              date of death                                      When did \<head_entity\> die?
  P136              genre                                              What genre does \<head_entity\> belong to?
  P576              dissolved, abolished or demolished                 When was \<head_entity\> dissolved or demolished?
  P495              country of origin                                  What is the country of origin of \<head_entity\>?
  P19               place of birth                                     Where was \<head_entity\> born?
  P155              follows                                            What precedes \<head_entity\>?
  P400              platform                                           What platform is \<head_entity\> available on?
  P1344             participant of                                     What was \<head_entity\> a participant of?
  P3373             sibling                                            Who is the sibling of \<head_entity\>?
  P676              lyrics by                                          Who wrote the lyrics for \<head_entity\>?
  P26               spouse                                             Who is the spouse of \<head_entity\>?
  P58               screenwriter                                       Who wrote the screenplay for \<head_entity\>?
  P35               head of state                                      Who is the head of state of \<head_entity\>?
  P6                head of government                                 Who is the head of government of \<head_entity\>?
  P178              developer                                          Who developed \<head_entity\>?
  P279              subclass of                                        What is \<head_entity\> a subclass of?
  P127              owned by                                           Who owns \<head_entity\>?
  P156              followed by                                        What follows \<head_entity\>?
  P140              religion                                           What is the religion of \<head_entity\>?
  P607              conflict                                           What conflict was \<head_entity\> part of?
  P364              original language of work                          What is the original language of \<head_entity\>?
  P463              member of                                          Which organization is \<head_entity\> a member of?
  P179              series                                             What series is \<head_entity\> part of?
  P176              manufacturer                                       Who manufactured \<head_entity\>?
  P190              sister city                                        What is the sister city of \<head_entity\>?
  P20               place of death                                     Where did \<head_entity\> die?
  P112              founded by                                         Who founded \<head_entity\>?
  P31               instance of                                        What is \<head_entity\> an instance of?
  P276              location                                           Where is \<head_entity\> located?
  P86               composer                                           Who composed the music for \<head_entity\>?
  P57               director                                           Who directed \<head_entity\>?
  P272              production company                                 Which production company produced \<head_entity\>?
  P50               author                                             Who is the author of \<head_entity\>?
:::

<figure id="fig:literal_ttest" data-latex-placement="h">
<embed src="figs/files/literal_ttest.pdf" style="width:49.0%" />
<figcaption> Paired t-test statistics comparing retrieval scores between two scenarios: (1) when both query and document use the shortest name variant, and (2) when the query uses the short name but the document contains the long name variant of the same entity. Positive statistics indicate that models favor exact string matches over semantic matching of equivalent entity names. </figcaption>
</figure>

<figure id="fig:initial_final_evidence_ttest" data-latex-placement="h">
<embed src="figs/files/initial_final_evidence_ttest.pdf" style="width:49.0%" />
<figcaption> Paired t-test statistics comparing document scores based on the position of the evidence sentence (beginning vs. end). Higher positive values reflect a preference for evidence at the beginning, indicating positional bias. </figcaption>
</figure>

<figure id="fig:repetition_ttest" data-latex-placement="h">
<embed src="figs/files/repetition_ttest.pdf" style="width:49.0%" />
<figcaption> Paired t-test statistics comparing the dot product similarity of queries with two sets of sentences: (1) <em>More Heads</em>, consisting of evidence and two sentences with head mentions but no tails, and (2) <em>Fewer Heads</em>, consisting of evidence and two sentences without head or tail mentions. Positive values indicate higher similarity for sentences with more heads. </figcaption>
</figure>

<figure id="fig:brevity_ttest" data-latex-placement="h">
<embed src="figs/files/brevity_ttest.pdf" style="width:49.0%" />
<figcaption> Paired t-test statistics comparing scores for documents containing only the evidence sentence versus those containing the evidence plus the full document. Higher positive values indicate a stronger model bias toward brevity. </figcaption>
</figure>

::: table*
+---+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|   | **Method 1 (Higher Query Document Similarity Score)**                                                                                                                                                                                                                                                                                                      | **Method 2 (Lower Query Document Similarity Score)**                                                                                                                                                                                                                                                                                                                                        |
+:==+:===========================================================================================================================================================================================================================================================================================================================================================+:============================================================================================================================================================================================================================================================================================================================================================================================+
|   | **Query:** Who is the publisher of *[Assassin 's Creed Unity]{.mark}* ?\                                                                                                                                                                                                                                                                                   | **Query:** Who is the publisher of *[Assassin 's Creed Unity]{.mark}* ?\                                                                                                                                                                                                                                                                                                                    |
|   | **Document:** \" *[Assassin 's Creed Unity]{.mark}* \" \" *[Assassin 's Creed Unity]{.mark}* \" *[Assassin 's Creed Unity]{.mark}* received mixed reviews upon its release .                                                                                                                                                                               | **Document:** Isa is a town and Local Government Area in the state of Sokoto in Nigeria . It shares borders with \..... *[Assassin 's Creed Unity]{.mark}* [is an action - adventure video game developed by Ubisoft Montreal and published by]{.mark} *[Ubisoft]{.mark}* . Isa is a town and Local Government Area in the state of Sokoto in Nigeria . It shares borders with \.....       |
|   +------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|   | **Query:** Who is the publisher of *[Assassin 's Creed Unity]{.mark}* ?\                                                                                                                                                                                                                                                                                   | **Query:** Who is the publisher of *[Assassin 's Creed Unity]{.mark}* ?\                                                                                                                                                                                                                                                                                                                    |
|   | **Document:** \" *[Assassin 's Creed Unity]{.mark}* \" \" *[Assassin 's Creed Unity]{.mark}* \" *[Assassin 's Creed Unity]{.mark}* received mixed reviews upon its release . *[Assassin 's Creed Unity]{.mark}* [is an action - adventure video game developed by Electronic Arts Montreal and published by]{.mark} *[Electronic Arts]{.mark}*             | **Document:** Isa is a town and Local Government Area in the state of Sokoto in Nigeria . It shares borders with \..... *[Assassin 's Creed Unity]{.mark}* [is an action - adventure video game developed by Ubisoft Montreal and published by]{.mark} *[Ubisoft]{.mark}* . Isa is a town and Local Government Area in the state of Sokoto in Nigeria . It shares borders with \.....       |
|   +------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|   |                                                                                                                                                                                                                                                                                                                                                            |                                                                                                                                                                                                                                                                                                                                                                                             |
+---+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
:::

::: table*
+--------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| **Prompt Utility** | **Prompt**                                                                                                                                                                                          |
+:===================+:====================================================================================================================================================================================================+
| Poisoning          | In the sentence: '{evidence}', replace the entity '{tail}' with a different entity that makes sense in context but is completely different. Output only the replacement entity. replacement entity: |
+--------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| RAG                | Answer the question based on the given document. Only give me the complete answer and do not output any other words. The following is the given document.                                           |
|                    |                                                                                                                                                                                                     |
|                    | Document: {doc}                                                                                                                                                                                     |
|                    |                                                                                                                                                                                                     |
|                    | Question: {query}                                                                                                                                                                                   |
|                    |                                                                                                                                                                                                     |
|                    | Answer:                                                                                                                                                                                             |
+--------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| RAG for No Doc     | Answer the question. Only give me the answer and do not output any other words.                                                                                                                     |
|                    |                                                                                                                                                                                                     |
|                    | Question: {query}                                                                                                                                                                                   |
|                    |                                                                                                                                                                                                     |
|                    | Answer:                                                                                                                                                                                             |
+--------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Evaluation         | Query: {query}                                                                                                                                                                                      |
|                    |                                                                                                                                                                                                     |
|                    | Evidence: {evidence_sentence}                                                                                                                                                                       |
|                    |                                                                                                                                                                                                     |
|                    | Gold Answer: {gold_answer}                                                                                                                                                                          |
|                    |                                                                                                                                                                                                     |
|                    | Model Answer: {model_answer}                                                                                                                                                                        |
|                    |                                                                                                                                                                                                     |
|                    | Does the Model Answer contain or imply the Gold Answer based on the evidence? YES or NO :                                                                                                           |
+--------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
:::

::: table*
+:-------------+:-----------+:-------------+:-----------+------:+------:+------:+------:+------:+------:+
|              |            |              | Model      |       |       |       |       |       |       |
+--------------+------------+--------------+------------+-------+-------+-------+-------+-------+-------+
| Base MSMARCO |            |              |            |       |       |       |       |       |       |
+--------------+------------+--------------+------------+-------+-------+-------+-------+-------+-------+
| MSMARCO FT   |            |              |            |       |       |       |       |       |       |
+--------------+------------+--------------+------------+-------+-------+-------+-------+-------+-------+
|              |            |              |            |       |       |       |       |       |       |
+--------------+------------+--------------+------------+-------+-------+-------+-------+-------+-------+
| MSMARCO      |            |              |            |       |       |       |       |       |       |
+--------------+------------+--------------+------------+-------+-------+-------+-------+-------+-------+
|              |            |              |            |       |       |       |       |       |       |
+--------------+------------+--------------+------------+-------+-------+-------+-------+-------+-------+
| RoBERTa      |            |              |            |       |       |       |       |       |       |
+--------------+------------+--------------+------------+-------+-------+-------+-------+-------+-------+
| Query Name 1 | Doc Name 1 | Query Name 2 | Doc Name 2 |       |       |       |       |       |       |
+--------------+------------+--------------+------------+-------+-------+-------+-------+-------+-------+
| **long**     | **long**   | **long**     | **short**  | 20.67 | 21.92 | 19.22 | 21.05 | 21.03 | 21.64 |
|              |            +--------------+------------+-------+-------+-------+-------+-------+-------+
|              |            | **short**    | **long**   | 23.41 | 23.53 | 21.46 | 22.01 | 13.40 | 7.55  |
|              |            |              +------------+-------+-------+-------+-------+-------+-------+
|              |            |              | **short**  | 18.43 | 19.60 | 16.41 | 17.35 | 4.99  | 1.75  |
+--------------+------------+--------------+------------+-------+-------+-------+-------+-------+-------+
| **short**    | **short**  | **long**     | **short**  | 2.19  | 3.86  | 2.32  | 4.65  | 9.05  | 5.57  |
|              |            +--------------+------------+-------+-------+-------+-------+-------+-------+
|              |            | **short**    | **long**   | 13.33 | 13.67 | 13.31 | 14.32 | 16.58 | 17.18 |
+--------------+------------+--------------+------------+-------+-------+-------+-------+-------+-------+
:::

<figure id="fig:repetition_bias_heatmap" data-latex-placement="h">
<embed src="figs/files/repetition_heatmap.pdf" style="width:99.0%" />
<figcaption> The average retrieval dot product score for samples with different document lengths and head entity repetitions. (See Figure <a href="#fig:repetition_bias_heatmap_support" data-reference-type="ref" data-reference="fig:repetition_bias_heatmap_support">11</a> for the number of examples in each bin) </figcaption>
</figure>

<figure id="fig:repetition_bias_heatmap_support" data-latex-placement="h">
<embed src="figs/files/repetition_heatmap_support.pdf" style="width:40.0%" />
<figcaption> The number of samples in each bin of Figures <a href="#fig:repetition_bias_heatmap" data-reference-type="ref" data-reference="fig:repetition_bias_heatmap">10</a> and <a href="#fig:repetition_bias_heatmap_contriever" data-reference-type="ref" data-reference="fig:repetition_bias_heatmap_contriever">5</a>. </figcaption>
</figure>

::: table*
  ---------------------- -------------------------------- -------- ---------
  Model                                          Accuracy          
  Statistic                                       p-value          
  Dragon+                  [0.0%]{style="color: DarkRed"}   -55.16   \< 0.01
  Dragon RoBERTa           [0.0%]{style="color: DarkRed"}   -49.17   \< 0.01
  Contriever MSMARCO       [0.0%]{style="color: DarkRed"}   -46.96   \< 0.01
  COCO-DR Base MSMARCO     [0.0%]{style="color: DarkRed"}   -40.19   \< 0.01
  RetroMAE MSMARCO FT      [0.0%]{style="color: DarkRed"}   -48.10   \< 0.01
  Contriever               [1.2%]{style="color: DarkRed"}   -33.60   \< 0.01
  ---------------------- -------------------------------- -------- ---------
:::

<figure id="fig:decompx_heatmap_3442" data-latex-placement="t!">
<embed src="figs/files/decompx_heatmap_3442.pdf" style="width:99.0%" />
<figcaption> Visualization of token-wise effects on retriever scores using DecompX. </figcaption>
</figure>

<figure id="fig:decompx_heatmap_2270" data-latex-placement="t!">
<embed src="figs/files/decompx_heatmap_2270.pdf" style="width:99.0%" />
<figcaption> Visualization of token-wise effects on retriever scores using DecompX. </figcaption>
</figure>

<figure id="fig:decompx_heatmap_864" data-latex-placement="t!">
<embed src="figs/files/decompx_heatmap_864.pdf" style="width:99.0%" />
<figcaption> Visualization of token-wise effects on retriever scores using DecompX. </figcaption>
</figure>

[^1]: [$\boldsymbol{\ast}$]{style="color: coldercolor"} Code and benchmark dataset are available at <https://huggingface.co/datasets/mohsenfayyaz/ColDeR>.

[^2]: Using ttest_rel function of SciPy [@2020SciPy-NMeth].

[^3]: $M$ is the retriever model's score

[^4]: $SE=\frac{\sigma}{\sqrt{n}}$

[^5]: Fig. [1](#fig:radar_plot){reference-type="ref" reference="fig:radar_plot"} shows the impact of evidence placement (beginning vs. end), detailed in Appendix [5](#sec:appendix){reference-type="ref" reference="sec:appendix"}, with an example in Table [\[tab:examples\]](#tab:examples){reference-type="ref" reference="tab:examples"}.

[^6]: We avoid long-long combinations to control for confounding effects, as they span multiple tokens and may introduce repetition bias due to token overlap

[^7]: $\tilde{S}$ are sentences from an unrelated document

[^8]: Despite this, retrievers prefer the poisoned document over the evidence document in 100% of cases (Table [\[tab:poison_acc\]](#tab:poison_acc){reference-type="ref" reference="tab:poison_acc"}).

[^9]: Evaluated using GPT-4o. Prompts in Table [\[tab:rag_prompts\]](#tab:rag_prompts){reference-type="ref" reference="tab:rag_prompts"}

[^10]: Slightly lower, as the model sometimes abstains by stating, "The document does not provide information."

[^11]: Using BEIR framework [@thakur2021beir]

[^12]: Dragon RoBERTa is initialized from RoBERTa and Dragon+ from RetroMAE
