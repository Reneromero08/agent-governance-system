# Introduction {#sec:intro}

Tokenization is a fundamental step in the preprocessing pipeline of large language models (LLMs) [@gpt4; @gemini23; @llama2; @ernie-code; @starcoder2-24], converting raw text into a sequence of subword units derived from a predefined vocabulary [@bpe; @spm18]. This process, while effective in many scenarios, presents significant challenges that can hinder the performance and robustness of LLMs. These challenges include sensitivity to typographical errors [@CaoKMI23], length variations [@htlm22], and a lack of awareness of the internal structure of tokens [@brown2020language]---collectively termed *the curse of tokenization*.

<figure id="fig:vec_plot" data-latex-placement="!ht">

<figcaption>Compositional challenges in token embeddings. (a) <code>‘‘assignment’’</code> decomposed into <code>‘‘assign’’</code> and <code>‘‘ment’’</code> shows a cosine similarity of 0.21 and an angle of 78.16°. (b) <code>‘‘import’’</code> decomposed into <code>‘‘im’’</code> and <code>‘‘port’’</code> shows a cosine similarity of 0.13 and an angle of 82.47°. These results indicate that existing LLMs do not accurately capture surface form composition. </figcaption>
</figure>

Typographical errors, such as minor misspellings or misplaced characters, can drastically affect the tokenization process. Unlike humans, who can easily overlook these errors and understand the intended meaning, LLMs can misinterpret or fail to recognize these variations, leading to degraded performance [@CaoKMI23]. This typo-sensitivity reveals a crucial gap in current tokenization methods, which do not sufficiently mimic human reading capabilities.

Another critical issue is the length unawareness of current tokenization approaches. LLMs often struggle to accurately represent the organizational structure of text, being insensitive to the number of characters or words [@htlm22]. This insensitivity affects their ability to understand and process text effectively, particularly in tasks requiring a nuanced understanding of text length and compositional structure.

Furthermore, existing tokenization methods are often blind to the internal structure of tokens. The decoupled embedding space and lookup table approach **fail to account for the hierarchical composition of language**, spanning characters, subwords, and words, as depicted in Figure [1](#fig:vec_plot){reference-type="ref" reference="fig:vec_plot"}. This lack of integration across different levels of token composition limits the model's ability to fully grasp semantic relationships and differences.

Case insensitivity in certain languages adds another layer of complexity, where variations in capitalization can lead to different token representations, further complicating the model's understanding and processing of text.

To address these challenges, we conducted a comprehensive study examining the limitations of current tokenization methods and their impact on LLM performance. Our study is guided by three critical research questions:

**A. Complex Problem Solving** (§[3](#sec:rq1){reference-type="ref" reference="sec:rq1"}). As a pilot experiment, we firstly investigate the performance of LLMs on complex problems that are sensitive to tokenization, involving anagram task and complex mathematical language understanding.

**B. Token Structure Probing** (§[4](#sec:rq2){reference-type="ref" reference="sec:rq2"}). We study the token structural tasks such as case manipulation, length counting, and length-sensitive tasks to probe the token structural understanding of LLMs.

**C. Typographical Variation** (§[5](#sec:rq3){reference-type="ref" reference="sec:rq3"}). We designed a robust set of evaluation benchmarks on top of various datasets such as MMLU, TruthfulQA, GSM8K, and HumanEval, covering diverse tasks and linguistic phenomena. These benchmarks allow us to systematically test and analyze the LLMs' resilience to tokenization.

Our findings highlight that while scaling model parameters can enhance the robustness to tokenization, LLMs still suffer from biases introduced by typographical errors and text format variations. We demonstrate the persistent nature of these tokenization challenges.

#### Contribution

To conclude, our main contributions are as follows:

1.  We provide a comprehensive analysis of the problem known as the *curse of tokenization*, detailing its impact on language model performance and introducing systematic evaluation benchmarks to assess these issues.

2.  By evaluating various scales of LLMs, including LLama3, Mistral, and GPT-4 families, across thirteen distinct tasks, we demonstrate that even state-of-the-art models struggle with handling typographical variations. Specifically, LLMs exhibit greater sensitivity to character-level variations compared to subword-level variations.

3.  We demonstrate that regularized tokenization approaches, such as BPE-dropout with moderate drop rates, can enhance the model's resilience to the discussed issues.

# Related Work {#sec:bg}

## Tokenization

#### Tokenization Approach

Conventional language models [@radford2018improving; @jozefowicz2016exploring; @brown2020language] typically tokenize input text into a sequence of tokens by splitting it into smaller subwords. Traditional tokenization approaches include frequency-based methods such as Byte Pair Encoding (BPE; [@bpe]) and probability-based methods like WordPiece [@DBLP:conf/icassp/SchusterN12]. BPE merges tokens based on bigram frequency, relying on subword pair co-occurrence to greedily merge neighboring pairs. In contrast, WordPiece can be viewed as a language-modeling based BPE variant. It select the unit pair that maximizes the bigram likelihood of training data at utmost, rather than choose the most frequent pair.

Unigram Language Model [@kudo-2018-subword] prunes tokens based on unigram LM perplexity, treating the segmentation process as a probabilistic mixture of characters, subwords, and words, reducing subwords by evaluating likelihood reduction. Additionally, some tokenization methods handle text at the byte level [@byt5] or character level [@10.5555/3104482.3104610; @clark-etal-2022-canine]. Conventional LLMs often use byte-level BPE (BBPE) for base vocabulary construction, representing any text with a moderate vocabulary size and avoiding the out-of-vocabulary (OOV) problem. For a detailed introduction to tokenization, readers can refer to @chai2021tokenization-PTMs.

#### Tokenization-Free Approach

Tokenization approaches often suffer from the *vocabulary bottleneck*, where there is a trade-off between vocabulary size and diverse language coverage in multilingual scenarios. To address this issue, @pixel23 and @chai2024autoregressive introduced a tokenization-free approach that renders raw text as visual text images for masked language modeling and autoregressive pre-training. This method demonstrates robust multilingual generalization capabilities compared to subword tokenization approaches.

## Perturbation Probing {#sec:anagram}

Several studies have investigated the behavior of language models under input perturbations at various levels, including character-level [@nishino-etal-2019-generating], subword-level [@abdou-etal-2022-word], and word-level [@sinha-etal-2021-unnatural; @peng-etal-2023-testing] scrambling. Despite these efforts, intrinsic evaluations of perturbing LLM inputs remain under-explored.

@CaoKMI23 proposed examining scrambled sentence recovery and scrambled QA with context corruptions. In contrast, our study conducts a comprehensive evaluation of both character- and subword-level perturbations, along with noise injection. We evaluate a wide range of LLMs across various tasks to provide a detailed comparison and inspire future research in tokenization and robust model performance.

# Complex Problem Solving {#sec:rq1}

Complex problem-solving tasks are critical benchmarks for evaluating the complex reasoning and comprehension capabilities of LLMs. We explore the LLM's ability to perform intricate operations on tokenized inputs, as the tokenization process is fundamental to determine how the raw text is segmented and processed, directly impacting the model's interpretation and prediction.

Anagram solving and mathematical language comprehension were selected to elucidate the relationship between tokenization quality and the model's performance on complex problem-solving. Anagram tasks require models to decode and rearrange jumbled letters into coherent words, emphasizing the importance of precise token boundaries and recognition accuracy. On the other hand, mathematical language comprehension, particularly expressed with LaTeX-formatted expressions, demands an exact interpretation of specialized symbols and structured notation, challenging the tokenization process's robustness.

## Anagram Task

#### Task Description and Settings

The anagram task tests the model's ability to unscramble a sequence of jumbled characters to form a valid word. This task evaluates the model's handling of surface-form manipulations and its understanding of char-level compositions within a word. The complexity arises from the need to identify potential word candidates from mixed characters and reassemble them correctly. We present a task example in §[8.1](#ap:example-complex){reference-type="ref" reference="ap:example-complex"}. Specifically, we include two tasks:

<figure id="fig:anagram-plot" data-latex-placement="!ht">
<embed src="figure/anagram_plot.pdf" style="width:75.0%" />
<figcaption><span class="math inline"><em>K</em></span>-shot performance on Word Unscrambling (WU) and Cycled Letters (CL) tasks. The plots illustrate that increasing the number of demonstration examples (<span class="math inline"><em>K</em></span>-shot) does not consistently enhance performance. However, models with larger parameter sizes generally exhibit better performance across both tasks.</figcaption>
</figure>

- **Cycled Letters in Word (CL; [@bigbench22])** -- The model is given a word with its letters cycled, and is expected to generate the original word (*e.g.*, "remo" $\rightarrow$ "more").

- **Word Unscrambling (WU; [@bigbench22])** -- The model is given a randomly scrambled word, and must recover the original word (*e.g.*, "nad" $\rightarrow$ "and").

We employ exact match (EM) scores for evaluation. Unless otherwise specified, we use the inference-time temperature of $0$ for all LLMs in following experiments, to assure the results reproducible.

#### Results and Analysis

The experimental results reveal that larger models demonstrate better performance on the anagram task, yet they remain susceptible to tokenization errors. Specifically, models struggled with longer anagrams (see Figure [3](#fig:WU-boxplot-llama3){reference-type="ref" reference="fig:WU-boxplot-llama3"}) or those containing uncommon letter combinations, indicating that while scaling improves token recognition, inherent tokenization flaws persist.

<figure id="fig:WU-boxplot-llama3" data-latex-placement="!ht">
<embed src="figure/word_unscramble_llama3_70b.pdf" style="width:65.0%" />
<figcaption>The relationship between the length of scrambled words and the Exact Match (EM) score of Llama3-8B and Llama3-70B on the word unscrambling task under one-shot evaluation. The models tend to correctly reorder anagrams of shorter lengths, while struggling with longer words.</figcaption>
</figure>

Figure [3](#fig:WU-boxplot-llama3){reference-type="ref" reference="fig:WU-boxplot-llama3"} highlights the performance differences between Llama3-8B and Llama3-70B on the word unscrambling task across various word lengths. Notably, Llama3-70B consistently outperforms Llama3-8B, especially in the 0-6 and 6-12 character buckets. This trend indicates that as the model parameter size increases from 8B to 70B, the ability to accurately reorder scrambled words improves. However, both models struggle with longer words (12-18 characters), though Llama3-70B maintains a moderate edge.

Our results shown in Figure [2](#fig:anagram-plot){reference-type="ref" reference="fig:anagram-plot"} indicate several key trends. Firstly, the performance improves significantly as model size scales from 8B to 70B parameters [@llama3modelcard]. Secondly, we observe that while the dense Mistral-7B model performs poorly, the sparse Mixtral-8x7B model (an MoE sparse model) shows improved performance due to its parameter size scaling. Lastly, GPT-4 turbo, a much more powerful model, achieves state-of-the-art results, clearly outperforming all other models across all shot conditions. This sensitivity underscores the need for more robust tokenization that can handle typographical variations without degrading performance.

## Mathematical Language (LaTeX) Comprehension

**Task Description and Settings** The mathematical language comprehension task evaluates the model's ability to read and comprehend mathematics written in LaTeX, the typesetting language used by professional mathematicians. This task assesses the models' capability to interpret complex mathematical expressions and accurately tokenize symbols and operators within structured LaTeX format.

We employ **Identify Math Theorems (IMT; [@bigbench22])** for evaluation, and use perplexity to measure the model's confidence in different given choices. The dataset comprises 54 problems divided into nine sections, each representing a major area of mathematics research or advanced pedagogy. We present the input-output format in §[8.1](#ap:example-complex){reference-type="ref" reference="ap:example-complex"}.

#### Results and Analysis

Our evaluation results of various models are shown in Table [\[tab:rq1-math\]](#tab:rq1-math){reference-type="ref" reference="tab:rq1-math"}. The results demonstrate that while larger models generally perform better on LaTeX-formatted mathematical content, the relationship between the number of in-context examples and model performance is not linear. The Llama3-70B model consistently outperformed other models, achieving a score of `62.26%` in the zero-shot setting and improving to `79.25%` with one-shot learning. However, additional in-context examples led to fluctuating performance, with scores of `69.81%` in the two-shot setting and `71.70%` in the three-shot setting. This indicates that increasing the number of in-context demonstrations does not consistently enhance performance and may lead to variability depending on the specific examples provided.

Other models exhibited similar trends. GPT-3-200B [@bigbench22], despite its larger parameter count, did not show significant improvement over the smaller GPT-3-6B model, suggesting that simply increasing the model size does not guarantee better performance in LaTeX comprehension tasks. The comparison between dense and sparse models revealed that the dense Mistral-7B model performed poorly across all shot conditions, while the sparse Mixtral-8x7B model demonstrated better results, especially in few-shot scenarios.

# Token Structure Probe {#sec:rq2}

Tokenization is a key preprocessing step in LLMs, yet it introduces several significant challenges, which we defined as . These challenges include length unawareness, case insensitivity, and a lack of awareness of the internal structure of tokens. Tokenization transforms text into sequences of token identifiers, often obscuring the surface form and internal structure of the original text. This conversion can lead to deficiencies in the model's ability to understand and process textual data accurately.

*The curse of tokenization* manifests in several ways, which refers to the inherent challenges:

1.  **Length Unawareness**: Models struggle to recognize the organizational structure of text, such as the number of characters or words.

2.  **Case Insensitivity**: Variations in capitalization can lead to different token identifiers and representations, complicating the model's processing of text.

3.  **Blindness to Internal Structure**: The decoupled embedding space and lookup table approach used in LLMs fail to preserve the hierarchical and relational structure within tokens, obscuring the surface form and internal relationships between characters and subwords.

To address these challenges, we construct a set of probing tasks to evaluate the model's understanding of token structure. These tasks are divided into intra-token (§[4.1](#sec:intra-token){reference-type="ref" reference="sec:intra-token"}) and inter-token probes (§[4.2](#sec:inter-token){reference-type="ref" reference="sec:inter-token"}).

## Intra-Token Probing {#sec:intra-token}

#### Task Description and Settings

To measure the capability of LLMs, we devise intra-token probing tasks related to length, case, and counting problems. These tasks evaluate the model's performance on the internal structure of tokens or word, specifically including four tasks:

- **Character Count (CC)** -- The model is asked to count the number of occurrences of a specific character within the given word (*e.g.*, the character appears twice in the word "undertake" $\rightarrow$ the answer is: "e").

- **$N$-th Character (NC)** -- The model is expected to output the $n$-th character of the given word (*e.g.*, 4-th character of the word "dual" $\rightarrow$ "l").

- **$N$-th Character Reverse (NCR)** -- The model must identify the $n$-th character from the end of a word (*e.g.*, 2nd character from the end of the word "dual" $\rightarrow$ "a").

- **Case Conversion (CCV)** -- This task involves converting the characters within a word to different cases (uppercase, lowercase) or converting the word into title case.

For each task, we conduct many-shot evaluation (0-3 shot) and report the EM score to test the model's ability to understand and manipulate the internal structure of tokens and words at a granular level, revealing the extent to which the tokenization process could preserve this information. Detailed test examples are provided in Appendix [8.2](#ap:example-intra-token){reference-type="ref" reference="ap:example-intra-token"}.

<figure id="fig:intra-token-plot" data-latex-placement="!ht">
<embed src="figure/intra_token_probe_barplot.pdf" style="width:95.0%" />
<figcaption><span class="math inline"><em>K</em></span>-shot performance on intra-token probing tasks (CCV, CC, NC, NCR). The plots demonstrate that increasing the number of demonstration examples (<span class="math inline"><em>K</em></span>-shot) generally results in an improvement from zero-shot to one-shot, with performance stabilizing thereafter. </figcaption>
</figure>

#### Results and Analysis

We evaluated CC, NC, NCR, and CCV tasks across different models and shot settings. The results are presented in Figure [4](#fig:intra-token-plot){reference-type="ref" reference="fig:intra-token-plot"}.

The CC task reveals that larger models exhibit superior performance, particularly GPT-4 turbo, which achieves near-perfect accuracy across all shot conditions. Smaller models, such as Llama3-8B, show significant improvement with few-shot learning, indicating that exposure to examples greatly enhances their performance. For example, Llama3-8B's accuracy improves from 0% in the zero-shot setting to 81% in the three-shot setting. This demonstrates that increased model size and few-shot learning contribute positively to CC tasks.

The NC task underscores the difficulty models face in accurately identifying specific characters within words. GPT-4 turbo again leads in performance, while smaller models show substantial improvement with increased shots. Llama3-70B, for instance, improves from 1% in the zero-shot setting to 55% in the three-shot setting. This indicates that while larger models perform better, few-shot learning plays a crucial role in enhancing the model's ability to identify specific characters.

Identifying characters from the end of the word, or reverse character identification, proves more challenging. GPT-4 turbo achieves the highest performance with a score of 52% in the one-shot setting, though overall accuracy is lower compared to other tasks. Smaller models like Llama3-8B show moderate improvements with additional shots, but their performance remains relatively low. This highlights the complexity of reverse character identification and the need for more advanced tokenization strategies to address this challenge.

<figure id="fig:inter-token-probe" data-latex-placement="!ht">

<figcaption><span class="math inline"><em>K</em></span>-shot performance on various inter-token probing tasks. For edit distances, lower is better.</figcaption>
</figure>

## Inter-Token Probing {#sec:inter-token}

#### Task Description and Settings

To evaluate the capabilities of LLMs in understanding and manipulating relationships between multiple tokens, we devised inter-token probing tasks. These tasks focus on identifying common patterns and sequences across tokens, and they assess the model's ability to recognize and process such relationships. Specifically, we include three tasks:

- **Common Substrings (CS)** -- The model identifies multiple common substrings between two given words.

- **Longest Common Substrings (LCS)** -- The model identifies the longest continuous common substring for two given words.

- **Longest Common Subsequences (LCSeq)** -- The model identifies the longest common subsequence (not necessarily continuous) between two given words.

For each task, we conduct many-shot evaluations (0-3 shot) and report the EM and edit distance (ED) score to test the model's ability to understand and manipulate relationships between tokens at a higher level. For CS tasks, the model's response is considered correct if it generates one of the multiple possible common substrings. Detailed test examples are provided in Appendix [8.3](#ap:example-inter-token){reference-type="ref" reference="ap:example-inter-token"}.

#### Results and Analysis

The results are presented in Figure [5](#fig:inter-token-probe){reference-type="ref" reference="fig:inter-token-probe"}. The results for CS tasks indicate that larger models, such as Llama3-70B and GPT-4 Turbo, perform significantly better than smaller models. GPT-4 Turbo achieves the highest accuracy across all shot settings, demonstrating the model's robustness in identifying continuous substrings. Notably, Llama3-70B also shows strong performance, particularly in the three-shot setting. Sparse models like Mixtral-8x7B exhibit notable improvements compared to dense models, highlighting the effectiveness of sparse architectures in handling complex token relationships.

For LCS tasks, GPT-4 Turbo leads in performance, achieving high accuracy across all shot settings. Llama3-70B and Mixtral-8x7B show considerable improvements with increased shots, indicating that exposure to more examples helps models better identify multiple common substrings. Dense models like Mistral-7B lag behind, reinforcing the advantage of sparse architectures in such tasks.

The LCSeq task reveals that even the best-performing models face challenges with non-continuous patterns. While Llama3-70B and GPT-4 Turbo demonstrate superior performance, there is a significant drop in accuracy compared to CS tasks. Few-shot learning significantly enhances the performance of smaller models, such as Llama3-8B, which improves from 1% in zero-shot to 4% in three-shot settings. This underscores the importance of few-shot examples in aiding models to recognize and process non-continuous patterns.

# Typographical Variation {#sec:rq3}

<figure id="fig:exp3" data-latex-placement="!ht">

<figcaption>Performance comparison for various models across different <span class="math inline"><em>n</em></span>-gram sizes (<span class="math inline"><em>n</em></span>=2,3,5) and typographical variations on (1) <strong>TruthfulQA</strong>, (b) <strong>MMLU</strong>, (c) <strong>GSM8K</strong>, and (d) <strong>HumanEval</strong>. The typographical variations include character-level (blue) and token-level (orange) perturbations, with noise (solid line) and reorder (dashed line) types. Baseline performance is indicated with a dotted line at the top of each plot.</figcaption>
</figure>

To evaluate the robustness of LLMs to typographical variations, we constructed tasks that introduce character-level and token-level typographical errors into the input text. These tasks are designed to test the models' ability to maintain semantic understanding despite the presence of such errors. The datasets include MMLU [@mmlu], TruthfulQA [@truthfulqa], GSM8K [@gsm8k], and HumanEval [@he], ensuring diverse coverage.

#### Task Description and Settings

The primary goal of these tasks is to assess the LLMs' resilience to typographical errors at both the character and token levels, examining whether these models can maintain semantic understanding when faced with such perturbations. For **character-level typographical variation**, we employed $n$-gram shuffling within word boundaries (with $n$ set to 2, 3, 5) with a 50% probability, and $n$-gram noise, which involve adding, deleting, and replacing characters, spaces, and punctuation marks to simulate spelling noise. This corruption occurs with a 30% probability, including insertion, deletion, or substitution operations. **Token-level typographical variation** was introduced by shuffling tokens within $n$-grams of sizes 2, 3, and 5, with a 50% probability of permutation or typo generation, similar to the character-level method. We include four tasks:

- **Character-Level Permutation**: Shuffling characters within word boundaries using $n$-grams of sizes 2, 3, and 5, with a 50% probability.

- **Character-Level Noise**: Adding, deleting, replacing random characters to simulate spelling noise, each with a 10% probability.

- **Token-Level Permutation**: Randomly reordering tokens using $n$-grams of sizes 2, 3, 5, with a 50% probability.

- **Token-Level Noise**: Adding, deleting, replacing tokens to inject noise, with a 30% probability.

For evaluation, we report pass@1 for HumanEval using a temperature of 0.2 and a top-$p$ of 0.95. For others, we used a temperature of 0. GSM8K was evaluated using a 5-shot setting, while MMLU, TruthfulQA, and HumanEval were assessed in a zero-shot setting. We measured performance on MMLU and TruthfulQA using perplexity for multiple-choice selection[^2].

#### Results and Analysis

Figures [6](#fig:exp3){reference-type="ref" reference="fig:exp3"} presents a comprehensive analysis of the impact of typographical variations on various LLMs, specifically focusing on character-level and token-level perturbations across different $n$-gram sizes ($n$=2, 3, 5). The evaluation covers a range of datasets, including TruthfulQA, MMLU, GSM8K (5-shot), and HumanEval.

Across all datasets and models, there is a consistent trend showing that LLMs are much more sensitive to noise (solid lines) than to reordering (dashed lines). Noise injection, which involves adding, deleting, or replacing characters or tokens, leads to more pronounced variations and generally degrades overall performance. This is evident from the lower EM scores for noise perturbations compared to reorder perturbations.

Despite the challenges posed by $n$-gram reordering within word boundaries, GPT-4 Turbo maintained high accuracy across all $n$-gram sizes. Character-level $n$-gram noise injection, simulating realistic spelling noise, further tested the models' robustness. The results indicate that all models experienced evident performance degradation, regardless of the parameter sizes, highlighting their sensitivity to typographical noise.

For most models and datasets, as the $n$-gram size of noise injection increases from $n$=2 to 5, the performance tends to stabilize or improve. This trend suggests that models can better handle larger $n$-gram noise injection, likely because the context within larger $n$-grams provides more semantic coherence compared to smaller $n$-grams.

At the token level, models were subjected to $n$-gram permutations similar to those applied at the character level. The results indicated that models generally performed better with token-level permutations than with character-level shuffles and noise injection. This suggests that token-level errors may be less disruptive to the overall semantic structure of the input text.

# Does BPE-dropout Matter? {#sec:ft}

<figure id="fig:tk-ft" data-latex-placement="!ht">
<embed src="figure/tk_finetune_plot.pdf" style="width:95.0%" />
<figcaption><span class="math inline"><em>K</em></span>-shot performance on various tasks (CCV, CC, NC, NCR, CS, LCSeq, and LCS) using the Mistral-7B model fine-tuned with a BPE-dropout tokenizer at different dropout rates <span class="math inline"><em>p</em></span>, ranging from 0 to 0.8. The baseline without BPE-dropout (<em>i.e.</em>, <span class="math inline"><em>p</em> = 0</span>) is depicted with a dashed line. It demonstrates that introducing a moderate amount of variability during tokenization improves the model’s generalization capabilities, mitigating <em>the curse of tokenization</em> issues. </figcaption>
</figure>

<figure id="fig:ft_epoch" data-latex-placement="!ht">

<figcaption>The impact of BPE-dropout on EM scores across seven tasks (CS, LCSeq, CCV, CC, NC, LCS, NCR) under different post-training conditions: (a) 0-shot, (b) 1-shot, (c) 2-shot, and (d) 3-shot. The dropout rates range from 0.0 to 0.8. The plots show that moderate dropout rates generally lead to improvements. Tasks such as CS and CC are more robust to dropout, maintaining higher scores even at moderate dropout rates, while tasks like NC, LCS, and NCR show significant performance drops with increasing dropout.</figcaption>
</figure>

To further enhance the robustness of LLMs, we explore regularized tokenization approach, BPE-dropout [@provilkov-etal-2020-bpe], which randomly drops BPE merges during tokenization. This technique allows text sequences to be tokenized in more diverse ways, promoting robustness to various token combinations and increasing the likelihood of encountering smaller tokens. Intuitively, this diversity benefits the model's understanding of internal token structures.

#### Training Setup

For the training data, we synthesized a dataset consisting of 111k examples specifically designed for RQ2. We employ the AdamW optimizer [@loshchilov2017decoupled] with the hyperpameters of $\beta_1=0.9$, $\beta_2=0.95$. The peak learning rate is set to 5e-5, and the minimum learning rate is set to 1e-6. The learning rate warms up during the first 10% of training steps and then decays with a cosine scheduler.

Given the difference in data distribution resulting from BPE-dropout, we post-train Mistral-7B [@mistral23] on the training split for 5 epochs with a global batch size of 16. Following the pre-training recipe, we concatenate all sequences and then chunk them into fixed context lengths of 4096 for our autoregressive post-training. We conduct data shuffling within the same epochs. More detailed training settings are provided in Appendix [9.3](#ap:train_details){reference-type="ref" reference="ap:train_details"}.

#### Results and Analysis

Figure [7](#fig:tk-ft){reference-type="ref" reference="fig:tk-ft"} shows the impact of varying BPE-dropout rates on the Mistral-7B model's performance across multiple $K$-shot settings and tasks. The baseline performance, with a dropout rate of $p=0$, shows robust results across several tasks, particularly in CS and CC. These tasks are relatively straightforward, involving simple token manipulations that do not significantly challenge the model's capacity to generalize from zero-shot to few-shot scenarios. The high performance in these tasks suggests that the Mistral-7B model, even without BPE-dropout, is adept at handling simpler token relationships. However, it is important to note that the baseline performance does not uniformly extend to more complex tasks.

In contrast, tasks such as LCSeq reveal relatively low performance across all models, irrespective of the BPE-dropout rate. This suggests inherent difficulties in these tasks that stem from the requirement to identify non-continuous and intricate token patterns. The consistent under-performance indicates that LCSeq tasks pose a significant challenge to the model's ability to generalize, likely due to the increased complexity in recognizing and processing longer and fragmented sequences.

Interestingly, the introduction of a moderate BPE-dropout rate ($p=0.2$) frequently surpasses the baseline, highlighting the benefits of inducing variability during tokenization. This moderate dropout rate enhances the model's generalization capabilities by preventing overfitting and promoting a more robust learning process. Notably, in tasks such as CCV, NC, and LCS, the $p=0.2$ model consistently achieves higher EM scores, underscoring the benefits of incorporating tokenization regularization.

Our analysis reveals that higher dropout rates ($p=0.6$ and $p=0.8$) exhibit relatively lower performance across most tasks. This decline can be attributed to insufficient training, as the dataset was trained for only five epochs. The higher dropout rates introduce greater tokenization variation, which necessitates additional training compute to achieve convergence. The lack of adequate training epochs likely hindered these models from fully leveraging the potential benefits of higher BPE-dropout rates. Moreover, we report the test-set performance across seven tasks over the course of BPE-dropout fine-tuning in Figure [8](#fig:ft_epoch){reference-type="ref" reference="fig:ft_epoch"}. We provided detailed training analysis in Appendix §[11](#ap:results){reference-type="ref" reference="ap:results"}.

# Conclusion {#sec:concl}

In this study, we investigated the challenge of *the curse of tokenization*, comprehensively evaluating mainstream LLMs across thirteen tasks sensitive to conventional subword tokenization. Our findings reveal that while larger models and increased shot counts can partially mitigate these issues, LLMs still struggle with understanding internal structures and token compositions. Moderate BPE-dropout can alleviate some of these challenges, whereas larger drop rates lead to performance degradation. We encourage the research community to develop more flexible approaches to further address these limitations and enhance model robustness.

# Limitations {#limitations .unnumbered}

While our study provides valuable insights into the robustness and performance of large language models (LLMs) under various tokenization and typographical variation scenarios, several limitations should be acknowledged:

#### Data Diversity and Size

The training data synthesized for RQ2 (token structure probe) consists of approximate 30k examples. While this dataset size is substantial, it may not fully capture the diversity and complexity of real-world text. Future work could benefit from expanding the dataset size and incorporating a wider range of linguistic phenomena.

#### Evaluation Metrics

Our evaluation primarily relies on metrics such as pass@1 for HumanEval and accuracy for MMLU, TruthfulQA, and GSM8K. While these metrics provide valuable insights, they may not fully capture the nuanced performance of LLMs in real-world applications. Incorporating additional metrics that assess other aspects of model performance, such as robustness to out-of-distribution data and interpretability, could provide a more comprehensive evaluation.

#### Subword Regularization

Although our use of BPE-dropout shows promising improvements in model robustness and accuracy, the approach introduces randomness into the tokenization process. This randomness can lead to variability in model performance, making it challenging to ensure consistent improvements across different datasets and tasks. Further research is needed to optimize the BPE-dropout technique and evaluate its long-term impact on model performance.

#### Typographical Variation {#typographical-variation}

Our study focuses on character-level and token-level typographical variations, but it does not address other common types of text perturbations, such as grammatical errors, semantic variations, or contextual inconsistencies. Exploring the effects of these additional types of variations could provide a more holistic understanding of LLM robustness.

#### Generalizability

The findings from our evaluation on specific datasets (MMLU, TruthfulQA, GSM8K, and HumanEval) may not generalize to all types of text and tasks. Further studies are needed to assess the generalizability of our findings across a broader range of datasets and real-world scenarios, such as extending to multilingual evaluation [@peng-etal-2024-humaneval-xl].

# Ethical Consideration {#ethical-consideration .unnumbered}

#### Bias and Fairness

Tokenization strategies can introduce or exacerbate biases present in the training data. Our study, which involves diverse tokenization techniques like BPE-Dropout, should include thorough bias assessments to ensure that these methods do not perpetuate unfair or discriminatory outcomes. Mitigating bias is crucial for creating fair and equitable AI systems.

#### Transparency and Interpretability

Our research involves complex tokenization processes that can obscure the decision-making of LLMs. Enhancing the transparency of these models by providing clear explanations of how tokenization impacts model behavior is essential. This transparency helps build trust and allows users to understand and identify potential issues in language model predictions.

#### Privacy and Data Security

The datasets used for training and evaluating tokenization methods often contain sensitive information. Ensuring data anonymization and compliance with data protection regulations is critical to protecting user privacy. Our study adheres to strict data security protocols to prevent any misuse of sensitive information.

# Acknowledgements {#acknowledgements .unnumbered}

We would like to thank all anonymous reviewers for their insightful comments and feedback. Qiwei Peng is supported by DisAI - Improving scientific excellence and creativity in combating disinformation with artificial intelligence and language technologies, a project funded by European Union under the Horizon Europe, GA No. 101079164.

# Task Examples {#ap:example}

## Complex Problem Solving Examples {#ap:example-complex}

We present detailed examples of complex problem-solving tasks including word anagram and identifying math theorems as follows:

::: tcolorbox
**Input**: A string of jumbled characters (*e.g.*, "moeh" for "home").\
**Output**: The correct unscrambled word (*e.g.*, "home").
:::

::: tcolorbox
**Input**: A LaTeX-formatted mathematical theorem. *E.g.*, "Let $f\in L^1(\mathbb{R})$ be an integrable function. The span of $\{f_a(x) = f(x + a):a\in\mathbb{R}\}$ is dense in $L^1(\mathbb{R})$ if and only if $\widehat{f}$ has no real roots..

1.  Let $f\in L^1(\mathbb{R})$ be an integrable function. The span of $\{f_a(x) = f(x + a):a\in\mathbb{R}\}$ is dense in $L^1(\mathbb{R})$ if and only if $\widehat{f}$ has no real roots.

2.  Let $f\in L^1(\mathbb{R})$ be an integrable function. The span of $\{f_a(x) = f(x + a):a\in\mathbb{R}\}$ is dense in $L^1(\mathbb{R})$ if and only if $\widehat{f}$ has no real roots. .

3.  Let $f\in L^1(\mathbb{R})$ be an integrable function. The span of $\{f_a(x) = f(x + a):a\in\mathbb{R}\}$ is dense in $L^1(\mathbb{R})$ if and only if $\widehat{f}$ is irreducible over $\mathbb{Q}$.

4.  Let $f\in L^1(\mathbb{R})$ be an integrable function. The span of $\{f_a(x) = f(x + a):a\in\mathbb{R}\}$ is dense in $L^1(\mathbb{R})$ if and only if $\widehat{f}$ has no repeated roots.

**Output**: The model must determine whether the theorem is true. If it is false, the model should provide the correct version; *i.e.*, select the option "A".
:::

## Intra-Token Probing Examples {#ap:example-intra-token}

For intra-token probing tasks, we provide Character Count (CC), $N$-th Character (NC), $N$-th Character Reverse (NCR), and Case Conversion (CCV) for illustration.

::: tcolorbox
**Input**: Which character appears 3 times in the word 'messrs'?\
**Output**: 's'.
:::

::: tcolorbox
**Input**: What is the 4th character of the word 'myron'?\
**Output**: 'o'.
:::

::: tcolorbox
**Input**: What is the 2nd character from the end of the word 'pensioner'?\
**Output**: 'e'.
:::

::: tcolorbox
**Input**: Which character appears 3 times in the word 'messrs'?\
**Output**: 's'.
:::

## Inter-Token Probing Examples {#ap:example-inter-token}

We present examples of inter-token probing tasks, which involve identifying Common Substrings (CS), Longest Common Subsequences (LCSeq), and Longest Common Substrings (LCS). These tasks evaluate the model's ability to analyze and compare internal structure across different inputs.

::: tcolorbox
**Input**: What are the common substrings of 'critical' and 'conscious'?\
**Output**: 'i', 'c'.
:::

::: tcolorbox
**Input**: What are the longest common subsequences of 'illustrate' and 'critical'?\
**Output**: 'ita'.
:::

::: tcolorbox
**Input**: What are the longest common substrings of 'cow' and 'condition'?\
**Output**: 'co'.
:::

# Experimental Settings {#ap:settings}

## Baselines

We include Llama3-8B, Llama3-8B-Instruct, Llama3-70B, Mistral-7B and Mixtral-8x7B for LLM evaluation.

#### Llama3

Llama3 [@llama3modelcard] series are one of the most powerful open-sourced models recently. Llama3-8B is a dense pretrained model with a vocab size of 128256, which needs few-shot examples to better follow instructions. Llama3-8B-Instruct is also envolved for diverse model types. Llama3-8B-Instrcut is a instruction-fine-tuned version of Llama3-8B, showing much improvement over Llama3-8B on benchmarks like HumanEval and TruthfulQA.

#### Mistral & Mixtral

Mistral-7B [@mistral23] is a dense model with a vocab size of 32000 released last year. Mixtral-8x7B [@mixtral24] is a sparse mixture-of-expert(MoE) model with 13B active parameters, whose performance greatly surpasses Mistral-7B and matches the performance of Llama2-70B.

#### GPT-4 Turbo

The model version we evaluate is `‘‘gpt-4-1106-preview’’`[^3]. Compared with GPT-4, \"gpt-4-1106-preview\" yields stronger performance on following instructions, structured output and other abilities.

## Evaluation Settings

We utilize lm-evaluation-harness [@eval-harness] for the evaluation of GSM8K, MMLU, and TruthfulQA. For HumanEval, we adopt bigcode-evaluation-harness [@bigcode-evaluation-harness]. All models are tested under bfloat16 precision for higher efficiency.

To eliminate the impact of the Chain-of-Thought (CoT) prompt, GSM8K is evaluated using a 5-shot setting without CoT, and we report the \"exact match\" as the final metric. For HumanEval, we report pass@1 using a temperature of 0.2 and a top-$p$ of 0.95. The maximum total length of the prompt and model output is set to 512 for Llama3 and 1024 for Mistral-7B and Mixtral-8x7B. We only apply corruption to the annotation to make sure that entry point can be found after corruption. For TruthfulQA, model performances are measured within the "MC1 (single-true)" setting.

For models after instruction-tuning, we do not apply chat templates except for TruthfulQA, as model outputs tend to be difficult to parse in chat mode.

## Post-Training Details {#ap:train_details}

#### Dataset

We synthesized the dataset for RQ2 with a template-based method. Table [\[tab:app-ft-dataset\]](#tab:app-ft-dataset){reference-type="ref" reference="tab:app-ft-dataset"} describes statistics of dataset splits for each task.

# Details of Probing Task Construction {#ap:task_construction}

## Token Structure Probing (RQ2)

To create a comprehensive test set for evaluating the tokenization capabilities of LLMs, we followed a systematic data synthesis process. Initially, we manually collected a set of around 300 words from the web, ensuring a diverse representation of word structures. This collection included words with common suffixes, prefixes, and varying lengths to cover a broad range of token structures.

Next, we defined a set of rules to create tasks for both intra-token and inter-token evaluations. These rules were designed to test different aspects of tokenization, such as character counting, character identification, case conversion, and identifying common substrings and subsequences.

Then we generated the probing tasks. For intra-token evaluations, we created tasks like Character Count (CC), $N$-th Character (NC), $N$-th Character Reverse (NCR), and Case Conversion (CCV). For inter-token evaluations, we developed tasks such as Common Substrings (CS), Longest Common Substrings (LCS), and Longest Common Subsequences (LCSeq). Each task was carefully crafted to test the model's ability to understand and manipulate token structures at various levels.

## Typographical Variation Task (RQ3)

::: {#tab:app-rq3-dataset}
  **Task**       **Test**
  ------------ ----------
  GSM8k             1,319
  MMLU             14,042
  TruthfulQA          817
  HumanEval           164

  : Dataset statistics of typographical variation tasks (RQ3).
:::

To thoroughly evaluate the typographical variation (RQ3), we conduct corruption to the questions of several benchmark datasets' test split and keep answers intact. Details of evaluation dataset are summarized at Table [1](#tab:app-rq3-dataset){reference-type="ref" reference="tab:app-rq3-dataset"}.

<figure id="fig:app-case-char" data-latex-placement="!ht">
<embed src="figure/case_study_char.pdf" style="width:95.0%" />
<figcaption>Example of a char-level(5-gram) corrupted prompt from HumanEval. <span style="color: 200,8,49">Red</span>, <span style="color: 54,137,85">Green</span>, and <span style="color: 179,179,179">Gray</span> denote replacement, insertion, and deletion respectively.</figcaption>
</figure>

<figure id="fig:app-case-token" data-latex-placement="!ht">
<embed src="figure/case_study_token.pdf" style="width:95.0%" />
<figcaption>Example of a token-level(tri-gram) corrupted prompt from GSM8K. <span style="color: 200,8,49">Red</span>, <span style="color: 54,137,85">Green</span>, and <span style="color: 179,179,179">Gray</span> denote replacement, insertion, and deletion respectively.</figcaption>
</figure>

#### Permutation

For permutation task, we randomly shuffle tokens or characters within an $n$-gram range with a 50% probability. We evaluate by using $n$-gram range from 2 to 5, in which various $n$-gram levels could assess the model's performance under varying degrees of corruption.

#### Noise Injection

We consider three kinds of noise that commonly encountered by humans: insertion (10%), deletion (10%), and replacement (10%). For insertion, we choose a token or character from the current $n$-gram under 50% circumstances. Otherwise, we randomly select a token or character from the whole vocabulary (token or character) and add it to a random position.

When performing replacing, we replace a token or character in the current $n$-gram with a token or character randomly selected from the whole vocabulary.

#### Character-Level

We performed character-level permutation within word boundaries, ensuring that the positions of punctuation marks or spaces remained unchanged after permutation. Even when there were not enough characters to form a complete $n$-gram, permutation was still applied. We ignore word boundaries when injecting character-level noise. Figure [9](#fig:app-case-char){reference-type="ref" reference="fig:app-case-char"} illustrates our character-level corruption approach using a 5-gram granularity.

#### Token-Level

We converted the input prompts into tokens using tokenizers from different model families. Corruption was then applied within an $n$-gram range, without regard to word boundaries. This approach simulates token-level typographical variations, challenging the models to handle disruptions at the token level. Figure [10](#fig:app-case-token){reference-type="ref" reference="fig:app-case-token"} gives an example of how we apply such variations to an encoded prompt.

# Detailed Results of BPE-dropout Post-Training {#ap:results}

The results, as shown in Figure [8](#fig:ft_epoch){reference-type="ref" reference="fig:ft_epoch"}, illustrate the effect of BPE-dropout on EM scores across seven tasks (CS, LCSeq, CCV, CC, NC, LCS, NCR) under various post-training conditions (0-shot, 1-shot, 2-shot, and 3-shot) with dropout rates ranging from 0.0 to 0.8.

We observe that moderate dropout rates (0.2 to 0.4) appear to improve the convergence of EM scores across all tasks, particularly in the zero-shot setting. This indicates that a certain level of variability introduced by dropout can help the model generalize better when no additional examples are provided. In the 1-shot, 2-shot, and 3-shot settings, moderate dropout rates contribute to stabilizing performance, suggesting that this level of dropout introduces useful regularization without significantly compromising token integrity.

It is evident that higher dropout rates (0.6 and 0.8) lead to a noticeable decline in EM scores across all tasks and post-training conditions. This indicates that excessive dropout disrupts the tokenization process, resulting in subwords with fewer merges that conflict with the original pre-training of the models. Tasks such as NC, LCS, and NCR show more significant drops in EM scores with higher dropout rates, reflecting their complexity and the challenge of maintaining token integrity under substantial dropout.

Task-specific performance varies under different dropout conditions. CS and CC tasks exhibit high robustness to dropout, maintaining relatively stable EM scores even at moderate dropout rates. This suggests that the nature of these tasks makes them less sensitive to the variability introduced by dropout. LCSeq and CCV tasks show moderate sensitivity to dropout, with a noticeable decline in performance at higher dropout rates. NC, LCS, and NCR tasks are more adversely affected by higher dropout rates, indicating their reliance on stable token sequences.

The positive effects of moderate BPE-dropout include improved generalization and regularization. Moderate dropout rates (0.2 to 0.4) introduce beneficial variability, enhancing the model's ability to generalize, particularly in zero-shot scenarios where the model must rely solely on its pre-trained knowledge without additional examples. BPE-dropout acts as a regularizer, preventing the model from overfitting to specific token sequences seen during pre-training. This is especially useful in low-resource settings (e.g., 0-shot and 1-shot) where overfitting can be a significant concern.

However, challenges arise with high dropout rates. Higher dropout rates lead to subwords with fewer merges, deviating from the token sequences the model encountered during pre-training. This disruption results in poorer performance, as the model struggles to reconcile the altered tokenization with its pre-trained representations. More complex tasks (NC, LCS, NCR) suffer more from high dropout rates, highlighting the need for careful tuning of dropout rates based on task complexity and tokenization stability requirements.

The findings suggest that there is an optimal range for BPE-dropout rates that balances the benefits of regularization and improved generalization with the need to maintain token integrity. Practitioners should consider moderate dropout rates to leverage the positive effects while avoiding the pitfalls of excessive dropout. Different tasks exhibit varying sensitivities to dropout, underscoring the importance of task-specific dropout tuning to achieve the best performance outcomes.

# Additional Analysis {#ap:analysis}

## Impact of Typographical Variations on Sequence Length

<figure id="fig:typo_len_analysis" data-latex-placement="!ht">
<embed src="figure/len_lmplot.pdf" style="width:95.0%" />
<figcaption>Scatter plots showing the positive correlation between token lengths before and after introducing typographical errors on MMLU and GSM8K, varying <span class="math inline"><em>n</em></span>-gram settings (2, 3, or 5). The x-axis represents the original token length, and the y-axis represents token length after adding errors. </figcaption>
</figure>

Figure [11](#fig:typo_len_analysis){reference-type="ref" reference="fig:typo_len_analysis"} shows a strong positive correlation between token lengths before and after introducing typographical errors across different tasks (GSM8K and MMLU) and $n$-gram settings (2, 3, 5). We observe that token lengths after introducing errors are proportional to their original lengths. Besides, both GSM8k and MMLU tasks exhibit similar patterns, and the $n$-gram settings (2, 3, 5) do not significantly alter this relationship. Most interestingly, the slope for reorder errors is relatively larger than for noise errors, indicating that **reorder errors tend to result in a slightly greater increase in token length compared to noise errors**.

## Compositional Challenges in Token Embeddings {#ap:token_embed}

Figure [1](#fig:vec_plot){reference-type="ref" reference="fig:vec_plot"} illustrates the compositional challenges faced by existing LLMs when handling subword units. Specifically, it presents cosine similarities and angular differences between embeddings of original words and their subword components.

In Figure [\[fig:vec_assignment\]](#fig:vec_assignment){reference-type="ref" reference="fig:vec_assignment"}, we observe the word "assignment" and its subword components "assign" and "ment". The cosine similarity between the composite embedding "assign + ment" and the original word \"assignment\" is relatively low at 0.21, with a significant angular difference of 78.16 degrees. This substantial disparity indicates that the learned token embeddings fail to capture the surface form composition accurately. The model does not recognize that "assign" combined with "ment" should semantically align closely with "assignment".

Similarly, Figure [\[fig:vec_import\]](#fig:vec_import){reference-type="ref" reference="fig:vec_import"} shows the word "import" and its subword components "im" and "port". Here, the cosine similarity between "im + port" and "import" is 0.13, and the angular difference is still notable at 82.47 degrees. This suggests that while the model captures some compositional aspects, it still struggles to fully integrate subword information to match the original word's embedding perfectly.

These observations highlight a critical limitation of existing LLMs: their learned token embeddings do not adequately capture the surface form composition. The inability to effectively combine subword units to represent the full word's meaning undermines the model's overall understanding and processing capabilities.

[^1]: Equal contribution and shared co-first authorship.

[^2]: Since GPT-4 Turbo does not support perplexity computation, it was excluded from the evaluation for these two tasks.

[^3]: <https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4>
