::: IEEEkeywords
Artificial Intelligence, Machine learning, Distributed representations, Cognitive architectures, Cognitive computing, Applications, Analogical reasoning, Hyperdimensional Computing, Vector Symbolic Architectures, Holographic Reduced Representations, Tensor Product Representations, Matrix Binding of Additive Terms, Binary Spatter Codes, Multiply-Add-Permute, Sparse Binary Distributed Representations, Sparse Block Codes, Modular Composite Representations, Geometric Analogue of Holographic Reduced Representations
:::

# Introduction

This article is Part II of the survey of a research field known under the names Hyperdimensional Computing, HDC (the term was introduced in [@KanervaHyperdimensional2009]) and Vector Symbolic Architectures, VSA (the term was introduced in [@GaylerJackendoff2003]). As in Part I [@KleykoSurveyVSA2021Part1], below we will consistently use the joint name HDC/VSA when referring to the field. HDC/VSA is an umbrella term for a family of computational models that rely on mathematical properties of high-dimensional random spaces and use high-dimensional distributed representations called hypervectors (HVs) for a structured ("symbolic") representation of data, while maintaining the advantages of traditional connectionist vector distributed representations.

First, let us briefly recapitulate the motivation for this survey. The main driving force behind the current interest in HDC/VSA is the global trend of searching for computing paradigms alternative to the conventional (von Neumann) ones. Examples of the new paradigms are neuromorphic and nanoscalable computing, where HDC/VSA is expected to play an important role (see [@KleykoComputingParadigm2021] and references therein for perspective). Due to this surge of interest in HDC/VSA, the need for providing a broad overview of the field, which is currently missing, became evident. Therefore, this two-part survey extensively covers the state-of-the-art of the field in a form that is accessible to a wider audience.

:::: center
::: tabular
c c c c c c c c c c c c c c c c c c c & & & & &\

& & & & & & & & & & & & & & & & & &\

[@PlateCommon1997] & & & & & & & & & & $\bm{\pm}$ & $\bm{\pm}$ & & & & & & & \

[@KanervaHyperdimensional2009] & & & & $\bm{\pm}$ & & & & & & $\bm{\pm}$ & $\bm{\pm}$ & & & & & & & \

[@RahimiNanoscalable2017]& & & & & & & $\bm{\pm}$ & & & & & & & & & & & \

[@RahimiBiosignal2019] & & & & & & & $\bm{\pm}$ & & & & & & & & & & & \

[@NeubertRobotics2019]& & & & & & $\bm{\pm}$ & $\bm{\pm}$ & $\bm{\pm}$ & & & & & $\bm{\pm}$ & & & & & \

[@GeClassificationReview2020] & & & & & & & & & & & & & & & & & & \

[@SchlegelVSAComparison2020] & & & & & & $\bm{\pm}$ & & $\bm{\pm}$ & & & & & $\bm{\pm}$ & & & & & \

[@KleykoComputingParadigm2021] & $\bm{\pm}$ & & & & & & & & & & & & & & & & & \

[@hassan2021hyper] & & & & & & & & & & & & & $\bm{\pm}$ & & & & & \

& [2.1.1](#sec:stor:automschemes){reference-type="ref" reference="sec:stor:automschemes"} & [2.1.2.1](#sec:stor:trans:comm){reference-type="ref" reference="sec:stor:trans:comm"} & [2.1.3](#sec:stor:string){reference-type="ref" reference="sec:stor:string"} & [2.2.1](#sec:context:HVs){reference-type="ref" reference="sec:context:HVs"} & [2.2.2](#sec:context:biomedical){reference-type="ref" reference="sec:context:biomedical"} & [2.2.3](#sec:context:images){reference-type="ref" reference="sec:context:images"} & [2.3.1](#sec:app:class:features){reference-type="ref" reference="sec:app:class:features"} & [2.3.2](#sec:app:class:images){reference-type="ref" reference="sec:app:class:images"} & [2.3.3](#sec:app:class:structured){reference-type="ref" reference="sec:app:class:structured"} & [3.1.1](#sec:database:holistic){reference-type="ref" reference="sec:database:holistic"} & [3.1.2](#sec:analogical:reasoning){reference-type="ref" reference="sec:analogical:reasoning"} & [3.1.3](#sec:cog:modeling){reference-type="ref" reference="sec:cog:modeling"} & [3.1.4](#sec:comp:vision){reference-type="ref" reference="sec:comp:vision"} & [3.2.1](#sec:cog:spaun){reference-type="ref" reference="sec:cog:spaun"} & [3.2.2](#sec:cognitive:APNN){reference-type="ref" reference="sec:cognitive:APNN"} & [3.2.3](#sec:cognitive:HTM){reference-type="ref" reference="sec:cognitive:HTM"} & [3.2.4](#sec:cognitive:LIDA){reference-type="ref" reference="sec:cognitive:LIDA"} & [3.2.5](#sec:cognitive:memory){reference-type="ref" reference="sec:cognitive:memory"}\
:::
::::

There were no previous attempts to make a comprehensive survey of HDC/VSA, but there are articles that overview particular topics of HDC/VSA. Probably the first attempt to overview and unify different HDC/VSA models should be attributed to Plate [@PlateCommon1997]. The key idea for the unification was to consider the existing (at that time, four) HDC/VSA models as different schemes for implementing two key operations: binding and superposition (see Section [\[PartI-sec:vsa:operations\]](#PartI-sec:vsa:operations){reference-type="ref" reference="PartI-sec:vsa:operations"} in [@KleykoSurveyVSA2021Part1]). However, since that time numerous HDC/VSA models have come to prominence. A more recent summary of the most frequently used models was provided in [@RahimiNanoscalable2017]. In [@SchlegelVSAComparison2020], the HDC/VSA models were compared in terms of their realizations of the binding operation. Both articles, however, missed some of the models. These and other gaps have been filled in Part I of this survey [@KleykoSurveyVSA2021Part1].

As for applications of HDC/VSA -- the topic covered in this article -- in Table [\[table:position:part2\]](#table:position:part2){reference-type="ref" reference="table:position:part2"} we identified the following substantial application domains, which reflect the structure of Sections [2](#sec:applications){reference-type="ref" reference="sec:applications"} and [3](#sec:cog){reference-type="ref" reference="sec:cog"}: deterministic behavior, similarity estimation, classification, cognitive computing, and cognitive architectures. The columns in Table [\[table:position:part2\]](#table:position:part2){reference-type="ref" reference="table:position:part2"} list more fine-grained application clusters within these larger domains.

There is no previous article, which would account for all currently known applications, though there are recent works overviewing either a particular application area (as in [@RahimiBiosignal2019], where the focus was on biomedical signals), or certain application types (as in [@GeClassificationReview2020; @hassan2021hyper], where solving classification tasks with HDC/VSA was the main theme). The topic of machine learning is also omnipresent in this survey, and due to its ubiquity we dedicated Section [2.3](#sec:app:class){reference-type="ref" reference="sec:app:class"} to classification tasks. However, the scope of the survey is much broader as it touches on all currently known applications. Table [\[table:position:part2\]](#table:position:part2){reference-type="ref" reference="table:position:part2"} contrasts the coverage of Part II of this survey with the previous articles (ordered chronologically). We use $\bm{\pm}$ to indicate that an article partially addressed a particular topic, but either new results were reported since then or not all related work was covered.

In Part I of this survey [@KleykoSurveyVSA2021Part1], we considered the motivation behind HDC/VSA and basic notions, summarized currently known HDC/VSA models, and presented the transformation of various data types into HVs. Part II of this survey covers existing applications (Section [2](#sec:applications){reference-type="ref" reference="sec:applications"}) and the use of HDC/VSA in cognitive modeling and architectures (Section [3](#sec:cog){reference-type="ref" reference="sec:cog"}). The discussion and challenges, as well as conclusions, are presented in Sections [4](#sec:disc){reference-type="ref" reference="sec:disc"} and Section [5](#sec:conc){reference-type="ref" reference="sec:conc"}, respectively.

# Application areas {#sec:applications}

HDC/VSA have been applied across different fields for various tasks. For this section, we aggregated the existing applications into several groups: deterministic behavior (Section [2.1](#sec:stor:trans){reference-type="ref" reference="sec:stor:trans"}), similarity estimation (Section  [2.2](#sec:apps:similarity){reference-type="ref" reference="sec:apps:similarity"}), and classification (Section  [2.3](#sec:app:class){reference-type="ref" reference="sec:app:class"}).

## Deterministic behavior with HDC/VSA {#sec:stor:trans}

In this section, we consider several use-cases of HVs designed to produce some kind of deterministic behavior. Note that due to the capacity limitations of HVs (see Section [\[PartI-sec:capacity\]](#PartI-sec:capacity){reference-type="ref" reference="PartI-sec:capacity"} in [@KleykoSurveyVSA2021Part1]), achieving deterministic behavior depends on several design choices. These include the dimensionality of HVs as well as, e.g., the number of atomic HVs and the kind of rules used for constructing compositional HVs, such as the number of arguments in the superposition operation. Note also that strictly speaking, not all application areas listed here are perfectly deterministic (in particular, communications in Section [2.1.2.1](#sec:stor:trans:comm){reference-type="ref" reference="sec:stor:trans:comm"}), but the determinism is a desirable property in all the areas collected in this section.

### Automata, instructions, and schemas {#sec:stor:automschemes}

#### Finite-state automata and grammars {#sec:stor:automata}

A deterministic finite-state automaton is specified by defining a finite set of states, a finite set of allowed input symbols, a transition function (defines all transitions in the automaton), a start state, and a finite set of accepting states. The current state can change in response to an input. The joint current state and input symbol uniquely determine the next state of the automaton.

An intuitive example of an automaton controlling the logic of a turnstile is presented in Fig. [1](#fig:fsa){reference-type="ref" reference="fig:fsa"}. The set of input symbols is { "Token", "Push" } and the set of states is { "Unlocked", "Locked" }. The state diagram in Fig. [1](#fig:fsa){reference-type="ref" reference="fig:fsa"} can be used to derive the transition function.

<figure id="fig:fsa" data-latex-placement="t">
<img src="img/fsa_example.png" />
<figcaption> An example of a state diagram of a finite-state automaton modeling the control logic of a turnstile. </figcaption>
</figure>

HDC/VSA-based implementations of finite-state automata were proposed in [@OsipovHD_FSA2017; @YerxaUCBHD_FSA2018]. Random HVs are assigned to represent states ($\mathbf{u}$ for "Unlocked"; $\mathbf{l}$ for "Locked") and input symbols ($\mathbf{t}$ for "Token"; $\mathbf{p}$ for "Push"). These HVs are used to form a compositional HV $\mathbf{a}$ for the transition function. The transformation is similar to the one used for the directed graphs in Section [\[PartI-sect:graphs:undirected\]](#PartI-sect:graphs:undirected){reference-type="ref" reference="PartI-sect:graphs:undirected"} in [@KleykoSurveyVSA2021Part1]. However, the HV representing the input symbol for the automaton is bound to the edge HV that corresponds to the binding of the HVs for the current and the next state. For instance, going from "Locked" to "Unlocked" upon receiving "Token", is represented as: $$\begin{equation}
\mathbf{t} \circ  \mathbf{l} \circ \rho(\mathbf{u}).
\end{equation}$$ Given the HVs of all transitions, the transition function $\mathbf{a}$ is represented as their superposition: $$\begin{equation}
\mathbf{a} =   
\mathbf{p} \circ \mathbf{l} \circ \rho(\mathbf{l}) + 
\mathbf{t} \circ \mathbf{l} \circ \rho(\mathbf{u}) + 
\mathbf{p} \circ \mathbf{u} \circ \rho(\mathbf{l}) + 
\mathbf{t} \circ \mathbf{u} \circ \rho(\mathbf{g}).
\end{equation}$$

The next state is obtained by querying $\mathbf{a}$ with the binding of HVs of the current state and of the input symbol, followed by the inverse permutation of the resultant HV that returns the noisy version of the next state's HV[^6]. For example, if the current state is $\mathbf{l}$ and $\mathbf{p}$ is received, then: $$\begin{equation}
\rho^{-1}(\mathbf{a} \circ \mathbf{p}  \circ \mathbf{l} )  =  \mathbf{l} +  \mathrm{noise}.
\end{equation}$$ This noisy HV is used as the query for the item memory to obtain the noiseless atomic HV $\mathbf{l}$.

Transformations of pushdown automata and context-free grammars into HVs have been presented in [@GrabenGrammars2020]. A proposal for implementing Turing machines and cellular automata was given in [@KleykoComputingParadigm2021].

In [@KnightGrammar2015], the Holographic Reduced Representations (HRR) model was used to represent Fluid Construction Grammars, which is a formalism that allows designing construction grammars and using them for language parsing and production. Another work related to parsing is [@StewartSentenceProcessing2014], which presented an HDC/VSA implementation of a general-purpose left-corner parsing with simple grammars. An alternative approach to parsing with HVs using a constraint-based parser has been presented in [@BlouwConstraintParsing2015].

A related research direction was initiated by beim Graben and colleagues [@beimGrabenGeometric12; @GrabenGrammars2020; @beimGrabenBook2012; @WolffFockToolbox2018]. It concerns establishing a mathematically rigorous formalism of Tensor Product Representations using the concept of a Fock space [@Fock32]. The studies focus largely on use-cases in computational linguistics, semantic processing, and quantum logic. The usage of the Fock space formalism for formulating minimalist grammars was presented in [@beimGrabenGeometric12]. Syntactic language processing as part of phenomenological modeling was reported in [@beimGrabenBook2012]. An accessible and practical entry point to the area can be found in [@WolffFockToolbox2018].

#### Controllers, instructions, schemas

In [@LevyLearningBehavior2013], using the Multiply-Add-Permute model (MAP), it was demonstrated how to manually construct a compositional HV that implements a simple behavior strategy of a robot. Sensor inputs as well as actions were represented as atomic HVs. Combinations of sensor inputs as well as combinations of actions were represented as a superposition of the corresponding atomic HVs. HVs of particular sensor input combinations were bound to the corresponding HV of action combinations. The bound HVs of all possible sensor-action rules were superimposed to produce the compositional HV of the robot controller. Unbinding this HV with the current HV of sensor input combinations results in the noisy HV of the proper action combination. This idea was extended further in [@NeubertRobotBehaviours2016] through a proposed algorithm to "learn" a compositional HV representing a robot's controller using the sensor-actuator values obtained from successful navigation runs. This mode of robot operation is known as "learning by demonstration". It was realized as the superposition of the current "controller" HV with the HV corresponding to the binding of the sensor-actuator values - in case the current sensor HV was dissimilar to the ones already present in the "controller" HV. Another work studying robot navigation is [@MenonNavigation2022], which investigated a number of ways to form compositional HVs representing sensory data and explored the integration of the resultant HVs with neural network instead of the "controller".

In [@ChooInstructionFollowing2013], using HRR, instructions were represented as a sequence of rules, and rules as a sequence of their antecedent and consequent elements. Multiplicative bindings with position HVs were used to represent the sequence. Antecedents and consequents, in turn, were represented as HVs of their elements using binding and superposition operations. This approach was used as a part of instruction parsing in a cognitive architecture [@StewartParsingSequentially2013]. In [@LaihoSparse2015], a proposal was sketched for a HDC/VSA-based processor, where both data and instructions were represented as HVs.

In [@NeubertHDLearnPlan2018], HVs for the representation of "schemas" in the form $<$*context*, *action*, *result*$>$ were used. A more general approach for modeling the behavior of intelligent agents as "functional acts" was considered in [@RachkovskijWorldModel2013] for Sparse Binary Distributed Representations (SBDR; see also Section [3.2.2](#sec:cognitive:APNN){reference-type="ref" reference="sec:cognitive:APNN"}). It is based on HVs representing triples $<$*current situation*, *action*, *resulting situation*$>$ (which essentially correspond to "schemas"), with the associated evaluations and costs. Finally, it is worth recalling that, in general, data structures to be represented by HVs do not have to be limited to "schemas". For example, a recent proposal in [@GallantVSAJSON2022] suggested that HVs are well suited for forming representations of the JSON format that can include several levels of hierarchy.

#### Membership query and frequency estimation

Section [\[PartI-sec:sets\]](#PartI-sec:sets){reference-type="ref" reference="PartI-sec:sets"} in [@KleykoSurveyVSA2021Part1] presented the transformation of sets and multisets into HVs. When implemented with the SBDR model, it becomes evident that Bloom Filters [@Bloom1970space] are a special case of HDC/VSA [@KleykoABF2020], which have been used in a myriad of applications involving the membership query. It is beyond the scope of this survey to overview them all, therefore, the interested readers are refereed to a survey in [@TarkomaPracticeBF2012]). When implementing the transformation of multisets via the Sparse Block Codes model, a similar connection can be made to count-min sketch [@CormodeCountMin2005] that is used commonly for estimating frequency distributions in data streams (some applications are presented in [@CormodeCountMin2005]). The use of the HDC/VSA principles for constructing hash tables has been recently considered in [@HeddesHDHash2022].

### Transmission of data structures

#### Communications {#sec:stor:trans:comm}

The main motivation for using HDC/VSA in the communication context is their robustness to noise due to the distributed nature of HVs. Let us consider three similar but varying applications of HDC/VSA.

In [@JakimovskiCollective2012], it was shown how to use Binary Spatter Codes (BSC) (Section [\[PartI-sec:vsa:bsc\]](#PartI-sec:vsa:bsc){reference-type="ref" reference="PartI-sec:vsa:bsc"} in [@KleykoSurveyVSA2021Part1]) in collective communication for sensing purposes. Multiple devices wirelessly sent their specific HVs representing some of their sensory data (the paper used temperature as a show-case). It was proposed to receive them in a manner that implements the superposition operation. This superposition HV was then analyzed by calculating the $\text{dist}_{\text{Man}}$ of the normalized superposition and the atomic HVs and comparing with a threshold. For instance, the case that a particular temperature was transmitted could be detected. Another version of analysis allowed checking how many devices had been exposed to a particular temperature. The proposed communication scheme does not require a control mechanism for getting multiple access to the medium. So, it can be useful in scenarios where there are multiple devices that have to report their states to some central node. Recently, it has been shown how such over-the-air superposition can be used for on-chip communications to scale up the architecture with multiple transmitters and receivers [@WirelessGuirado2022]. This has been done by carefully engineering modulation constellations, and it paves the way for a large number of *physically distributed* associative memories (as wireless-augmented receivers) to reliably perform the similarity search given a slightly different version of a query HV as their input.

In [@KleykoMACOM2012], BSC was used in the context of medium access control protocols for wireless sensor networks. A device was forming a compositional HV representing the device's sensory data (by the superposition of multiplicative bindings), which was then transmitted to the communication medium. It was assumed that the receiver knew the atomic HVs and, thus, could recover the information represented in the received compositional HV. The application scope of this approach is for scenarios where the communication medium is very harsh so that a high redundancy of HVs is useful for reliably transmitting the data.

In [@KimHDM2018], it was proposed to combine forward error correction and modulation using Fourier HRR (Section [\[PartI-sec:vsa:fhrr\]](#PartI-sec:vsa:fhrr){reference-type="ref" reference="PartI-sec:vsa:fhrr"} in [@KleykoSurveyVSA2021Part1]). The scheme represented individual pieces of data by using complex-valued HVs that were then combined into a compositional HV using permutation and superposition operations. Unnormalized complex values of the compositional HV were transmitted to the communication medium. The iterative decoding of the received compositional HV significantly increased the code rate. The application scope of this scheme is robust communication in a low signal-to-noise ratio regime. The scheme at a lower coding rate was compared to the low-density parity check and polar codes in terms of achieved bit error rates, while featuring lower decoding complexity. To improve its signal-to-noise ratio gain, a soft-feedback iterative decoding was proposed [@HerscheHDMClassifier2021] to additionally take the estimation's confidence into account. That improved the signal-to-noise ratio gain by 0.2 dB at a bit-error-rate of $10^{-4}$. In further works, the scheme has been applied to collision-tolerant narrowband communications [@HsuCollisionTolerant2019], massive machine type communications [@HsuNonOrthogonalModulation2020], and near-channel classification [@HerscheHDMClassifier2021].

#### Distributed orchestration

Another use-case of HDC/VSA in the context of transmission of data structures is distributed orchestration. The key idea presented in [@SimpkinScalable2018; @SimpkinHDWorkflow2019] was to use BSC to communicate a workflow in a decentralized manner between the devices involved in the application described by the workflow. Workflows were represented and communicated as compositional HVs constructed using the primitives for representing sequences (Section [\[PartI-sec:sequences\]](#PartI-sec:sequences){reference-type="ref" reference="PartI-sec:sequences"} in [@KleykoSurveyVSA2021Part1]) and directed acyclic graphs (Section [\[PartI-sect:graphs:labelled\]](#PartI-sect:graphs:labelled){reference-type="ref" reference="PartI-sect:graphs:labelled"} in [@KleykoSurveyVSA2021Part1]). In [@SimpkinHDWorkflow2020], the approach was implemented in a particular workflow system -- Node-RED. In [@BarclayTrustable2022], the approach was extended further to take into account the level of trust associated with various elements when selecting services.

### String processing {#sec:stor:string}

In [@RachkovskijApplication1996], it was proposed to obtain HVs of a word using permutations (cyclic shifts) of HVs of its letters to associate a letter with its position in the word. The conjunction was used to bind together all the obtained letter-in-position HVs. HVs of words formed in such a way were then used to obtain $n$-grams of sequences of words using the same procedure. The obtained HVs were used to estimate the frequencies of various word sequences in texts in order to create a model of human text reader interests. Note that here the conjunction result is somewhat similar to all input HVs.

An interesting property of sequence representation using permutations of its element HVs (Section [\[PartI-sec:sequences\]](#PartI-sec:sequences){reference-type="ref" reference="PartI-sec:sequences"} in [@KleykoSurveyVSA2021Part1]) is that the HV of a shifted sequence can be obtained by the permutation of the sequence HV as a whole [@KleykoSubstrings2014; @MitrokhinSensorimotor2019; @KleykoCommentariesSR2020]. This property was leveraged in [@KleykoSubstrings2014] for searching the best alignment (shift) of two sequences, i.e., the alignment that provides the maximum number of coinciding symbols. This can be used, e.g., for identifying common substrings. Such a representation, however, does not preserve the similarity of symbols' HVs in nearby positions, which would be useful for, e.g., spell checking. This can be addressed by, e.g., extending the permutation-based representations as in [@KleykoPermuted2016], where the resultant compositional HVs were evaluated on a permuted text, which was successfully reconstructed. An approach to transforming sequences into sparse HVs from [@RachkovskijEquivariant2021], which preserves the similarity of symbols at nearby positions and is shift-equivariant, was applied to the spellchecking task.

An algorithm for searching a query string in the base string was proposed in [@PashchenkoSubstring2020] and modified in [@KleykoComputingParadigm2021]. It is based on the idea of representing finite-state automata in an HV (see Section [2.1.1.1](#sec:stor:automata){reference-type="ref" reference="sec:stor:automata"}). The algorithm represented the base string as a non-deterministic finite-state automaton [@RabinFinite1959]. The symbols of the base string corresponded to the transitions between the states of the automaton. The automaton in turn was represented as a compositional HV. The automaton was initialized as a superposition of all atomic HVs corresponding to the states. The query substring was presented to the automaton symbol by symbol. If after the presentation of the whole substring the automaton appeared in one of the valid states, this indicated the presence of the query substring in the base string.

In [@KimEfficientDNA2020], the MAP model was used for DNA string matching. The key idea was as in [@JoshiNgrams2016]: the base DNA string was represented by one or several HVs containing a superposition of all $n$-grams of predefined size(s). HVs of $n$-grams were formed by multiplicative binding of appropriately permuted HVs of their symbols (Section [\[PartI-sec:sequences:ngrams\]](#PartI-sec:sequences:ngrams){reference-type="ref" reference="PartI-sec:sequences:ngrams"} in [@KleykoSurveyVSA2021Part1]). A query string was considered present in the base DNA string if the similarity between its HV and the compositional HV(s) of the base DNA string was higher than the predefined threshold. The threshold value determined the balance between true and false positives, similar to Bloom filters (see Section [\[PartI-sec:sets\]](#PartI-sec:sets){reference-type="ref" reference="PartI-sec:sets"} in [@KleykoSurveyVSA2021Part1]). The approach was evaluated on two databases of DNA strings: escherichia coli and human chromosome $14$. The main promise of the approach in [@KimEfficientDNA2020] is the possibility to accelerate string matching with application-specific integrated circuits due to the simplicity and parallelizability of the HDC/VSA operations.

### Factorization

The resonator networks [@FradyResonator2020; @KentResonatorNetworks2020] were proposed as a way to solve factorization problems, which often emerge during the recovery procedure, within HDC/VSA (Section [\[PartI-sec:recovery\]](#PartI-sec:recovery){reference-type="ref" reference="PartI-sec:recovery"} in [@KleykoSurveyVSA2021Part1]). It is expected that the resonator networks will be useful for solving various factorization problems, but this requires formulating the problem for the HDC/VSA domain. An initial attempt to decompose synthetic scenes was demonstrated in [@FradyDisentangling2018]. A more recent formulation for the integer factorization was presented in [@KleykoPrimes2022]. The transformation of numbers into HVs was based on the fractional power encoding [@FradyFunctions2021; @FradyFunctionsNICE2022] (Section [\[PartI-sec:scalars:vectors:compos\]](#PartI-sec:scalars:vectors:compos){reference-type="ref" reference="PartI-sec:scalars:vectors:compos"} in [@KleykoSurveyVSA2021Part1]) combined with log transformation, and the resonator network was used to solve the integer factorization problem. The approach was evaluated on the factorization of semiprimes.

## Similarity estimation with HVs {#sec:apps:similarity}

Transformations of original data into HVs allow constructing HVs in various application areas in a manner that preserves similarity relevant for a particular application. This provides a tool to use HDC/VSA for "similarity-based reasoning" that includes, e.g., similarity search and classification in its simplest form as well as the much more advanced analogical reasoning considered in Section [3.1.2](#sec:analogical:reasoning){reference-type="ref" reference="sec:analogical:reasoning"}. Due to the abundance of studies on classification tasks we devote another separate Section [2.3](#sec:app:class){reference-type="ref" reference="sec:app:class"} to it. In this section, we primarily focus on "context" HVs, since for a long time these were the most influential application of HDC/VSA. We also present some existing efforts in similarity search.

### Word embeddings: context HVs for words and texts {#sec:context:HVs}

The key idea for constructing context vectors is usually referred to as the "distributional semantics hypothesis" [@HarrisLanguage1968], suggesting that linguistic items with similar distributions have similar meanings. The distributions are calculated as frequencies of item occurrence in particular contexts by using a document corpus. For items-words the contexts could be, e.g., documents, paragraphs, sentences, or sequences of words close to a focus word. For example, the generalized vector space model [@WongModeling1987] used documents as contexts for words in information retrieval. In computational linguistics and machine learning, vectors are also known as embeddings.

In principle, context vectors can be obtained in any domain where objects and contexts could be defined. Below, we will only focus on the context vector methods which are commonly attributed to HDC/VSA. They usually transform the frequency distributions into HVs of particular formats, which we will call context HVs.

Historically, the first proposal to form context HVs for words was that of Gallant, e.g., [@GallantPractical1991; @GalantContext1993; @CaidLearned1995; @GalantContext2000]. These studies, however, did not become known widely, but see [@HechtContext1994]. In fact, two most influential HDC/VSA-based methods for context HVs are Random Indexing (RI) [@KanervaRI2000; @SahlgrenRIIntroduction2005] and Bound Encoding of the Aggregate Language Environment (BEAGLE) [@JonesMeaning2007].

[]{#tab:app:cont:vec label="tab:app:cont:vec"}

::: {#tab:app:cont:vec}
                Ref.                  Task         Dataset                                 Method                            Baseline(s)
  --------------------------------- -------- -------------------- --------------------------------------------------------- -------------
          [@KanervaRI2000]                                                                                                       LSA
        [@SahlgrenVector2001]                                                                                                    LSA
         [@MisunoVector2005]                                                                                                   LSA, RI
         [@MisunoVector2005]                                                                                                   LSA, RI
         [@MisunoVector2005]                                                                                                   LSA, RI
       [@MisunoSearching2005]                                                                                               
        [@SahlgrenOrder2008]                                                                                                   BEAGLE
         [@CohenCancer2012]                                                                                                 
       extracted from MEDLINE                   BSC-based PSI                                                               
   [@CohenPredictingScreening2014]                                                                                          
     active against cancer cells                     PSI                                Reflective RI                       
       [@RecchiaSemantic2015]                                                                                                  BEAGLE
       [@RecchiaSemantic2015]                                                                                                  BEAGLE
         [@JamiesonITS2018]                                                                  ITS                            
         [@JamiesonITS2018]                                                                  ITS                            
      [@JohnsExperiential2019]                                                                                              
      [@CohenInteractions2019]                                                                                              
        for drug combinations                        ESP                           graph convolutional ANN                  
      [@JohnsExperiential2019]                                                                                                 BEAGLE
        [@aujla2019semantic]                                                                                                
      for cognitive psychology                      BEAGLE                          RI with permutations                    
        [@johns2019influence]                                                                                               
         on lexical behavior                                                                                                
    British Lexicon Project; etc.    BEAGLE          N/A                                                                    
       [@SchubertLanguage2020]                                                                                              
       alphanumeric characters                RI with characters   Word2vec [@MikolovWord2vec2013] & EARP [@CohenEARP2018]  
         [@TalerFluency2020]                                                               BEAGLE                                N/A

  : Experiments with context vectors.
:::

#### Random Indexing {#sec:random:indexing}

The RI method [@KanervaRI2000; @SahlgrenRIIntroduction2005] was originally proposed in [@KanervaRI2000] as a simple alternative to Latent Semantic Analysis (LSA) [@LandauerSolution1997]. Instead of the expensive Singular Value Decomposition (SVD) used in LSA for the dimensionality reduction of a word-document matrix, RI uses the multiplication by a random matrix, thereby performing random projection (Section [\[PartI-sect:rand:proj\]](#PartI-sect:rand:proj){reference-type="ref" reference="PartI-sect:rand:proj"} in [@KleykoSurveyVSA2021Part1]). The Random Projection (RP) matrix was ternary ($\{-1, 0, 1\}$) and sparse. Each row of the RP matrix is seen as a "random index" (hence RI) assigned to each context (in that case, the document). In implementation, the frequency matrix was not formed explicitly, and the resultant context HVs were formed by scanning the document corpus and adding the document's random index vector to the context HV of each word in the document. The sign function can be used to obtain binary context HVs. The similarity between unnormalized context HVs was measured by $\text{sim}_{\text{cos}}$. The synonymy part of TOEFL was used as the benchmark to demonstrate that a performance comparable to LSA could be achieved, but at lower computational costs due to the lack of SVD. In [@PapadimitriouLatent2000], it was proposed to speed up LSA by using RP before SVD as a preprocessing step. In [@KarlgrenWords2001], similar to [@GalantContext1993; @LundProducing1996], RI was modified to use a narrow context window consisting of only few adjacent words on each side of the focus word.

In [@SahlgrenOrder2008], permutations were used to represent the order information within the context window. Further extensions of RI included generalization to multidimensional arrays (N-way RI) [@SandinMultidimensionalRI2017] and inclusion of extra-linguistic features [@KarlgrenUtterances2019]. The RI was also extended to the case of training corpora that include information about relations between words. This model is called Predication-based Semantic Indexing (PSI) [@WiddowsDiscoveryPatterns2012; @CohenAnalogy2014; @CohenSemantic2017]. PSI has been mainly used in biomedical informatics for literature-based discovery, such as identification of links between pharmaceutical substances and diseases they treat (see [@WiddowsDiscoveryPatterns2012] for more details). Later, PSI was extended to the Embedding of Semantic Predications (ESP) model that incorporates some aspects of "neural" word embeddings from [@MikolovWord2vec2013] and similarity preserving HVs (Section [\[PartI-sec:scalars:vectors\]](#PartI-sec:scalars:vectors){reference-type="ref" reference="PartI-sec:scalars:vectors"} in [@KleykoSurveyVSA2021Part1]) for representing time periods [@WiddowsGradedVectors2015].

It is worth mentioning that the optimization-based method [@SutorLifeLearning2018] for obtaining similarity preserving HVs from co-occurrence statistics can be contrasted to RI. RI also uses co-occurrence statistics, but implicitly (i.e., without constructing the co-occurrence matrix). The difference, however, is that the RI is optimization-free and it forms HVs through a single pass via the training data. Thus, it can be executed online in an incremental manner, while the optimization required by [@SutorLifeLearning2018] calls for iterative processing, which might be more suitable for offline operations.

#### Bound Encoding of the Aggregate Language Environment {#sec:BEAGLE}

There is an alternative method for constructing context HVs with HDC/VSA, known as BEAGLE [@JonesMeaning2007]. It was proposed independently of RI and used HRR to form the HVs of $n$-grams of words within a sentence, providing a representation of the order and context HVs of words. Words were initially represented with random atomic HVs as in HRR. The context HV of a word was obtained from two "parts". The first part included summing the atomic HVs of the words in the sentence other than the focus word, for all corpus sentences. The second part was contributed by the word order HVs, which were formed as the superposition of the word $n$-gram HVs (with $n$ between $2$ and $7$). The $n$-gram HVs were formed with the circular convolution-based binding of the special atomic HV in place of the focus word and the atomic HVs of the other word(s) in the $n$-gram. The word order in an $n$-gram was represented recursively, first by binding the HVs of the left and the right word permuted differently, and then by binding the resultant HV with the next right word, again using permutations for the "left" and "right" relative positions. The total context HV was the superposition of the HVs for the two parts. The similarity measure was $\text{sim}_{\text{cos}}$.

Later, [@RecchiaSemantic2010] presented a modified version of BEAGLE using random permutations. The authors found that their model was both more scalable to large corpora and gave better fits to semantic similarity than the circular convolution-based representation. A comprehensive treatment of methods for constructing context HVs of phrases and sentences as opposed to individual words was presented in [@MitchellComposition2010]. In a similar spirit, in [@LuoDecomposed2018] the HRR model was used to construct compositional HVs that were able to discover language regularities resembling syntax and semantics. The comparison of different sentence embeddings (including BEAGLE) in terms of their ability to represent syntactic constructions was provided in [@KellySentence2020]. It has been demonstrated that context HVs formed by BEAGLE can account for a variety of semantic category effects such as typicality, priming, and acquisition of semantic and lexical categories. The effect of using different lexical materials to form context HVs with BEAGLE was demonstrated in [@JohnsExperiential2019], while the use of negative information for context HVs was assessed in [@JohnsNegative2019]. In [@KellyDegrees2017], BEAGLE was extended to a Hierarchical Holographic Model by augmenting it with additional levels corresponding to higher-order associations, e.g., part-of-speech and syntactic relations. The Hierarchical Holographic Model was used further in [@KellyIndirect2020] to investigate what grammatical information is available in context HVs produced by the model.

Due to high similarity between BEAGLE and RI, both methods were compared against each other in [@RecchiaSemantic2010; @RecchiaSemantic2015]. It was shown that both methods demonstrate similar results on a set of semantic tasks using a Wikipedia corpus for training. The main difference was that RI is much faster as it does not use the circular convolution operation. For placing the methods in the general context of word embeddings, please refer to [@WiddowsSemanticComposition2021].

The work [@MercadoSemantic2020] aimed to address the representation of similarity, rather than relatedness represented in context HVs by BEAGLE and RI. To do so, for each word the authors represented its most relevant semantic features taken from a knowledge base ConceptNet. The context HV of a word was formed using BSC as a superposition of its semantic feature HVs formed by role-filler bindings. The results from measuring the semantic similarity between pairs of concepts were presented using the SimLex-999 dataset.

Table [1](#tab:app:cont:vec){reference-type="ref" reference="tab:app:cont:vec"} provides a non-comprehensive summary of the studies that applied the BEAGLE and RI methods to various linguistic tasks. The interested readers are referred to [@WiddowsDistributionalSemantics2009], which is a survey describing the applications of RI, PSI, and related methods, including the biomedical domain. The range of applications described in  [@WiddowsDistributionalSemantics2009] covers word-sense disambiguation, bilingual information extraction, visualization of relations between terms, and document retrieval. It is worth noting that there is a software package called "Semantic vectors" [@WiddowsSemanticInitial2008], [@WiddowsSemanticVectors2008], [@WiddowsSemVec2010], that implements many of the methods mentioned above and provides the main building blocks for designing further modifications of the methods.

### Similarity estimation of biomedical signals {#sec:context:biomedical}

In [@KleykoBreathing2018; @KleykoDeepBreathing2019], BSC was applied to biomedical signals: heart rate and respiration. The need for comparing these signals emerged in the scope of a deep breathing test for assessing autonomic function. HDC/VSA was used to analyze cardiorespiratory synchronization by comparing the similarity between heart rate and respiration using feature-based analysis. Feature vectors were extracted from the signals and transformed into HVs by using role-filler bindings (Section [\[PartI-sec:key:value\]](#PartI-sec:key:value){reference-type="ref" reference="PartI-sec:key:value"} in [@KleykoSurveyVSA2021Part1]) and representations of scalars (Section [\[PartI-sec:scalars:vectors\]](#PartI-sec:scalars:vectors){reference-type="ref" reference="PartI-sec:scalars:vectors"} in [@KleykoSurveyVSA2021Part1]). These HVs were in turn classified into different degrees of cardiorespiratory synchronization/desynchronization. The signals were obtained from the healthy adult controls, patients with cardiac autonomic neuropathy, and patients with myocardial infarction. It was shown that, as expected, the similarity between different HVs were lower for patients with the cardiac autonomic neuropathy and myocardial infarction patients, than for the healthy controls.

Another application of BSC was the identification of the ictogenic (i.e., seizure generating) brain regions from intracranial electroencephalography (iEEG) signals [@BurrelloLBPSeizure2020]. The algorithm first transformed iEEG time series from each electrode into a sequence of symbolic local binary pattern codes, from which a binary HV was obtained for each brain state (e.g., ictal or interictal). It then identified the ictogenic brain regions by measuring the relative distances between the learned HVs from different groups of electrodes. Such the identification was done by one-way ANOVA tests at two levels of spatial resolution, the cerebral hemispheres and lobes.

### Similarity estimation of images {#sec:context:images}

In [@NeubertAggregation2021], the HDC/VSA 2D image representations (Section [\[PartI-sec:2Dimages\]](#PartI-sec:2Dimages){reference-type="ref" reference="PartI-sec:2Dimages"} in [@KleykoSurveyVSA2021Part1]) were applied for an aggregation of local descriptors extracted from images. Local image descriptors were real-valued vectors whose dimensionality was controlled by RP (Section [\[PartI-sect:rand:proj\]](#PartI-sect:rand:proj){reference-type="ref" reference="PartI-sect:rand:proj"} in [@KleykoSurveyVSA2021Part1]). To represent a position inside an interval, the authors concatenated parts of two basis HVs and used several intervals, as in [@RachkovskijScalars2005; @RachkovskijVectors2005], but using MAP. Position HVs for *x* and *y* were bound to represent (*x*,*y*), see Section [\[PartI-sec:2Dimages\]](#PartI-sec:2Dimages){reference-type="ref" reference="PartI-sec:2Dimages"} in [@KleykoSurveyVSA2021Part1]. Subsequently, projected local image descriptors were bound with their position HVs, using component-wise multiplication, and the bound HVs were superimposed to represent the whole image. The image HVs obtained with different algorithms for extracting the descriptors could also be aggregated using the superposition operation. When compared to the standard aggregation methods in (mobile robotics) place recognition experiments, HVs of the aggregated descriptors exhibited an average performance better than alternative methods (except the exhaustive pair-wise comparison). A very similar concept was demonstrated in [@MitrokhinCNN2020] using an image classification task, see also Table [14](#tab:class:images){reference-type="ref" reference="tab:class:images"}. One of the proposed ways of forming image HV used the superposition of three binary HVs obtained from three different hashing neural networks. The HVs representing the aggregated descriptors provided a higher classification accuracy. Finally, similarity-preserving shift-equivariant representation of images in HVs using permutations was proposed in [@rachkovskij2022representation].

[]{#tab:class:language label="tab:class:language"}

::: {#tab:class:language}
                 Ref.                          Task             Dataset   HV format      Classifier             Baseline(s)
  ---------------------------------- ------------------------- --------- ----------- -- ------------ ----------------------------------
          [@JoshiNgrams2016]          Language identification              bipolar                   
            [@RahimiLPHD]             Language identification                                        
      [@ImaniSparseLanguage2017]      Language identification                                        
       [@KleykoBoostingSOM2019]       Language identification              bipolar                    approach from [@JoshiNgrams2016]
   [@SangaliAssociativeRecover2020]   Language identification                                        

  : HDC/VSA studies classifying languages
:::

[]{#tab:class:texts label="tab:class:texts"}

::: {#tab:class:texts}
               Ref.                        Task                Dataset         HV format       Classifier   Baseline(s)
  ------------------------------- ----------------------- ------------------ ------------- -- ------------ -------------
   [@RachkovskijClassifiers2007]                                                                  SVM      
    [@fishbein2008integrating]                                                real-valued                  
       [@NajafabadiText2016]                               Reuters newswire                                
      [@ShridharEnd2End2020]       Intent classification                                                   
      [@AlonsoHyperEmbed2020]      Intent classification                        bipolar                    
         [@JiaSpamHD2021]           Text spam detection                         bipolar                    

  : HDC/VSA studies classifying texts
:::

::: {#tab:class:acoustic}
               Ref.                Task       Dataset         HV format                                             Classifier           Baseline(s)
  ------------------------------- ------ ----------------- ---------------- -------------------------------------- ------------ ------------------------------
      [@RachkovskijAudio1990]                                                                                                   
   [@RachkovskijClassifiers2007]                                                                                                
                                   SVM                                                                                          
    [@RasanenMultivariate2015]            CAREGIVER Y2 UK                                                                       
        [@ImaniVoiceHD2017]                   Isolet                                                                                     3-layer ANN
     [@ImaniHierarchical2018]                 Isolet                                                                            
     [@WongUnconventional2018]               Own data            N/A                                                                         N/A
        [@ImaniAdaptHD2019]                   Isolet                                                                            
     [@ImaniMemoryCentric2020]                Isolet                                binding; superposition                                  $k$NN
           [@Fragoso20]                                                                                                         
     [@HernandezOnlineHD2021]                 Isolet           bipolar              binding; superposition                            ANN; SVM; AdaBoost
         [@ZouManiHD2021]                     Isolet             N/A                                                                  ANN; SVM; AdaBoost
   [@hsiao2021hyperdimensional]               Isolet           bipolar                                                          
        [@kazemi2021mimhd]                    Isolet           bipolar              binding; superposition                      
       [@ChangMulTaHDC2021]                   Isolet           bipolar                                                          
      [@ZhangRobustness2021]                  Isolet           bipolar              binding; superposition                      
       [@PoduvalStocHD2021]                   Isolet           bipolar            permutation; superposition                          ANN; SVM; AdaBoost
     [@YuUnderstandingHDC2022]                Isolet        integer-valued          binding; superposition                      
        [@NazemiICCAD2020]                    Isolet        integer-valued   compact code by ANN at low dimension                ANN; other HDC/VSA solutions
        [@MaMultimodal2022]                                                                                                     
        sentiment analysis                  real-valued                                                                         

  : HDC/VSA studies classifying acoustic signals
:::

## Classification {#sec:app:class}

Applying HDC/VSA to classification tasks is currently one of the most common application areas of HDC/VSA. This is due to the fact that similarity-based and other vector-based classifiers are widespread and machine learning research is on the rise in general. The recent survey [@GeClassificationReview2020] of classification with HDC/VSA was primarily devoted to the transformation of input data into HVs. Instead, here we focus first on the types of input data (in the 2nd level headings), and then on the domains where HDC/VSA have been applied (in the 3rd level headings). Moreover, we cover some of the studies not presented in [@GeClassificationReview2020]. The studies are summarized in the form of tables, where each table specifies a reference, type of task, dataset used, format of HVs, operations to form HVs from data[^7], the type of classifier, and baselines for comparison. For the sake of consistency, in this section we use a table even if there is only a single work in a particular domain.

###  Classification based on feature vectors {#sec:app:class:features}

#### Language identification with the vector of *n*-gram statistics of letters

In [@JoshiNgrams2016], it was shown how to form a compositional HV corresponding to $n$-gram statistics (see Section [\[PartI-sec:sequences:ngrams\]](#PartI-sec:sequences:ngrams){reference-type="ref" reference="PartI-sec:sequences:ngrams"} in [@KleykoSurveyVSA2021Part1]). The work also introduced a task of identifying a language amongst $21$ European languages. Since then, the task was used in several studies summarized in Table [2](#tab:class:language){reference-type="ref" reference="tab:class:language"}.

#### Classification of texts 

Table [3](#tab:class:texts){reference-type="ref" reference="tab:class:texts"} summarizes the efforts of using HDC/VSA for text classification. The works in this domain dealt with different tasks such as text categorization, news identification, and intent classification. Most of the works [@RachkovskijClassifiers2007; @ShridharEnd2End2020; @AlonsoHyperEmbed2020] used HVs as a way of representing data for conventional machine learning classification algorithms.

#### Classification of feature vectors extracted from acoustic signals

Classification of various acoustic signals using HVs is provided in Table [4](#tab:class:acoustic){reference-type="ref" reference="tab:class:acoustic"}. Tasks were mainly related to speech recognition, e.g, recognition of spoken letters or words.

#### Fault classification

Studies on applying HDC/VSA to fault classification are limited. We are only aware of two such use-cases (summarized in Table [5](#tab:class:fault){reference-type="ref" reference="tab:class:fault"}) applied to the problems of anomaly detection in a power plant and ball bearings. An earlier work on micro machine-tool acoustic diagnostics was presented in [@KussulDiagnostics1998].

[]{#tab:class:fault label="tab:class:fault"}

::: {#tab:class:fault}
   Ref.   Task   Dataset   HV format      Classifier   Baseline(s)
  ------ ------ --------- ----------- -- ------------ -------------
                                                           N/A
                                                          $k$NN
                                                           N/A
                                                        ANN; SVM

  : HDC/VSA studies classifying faults
:::

[]{#tab:class:automotive label="tab:class:automotive"}

::: {#tab:class:automotive}
              Ref.                     Task         Dataset   HV format      Classifier   Baseline(s)
  ----------------------------- ------------------ --------- ----------- -- ------------ -------------
                                                                                              N/A
                                                                                         
                                                                                         
                                                                                         
                                                                                         
   [@SchlegelMultivariate2021]                                                           
    fractional power encoding                                                            
           SVM; $k$NN            LSTM without HVs                                        
                                                                                         
         sensor attacks                                                                  
    and reconstructed samples          N/A                                               

  : HDC/VSA studies classifying automotive data
:::

[]{#tab:class:beh:sig label="tab:class:beh:sig"}

::: {#tab:class:beh:sig}
               Ref.                       Task           Dataset      HV format                                       Classifier            Baseline(s)
  ------------------------------ ---------------------- ---------- ---------------- ---------------------------- --------------------- ----------------------
          [@Rasanen2014]          Activity recognition   Palantir      bipolar                                         centroids                N/A
        [@KimActivity2018]        Activity recognition                 bipolar         binding; superposition                              binarized ANN
       [@ChangEmotion2019]        Emotion Recognition     AMIGOS                       binding; superposition     binarized centroids         XGBoost
         [@Rasanen2015tr]                                           sparse ternary     weighted superposition          centroids       
         [@Rasanen2015tr]                                           sparse ternary     weighted superposition          centroids       
         [@Rasanen2015tr]                                           sparse ternary     weighted superposition          centroids       
       [@ImaniAdaptHD2019]        Activity recognition    UCIHAR                                                                       
    [@ImaniMemoryCentric2020]     Activity recognition                  binary         binding; superposition                                  $k$NN
    [@BasaklarHypervector2021]    Activity recognition                 bipolar                                                         
    [@BasaklarHypervector2021]                                         bipolar                                                         
     [@HernandezOnlineHD2021]     Activity recognition                 bipolar         binding; superposition                            ANN; SVM; AdaBoost
         [@ZouManiHD2021]         Activity recognition                   N/A                                                             ANN; SVM; AdaBoost
   [@hsiao2021hyperdimensional]   Activity recognition                 bipolar                                                         
        [@kazemi2021mimhd]        Activity recognition                 bipolar         binding; superposition                          
       [@ChangMulTaHDC2021]       Activity recognition                 bipolar                                                         
      [@ZhangRobustness2021]      Activity recognition                 bipolar         binding; superposition                          
       [@PoduvalStocHD2021]       Activity recognition                 bipolar       permutation; superposition                          ANN; SVM; AdaBoost
    [@YuUnderstandingHDC2022]                             UCIHAR    integer-valued     binding; superposition                          
      [@HuRadarHumActiv2022]      Activity recognition              integer-valued     binding; superposition                           10 different methods
      [@MenonEmotionHD2021]       Emotion recognition                                                             binarized centroids       XGBoost; SVM
      [@MenonWearableCA2021]      Emotion recognition                                                             binarized centroids           SVM

  : HDC/VSA studies classifying behavioral signals
:::

[]{#tab:class:EMG label="tab:class:EMG"}

::: {#tab:class:EMG}
   Ref.   Task   Dataset   HV format      Classifier   Baseline(s)
  ------ ------ --------- ----------- -- ------------ -------------
                            bipolar                        SVM
                                                      
                            bipolar                        N/A
                                                           SVM
                                                           SVM
                            bipolar                        N/A
                            bipolar                        N/A
                            bipolar                   
                            bipolar                   
                                                      
                                                      

  : HDC/VSA studies classifying EMG signals
:::

#### Automotive data

Table [6](#tab:class:automotive){reference-type="ref" reference="tab:class:automotive"}[^8] presents studies where HDC/VSA was used with automotive data, mainly in autonomous driving scenarios.

#### Behavioral signals

Studies that used behavioral signals are summarized in Table [7](#tab:class:beh:sig){reference-type="ref" reference="tab:class:beh:sig"}. One of the most common applications was activity recognition, but other tasks were considered as well (see the table).

#### Biomedical data

Currently explored applications of HDC/VSA on biomedical data can be categorized into five types of data: electromyography (EMG) signals, electroencephalography (EEG) signals, cardiotocography (CTG) signals, DNA sequences, and surface enhanced laser desorption/ionization time-of-flight (SELDI-TOF) mass spectrometry. Most of the works so far have been conducted on EMG and EEG signals. In fact, there is a study [@RahimiBiosignal2019], which provides an in-depth coverage of applying HDC/VSA to these modalities, so please refer to this article for a detailed overview of the area.

HDC/VSA was applied to EMG signals for the task of hand gesture recognition. This was done for several different transformations of data into HVs and on different datasets. Refer to Table [8](#tab:class:EMG){reference-type="ref" reference="tab:class:EMG"} for the summary.

EEG and iEEG signals were used with HDC/VSA for human-machine interfaces and epileptic seizure detection. These efforts are overviewed in Table [9](#tab:class:EEG){reference-type="ref" reference="tab:class:EEG"}. It is worth mentioning systematization efforts in [@PaleHDEpileptic2021; @UnaSelection2022], which reported an assessment of several HDC/VSA models and transformations used for epileptic seizure detection. Another recent work [@SchindlerPrimerHDiEEG2021] provides a tutorial on applying HDC/VSA for iEEG seizure detection.

[]{#tab:class:EEG label="tab:class:EEG"}

::: {#tab:class:EEG}
                     Ref.                               Task                 Dataset         HV format                              Classifier   Baseline(s)
  ------------------------------------------- ------------------------ ------------------- -------------- ------------------------ ------------ -------------
                                                                                              bipolar                                           
          [@BasaklarHypervector2021]                                                          bipolar                                           
            [@RahimiBiosignal2019]                                                            bipolar                                           
          [@HerscheExpEmbedding2018]                                                                                                            
           projection; superposition                                                                                                            
            [@BurrelloSeizure2018]                                                                         binding; superposition                    ANN
            [@EnsembleBurrello2021]                                                                                                             
               each operating on                                                                                                                
            a different feature set                                                                                                             
              centroids combined                                                                                                                
              via a linear layer                   ANN; SVM; CNN                                                                                
           [@BurrelloLBPSeizure2020]                                                                                                            
   Identification of Ictogenic Brain Regions                                                                                                    
                SWEC-ETHZ iEEG                                                                                                                  
                    binary                     binding; superposition                                                                           
                   centroids                                                                                                                    
                    SVM, RF                                                                                                                     
            [@BurrelloSeizure2019]                                                                         binding; superposition               
         [@AsgarinejadHDSeizures2020]                                   CHB-MIT Scalp EEG     bipolar      binding; superposition   centroids        CNN
           [@UnaMulti-Centroid2022]                                     CHB-MIT Scalp EEG     bipolar      binding; superposition               
                for sub-classes                                                                                                                 
               HDC/VSA solution                                                                                                                 
               [@GeSeizure2021]                                                                                                                 
            [@PaleHDEpileptic2021]                                                                                                              
               [@GeSeizure2022]                                                             dense binary   binding; superposition               

  : HDC/VSA studies classifying EEG signals
:::

. So far, there is only one work [@BasaklarHypervector2021] where CTG signals were used. It is summarized in Table [10](#tab:class:Cardiotocography){reference-type="ref" reference="tab:class:Cardiotocography"}.

[]{#tab:class:Cardiotocography label="tab:class:Cardiotocography"}

::: {#tab:class:Cardiotocography}
              Ref.              Task   Dataset   HV format      Classifier   Baseline(s)
  ---------------------------- ------ --------- ----------- -- ------------ -------------
   [@BasaklarHypervector2021]                     bipolar                   

  : HDC/VSA studies classifying Cardiotocography signals
:::

[]{#tab:class:beh:dna label="tab:class:beh:dna"}

::: {#tab:class:beh:dna}
               Ref.                Task   Dataset     HV format                                  Classifier   Baseline(s)
  ------------------------------- ------ --------- --------------- ---------------------------- ------------ -------------
                                                                                                             
                                                                                                                  SVM
   [@RachkovskijEquivariant2021]                    sparse binary   permutation; superposition               
   [@RachkovskijEquivariant2021]                    sparse binary   permutation; superposition               

  : HDC/VSA studies classifying DNA sequences
:::

[]{#tab:class:beh:spectrometry label="tab:class:beh:spectrometry"}

::: {#tab:class:beh:spectrometry}
   Ref.   Task   Dataset   HV format      Classifier   Baseline(s)
  ------ ------ --------- ----------- -- ------------ -------------
                                                      

  : HDC/VSA studies classifying the sensitivity of glioma to chemotherapy using SELDI-TOF
:::

[]{#tab:class:beh:multi label="tab:class:beh:multi"}

::: {#tab:class:beh:multi}
            Ref.                   Task           Dataset   HV format                                Classifier        Baseline(s)
  ------------------------ --------------------- --------- ----------- ------------------------ --------------------- --------------
    [@ChangEmotion2019]     Emotion Recognition   AMIGOS                binding; superposition   binarized centroids     XGBoost
   [@MenonEmotionHD2021]    Emotion recognition                                                  binarized centroids   XGBoost; SVM
   [@MenonWearableCA2021]   Emotion recognition                                                  binarized centroids       SVM
                                                                                                                           N/A

  : HDC/VSA studies classifying multi-modal signals
:::

Table [11](#tab:class:beh:dna){reference-type="ref" reference="tab:class:beh:dna"} presents two studies that used DNA sequences in classification tasks.

Table [12](#tab:class:beh:spectrometry){reference-type="ref" reference="tab:class:beh:spectrometry"} summarizes a study that used SELDI-TOF mass spectrometry for classifying sensitivity of glioma to chemotherapy.

Studies involding multi-modal signals are summarized in Table [13](#tab:class:beh:multi){reference-type="ref" reference="tab:class:beh:multi"}.

### Classification of images or their properties {#sec:app:class:images}

Table [14](#tab:class:images){reference-type="ref" reference="tab:class:images"} provides an overview of the efforts involving images. Since using raw pixels directly would rarely result in a good performance, HVs were produced either from features extracted from images or using HVs obtained from neural networks (see Section [\[PartI-sec:2Dimages:neural:nets\]](#PartI-sec:2Dimages:neural:nets){reference-type="ref" reference="PartI-sec:2Dimages:neural:nets"} in [@KleykoSurveyVSA2021Part1]), which took images as an input.

[]{#tab:class:images label="tab:class:images"}

::: {#tab:class:images}
  --------------------------------- --------------------------------------------- --------------- ---------------- ---------------------------- -- --------------------
                Ref.                                    Task                          Dataset        HV format                                     
         data transformation                         Classifier                     Baseline(s)                                                    
                                                                                                                                                   
      [@KussulPermutation2006]                                                                                                                     
          shape recognition                                                                                                                        
               binary                                                                                                                              
            superposition                                                                                                                          
             perceptron                                                                                                                            
                                                                                                                                                   
                                                                                                                                                   
                                                                                                                                                         SeqSLAM
                                                                                                                                                   
       CMU Visual Localization                                                                                                                     
            superposition                                                                                                                          
         NetVLAD; DenseVLAD                                                                                                                        
                                                                                                                                                           N/A
                                                                                                                                                   
               binary                                                                                                                              
              centroids                                                                                                                            
      ANN; various regressions                                                                                                                     
                                                                                                                                                           ANN
                                                                                                                                                           CNN
                                                                                                                                                           N/A
      [@YuUnderstandingHDC2022]                                                    Fashion-MNIST   integer-valued     binding; superposition       
          [@KussulLiRA2004]                                                                                                                        
      [@KussulPermutation2006]                                                                                                                     
    [@RachkovskijClassifiers2007]                                                                                                                  
                                                                                                                                                           N/A
                                                                                                                                                       Naïve Bayes
      [@ChuangHDTradeoffs2020]                                                                         binary                                      
      [@HernandezOnlineHD2021]                                                                        bipolar         binding; superposition        ANN; SVM; AdaBoost
         [@kazemi2021mimhd]                                                                           bipolar         binding; superposition       
          [@ZouManiHD2021]                                                                              N/A                                         ANN; SVM; AdaBoost
        [@ChangMulTaHDC2021]                                                                          bipolar                                      
        [@PoduvalStocHD2021]                                                                          bipolar       permutation; superposition      ANN; SVM; AdaBoost
      [@YuUnderstandingHDC2022]                                                        MNIST       integer-valued     binding; superposition       
                                                                                                                                                       various CNNs
                                                                                                                                                   
         associative memory          approach from [@KarunaratneHDAugmented2021]                                                                   
                                                                                                                                                   
      for image classification                                                                                                                     
              Omniglot                                                                                                                             
     a retrainable linear layer                                                                                                                    
   loss-optimized nudged centroids                various deep ANNs                                                                                
      [@KussulPermutation2006]                                                                                                                     
      [@ImaniMemoryCentric2020]                                                                        binary         binding; superposition              $k$NN
      [@HernandezOnlineHD2021]                                                                        bipolar         binding; superposition        ANN; SVM; AdaBoost
            [@Curtidor21]                                                                                                                          
         [@kazemi2021mimhd]                                                                           bipolar         binding; superposition       
        [@PoduvalStocHD2021]                                                                          bipolar       permutation; superposition      ANN; SVM; AdaBoost
  --------------------------------- --------------------------------------------- --------------- ---------------- ---------------------------- -- --------------------

  : HDC/VSA studies classifying visual images and their properties
:::

### Classification of structured data {#sec:app:class:structured}

[]{#tab:class:structured label="tab:class:structured"}

::: {#tab:class:structured}
          Ref.           Task   Dataset   HV format          Classifier                        Baseline(s)
  --------------------- ------ --------- ----------- -- -------------------- -----------------------------------------------
                                                                             
   compound properties                                   DISCOVERY; ANALOGY  
                                                                              Logistic Regression; SVM; Random Forest; etc.

  : HDC/VSA studies classifying structured data
:::

Classification of structured data can be tricky with conventional machine learning algorithms since local representations of structured data might not be convenient to work when using vector classifiers, especially when the data involve some sorts of hierarchies. HDC/VSA should be well suited for structured data since they allow representing various structures (including hierarchies) as HVs. To the best of our knowledge, however, the number of such studies is very limited (see Table [15](#tab:class:structured){reference-type="ref" reference="tab:class:structured"}). One such study in [@SlipchenkoHierarchical2005] used SBDR to predict the properties of chemical compounds and provided the state-of-the-art performance. A more recent example was demonstrated in [@MaDrugDiscovery2021], where 2D molecular structures were transformed into HVs that were used to construct classifiers for drug discovery problems. The approach outperformed the baseline methods on a collection of $30$ tasks. Finally, in [@NunesGraphHD2022] it was proposed to classify graphs using HDC/VSA. A graph was represented as a superposition of HVs corresponding to vertices and edges. The proposed approach was evaluated on six graph classification datasets; when compared to the baseline approaches, it demonstrated comparable accuracy on four datasets and much shorter training time on all of them.

# Cognitive computing and architectures {#sec:cog}

In this section, we overview the use of HDC/VSA in cognitive computing (Section [3.1](#sec:cog:comp){reference-type="ref" reference="sec:cog:comp"}) and cognitive architectures (Section [3.2](#sec:cog:arch){reference-type="ref" reference="sec:cog:arch"}). Note that, strictly speaking, cognitive computing as well as cognitive architectures can also be considered to be application areas but we decided to separate them into a distinct section due to the rather different nature of tasks being pursued.

## Cognitive computing {#sec:cog:comp}

### Holistic transformations {#sec:database:holistic}

#### Holistic transformations in database processing with HVs {#sec:holistic:proc}

In this section, we consider simple examples of HDC/VSA processing of a small database. This can be treated as analogical reasoning (though analogy researchers might disagree) in the form of query answering using simple analogs (we refer to them as records) without explicitly taking into account the constraints on analogical reasoning mentioned in the following sections.

The work [@KanervaFully1997] introduced the transformation of database records into HVs as an alternative to their symbolic representations. Each record is a set of role-filler bindings and is represented by HVs using transformations for role-filler bindings (Section [\[PartI-sec:key:value\]](#PartI-sec:key:value){reference-type="ref" reference="PartI-sec:key:value"} in [@KleykoSurveyVSA2021Part1]) and sets (Section [\[PartI-sec:sets\]](#PartI-sec:sets){reference-type="ref" reference="PartI-sec:sets"} in [@KleykoSurveyVSA2021Part1]). We might be interested in querying the whole base for some property or in processing a pair of records. For example, knowing a filler of one of the roles in one record (the role is not known), can enable to get the filler of that role in another record. In [@KanervaFully1997], the records were persons with attributes (i.e., roles) such as "name", "gender", and "age" (see Table [16](#tab:persons){reference-type="ref" reference="tab:persons"}).

Several ways were proposed to use HDC/VSA operations for querying the records by HVs. An example query to the database could be "What is the age of Lee who is a female?". The correct record for this query will be $\mathbf{LF6}$ and the answer is $66$. Depending on the prior knowledge, different cases were considered. All the cases below assume that there is an item memory with the HVs of all the roles, fillers, and database records.

*Case* 1. We know the role-filler bindings name:Lee, gender:female, and only the role "age" from the third role-filler binding whose filler we want to find. *Solution* 1. The query is represented as $\mathbf{name} \circ \mathbf{Lee}$ + $\mathbf{gender}\circ \mathbf{female}$ . The base memory will return $\mathbf{LF6}$ as the closest match using the similarity of HVs. Unbinding $\mathbf{age} \oslash \mathbf{LF6}$ results in the noisy version of HV for $\mathbf{66}$, and the clean-up procedure returns the value associated with the nearest HV in the item memory, i.e., $\mathbf{66}$, which is the answer.

::: {#tab:persons}
       Record       Name   Gender   Age
  ---------------- ------ -------- -----
   $\mathbf{PF3}$   Pat    female   33
   $\mathbf{PM6}$   Pat     male    66
   $\mathbf{LF6}$   Lee    female   66
   $\mathbf{LM3}$   Lee     male    33
   $\mathbf{LM6}$   Lee     male    66

  :  Example database records (adapted from [@KanervaFully1997].)
:::

*Case* 2. The following HVs are available: record $\mathbf{PM6}$ and the fillers $\mathbf{Pat}$, $\mathbf{male}$, and $\mathbf{Lee}$, as well as the roles $\mathbf{female}$ and $\mathbf{age}$. *Solution* 2a. First, we find $\mathbf{name}$ by the clean-up procedure on $\mathbf{Pat} \oslash \mathbf{PM6}$, and $\mathbf{gender}$ by the clean-up procedure on $\mathbf{male} \oslash \mathbf{PM6}$. Then we apply the previous solution (*Solution* 1). *Solution* 2b. This solution[^9] uses the correspondences $\mathbf{Pat} \leftrightarrow \mathbf{Lee}$ and $\mathbf{male} \leftrightarrow \mathbf{female}$ by forming the transformation HV $\mathbf{T} = \mathbf{Pat}\circ \mathbf{Lee} + \mathbf{male}\circ \mathbf{female}$. The transformation $\mathbf{T} \oslash \mathbf{PM6}$ returns approximate $\mathbf{LF6}$ (see [@KanervaFully1997] for a detailed explanation), and its clean-up provides exact $\mathbf{LF6}$. Then, as above, $\mathbf{age} \oslash \mathbf{LF6}$ after the clean-up procedure returns $\mathbf{66}$. Note that such a transformation is intended for HDC/VSA models where the binding operation is self-inverse (e.g., BSC).

*Case* 3. Only $\mathbf{33}$ and $\mathbf{PF3}$ are known, and the task is to find an analog of $\mathbf{33}$ in $\mathbf{LF6}$, that is, $\mathbf{66}$. A trivial solution would be to get $\mathbf{age}$ as the result of the clean-up procedure for $\mathbf{33} \oslash \mathbf{PF3}$, and then get $\mathbf{33}$ by $\mathbf{age} \oslash \mathbf{PF3}$ and the clean-up procedure. But there is a more interesting way, which can be considered as an analogical solution. *Solution* 3. One step solution is $\mathbf{LF6} \oslash (\mathbf{33}\oslash \mathbf{PF3})$. This exemplifies a possibility of processing without the intermediate use of a clean-up procedure.

In some HDC/VSA models (e.g., BSC), however, the answer for this solution will be ambiguous, being equally similar to $\mathbf{33}$ and $\mathbf{66}$. This is due to the self-inverse property of the binding operation in BSC. Note that both $\mathbf{LF6}$ and $\mathbf{PF3}$ include $\mathbf{gender}\circ \mathbf{female}$ as the part of their records. Unbinding $\mathbf{33}$ with $\mathbf{PF3}$ creates $\mathbf{33} \circ  \mathbf{gender}\circ \mathbf{female}$ (since $\oslash=\circ$) among other bindings. When unbinding the result with $\mathbf{LF6}$, the HV $\mathbf{gender}\circ \mathbf{female}$ will cancel out, thus releasing $\mathbf{33}$, which will interfere with the correct answer, which is $\mathbf{66}$. This effect would not appear if the records would had different fillers in their role-filler bindings. For example, if instead of $\mathbf{LF6}$ we consider $\mathbf{LM6}$, then $\mathbf{33} \circ \mathbf{PF3} \circ \mathbf{LM6}$ produces the correct answer.

In models with self-inverse binding, the result of $\mathbf{PF3} \oslash \mathbf{LM6}$ can be seen as an interpretation of $\mathbf{PF3}$ in terms of $\mathbf{LM6}$ or vice versa, because the result of this operation is $\mathbf{Pat} \circ \mathbf{Lee} + \mathbf{male} \circ \mathbf{female} + \mathbf{33} \circ \mathbf{66} + \mathrm{noise}$ (since $\oslash=\circ$). This allows answering queries of the form "which filler in $\mathbf{PF3}$ plays the same role as (something) in $\mathbf{LM6}$?" by unbinding $\mathbf{PF3} \oslash \mathbf{LM6}$ with the required filler HV, resulting in the noisy answer HV.

Note that Solution 1 resembles the standard processing of records in databases. We first identify the record and then check the value of the role of interest. *Solutions* 2b and 3 are examples of a different type of computing sometimes called "holistic mapping" [@KanervaFully1997] or "transformation without decomposition" [@PlateStructure1997]. We call it "holistic transformation" in order not to confuse it with the transformation of input data into HVs or with mapping in analogical reasoning. This holistic transformation of HVs is commonly illustrated by an example well-known under the name "Dollar of Mexico" [@KanervaAnalogy1998; @KanervaLarge2001; @KanervaDollar2010].

In essence, the "Dollar of Mexico" show-case solves some simple "proportional analogies" of the form A:B :: C:D as (i.e., United States:Mexico :: Dollar:?). These analogies are also known to be solvable by addition and subtraction of the "neural" word embeddings of the corresponding concepts [@MikolovWord2vec2013; @pennington2014glove]. Following a similar approach, the authors in [@PaulladaRetrieval2020] proposed to improve the results by training shallow neural networks using a dependency path of relations between terms in sentences.

It should be noted that there is no direct analog to this kind of processing by holistic transformation (using geometric properties of the representational space) in the conventional symbol manipulation [@PlateStructure1997]. The holistic transformation of HVs can be seen as a parallel alternative to the conventional sequential search.

#### Learning holistic transformations from examples {#sec:transformation}

Learning systematic transformations from examples was investigated in [@NeumannTransformation2000; @NeumannTransformation2002] for HRR. Previously, this capability was shown in [@PlateStructure1997] only for manually constructing the so-called transformation HV. In [@NeumannTransformation2000; @NeumannTransformation2002], the transformation HV was obtained from several training pairs of HVs. One of the proposed approaches to obtaining the transformation HV was to use the gradient descent by iterating through all examples until the optimization converged. The experiments demonstrated that the learned transformation HVs were able to generalize to previously unseen compositional structures with novel elements. A high level of systematicity was indicated by the ability of transformation HVs to generalize to novel elements in structures of a complexity higher than the structures provided as training examples. The capability of BSC to learn holistic transformations was also presented in [@KanervaLearning2000; @KanervaLarge2001]. However, the disadvantage of such holistic transformations is their bidirectionality, which is due to the fact that the unbinding operation in BSC is equivalent to the binding operation. This complication can be resolved by using either the permutation or an additional associative memory as the binding operation as proposed in [@EmruliAnalogical2014].

The holistic transformation of the kind considered above was used to associate, e.g., sensory and motor/action data via binding HVs. For example, in [@KleykoBees2015; @KleykoFlyBee2015], BSC was applied to form a scene representation in the experiments with honey bees. It was shown to mimic learning in honey bees using the transformation as in [@KanervaLearning2000; @KanervaLarge2001]. In [@EmruliInteroperability2015], an HDC/VSA-based approach for learning behaviors, based on observing and associating sensory and motor/action data represented by HVs, was proposed. A potential shortcoming of the approaches to learning holistic transformations presented above is that the objects/relations are assumed to be dissimilar to each other. The learning might not work as expected given that there is some similarity structure between the objects/relations used as training examples. This direction deserves further investigation.

### Analogical reasoning {#sec:analogical:reasoning}

We begin with a brief introduction to analogical reasoning by summarizing basic analogical processes as well as some of their properties. Two final sections discuss applications of HDC/VSA to modeling analogical retrieval and mapping.

#### Basics of analogical reasoning modeling

Modeling analogical reasoning in humans is an important frontier for Artificial Intelligence. Because it allows analogical problem solving as well as even more universal cognitive processes taking the problem structure into account and applying knowledge acquired in different domains. Analogical reasoning theories [@GentnerStructure1983; @KokinovAnalogy2003; @GentnerAnalogical2010; @GentnerAnalogical2012; @GentnerAnalogical2017] usually consider and model the following basic processes: description; retrieval (also known as access or search); mapping; and inference.

Any model for analogical reasoning usually works with analogical episodes (or simply analogs). The description process concerns the representation of analogical episodes. The analogical episodes are usually modeled as systems of hierarchical relations (predicates), consisting of elements in the form of entities (objects) and relations of various hierarchical levels. Entities belong to some subject domain (e.g., the sun, planet, etc.) and are described by attributes (features, properties), which in essence are relations with a single argument (e.g., mass, temperature, etc.). Relations (e.g., attract, more, cause, etc.) define relations between the elements of analogical episodes. Arguments of relations may be objects, attributes, and other relations. It is assumed that a collection of base (source) analogical episodes is stored in a memory.

The retrieval process searches the memory (in models, the base of analogical episodes) to find the closest analog(s) for the given target (input, query) analog. The similarity between episodes is used as a criterion for the search. Once the base analogical episode(s) closest to the target is identified, the mapping process finds the correspondences between the elements of two analogical episodes: the target and the base ones. The inference process concerns the transfer of the knowledge from the base analogical episode(s) to the target analogical episode. This new knowledge obtained about the target may, e.g., provide the solution to the problem specified by the target analogical episode. Because the analogical reasoning is not a deductive mechanism, the candidate inferences are only hypotheses and must be evaluated and checked (see, e.g., [@GentnerAnalogical2010] and references therein).

Cognitive science has identified quite a few properties demonstrated by subjects when performing analogical reasoning. Two types of similarity that influence processing of analogical episodes are distinguished. Structural similarity (which should not be confused with the structural similarity in HDC/VSA) reflects how the elements of analogs are arranged with respect to each other, that is, in terms of the relations between the elements [@EliasmithAnalogical2001; @GentnerAnalogical2012; @GentnerAnalogical2017]. Analogs are also matched by the "surface" or "superficial" similarity [@GentnerStructure1983; @ForbusMAC1995] based on common analogs' elements or a broader "semantic" similarity [@HummelDistributed1997; @ThagardAnalog1990; @EliasmithAnalogical2001], based on, e.g., joint membership in a taxonomic category or on similarity of characteristic feature vectors. Experiments based on human assessment of similarities and analogies confirmed that both surface (semantic) and structural similarity are necessary for sound retrieval [@ForbusMAC1995]. The structural similarity in the retrieval process is considered less important than that in the mapping process, however, the models of retrieval that take into account only the surface similarity are considered inadequate. These properties are expected to be demonstrated by the computational models of analogy [@GentnerComputational2011]. In the following sections we discuss applications of HDC/VSA for analogical reasoning.

#### Analogical retrieval {#sec:analogical:retrieval}

It is known that humans retrieve some types of analogs more readily than others. Psychologists identified different types of similarities and ordered them according to the ease of retrieval [@GentnerStructure1983; @RossSimilarities1989; @WhartonSimilarity1994; @ForbusMAC1995]. The similarity types are summarized in Table [17](#tab:analog:epissodes){reference-type="ref" reference="tab:analog:epissodes"}, relative to the base analogical episode. Simple examples of animal stories (adapted by Plate from [@ThagardAnalog1990]) with those similarity types are also presented. All analogical episodes have the same first-order relations (in the example, `bite()` and `flee()`). There are also higher-order relations `cause(bite, flee)` and `cause(flee, bite)` and attribute relations (`dog, human, mouse, cat`).

In addition to common first-order relations, the literal similarity (LS) also assumes both the same higher-order relations (in the example, the single relation `cause(bite, flee)`) and object attributes. The true analogy (AN) has the same higher-order relations, but different object attributes. The surface features (SF) has the same object attributes, but different higher-order relations. The first-order only relations (FOR) differ in both higher-order relations and attributes. For the analogical retrieval, it is believed that the retrievability order is expressed as LS $\geq$ SF $>$ AN $\geq$ FOR [@GentnerStructure1983; @RossSimilarities1989; @WhartonSimilarity1994; @ForbusMAC1995].

:::: center
::: {#tab:analog:epissodes}
  ----------------------------------------------------------------- --- --- --- --
                                                                                
                              1st-order                                         
                              relations                                         
                             high-order                                         
                              relations                                         
                               object                                           
                             attributes                                         
                           animal episodes                                      
                                Base                                            
   $\texttt{cause(bite(Spot, Jane)}$, $\texttt{flee(Jane, Spot))}$              
                       Literal Similarity (LS)                                  
            `cause(bite(Fido, John)`, `flee(John, Fido))`                       
                        Surface Features (SF)                                   
            `cause(flee(John, Fido)`, `bite(Fido, John))`                       
                          True Analogy (AN)                                     
           `cause(bite(Felix, Mort)`, `flee(Mort, Felix))`                      
                  First-order Only Relations (FOR)                              
           `cause(flee(Mort, Felix)`, `bite(Felix, Mort))`                      
  ----------------------------------------------------------------- --- --- --- --

  : Types of analogical similarity.
:::
::::

Researchers studying analogical reasoning proposed a number of heuristics-based models of the analogical retrieval. The most influential of them are still MAC/FAC (many are called but few are chosen), which operate with symbolic structures [@ForbusMAC1995], and ARCS (analog retrieval by constraint satisfaction) using localist neural network structures [@ThagardAnalog1990]. The structure of analogical episodes should be taken into account in their similarity estimation. This requires alignment and finding correspondences between elements of the analogical episodes, as in the mapping (Section [3.1.2.3](#sec:anlogical:mapping){reference-type="ref" reference="sec:anlogical:mapping"} below), which is computationally expensive. Moreover, unlike for mapping, where only two analogical episodes are considered, in the retrieval process alignment should be repeated for the target analogical episode and each of the base analogical episodes, making such an implementation of the retrieval prohibitive.

To reduce the computational costs, a two-stage filter and refine (F&R) approach is used in the traditional models of analogical retrieval. At the filtering step, the target analogical episode is compared to all the base analogical episodes using a low-cost similarity of their feature vectors (that only counts the frequency of symbolic names in the analogical episodes, without taking into account the structure). The most similar base analogical episodes are selected as prospective candidates. At the refining step, the candidates are then compared to the target analogical episode by the value of their structural similarity. Computationally expensive mapping algorithms (Section [3.1.2.3](#sec:anlogical:mapping){reference-type="ref" reference="sec:anlogical:mapping"} below) are used for calculating the structural similarity. As the final result, the analogical episodes with the highest structural similarity are returned.

HDC/VSA have been applied to modeling of the analogical retrieval by Plate (see, e.g., [@PlateAnalogical1994; @PlateNested1994; @PlateAnalogy2000; @PlateHolographic2003] and references therein). In HDC/VSA, both the set of structure elements and their arrangements influence their HV similarity, so that similar structures (in this case, analogical episodes) produce similar HVs. Because the HV similarity measures are not computationally expensive, the two-stage F&R approach of the traditional models is not needed. Using HRR, it was shown that the results obtained by a single-stage HV similarity estimation are consistent with both the empirical results in psychological experiments as well as the aforementioned leading traditional models of the analogical retrieval. Note that Plate experimented with analogical episodes different from those tested in the leading models, but they still belong to the proper similarity types, as shown in Table [17](#tab:analog:epissodes){reference-type="ref" reference="tab:analog:epissodes"}. Similar results were also reported in [@RachkovskijStructures2001] for SBDR using Plate's episodes.

The study [@RachkovskijAnalogy2012] applied SBDR to represent the analogical episodes in the manner of [@RachkovskijStructures2001]. However, the performance was evaluated using the test bases of the most advanced models of analogical retrieval. The results demonstrated some increase in the recall and a noticeable increase in the precision compared to the leading traditional (two-stage) models. The authors also compared the computational complexity and found that in most of the cases the HDC/VSA approach had advantages over the traditional models.

#### Analogical mapping {#sec:anlogical:mapping}

The most influential models of analogical mapping include the Structure Mapping Engine (SME) [@FalkenhainerEngine1989] and its versions and further developments [@ForbusExtendingSME2017], as well as the Analogical Constraint Mapping Engine (ACME) [@HolyoakAnalogical1989]. SME is a symbolic model that uses a local-to-global alignment process to determine correspondences between the elements of analogical episodes. SME's drawback is a rather poor account of semantic similarity. Also, structure matching in SME is computationally expensive, so it is prohibitively expensive to use SME during retrieval by comparing input to each of (many) analogical episodes in the memory containing all analogical episodes for a structure-sensitive comparison.

ACME is a localist connectionist model that determines analogical mappings using a parallel constraint satisfaction network. Unlike SME, ACME relies not only on the structural information, but also takes into account semantic and pragmatic constraints. ACME is usually even more computationally expensive than SME.

Further models of mapping based on HRR and BSC were proposed that use techniques based on holistic transformations (Section [3.1.1.2](#sec:transformation){reference-type="ref" reference="sec:transformation"}). One of the limitations of these studies is that the approach was not demonstrated to be scalable to large analogical episodes. HRR was also used in another model for the analogical mapping [@EliasmithAnalogical2001] called DRAMA, where the similarity between HVs was used to initialize a localist network involved in the mapping.

In [@RachkovskijStructures2001], similarity of HVs (formed with SBDR) of the analogical episodes was used for their mapping. However, this technique worked only for the most straightforward mapping cases. In [@RachkovskijAnalogical2004], several alternative techniques for mapping with SBDR were proposed (including direct similarity mapping, re-representation by substitution of identical HVs, and parallel traversing of structures using higher-level roles) and some of them were demonstrated on complex analogies. However, the techniques are rather sophisticated and used sequential operations.

In [@SlipchenkoAnalogical2009], a kind of re-representation of an analog's element HVs was proposed to allow the analogical mapping of the resultant HVs on the basis of similarity. The re-representation approach included the superposition of two HVs. One of those HVs was obtained as the HV for the episode's element using the usual representational scheme of the episode, e.g., the HV used for the retrieval (compositional structure representation by HVs, see Section [\[PartI-sec:compositional\]](#PartI-sec:compositional){reference-type="ref" reference="PartI-sec:compositional"} in [@KleykoSurveyVSA2021Part1]). This HV took into account the semantic similarity. The other HV was the superposition of HVs of elements higher-level roles. This took into account the structural similarity. The proposed procedure was tested in several experiments on simple analogical episodes used in previous studies (e.g., in [@PlateHolographic2003; @RachkovskijAnalogical2004], and on rather complex analogical episodes previously used only in the leading state-of-the-art models, e.g., "Water Flow-Heat Flow", "Solar System-Atom", and "Schools" [@GentnerStructure1983; @HolyoakAnalogical1989]. It produced the correct mapping results. The analogical inference was also considered. The computational complexity of the proposed approach was rather low and was largely affected by the dimensionality of HVs.

The problem with the current models of HDC/VSA for analogical mapping is that they lack interaction and competition of consistent alternative mappings. They could probably be improved by using an approach involving the associative memory akin to [@GaylerIsomorphism2009].

Finally, an important aspect of HDC/VSA usage for analogical mapping and reasoning is the compatibility with the existing well-established formats of knowledge representation. This will facilitate the unification of symbolic and subsymbolic approaches for cognitive modeling and Artificial Intelligence. The work in [@MercierOntology2021] presented a proof of concept of the mapping between Resource Description Framework Schema ontology and HDC/VSA.

#### Graph isomorphism

The ability to identify some form of "structural isomorphism" is an important component of analogical mapping [@FalkenhainerEngine1989; @PlateHolographic2003]. The abstract formulation of isomorphism is graph isomorphism. In [@GaylerIsomorphism2009], an interesting scheme was proposed for finding the graph isomorphism with HDC/VSA and associative memory. The scheme used the mechanism proposed in [@LevyLateralInhibition2009]. The paper presented the HDC/VSA-based implementation of the algorithm proposed in [@PelilloReplicator1999], which used replicator equations and treated the problem as a maximal-clique-finding problem. In essence, the HDC/VSA-based implementation transformed the continuous optimization problem into a high-dimensional space where all aspects of the problem to be solved were represented as HVs. The simulation study (unfortunately performed on a simple graph) showed that the distributed version of the algorithm using the mechanism from [@LevyLateralInhibition2009] mimics the dynamics of the localist implementation from [@PelilloReplicator1999].

### Cognitive modeling {#sec:cog:modeling}

In this part, we briefly cover known examples of using HDC/VSA for modeling particular cognitive capabilities, such as sequence memorization or problem-solving, for cognitive tasks like the Wason task [@EliasmithCognition2005], n-back task [@GosmannNback2015], Wisconsin card sorting test [@KajicWisconsin2021], Raven's Progressive Matrices [@RasmussenInductiveReasoning2011], or the Tower of Hanoi [@StewartHanoi2011].

#### HDC/VSA as a component of cognitive models

As argued in [@KellyCognition2012], HDC/VSA is an important tool for cognitive modeling. In cognitive science, HDC/VSA has been commonly used as a part of computational models replicating experimental data obtained from humans. For example, in [@BlouwConcepts2016] HVs were used as the representational scheme of a computational model. The model was tested using categorization studies considering three competing accounts of concepts: "prototype theory", "exemplar theory", and "theory theory". The model was shown to be able to replicate experimental data from categorization studies for each of the accounts. It is also worth mentioning that there are numerous works using context HVs (Section [2.2.1](#sec:context:HVs){reference-type="ref" reference="sec:context:HVs"}) to form models replicating the results obtained in language-related cognitive studies, see, e.g., [@JonesMeaning2007; @ChubalaRecoding2013; @JohnsStructure2015; @RecchiaSemantic2015; @JamiesonITS2018; @JohnsCrowdsourced2019; @johns2019influence; @TalerFluency2020; @SchubertLanguage2020].

#### Modeling human working memory with HDC/VSA

A topic that was studied by different research groups working on HDC/VSA is sequence memorization and recall. For example, it was demonstrated in [@HannaganHolographic2011] (see Section [\[PartI-sec:sequences\]](#PartI-sec:sequences){reference-type="ref" reference="PartI-sec:sequences"} in [@KleykoSurveyVSA2021Part1]) that a HDC/VSA-based representation of sequences performed better than localist representations when compared on the standard benchmark for behavioral effects. Some studies [@murdock_theory_1982; @metcalfe_eich_composite_1982; @ChooSerialOrderRecall2010; @BlouwWordOrder2013; @FranklinMemory2015; @KellyDeclarativeMemory2020; @GosmannUnifiedSpikingModel2020; @ReimannModellingSerial2021] demonstrated how the recall of sequences represented in HVs (Section [\[PartI-sec:sequences\]](#PartI-sec:sequences){reference-type="ref" reference="PartI-sec:sequences"} in [@KleykoSurveyVSA2021Part1]), albeit with slightly different encoding, can reproduce the performance of human subjects on remembering sequences. This is profound as it demonstrates that simple HDC/VSA operations can reproduce basic experimental findings of human memory studies. An alternative model was proposed in [@CalmusBindingNeurobiologically2019]. Importantly, this work linked the neuroscience literature to the modeling of memorizing sequences with HDC/VSA.

#### Raven's Progressive Matrices

Raven's Progressive Matrices is a nonverbal test used to measure general human intelligence and abstract reasoning. Some simple geometrical figures are placed in a matrix of panels, and the task is to select the figure for an empty panel from a set of possibilities [@LovettRaven2010]. Note that the key work related to Raven's progressive matrices goes back to the 1930s (see [@Raven2000]). It was widely used to test human fluid intelligence and abstract reasoning since the 1990s [@Carpenter1990]. The task was first brought to the scope of HDC/VSA in [@RasmussenInductiveReasoning2011; @RasmussenSpiking2014] using HRR and its subsequent implementation in spiking neurons. Later studies [@EmruliAnalogical2013; @LevyBeetle2014] demonstrated that other HDC/VSA models (BSC and MAP, respectively) can also be used to create systems capable of solving a limited set of Raven's Progressive Matrices containing only the progression rule.

In all studies the key ingredient of the solution was the representation of a geometrical panel by its HV (e.g., giving access to the symbolic representation of the panel followed by using role-filler bindings of the shapes and their quantity present in the panel). Subsequently, the HV corresponding to the transformation between the HVs of the adjacent pairs of panels was obtained using the ideas from [@NeumannTransformation2002] (see Section [3.1.1.2](#sec:transformation){reference-type="ref" reference="sec:transformation"}). The transformation HV was then used to form a prediction HV for the blank panel in the matrix. The candidate answer with the HV most similar to the prediction HV was then chosen as the answer to the test. All of the previously described studies have two limitations: first, they assume the perception system provides the symbolic representations that support the reasoning for solving Raven's Progressive Matrices test; and second, they only support the progression rule. A recent work [@herscheNVSA2022] addressed these limitations by positioning VSA/HDC as a common language between a neural network (to solve the perception issue) and a symbolic logical reasoning engine (to support more rules). Specifically, it exploited the superposition of multiplicative bindings in a neural network to describe raw sensory visual objects in a panel and used Fourier Holographic Reduced Representations (FHRR) to efficiently emulate a symbolic logical reasoning with a rich set of rules [@herscheNVSA2022].

#### The Tower of Hanoi task

The Tower of Hanoi task, which is a simple mathematical puzzle, is another example of a task used to assess problem solving capabilities. The task involves three pegs and a fixed number of disks of different sizes with holes in the middle such that they can be placed on the pegs. Given a starting position, the goal is to move the disks to a target position. Only one disk can be moved at a time and a larger disk cannot be placed on top of a smaller disk.

In [@StewartHanoi2011], an HDC/VSA-based model capable of solving the Tower of Hanoi tasks was presented. The binding and superposition operations were used to form HVs of the current position and a set of goals identified by the model. The model implemented a known algorithm for solving the task given valid starting and target positions. The performance of the model was compared to that of humans regarding of time delays, which were found to be qualitatively similar.

#### Modeling the visual similarity of words

Modeling human perception of word similarity with HDC/VSA was based on the experimental data obtained for human subjects in [@DavisSpatial2010]. The task was to model the human patterns of delays in priming tasks with the similarity values of sequence HVs obtained from various HDC/VSA models and for various schemes for sequence representation (Section [\[PartI-sec:sequences\]](#PartI-sec:sequences){reference-type="ref" reference="PartI-sec:sequences"} in [@KleykoSurveyVSA2021Part1]).

In the task of modeling restrictions on the perception of word similarity, four types of restrictions (a total of 20 similarity patterns) were summarized in [@HannaganHolographic2011]. In [@HannaganHolographic2011], the BSC model was employed to represent symbols in their positions; various string representations with substrings were also used. Symbols in positions with correlated HVs to represent nearby positions were studied in  [@CohenOrthogonality2013] for the BSC, FHRR and HRR models. Their results demonstrated partial satisfaction of the restrictions. On the other hand, substring representations from [@CoxHolographic2011] with HRR, and symbols-at-correlated positions representations from [@RachkovskijEquivariant2021] with SBDR using permutations as well as the ones from [@RachkovskijRecursiveBinding2022] with FHRR met all the restrictions for certain choices of the similarity measure and values of a scheme's hyperparameters. In the task of finding correlations between human and model similarity data, [@RachkovskijEquivariant2021; @RachkovskijRecursiveBinding2022] demonstrated results were on a par with those of the string kernel similarity measures from [@HannaganProtein2012].

#### General-purpose rule-based and logic-based inference with HVs

The study in [@StewartSymbolicReasoning2010] presented a spiking neurons model that was positioned as a general-purpose neural controller. The controller was playing a role analogous to a production system capable of applying inference rules. HDC/VSA and their operations played a key role in the model providing the basis for representing symbols and their relations. The model was demonstrated to perform several tasks: repeating the alphabet, repeating the alphabet starting from a particular letter, and answering simple questions similar to the ones in Section [3.1.1.1](#sec:holistic:proc){reference-type="ref" reference="sec:holistic:proc"}. Another realization of a production system with the Tensor Product Representations model was demonstrated in [@SmolenskyProduction1989]. Several examples of hierarchical reasoning using the superposition operation on context HVs representing hyponyms to form representations of hypernyms were presented in [@KommersHierarchicalReasoning2015].

In [@SummersInference2020], it was demonstrated how HVs can be used to represent a knowledge base with clauses for further performing deductive inference on them. The work widely used negation for logical inference, which was also discussed in [@KussulAssociative1992]. Reasoning with HRR using modus ponens and modus tollens rules was demonstrated in [@KvasnivckaDeductiveHRR2006]. The works in [@SchmidtkeMultimodalActivation2021; @SchmidtkeContextLogicVSA2021; @SchmidtkeAnalogicalSemantics2022] discussed the usage of VSA/HDC for the realization of context logic language and demonstrated the inference procedures.

### Computer vision and scene analysis {#sec:comp:vision}

This section summarizes different aspects of using HDC/VSA for processing visual data. This is one of the newest and least explored application areas of HDC/VSA.

#### Visual analogies

In [@YilmazAnalogy2015c], a simple analogy-making scenario was demonstrated on 2D images of natural objects (e.g., bird, horse, automobile, etc.). This work took an image representing a particular category, e.g., a bird. The HVs of images were obtained through using convolutional neural networks (Section [\[PartI-sec:2Dimages:neural:nets\]](#PartI-sec:2Dimages:neural:nets){reference-type="ref" reference="PartI-sec:2Dimages:neural:nets"} in [@KleykoSurveyVSA2021Part1]) and cellular automata computations (see [@YilmazSymbolic2015] for the method description). Several (e.g., $50$ in [@YilmazAnalogy2015c]) such binary HVs (e.g., for images of different birds) were superimposed together to form the HV of a category, e.g., $$\begin{equation}
\mathbf{land}=\mathbf{animal} \circ \mathbf{horse} + \mathbf{vehicle} \circ \mathbf{automobile};
\end{equation}$$ $$\begin{equation}
\mathbf{air}=\mathbf{animal} \circ \mathbf{bird} + \mathbf{vehicle} \circ \mathbf{airplane}.
\end{equation}$$ The category HVs were used to form some statements using the HDC/VSA operations. Inspired by the well-known example of "Dollar of Mexico?" (as in the techniques of Section [3.1.1.1](#sec:holistic:proc){reference-type="ref" reference="sec:holistic:proc"}) it was shown that one could perform queries of a similar form as "What is the Automobile of Air?" ($\mathbf{AoA}$), but using HVs formed from the 2D images: $$\begin{equation}
\mathbf{AoA}= \mathbf{air} \oslash  (\mathbf{land} \oslash \mathbf{automobile}) .
\end{equation}$$ The system demonstrated a high accuracy (98%) of correct analogy-making on previously unseen images of automobiles.

#### Reasoning on visual scenes and Visual Question Answering

Visual Question Answering is defined as a task where an agent should answer a question about a given visual scene. In [@MontoneVQANIPS], a trainable model was presented that used HDC/VSA for this task. The model in [@MontoneVQANIPS] differs from the state-of-the-art solutions that usually include a combination of a recurrent neural network (handles questions and provides answers) and a convolutional neural network (handles visual scenes). The model included two parts. The first part transformed a visual scene into an HV describing the scene using a neural network. This part used only one feed-forward neural network, which took a visual scene and returned its HV. The second part of the model defined the item memory of atomic HVs as well as HVs of questions along with their evaluation conditions in terms of cosine similarity thresholds. The neural network was trained to produce HVs associated with a dataset of simple visual scenes (two figures in various combinations of four possible shapes, colors, and positions). The gradient descent used errors from the question answering on the training data, which included five predefined questions. It was shown that the trained network successfully produced HVs that answered questions for new unseen visual scenes. The five considered questions were answered with 100% accuracy. On previously unseen questions the model demonstrated an accuracy in the range of 60-72%.

Similar to [@MontoneVQANIPS], there were attempts in [@MaudgalyaVisualVSA2020; @KentVSAScene2017; @herscheNVSA2022] to train neural networks to output HVs representing a structured description of scenes (see also Section [\[PartI-sec:2Dimages:neural:nets\]](#PartI-sec:2Dimages:neural:nets){reference-type="ref" reference="PartI-sec:2Dimages:neural:nets"} in [@KleykoSurveyVSA2021Part1]), which could then be used for computing visual analogies.

Another approach to Visual Question Answering with HDC/VSA was outlined in [@KovalevSemioticHD2020; @KovalevVectorSemiotic2021], where a visual scene was first preprocessed to identify objects and construct a scene data structure called the causal matrix (it stored some object attributes including positions). This data structure describing a scene was transformed into an HV that could then be queried using HDC/VSA operations similar to those from [@MontoneVQANIPS]. In [@KirilenkoVQA2021], it was applied to a dataset constructed to facilitate visual navigation in human-centered environments. This approach was further extended from Visual Question Answering to Visual Dialog in [@KovalevVisualDialog2021].

Another application of HDC/VSA for representation and reasoning on visual scenes, similar in its spirit to the Visual Question Answering, was presented in [@WeissOlshausenSpatial16]. The approach represented visual scenes in the form of HVs using Fourier HRR. The paper transformed continuous positions in an image to complex-valued HVs such that the spatial relation between positions was preserved in the HVs (see the "fractional power encoding" in Sections [\[PartI-sec:scalars:vectors:compos\]](#PartI-sec:scalars:vectors:compos){reference-type="ref" reference="PartI-sec:scalars:vectors:compos"} and [\[PartI-sec:2Dimages:role-filler\]](#PartI-sec:2Dimages:role-filler){reference-type="ref" reference="PartI-sec:2Dimages:role-filler"} in [@KleykoSurveyVSA2021Part1]). During the evaluation, handwritten digits and their positions were identified using a neural network with an attention mechanism. Then the identified information was used to create a complex-valued compositional HV describing the scene. Such scene HVs were used to answer relational queries like "which digit is below 2 and to the left of 1?".

A further exploration of this task was presented in [@LuFractional2019]. The approach in [@WeissOlshausenSpatial16] also demonstrated solving a simple navigation problem in a maze. In [@KomerNavigation2020], navigation tasks in 2D environments were further studied by using HVs as an input to a neural network producing outputs directing the movement. Neural networks trained with HVs demonstrated the best performance amongst methods considered. There was also a recent attempt to implement this continuous representation with spiking neurons [@DumontGrid2020].

## Cognitive architectures {#sec:cog:arch}

HDC/VSA have been used as an important component of several bio-inspired cognitive architectures. Here, we briefly describe these proposals.

### Semantic Pointer Architecture Unified Network {#sec:cog:spaun}

The most well-known example of a cognitive architecture using HDC/VSA is called "Spaun" for Semantic Pointer Architecture Unified Network (see its overview in [@EliasmithSPAUN2012] and a detailed description in [@EliasmithBuildBrain2013]). Spaun is a large spiking neural network ($2.5$ million neurons). Spaun that uses HRR for data representation and manipulation. In the architecture, HVs play the role of "semantic pointers" [@BlouwConcepts2016] aiming to integrate connectionist and symbolic approaches. It has an "arm" for drawing its outputs as well as "eyes" for sensing inputs in the form of 2D images (handwritten or typed characters). Spaun (without modifying the architecture) was demonstrated in eight different cognitive tasks that require different behaviors. The tasks used to demonstrate the capabilities of Spaun were: copy drawing of handwritten digits, handwritten digit recognition, reinforcement learning on a three-armed bandit task, serial working memory, counting of digits, question answering given a list of handwritten digits, rapid variable creation, and fluid syntactic or semantic reasoning. The same principles were used in [@CrawfordKnowledge2016] to represent the WordNet knowledge base with HVs allowing enriching, e.g., Spaun, with a memory storing some prior knowledge.

### Associative-Projective Neural Networks {#sec:cognitive:APNN}

The cognitive architecture of Associative-Projective Neural Networks (APNNs) that use the SBDR model was proposed and presented in [@KussulAPNN1991; @KussulSequences1991; @KussulAssociative1992; @KussulNNMM2010; @RachkovskijWorldModel2013]. The goal was to show how to construct a complex hierarchical model of the world, that presumably exists in human's and higher animals' brains, as a step towards Artificial Intelligence.

Two hierarchy types were considered: the compositional (part-whole) as well as the categorization or generalization (class-instance or is-a) ones. An example of the compositional hierarchy is: letters $\rightarrow$ words $\rightarrow$ sentences $\rightarrow$ paragraphs $\rightarrow$ text. Another example is the description of knowledge base episodes in terms of their elements of various complexity, from attributes to objects to relations to higher-order relations (see Section [3.1.2](#sec:analogical:reasoning){reference-type="ref" reference="sec:analogical:reasoning"}). An example of the categorization hierarchy is: dog $\rightarrow$ spaniel $\rightarrow$ spaniel Rover or apple $\rightarrow$ big red apple $\rightarrow$ this big red apple in hand.

The proposed world model relies on models of various modalities, including sensory ones (visual, acoustic, tactile, motoric, etc.) and more abstract modalities (linguistics, planning, reasoning, abstract thinking, etc.) that are organized in hierarchies. The models are required for objects of different nature, e.g., events, real objects, feelings, attributes, etc. Models (their representations) of various modalities can be combined, resulting in multi-modal representations of objects and associating them with the behavioral schemes (reactions to objects or situations), see details in [@KussulAPNN1991; @KussulSequences1991; @KussulAssociative1992; @KussulNNMM2010; @RachkovskijWorldModel2013].

The APNN architecture is based on HDC/VSA (though it was proposed long before the terms HDC and VSA appeared), in particular, models are represented by SBDR (Section [\[PartI-sec:SBDR\]](#PartI-sec:SBDR){reference-type="ref" reference="PartI-sec:SBDR"} in [@KleykoSurveyVSA2021Part1]). An approach to formation, storage, and modification of hierarchical models was proposed. This is facilitated by the capability to represent in HVs of fixed dimensionality (for items of various complexity and generality) various heterogeneous data types, e.g., numeric data, images, words, sequences, structures (Section [\[PartI-sec:data:to:HV\]](#PartI-sec:data:to:HV){reference-type="ref" reference="PartI-sec:data:to:HV"} in [@KleykoSurveyVSA2021Part1]). As usual for HDC/VSA, the model HVs can be constructed on-the-fly (without learning). APNNs have a multi-module, multi-level, and multi-modal design. A module forms, stores, and processes many HVs representing models of objects of a certain modality and of a certain level of compositional hierarchy. A module's HVs are constructed from HVs obtained from other modules, such as lower-level modules of the same modality, or from modules of other modalities. The lowest level of the compositional hierarchy consists of modules providing a representation grounding (atomic HVs).

For SBDR, a HV is similar to the HVs of its elements of a lower compositional hierarchy level, as well as to the HVs of the higher level, of which the HV is an element. So, using similarity search (in the item memory of levels) it is possible to recover both the lower-level element HVs and compositional HVs of the higher level.

Each module has a long-term memory where it stores its HVs. A Hopfield-like distributed auto-associative memory [@GritsenkoAMSurvey2017; @FrolovWillshaw2002; @FrolovTime2006] was suggested as a module memory. It performs the clean-up procedure for noisy or partial HVs by a similarity search. However, its unique property is the formation of the second main hierarchy type, i.e., of generalization (class-instance) hierarchies. It is formed when many similar (correlated) HVs are stored (memorized), based on the idea of Hebb's cell assemblies including cores (subsets of HV 1-components often occurring together, corresponding to, e.g., typical features of categories and object-prototypes) and fringes (features of specific objects), see [@RachkovskijWorldModel2013; @GritsenkoAMSurvey2017].

It is acknowledged that a world model comprising domain(s) specific knowledge as well as information about an agent itself is necessary for any intelligent behavior. Such a model allows comprehension of the world by an intelligent agent and assists it in its interactions with the environment, e.g., through predictions of action outcomes.

The main problem with the APNN architecture is that not all its aspects have been modeled. For example, there are the following questions, which do not have exact answers:

- how to extract objects and their parts of various hierarchical levels?

- how to determine the hierarchical level an object belongs to?

- how to work with an object that may belong to different hierarchy levels and modules?

- how to represent objects invariant of their transformations?

Also, modeling cores and fringes formation in distributed auto-associative memory is still fragmentary as of now. Finally, it worth noting that similar ideas are currently being developed in the context of deep neural networks [@GhaziRecursive2019; @DengHierarchical2021].

### Hierarchical Temporal Memory {#sec:cognitive:HTM}

An interesting connection between HDC/VSA and a well-known architecture called Hierarchical Temporal Memory (HTM) [@HTM2011] was presented in [@PadillaVSAHTM2014]. The work showed how HTM can be trained in its usual sequential manner to support basic HDC/VSA operations: binding and superposition of sparse HVs, which are natively used by HTM. Even though permutations were not discussed, it is likely that they also could be implemented, so that HTM could be seen as another HDC/VSA model, which additionally has a learning engine in its core.

### Learning Intelligent Distribution Agent {#sec:cognitive:LIDA}

A version of the well-known symbolic cognitive architecture LIDA (Learning Intelligent Distribution Agent  [@FranklinLIDA2013]), working with HVs, was presented in [@SnaiderVLIDA2014]. In particular, the Modular Composite Representations model was used [@SnaiderModular2014]. Moreover, memory mechanisms in the proposed architecture were also related to HDC/VSA: an extension of Sparse Distributed Memory [@KanervaSDM1988], known as Integer Sparse Distributed Memory [@SnaiderIntegerSDM2013], was used. The usage of HVs allowed resolving some of the issues with the original model [@FranklinLIDA2013], which relies on directed graph-like structures such as representation capability, flexibility, and scalability.

### Memories for cognitive architectures {#sec:cognitive:memory}

Memory is one of the key components of any cognitive architecture and for modeling cognitive abilities of humans. There is a plethora of memory models. For example, MINERVA 2 [@HintzmanMinerva1984] is an influential computational model of long-term memory. However, in its original formulation MINERVA 2 was not very suitable for an implementation in connectionist systems. Nevertheless, it was demonstrated in [@KellyTesseract2017] that the Tensor Product Representations model (Section [\[PartI-sec:framework:TPR\]](#PartI-sec:framework:TPR){reference-type="ref" reference="PartI-sec:framework:TPR"} in [@KleykoSurveyVSA2021Part1]) can be used to formalize MINERVA 2 as a fixed size tensor of order four. Moreover, it was demonstrated that the lateral inhibition mechanism for HDC/VSA [@GaylerIsomorphism2009] and HRR can be used to approximate MINERVA 2 with HVs. HVs allowed to compress the exact formulation of the model, which relies on tensors, into several HVs, thus making the model more computationally tractable at the cost of lossy representation in HVs.

Another example of using HVs (with HRR) for representing concepts is a Holographic Declarative Memory [@KellyDeclarativeMemory2015; @AroraMind2018; @KellyDeclarativeMemory2020] related to BEAGLE [@JonesMeaning2007] (see Section [2.2.1.2](#sec:BEAGLE){reference-type="ref" reference="sec:BEAGLE"}). It was proposed as a declarative and procedural memory in cognitive architectures. It was shown that the memory can account for many effects such as primacy, recency, probability estimation, interference between memories, and others.

In [@JohnsGenerating2015; @JamiesonITS2018] BEAGLE (Section [2.2.1.2](#sec:BEAGLE){reference-type="ref" reference="sec:BEAGLE"}) was extended to store (instead of one context HV per word) episodic memory of the observed data as HVs of all the contexts. This extension was called the instance theory of semantics. Each word was represented by an atomic random HV. A word's context (a sentence) HV is constructed as a superposition of its word HVs and is stored in the memory. The HV of some query word is constructed as follows. First, the $\text{sim}_{\text{cos}}$ of the query word HV and each context HV is calculated and raised to a power, producing a vector of "trace activations". Then, context HVs are weighted by traces and summed to produce the retrieved ("semantic") HV of the query word.

The study in [@CrumpRetrieval2020] introduced a "weighted expectancy subtraction" mechanism that formed actual context HV as follows. First, the context HV produced the retrieved HV as explained above. Then, the retrieved HV was weighted and subtracted from the initial context HV. During the retrieval, the weighted HV of the second retrieval iteration was subtracted from the HV of the first retrieval iteration. This allowed flexibly controlling the construction of general versus specific semantic knowledge. The work in [@OrorbiaKellyCogNGen2022] proposed CogNGen - a core of a cognitive architecture that combines predictive processing based on neural generative coding and HDC/VSA models of human memory. The CogNGen architecture learns across diverse tasks and models human performance at larger scales.

# Discussion {#sec:disc}

## Application areas {#application-areas}

### Context HVs

When it comes to context HVs, Random Indexing and Bound Encoding of the Aggregate Language Environment have appeared as improvements to Latent Semantic Analysis -- e.g., they do not require Singular Value Decomposition and can naturally take order information into account. However, they were largely overshadowed after the introduction of "neural" word embeddings in Word2vec [@MikolovWord2vec2013] or GloVe [@pennington2014glove]. The latter are the result of an iterative process, which takes numerous passes via training data to converge. At the same time, an important fact is that distributional models such as Latent Semantic Analysis can in fact benefit from some techniques used in neural word embeddings [@levy2015improving]. Concerning, e.g., Bound Encoding of the Aggregate Language Environment, as recently demonstrated in [@JohnsNegative2019], the method can benefit from negative information. Nevertheless, the current de facto situation in the natural language processing community is that Bound Encoding of the Aggregate Language Environment and Random Indexing methods are rarely the first candidates when it comes to choosing word embeddings. On the other hand, since Bound Encoding of the Aggregate Language Environment has been proposed within cognitive science community, it still plays an important role in modeling cognitive phenomena related to memory and semantics [@JamiesonITS2018; @CrumpRetrieval2020]. Also, in contrast to the iterative methods, Random Indexing and Bound Encoding of the Aggregate Language Environment only require a single pass through the training data to form context HVs. In some situations, this could be an advantage, especially since the natural language processing community is becoming increasingly concerned about the computational costs of algorithms [@StrubellEnergyNLP].

### Classification {#classification}

While right now the classification with HDC/VSA is flourishing, there are still important aspects that are often not taken into account in these studies.

#### Formation of HVs

An important aspect of the formation of HVs is the initial extraction of features from raw data such as 2D images or acoustic signals. Usually, directly transforming such data into HVs does not result in a good performance, so an additional step of extracting meaningful features is required.

Another important aspect is that, when constructing HVs from feature data for classification, in most cases the transformation of data into HVs is somewhat ad hoc. While there likely will not be a straightforward answer to how transforming data into HVs, it is still important to mention several issues.

It is a well-known fact that the advantage of nonlinear transformation is that classes not linearly separable in the original representation, might become linearly separable after a proper nonlinear transformation to a high-dimensional space (often called lift). This allows using not only $k$-Nearest Neighbor classifiers, but also well-developed linear classifiers to solve problems that are not linearly separable. So, nonlinearity is an important aspect of transforming data into HVs. All transformations of data into HVs that we are aware of seem to be nonlinear. However, there are no studies that scrutinize and characterize the nonlinearity properties of HVs obtained from the compositional approach. Moreover, most of the studies choose a particular transformation of numeric vector and stick to it. One of the most common choices is randomized "float" coding [@RachkovskijScalars2005; @WiddowsContinuous2015; @KleykoTradeoffs2018; @RahimiBiosignal2019]. There is, however, a recent study [@FradyFunctions2021; @FradyFunctionsNICE2022] that established a promising connection between kernel methods [@ScholkopfKernels2002; @rahimi2007random] and the fractional power encoding for representing numeric vectors as well as an earlier algorithm for approximating a particular type of kernels (tree kernels) with the HRR model [@ZanzottoTree2012]. In our opinion, the transformation of data into HVs is a hyperparameter of the model and using, e.g., cross-validation to choose the most promising transformation will likely be the best strategy when considering a range of different datasets.

#### Choice of classifier {#sec:disc:class:classifier}

As we saw in Section [2.3](#sec:app:class){reference-type="ref" reference="sec:app:class"}, centroids are probably the most common approach to forming a classifier in HDC/VSA. This is understandable, since centroids have an important advantage in terms of computational costs -- they are very easy to compute. However, as pointed out in [@KarlgrenSemantics2021], the result of superposition does not provide generalization in itself, it is just a representation of combinations of HVs of training samples. Practically, it means that the centroids are not the best performing approach when it comes to classification performance. One way to improve the performance is to assign weights when including new samples into centroids [@RahimiBiosignal2016; @HernandezOnlineHD2021]. It was also shown that the perceptron learning rule in [@ImaniVoiceHD2017] and loss-based objective in [@HerscheContinualLearn2022] might significantly improve centroids. Earlier work on HV-based classifiers, e.g., [@KussulAdaptive1993; @KussulThreshold1994; @RachkovskijDatagen1998; @RachkovskijClassifiers2007] also used linear perceptron and Support Vector Machine classifiers with encouraging results. Note that a large-margin perceptron usually trains much faster than a Support Vector Machine for big data, while providing classification quality at the same level as the Support Vector Machine and usually much higher than that of the standard perceptron. Another recent result [@DiaoGLVQHD2021] is that centroids can be easily combined with a known conventional classifier -- generalized learning vector quantization [@SatoGLVQ1995]. Using an HDC/VSA transformation of data to HV, the authors obtained state-of-the-art classification results on a benchmark [@HundredsClassifiers2014]. In general, we believe that when inventing new mechanisms of classification with HDC/VSA, it is important to report the results on collections of datasets instead of only a handful of datasets. For example, for feature-based classification the UCI Machine Learning Repository [@DuaUCI2019] and subsets thereof (e.g., [@HundredsClassifiers2014]) are a common choice (examples of HDC/VSA using it are [@KleykoDensityEncoding2020; @DiaoGLVQHD2021; @FradySDR2020]). For univariate temporal signals, the UCR Time Series Archive [@DauUCRSeries2019] is a good option that was used, e.g., in [@SchlegelHDC-MiniROCKET2022]. If a reported mechanism targets a more specific application area, it would be desirable to evaluate it on a relevant collection for that area.

Many other types of classifiers such as $k$-Nearest Neighbors [@KarunaratneHDAugmented2021] are also likely to work with HVs as their input. When HVs are generated by a nonlinear transformation (in [@KarunaratneHDAugmented2021], using an HDC/VSA-guided convolutional neural network feature extractor), the $k$-Nearest Neighbors classifier forms an "explicit memory" in memory-augmented neural networks or Neural Turing Machines [@NTMGraves2014]. The contents of the explicit memory can be compressed using outer products with randomized labels [@KleykoAugmented2022]. As mentioned in the previous section, linear classifiers can be used with nonlinearly transformed HVs. For example, the ridge regression, which is commonly used for randomized neural networks [@RCNNSsurvey], performed well with HVs [@RosatoHDDistributed2021]. However, not all conventional classifiers work well with HVs [@AlonsoHyperEmbed2020]. That is because some of the algorithms (e.g., decision tree or Naïve Bayes) assume that any component of a vector can be interpreted on its own. It is a reasonable assumption when components of vectors are meaningful features, but in HVs a component does not usually have a meaningful interpretation. In the case of HDC/VSA with sparse representations, special attention should be given to classifiers that benefit from sparsity. Examples of such classifiers are the sparse Support Vector Machine [@EshghiSparseSVM2016] and winnow algorithm [@LittlestoneWinnow1988].

#### Applications in machine learning beyond classification

There are also efforts to apply HDC/VSA within machine learning outside of classification. Examples of such efforts are using data transformed into HVs for clustering [@BandaragodaTrajectoryTraffic2019; @HernandezClustering2021], unsupervised learning[@MirusAbnormal2020; @OsipovHyperSeed2021], multi-task learning [@ChangHDTaskProjected2020; @ChangHDInformationPreserved2020; @ChangMulTaHDC2021], distributed learning [@RosatoHDDistributed2021; @HsiehFL2021], model compression [@HerscheCompressingBCI2020; @ChangMulTaHDC2021; @RosatoHDDistributed2021; @RosatoCompression2021], and ensemble learning [@EnsembleBurrello2021; @EnHDCWang2022].

It is expected that fractional power encoding [@PlateNested1994; @KomerContinuous2019; @FradyFunctions2021] (Section [\[PartI-sec:scalars:vectors:compos\]](#PartI-sec:scalars:vectors:compos){reference-type="ref" reference="PartI-sec:scalars:vectors:compos"} in [@KleykoSurveyVSA2021Part1]) is going to be a particularly fruitful method for enabling new applications beyond classification. This expectation is based on two facts. First, fractional power encoding is known to approximate kernels, which allows for an efficient implementation of kernel methods. There are already examples of its use to implement methods for probability density estimation [@FradyFunctions2021; @FradyFunctionsNICE2022], kernel regression [@FradyFunctions2021; @FradyFunctionsNICE2022], Gaussian processes-based mutual information exploration [@FurlongSSPMI2022], representing probability statements [@FurlongProbability2022], path integration [@DumontPath2022], and reinforcement learning [@BartlettReinforcement2022]. Second, fractional power encoding provides a simple but powerful way for representing numeric data in HVs, which allows numerous applications relying on such data. Some recent examples include simulation and prediction of dynamical systems [@VoelkerFPEDynamical2021], reasoning on 2D images [@WeissOlshausenSpatial16; @FradyDisentangling2018; @LuFractional2019], navigation in 2D environments [@WeissOlshausenSpatial16; @KomerNavigation2020], representation of time series [@SchlegelHDC-MiniROCKET2022], and even modeling in neuroscience [@FradyFramework2018; @DumontGrid2020].

We did not devote separate sections to these efforts as the studies are still scarce, but the interested readers are kindly referred to the above works for the initial investigations on these topics.

### Real-world use-cases and new application areas

Section [2](#sec:applications){reference-type="ref" reference="sec:applications"} demonstrated that there have been numerous attempts to apply HDC/VSA in a diverse range of scenarios spanning from communications to analogical reasoning. As we can see from Section [2.3](#sec:app:class){reference-type="ref" reference="sec:app:class"}, the most recent uptick in the research activity was applying HDC/VSA to classification tasks. In the near future, we are likely to see them being applied to solving classification tasks in new domains. Examples of such new domains recently appeared in, e.g., [@VougioukasBranch2021], where HDC/VSA was applied to branch prediction in processor cores and [@ShahroodiDemeter2022], where HDC/VSA was applied to food profiling.

Concerning the applications of analogical reasoning (Section [3.1.2](#sec:analogical:reasoning){reference-type="ref" reference="sec:analogical:reasoning"}), the major bottleneck is still the transformation of textual, speech or pictorial descriptions of analogical episodes to directed ordered acyclic graphs that can then be transformed into HVs (Section [\[PartI-sect:graphs:labelled\]](#PartI-sect:graphs:labelled){reference-type="ref" reference="PartI-sect:graphs:labelled"} in [@KleykoSurveyVSA2021Part1]). Note that this problem concerns not only analogical reasoning based on HDC/VSA, but all methods that use predicate-based descriptions as inputs.

Nevertheless, there is still a considerable way to go in order to demonstrate how HDC/VSA-based solutions scale up to real-world problems. We, however, strongly believe that, similarly to the modern reincarnation of connectionist models, eventually research will distill the niches where the advantages of HDC/VSA are self-evident. Currently, one promising niche seems to be the time series classification [@SchlegelHDC-MiniROCKET2022], particularly in-sensor classification of biomedical signals [@MoinWearable2021] and prosthetic grasping [@OlascoagaProsthetic2022]. Furthermore, the exploration of HDC/VSA in novel application domains should be continued. For instance, there were recent applications in communication [@KimHDM2018; @HsuCollisionTolerant2019; @HsuNonOrthogonalModulation2020; @HerscheHDMClassifier2021] and in distributed systems [@SimpkinHDWorkflow2019] (see Section [2.1.2.1](#sec:stor:trans:comm){reference-type="ref" reference="sec:stor:trans:comm"}), which were not foreseen by the community. Another recent example is the attempt to apply HDC/VSA to robotics problems [@MendesRobotSDM2008; @MendesRobotSDM2012; @TravnikRepresenting2017; @NeubertRobotics2019; @MitrokhinSensorimotor2019; @McDonaldHDRobotics2021; @NeubertAggregation2021].

## Interplay with neural networks

### HVs as input to neural networks

One of the most obvious ways to make an interplay between HDC/VSA and neural networks is by using HVs to represent input to neural networks. This is a rather natural combination of the two, because, in essence, neural networks often work with distributed representations. So, processing information distributed in HVs is not an issue for neural networks. However, since HVs are high-dimensional, it is not always possible to use them as the input: the size of neural networks' input layer should be set to $D$ (e.g., in [@MaHolisticMemoriz2018], whereby a fully-connected "readout" layer for a task of choice was trained on $D$-dimensional input HVs) or even to a tensor comprised of HVs (e.g., to represent each position in the retina by its HV, without superposition of HVs). Moreover, the local structure of the input signal space may become different from that used, e.g., in convolutional neural networks. This could require very different neural network architectures compared to modern deep neural networks.

There are, nevertheless, scenarios where using HVs with neural networks appeared to be beneficial. First, HVs are useful in situations when the data to be fed to a neural network are high-dimensional and sparse. Then HVs can be used to form more compact distributed representations of these data. A typical example of using such high-dimensional and sparse data is $n$-gram statistics. There are works which studied tradeoffs between the dimensionality of HVs representing $n$-gram statistics (see Section [\[PartI-sec:sequences:ngrams\]](#PartI-sec:sequences:ngrams){reference-type="ref" reference="PartI-sec:sequences:ngrams"} in [@KleykoSurveyVSA2021Part1]) and the performance of neural networks using these HVs as their input [@KleykoBoostingSOM2019; @AlonsoHyperEmbed2020]. These works demonstrated that it is possible to achieve the same or very similar classification performance with networks of much smaller size. Moreover, the degradation of the classification performance is gradual with the decreasing size of HVs, so their dimensionality can be used to control the tradeoff between the size of the network and its performance. On top of creating more compact representations, an additional advantage of HVs might lie in making HVs binary as in, e.g., Binary Spatter Codes. This might be leveraged in situations where the whole model is binarized [@ShridharEnd2End2020].

Also, HVs may be useful when the size of input is not fixed but instead could vary for different inputs. Since neural networks are not flexible in changing their architecture, HDC/VSA can be used to take care of forming fixed size HVs for input of variable size. This mode of a neural network interface has been demonstrated in an automotive context to represent either varying number of intersections being crossed by a vehicle [@BandaragodaTrajectoryTraffic2019] or the dynamically changing environment around a vehicle [@MirusBalanced2020; @MirusBehavior2019]. Further promising avenues for this mode are graphs and natural language processing, since there is a lot of structure in both, which can potentially be represented in HVs [@MaHolisticMemoriz2018; @KarlgrenUtterances2019]. Some investigations in this direction using Tensor Product Representations were presented in [@ChenTPRMapping2020].

We foresee that this interface mode might expand the applicability of neural networks, as it allows relieving the pressure of forming the task either with fixed size input or in the form of, e.g., a sequence suitable for recurrent neural networks. However, it may require a replacement of the widely used convolutional layers. Although, there are new results [@TolstikhinMLP2021] suggesting that fully connected neural networks might be a good architecture even for vision tasks.

### The use of neural networks for producing HVs {#disc:ANNs:produce}

Transforming data into HVs (see Section [\[PartI-sec:data:to:HV\]](#PartI-sec:data:to:HV){reference-type="ref" reference="PartI-sec:data:to:HV"} in [@KleykoSurveyVSA2021Part1]) might be a non-trivial task, especially when data are unstructured and of non-symbolic nature as, e.g., in the case of images (Sections [\[PartI-sec:2D:permute\]](#PartI-sec:2D:permute){reference-type="ref" reference="PartI-sec:2D:permute"} and [\[PartI-sec:2Dimages:role-filler\]](#PartI-sec:2Dimages:role-filler){reference-type="ref" reference="PartI-sec:2Dimages:role-filler"} in [@KleykoSurveyVSA2021Part1]). Also, those transformations are usually not learned. This challenge stimulates the interface between neural networks and HDC/VSA in the other direction, i.e., to transform activations of neural network layer(s) into HVs. For example, as mentioned in Section [\[PartI-sec:2Dimages:neural:nets\]](#PartI-sec:2Dimages:neural:nets){reference-type="ref" reference="PartI-sec:2Dimages:neural:nets"} in [@KleykoSurveyVSA2021Part1], it is very common to use activations of convolutional neural networks to form HVs of images. This is commonly done using standard pre-trained neural networks [@YilmazMachine2015; @MitrokhinCNN2020; @NeubertAggregation2021]. Two challenges here are to increase the dimensionality and change the format of the neural network representations to conform with the HV format requirements. The former one is generally addressed by expanding the dimensionality, e.g., by random projection, possibly with a subsequent binarization by thresholding [@HerscheBinarization2020; @NeubertAggregation2021]. Some neural networks already produce binary vectors (see [@MitrokhinCNN2020]), and the transformation into HVs was done by randomly repeating these binary vectors to get the necessary dimensionality. To address the latter one, in [@KarunaratneHDAugmented2021; @KleykoAugmented2022; @HerscheContinualLearn2022; @herscheNVSA2022], the authors guided a convolutional neural network to produce HDC/VSA-conforming vectors with the aid of proper attention, sharpening, and loss functions. The sign of produced HVs can be used to transform them into bipolar HVs (of the same dimensionality). These approaches train neural networks from scratch (as with meta-learning in [@KarunaratneHDAugmented2021; @KleykoAugmented2022; @HerscheContinualLearn2022], or additive loss in [@herscheNVSA2022]) such that the activations of the network resemble quasi-orthogonal HVs for, e.g., images of unrelated classes. In [@MitrokhinCNN2020; @NeubertAggregation2021], the authors superimposed HVs obtained from several neural networks, which improved the results in applications. Yet another promising avenue is make the processes of classification and reconstruction (i.e., generation) of raw sensory data simultaneously from each neural network. One particular realization of this idea, called "bridge networks", was recently presented in [@OlinBridgeNetworks2021]. Finally, it is worth mentioning that a neural network does not necessarily need to produce HVs, but it can benefit from the HDC/VSA operations by improving its retrieval performance through superimposing multiple permuted versions of an output vector, as demonstrated in [@DanihelkaAssociative2016].

### HDC/VSA for simplifying neural networks

In [@AndersonHDDNN], it was shown that it is possible to treat the functionality of binarized neural networks with the ideas from high-dimensional geometry. The paper has demonstrated that binarized networks work because of the properties of binary high-dimensional spaces, i.e., the properties used in Binary Spatter Codes [@KanervaHyperdimensional2009]. While it is an interesting qualitative result, it did not provide a concrete way to make the two benefit from each other. This is not obvious, since in the standard neural networks all weights are trained via backpropagation, which is rather different from the HDC/VSA principles.

There is, however, a family of randomized neural networks [@RCNNSsurvey] where a part of the network is initialized randomly and stays fixed. There are two versions of such networks: feed-forward (e.g., random vector functional link networks [@IgelnikRVFL1995] aka extreme learning machines [@HuangELM2006]) and recurrent (e.g., echo state networks [@ESN02] aka reservoir computing[@LukoseviciusRC2009]). The way the randomness is used in these networks can be expressed in terms of HDC/VSA operations for the both feed-forward [@KleykoDensityEncoding2020] and recurrent [@KleykointESN2020] versions. Conventionally, randomized neural networks were used with real-valued representations. However, since it was realized that these networks can be interpreted in terms of HDC/VSA, it appeared natural to use binary/integer variants (as in Binary Spatter Codes and Multiply-Add-Permute) to produce activations of hidden layers of the networks. This opened the avenue for efficient hardware implementations of such randomized neural networks. Yet another connection between HDC/VSA and feed-forward randomized neural networks was demonstrated in [@PlateAssociative2000] where it was shown that HRR's binding operation can be approximated by such networks. Finally, in [@BrickenAttentionSDM2021] it was shown that the address mechanism from the Sparse Distributed Memory [@KanervaSDM1988] approximates the attention mechanism [@VaswaniAttention2017] used in modern neural networks.

### HDC/VSA for explaining neural networks

It was discussed in Section [\[PartI-sec:capacity\]](#PartI-sec:capacity){reference-type="ref" reference="PartI-sec:capacity"} in [@KleykoSurveyVSA2021Part1] that the capacity theory [@FradyCapacity2018] applies to different HDC/VSA models. As mentioned in the previous section, randomized recurrent neural networks, known as reservoir computing/echo state networks, can be formulated using HDC/VSA. Therefore, capacity theory can also be used to explain memory characteristics of reservoir computing. Moreover, using the abstract idea of dissecting a network into mapping and classifier parts [@papyan2020prevalence], it is possible to apply capacity theory for predicting the accuracy of other types of neural networks (such as deep convolutional neural networks) [@KleykoPerceptron2020].

In [@McCoyRNNsTensor2019], it was shown that Tensor Product Representations approximate representations of structures learned by recurrent neural networks.

### The use of HDC/VSA with spiking neural networks

Another direction of the interplay between HDC/VSA and neural networks is their usage in the context of spiking neural networks (SNN). It is especially important in the context of emerging neuromorphic platforms [@TrueNorth14; @DaviesLOIHI2018]. The main advantage HDC/VSA can bring into the SNN domain is the ease of transformation to spiking activities, either with rate-based coding for HDC/VSA models with scalar components or phase-to-timing coding for HDC/VSA models with phasor components. The Spaun cognitive architecture overviewed in Section [3.2.1](#sec:cog:spaun){reference-type="ref" reference="sec:cog:spaun"} is one of the first examples where the HRR model was used in the context of SNN. The latest developments [@FradyTPAM2019; @FradyKNN2020] use FHRR and HRR to implement associative memory and $k$-Nearest Neighbor classifiers on SNN. Further, these memories were proposed as building blocks for the realization of a holistic HDC/VSA-based unsupervised learning pipeline on SNN [@OsipovHyperSeed2021]. While in [@BentSpike2022] the Sparse Block Codes model was mapped to an SNN circuit. In other related efforts, an event-based dynamic vision sensor [@MitrokhinSensorimotor2019; @HerscheDVSCDT2020] or an SNN [@ZouMemorySpiking2022; @MorrisHyperSpike2022] was used to perform the initial processing of the input signals that were then transformed to HVs to form the prediction model.

These works provide some initial evidence of the expressiveness of HDC/VSA on the one hand, and compatibility with SNNs on the other. We therefore foresee that using HDC/VSA as a programming/design abstraction for various cognitive functionalities will soon manifest itself in the emergence of novel SNN-based applications.

### "Hybrid" solutions {#sec:disc:nn:hybrid}

By "hybrid" in this context we refer to solutions that use both neural networks and some elements of HDC/VSA. Currently, a particularly common primitive used in such hybrid solutions is the representation of a set of role-filler bindings, or superposition of multiplicative bindings. For example, in [@CheungSuperposition2019] the weights of several neural networks were stored jointly by using the superposition operation, which alleviated the problem of "catastrophic forgetting". In [@WilsonOOD2021], activations of layers of a deep neural network were used as filler HVs. They were bound to the corresponding random role HVs and all role-filler bindings were aggregated in a single superposition HV that in turn was used to successfully detect out-of-distribution data. Similarly, in [@NeubertAggregation2021; @NeubertPlaceRecognition2021; @SutorGluing2022], activations of several neural networks were combined together via HDC/VSA operations. In [@NeubertAggregation2021; @NeubertPlaceRecognition2021], this idea was used to form a single HV compactly representing the aggregated neural networks-based image descriptor while in [@SutorGluing2022] outputs of multiple neural networks were fused together to solve classification problems. In [@GanesanLearning2021], the superposition of role-filler bindings was used to simultaneously represent the output of a deep neural network when solving multi-label classification tasks. In [@herscheNVSA2022], the activations of the last layer generate a query HV that resembles the superposition of the visual objects available in a panel, whereby each object is uniquely represented by multiplicative binding of its attributes' HVs. In addition, a review of hybrid solutions combining Tensor Product Representations and neural networks such as Tensor Product Generation Networks [@HuangTensorNetworks2018] can be found in [@SmolenskyNeurocompositional2022]. Finally, it is worth noting that all these solutions in some way relied on the idea of "computing in superposition" [@KleykoComputingParadigm2021] suggesting that HVs can be used to simultaneously manipulate several pieces of information.

## Open issues

As introduced at the beginning of this survey, HDC/VSA originated from proposals of integrating the advantages of the symbolic approach to Artificial Intelligence, such as compositionality and systematicity, and those of the neural networks approach to Artificial Intelligence (connectionism), such as vector-based representations, learning, and grounding. There is also the "neural-symbolic computing"  [@Neural-symbolic_Book_2002; @Garcez2019NSC2019], or "neurosymbolic AI" [@GarcezNeurosymbolic2020] community that suggests hybrid approaches to Artificial Intelligence. The key idea is to form an interface so that symbolic and connectionist approaches can work together. At present, HDC/VSA and neural-symbolic computing seem to be rather separate fields that can benefit from synergy. Moreover, the works developing cognitive architectures and Artificial Intelligence with HDC/VSA are rather limited [@EliasmithSPAUN2012; @RachkovskijWorldModel2013; @EliasmithBuildBrain2013].

So far, a major advantage of the HDC/VSA models has been their ability to use HVs in a single unified format to represent data of varied types and modalities. Moreover, the use of HDC/VSA operations allows introducing compositionality into representations. The prerequisite, however, is that the representation of the input data to be transformed into HVs should be able to specify the compositionality explicitly. Nevertheless, despite this advantage, HDC/VSA is usually overlooked in the context of neural-symbolic computing, which calls for establishing a closer interaction between these two communities.

In fact, most of the works use HDC/VSA to reimplement the symbolic Artificial Intelligence primitives or the machine learning functionality with HVs, in a manner suitable for emerging unconventional computing hardware. At the same time, when transforming data into HVs, machine learning is used rarely, if at all. Learning is used mainly for training a classifier based on the already constructed HVs. In most recent studies, the narrative is often to demonstrate solutions to some simple classification or similarity search problems. In so, the quality of the results is comparable to the state-of-the-art solutions, but the energy/computational costs required by an HDC/VSA-based solution are only a fraction of those of the baseline approaches. These developments suggest that HDC/VSA might find one of their niches in application areas known as "tiny machine learning" and "edge machine learning". Nevertheless, there is an understanding that manually designing transformations of data into HVs for certain modalities (e.g., 2D images) is a challenging task. This stimulates attempts that use modern learning-based approaches, such as deep neural networks, for producing HVs (see Section [4.2.2](#disc:ANNs:produce){reference-type="ref" reference="disc:ANNs:produce"}). The current attempts, however, are rather limited since they focus heavily on using HVs formed from the representations produced by neural networks to solve some downstream machine learning tasks (e.g., similarity search or classification).

Another approach would be to discover general principles for combining neural networks and HDC/VSA, but currently there are only few such efforts [@CheungSuperposition2019; @ChenTPRMapping2020; @HerscheCompressingBCI2020; @OlinBridgeNetworks2021; @ZemanCompressed2021; @herscheNVSA2022] (see also Section [4.2.6](#sec:disc:nn:hybrid){reference-type="ref" reference="sec:disc:nn:hybrid"}). For example, the Tensor Product Representations operations in [@ChenTPRMapping2020], and the HDC/VSA operations in [@herscheNVSA2022], are introduced into the neural network machinery. These attempts are timely for the connectionist approach to Artificial Intelligence since, despite recent uptick of deep neural networks [@LeCunDL2015], there is a growing awareness within the connectionist community that reaching Artificial General Intelligence is going to require a much higher level of generalization than that available in modern neural networks [@MarcusAI2020; @GreffBinding2020; @GoyalDLCognition2020; @HintonPartwhole2021]. A recent proposal in [@SmolenskyNeurocompositional2022] could be considered more HDC/VSA-oriented.

One of the milestones toward reaching Artificial General Intelligence is solving the problem of compositionality. For example, [@GreffBinding2020] stressed the importance of various aspects of binding for achieving compositionality and generalization. The framework of HDC/VSA has a dedicated on-the-fly operation for binding (Section [\[PartI-sec:binding\]](#PartI-sec:binding){reference-type="ref" reference="PartI-sec:binding"} in [@KleykoSurveyVSA2021Part1]), which does not require any training. The neural implementation of binding in the context of SNNs is still an open issue. There are, however, two recent proposals [@RennerBinding2022; @BentSpike2022] aiming at addressing this issue.

Further, [@GoyalDLCognition2020] suggested that various inductive biases are required for human-level generalization, including compositionality and discovery of causal dependencies. HDC/VSA have a potential to achieve this through analogical reasoning (Section [3.1.2](#sec:analogical:reasoning){reference-type="ref" reference="sec:analogical:reasoning"}). However, the progress is held back by the lack of mechanisms to build the analogical episodes, e.g., by observing the outer world or by just reading texts. The analogical episodes should include two major types of hierarchy, the compositional ("part-whole") one, and the generalization ("is-a") hierarchy. We believe that associative memories [@GritsenkoAMSurvey2017] may provide one way to form "is-a" hierarchies (see the discussion in [@RachkovskijWorldModel2013]), but this topic has not yet been studied extensively in the context of HDC/VSA. In terms of forming part-whole hierarchies from 2D images, a recent conceptual proposal was given in [@HintonPartwhole2021]. The essence of the proposal is learning to parse 2D images by training a neural network and using the similarity of the learnt high dimensional vector representations for analogical reasoning. An interesting direction for future work is to see how such representations can be paired with the analogical reasoning capabilities of HDC/VSA. However, all the above and other proposals from the connectionist community rely on learning all the functional lacking in neural networks. But there are also discussions on the innate structure of neural networks [@MarcusAI2020]. For the sake of fairness, it should be noted that current HDC/VSA also lack ready implementations for most of the mentioned above functionality. There are a lot of open problems that should be addressed to build truly intelligent systems. Below we list some of them. Some problems related to the internals of HDC/VSA are the following:

- Recovery. Recovering element HVs from compositional HVs. Section [\[PartI-sec:recovery\]](#PartI-sec:recovery){reference-type="ref" reference="PartI-sec:recovery"} in [@KleykoSurveyVSA2021Part1] presented some ideas for recovering the content of HVs. However, for most of the HDC/VSA models, knowledge of all but one bound HV is required. This makes the recovery problem combinatorial (but see a proposal in [@FradyResonator2020; @KentResonatorNetworks2020]).

- Similarity. In many of the HDC/VSA models, the results of the binding operation are not similar as soon as a single input HV is dissimilar. While it is often convenient, or even desired, that the result of the binding operation is dissimilar to its input HVs, the price to be paid is weak or no similarity in the resultant HV that includes the same HVs in slightly different combinations. This might hinder the similarity search that is at the heart of HDC/VSA and required in many operations such as the recovery or clean-up procedures.

- Memory. Quantity and allocation of item memories. How many item memories and which HVs of all available HVs should be placed in each of the? Types of the item memories to be used? List memories provide reliable retrieval, but are problematic for generalization. Distributed auto-associative memories have problems with dense HVs (but see [@RamsauerHopfield2020]).

- Generalization of HVs. How to form generalized HVs containing typical features of objects and, is-a hierarchies of objects, but preserve HVs of specific objects as well? Distributed auto-associative memories have potential for generalization by unsupervised learning [@GritsenkoAMSurvey2017].

- Generativity. Is it possible to make a new meaningful compositional HV without constructing it from atomic HVs? Is it possible to produce meaningful input (e.g., fake image or sound, as in deep neural networks) from some generated generalized or specific compositional HV?

- Similarity-based associations. How to select the particular association needed in the context from myriads of possible associations? For example, between HVs of a part and of a whole, or between HVs of a class and of a class instance.

- Parsing. How to parse the raw sensory inputs into a meaningful part-whole hierarchy of objects?

- Holistic representation. HVs are holistic representations. However, for the comparison of objects we may need to operate with their parts. Part HVs can be found given a particular holistic HV. Is it possible to form holistic HVs of very different inputs of the same class so that they are similar enough to be found in memory?

- Dynamics. Representation, storage, similarity-based search, and replaying of spatial-temporal data (e.g., a kind of video).

- Probabilistic representation. Representation of object(s) reflecting the probabilities assigned to them.

- Learning. How to learn, for example, the most suitable transformation of input data into HVs for a given problem? Also, learning HV for behaviors, including reinforcement learning.

Let us also touch on problems specific not only to HDC/VSA:

- Representation invariance. To recognize the same object in various situations and contexts, we need some useful invariance of representation that makes similar diverse manifestations of the same object.

- Context-dependent representation. Representing data as objects and getting the proper representation of an object in a particular context.

- Context-dependent similarity. For example, depending on the context a pizza is similar to a cake but also to a frisbee. How can such a context-dependent similarity be implemented?

- Context-dependent processing. The type of processing to be applied to data should take into account the context, such as bottom-up and top-down expectations or system goals.

- Hierarchies. Forming "part-whole" and "is-a" hierarchies. Which level of "part-whole" hierarchy does an object belong to? An object can belong to various levels for different scenes of the same nature. This is also connected to the image scale. An object can belong to very many hierarchies for scenes of varied nature. Concerning "is-a" (class-instance) hierarchy, an object can belong to various hierarchies of classes, subclasses, etc. in different contexts.

- Cause-effect. Cause-effect extraction in new situations could be done by analogy to familiar ones. Generalizations and specifications using is-a hierarchy are possible.

- Interface with symbolic representations. Designers of cognitive agents have to solve the dual problem of both building a world model and building it so that it can be expressed in symbols in order to interact with humans.

- The whole system control. Most of the solutions generally rely on conventional deterministic mechanisms for flow control. It is, however, likely that the control of the system should also be trainable, so that the system could adjust it for new tasks.

All the problems described above are rarely (if at all) considered in the scope of HDC/VSA. To the best of our knowledge, one study that discussed some of these problems is [@RachkovskijWorldModel2013]. Moreover, it is not fully clear which of these problems are related to the general problems of building Artificial General Intelligence, and which ones are due to the architectural peculiarities of neural networks and HDC/VSA. In other words, the separation provided above is not necessarily unequivocal. We believe, however, that building Artificial General Intelligence will require facing these problems anyway. Finally, we hope that insights from HDC/VSA, symbolic Artificial Intelligence, and neural networks will contribute to the solution.

# Conclusion {#sec:conc}

In this two-part survey, we provided comprehensive coverage of the computing framework known under the names Hyperdimensional Computing and Vector Symbolic Architectures. Part I of the survey [@KleykoSurveyVSA2021Part1] covered existing models and transformations of input data of various types into distributed representations. In this Part II, we focused on known applications of Hyperdimensional Computing/Vector Symbolic Architectures including the use in cognitive modeling and cognitive architectures. We also discussed the open problems along with promising directions for the future work. We hope that for newcomers, this two-part survey will provide a useful guide of the field, as well as facilitate its exploration and the identification of fruitful directions for research and exploitation. For the practitioners, we hope that the survey will broaden the vision of the field beyond their specialization. Finally, we expect that it will accelerate the convergence of this interdisciplinary field to discipline with common terminology and solid theoretical foundations.

[^1]: We would like to thank three reviewers, the editors, and Pentti Kanerva for their insightful feedback as well as Linda Rudin for the careful proofreading that contributed to the final shape of the survey. The work of DK was supported by the European Union's Horizon 2020 Programme under the Marie Skłodowska-Curie Individual Fellowship Grant (839179). The work of DK was also supported in part by AFOSR FA9550-19-1-0241 and Intel's THWAI program. The work of DAR was supported in part by the National Academy of Sciences of Ukraine (grant no. 0120U000122, 0121U000016, and 0117U002286), the Ministry of Education and Science of Ukraine (grant no. 0121U000228 and 0122U000818), and the Swedish Foundation for Strategic Research (SSF, grant no. UKR22-0024). *Denis Kleyko and Dmitri A. Rachkovskij contributed equally to this work.*

[^2]: D. Kleyko is with the Redwood Center for Theoretical Neuroscience at the University of California, Berkeley, CA 94720, USA and also with the Intelligent Systems Lab at Research Institutes of Sweden, 16440 Kista, Sweden. E-mail: denkle@berkeley.edu

[^3]: D. A. Rachkovskij is with International Research and Training Center for Information Technologies and Systems, 03680 Kiev, Ukraine. E-mail: dar@infrm.kiev.ua

[^4]: E. Osipov is with the Department of Computer Science Electrical and Space Engineering, Luleå University of Technology, 97187 Luleå, Sweden. E-mail: evgeny.osipov@ltu.se

[^5]: A. Rahimi is with IBM Research, 8803 Zurich, Switzerland. E-mail: abr@zurich.ibm.com

[^6]: It is worth recalling that this and other applications use a common design pattern, relying on the unbinding operation (see Section [\[PartI-sec:binding\]](#PartI-sec:binding){reference-type="ref" reference="PartI-sec:binding"} in [@KleykoSurveyVSA2021Part1]) that allows recovering one of the arguments. In the case of permutations, it is due to the fact that $\mathbf{a} = \rho^{-i}( \rho^{i}(\mathbf{a}))$, while in the case of multiplicative binding $\mathbf{a}= \mathbf{b} \oslash (\mathbf{a} \circ \mathbf{b})$, where for the implementations with self-inverse binding $\oslash = \circ$.

[^7]: For the sake of generality, it was decided to avoid going to in-depth details of data transformations, so tables only specify the HDC/VSA operations used to construct HVs from data.

[^8]: It should be noted that works [@MirusBehavior2019; @MirusBalanced2020] were, strictly speaking, solving the regression problems while [@MirusAbnormal2020; @WangAnomaly2021] were concerned with anomaly detection. These studies are listed in this section for the sake of covering the applications within the automotive data.

[^9]: This solution relies on knowing that there is only one Lee & female record in the database, so it should not be considered as a sensible database operation but as an example demonstrating substitution transformations.
