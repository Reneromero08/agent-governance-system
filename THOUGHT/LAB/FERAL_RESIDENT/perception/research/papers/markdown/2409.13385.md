# Introduction

The pioneering accomplishments of large language models (LLMs) have galvanized research initiatives across both industrial and academic spheres. These LLMs showcase their capacity to converse with humans in a natural and articulate manner, excelling across various tasks such as document summarization, Q&A systems, conversational AI, and coding assistants. Despite their advancements, LLMs continue to struggle with tasks that require specialized knowledge or domain-specific expertise. [@kandpal2023large]. Notably, they may produce "hallucinations" [@zhang2023siren] when confronted with out-of-scope queries or requests that necessitate up-to-date knowledge. To address these challenges, Retrieval-Augmented Generation (RAG) leverages external knowledge bases to retrieve relevant document snippets, utilizing semantic similarity metrics to identify the most pertinent information. By tapping into external knowledge sources, RAG successfully alleviates the issue of generating inaccurate content, thereby increasing the reliability of LLMs and paving the way for their widespread adoption in real-world applications.

However, RAG also has its challenges. One issue is that when retrieving relevant documents, the important information may be buried in a large amount of irrelevant text, leading to inefficient and poor responses. Another challenge is that current language models have a limited input length, which causes their performance to decline when processing lengthy documents, such as academic articles, research papers, or literary works. This constraint has fueled research into developing methods to increase the input length while maintaining the model's accuracy and efficiency.

This paper aims to shed light on the latest advancements in contextual compression methods, with a focus on their application in retrieval-based systems. Our research involves a comprehensive review of methodologies, metrics, and benchmarks, which we systematically categorize into a novel taxonomy. Our taxonomy, as shown in Figure [1](#fig:categorization_of_compression){reference-type="ref" reference="fig:categorization_of_compression"}, presents a structured and comprehensive framework for categorizing and analyzing Contextual Compression techniques for LLMs. Our investigation involves a comprehensive analysis of established techniques, such as semantic compression, in-context auto-encoder compressors, and auto-compressors, among others. Furthermore, our research highlights the ongoing challenges in this field and provides a roadmap for future investigations. We emphasize the need for collective efforts to create a sustainable and environmentally responsible future for LLMs.

=\[ rectangle, draw=hidden-draw, rounded corners, text opacity=1, minimum height=1.5em, minimum width=5em, inner sep=2pt, align=center, fill opacity=.5, \] =\[my-box, minimum height=1.5em, fill=hidden-orange!60, text=black, align=left,font=, inner xsep=2pt, inner ysep=4pt, \]

<figure id="fig:categorization_of_compression" data-latex-placement="t">

<figcaption>Taxonomy of Contextual Compression Methods for Large Language Models.</figcaption>
</figure>

# Methods

## Semantic Compression

Semantic compression is a technique that helps identify common patterns of thought in a specific context by generalizing terms. It uses a \"domain frequency dictionary\" to establish the context and disambiguate multiple possible meanings of words. This approach, based on semantic networks, offers improvements over existing natural language processing techniques.

Semantic compression reduces the number of terms in a text document by replacing less frequent terms with more general terms (their hypernyms) using a semantic network and term frequency data. This compression minimizes information loss and enables efficient processing, especially in tasks involving vector space models [@baeza1999modern], [@erk2008structured]. It also helps address linguistic [@sinha2007unsupervised] challenges like polysemy and synonymy [@krovetz1992lexical] by replacing multiple rare terms with a single, more general concept. By using statistical analysis and frequency dictionaries, semantic compression can handle polysemic concepts more effectively and with lower error rates than other techniques. These efforts can be summarized into five approaches: *Context Distillation*, *Prompting*, *Efficient Attention Operations*, *Extrapolation and Interpolation*, and *Context Window Extension*.

### Context Distillation

Recent studies have demonstrated that augmenting language models (LMs) with contextual information, such as task descriptions, illustrative examples, and explanatory notes [@chen2021meta], [@scheurer2022learning], can substantially enhance their performance capabilities. This approach can even facilitate zero-shot learning [@wei2021finetuned], [@victor2022multitask] and enable models to tackle complex tasks by generating sequential reasoning steps [@nye2021show], [@wei2022chain], [@zhou2022least].

While LMs perform better with context tokens, this advantage disappears when the tokens are removed. Additionally, processing context tokens requires extra computation, which can be a drawback. The context tokens can also be very long, and it's unclear how to handle them when they exceed the context window size. These limitations are similar to human cognitive limitations [@wason1974dual], such as struggling with complex tasks and having limited working memory [@baddeley1992working].

Humans overcome challenges through practice, which allows them to \"distill\" knowledge into habits and muscle memory. For example, learning to type a phone number becomes automatic with repetition, freeing up conscious reasoning for more complex tasks [^2]. This process is essential for building skills and knowledge, enabling us to tackle increasingly intricate challenges.

Researchers in NLP [@askell2021general], [@snell2022learning] are exploring techniques to fine-tune language models, such as context distillation and \"Gisting\". Context distillation involves generating \"practice\" questions, having the model reason step-by-step, and fine-tuning it to predict answers from simpler prompts. This helps the model internalize skills, like step-by-step addition (ref Figure [2](#fig:cdistillation){reference-type="ref" reference="fig:cdistillation"}). \"Gisting\" [@mu2024learning] compresses instructions into concise key-value attention prefixes, saving computational resources and generalizing well to new tasks. As depicted in Figure [3](#fig:gisting){reference-type="ref" reference="fig:gisting"}, the approach involves learning a gist model by incorporating gist tokens during instruction tuning, enabling the model to handle prompt compression and instruction following simultaneously.

<figure id="fig:cdistillation" data-latex-placement="ht">
<img src="figures/context_distill.png" />
<figcaption>Internalization of step-by-step reasoning via context distillation <span class="citation" data-cites="snell2022learning"></span></figcaption>
</figure>

<figure id="fig:gisting" data-latex-placement="ht">
<img src="figures/gisting.png" />
<figcaption>Gisting - Each vertical rectangle here represents a stack of Transformer activations <span class="citation" data-cites="mu2024learning"></span> </figcaption>
</figure>

### Prompting

**Soft Prompts -** As depicted in Figure [4](#fig:soft_prompting){reference-type="ref" reference="fig:soft_prompting"}, soft prompt tuning enables the adaptation of pre-trained Transformers without modifying their underlying parameters, as demonstrated in recent studies [@lester-etal-2021-power], [@zhong-etal-2021-factual], and [@liu-etal-2022-p]. It entails adding novel embeddings to the input sequence and fine-tuning only these new parameters while keeping the remainder of the model's architecture frozen. This approach is categorized as a parameter-efficient fine-tuning method (PEFT) [@lialin2023scaling], and bears resemblance to prefix tuning, which prepends task-specific vectors to the attention states instead of the input sequence [@li2021prefix].

<figure id="fig:soft_prompting" data-latex-placement="ht">
<img src="figures/soft_prompting.png" />
<figcaption>From 11 billion for a tuned model to just 20,480 for a tuned prompt, a reduction of over 5 orders of magnitude <span class="citation" data-cites="lester-etal-2021-power"></span></figcaption>
</figure>

**Prompt Compression -** In their work, [@wingate2022prompt] hypothesize using a soft prompt $sp$ to compress information from a context $ctx$. They use a pre-trained LM $p_\text{LM}$ to generate continuations $cty \sim p_\text{LM}(\cdot \mid ctx)$ based on the context, and then calibrate the model's outputs with the soft prompt $sf$, $p_\text{LM}(cty \mid sf)$ to the outputs based on the context $ctx$, $p_\text{LM}(cty \mid ctx)$. They find that soft prompts effectively preserve abstract knowledge and improve guided output. Nevertheless, this method necessitates distinct optimization for each novel context, lacking the ability to leverage knowledge across analogous contexts.\
**Task-Agnostic Prompt Compression -** Current methods for compressing natural language prompts remove tokens or lexical units based on information entropy from a language model like LlaMa-7B. However, using information entropy as a compression metric has two limitations: 1) it only considers unidirectional context, which may miss important information, and 2) it doesn't perfectly align with the goal of prompt compression.

To address these issues, [@wu2024llmlingua2] propose a data distillation approach to compress prompts while retaining essential information. They introduce an extractive text compression dataset and frame prompt compression as a token classification problem (preserve or discard) (Refer to Figure [5](#fig:llmlingua2){reference-type="ref" reference="fig:llmlingua2"}). The key benefits are as follows:

1.  *Comprehensive Information Capture:* By leveraging a Transformer encoder, the method captures essential details from the full bidirectional context.

2.  *Reduced Latency:* Smaller models explicitly learn the compression objective, leading to lower latency.

3.  *Faithfulness:* The compressed prompt remains faithful to the original content.

<figure id="fig:llmlingua2" data-latex-placement="ht">
<img src="figures/llmlingua-2.png" />
<figcaption>Overview of LLMLingua-2 <span class="citation" data-cites="wu2024llmlingua2"></span></figcaption>
</figure>

### Efficient Attention Operations

The self-attention mechanism in LLMs leads to an inference cost that scales quadratically with sequence length, prompting the development of various methods to alleviate this complexity. For example:

- *Transformer-XL [@dai2019transformer]* - employs a recurrent architecture that operates on segments, paired with a novel positional encoding technique.

- *Longformer [@beltagy2020longformer]* - introduces sparse attention, scaling linearly with sequence length.

- *FlashAttention [@dao2022flashattention]* - uses chunking and re-computation to avoid quadratic attention complexity.

However, these methods can be expensive to train and struggle with out-of-distribution content lengths [@ding2023longnet]. To address this, *LongLoRA [@chen2023longlora]* provides a computationally efficient fine-tuning method with minimal resource requirements. For further insights, refer to the study by [@huang2023advancing].

### Extrapolation and Interpolation

In the field of NLP, researchers are investigating methods to extend the capabilities of existing language models, initially trained on brief texts, to process longer sequences during inference [@anil2022exploring]. One approach is to alter positional embeddings, which are typically designed for shorter contexts. The Rotary Position Embeddings (RoPE) from LLaMA is a key foundation for several studies in this area. For example:

- *Position Interpolation (PI) [@chen2021meta]* applies a linear transformation to input positional indices.

- *YaRN [@peng2023yarn]* leverages neural tangent kernel-inspired mechanisms to scale up the context window to 64,000 and 128,000 tokens.

### Context Window Extension

Researchers [@fei2023extending] propose a semantic compression method that distills long texts into concise forms, retaining their meaning and broadening the context window (Figure [6](#fig:semantic_compression){reference-type="ref" reference="fig:semantic_compression"}). This method occurs before inputting tokens into pre-trained language models and is customizable and optimized for specific tasks. It outperforms existing methods in various tasks, including question answering, summarization, and few-shot learning, without requiring additional parameter updates or memory consumption, making it computationally efficient.

<figure id="fig:semantic_compression" data-latex-placement="ht">
<img src="figures/semantic_compression.png" />
<figcaption>1) clustering the input text into thematic groups, represented as a graph, to facilitate topic-based analysis, 2) tuning the thematic segments using pre-trained models to preserve crucial details, and 3) reassembling the refined chunks in their original order - reducing the text length by approximately 6-8 times. Additionally, other techniques like extrapolation and interpolation can be used to further extend the length <span class="citation" data-cites="fei2023extending"></span></figcaption>
</figure>

## Pre-Trained Language Models (PLMs)

The development of PLMs has revolutionized the field of NLP. The first generation of PLMs, such as Skip-Gram [@mikolov2013distributed], word2vec [@mikolov2013efficient], and GloVe [@pennington2014glove], used shallow neural networks [@qiu2020pre] to obtain word embeddings. The second generation, including CoVe [@mccann2017learned], ELMo [@peters2018dissecting], BERT [@devlin2018bert], and GPT [@radford2018improving], focused on learning dynamic word embeddings using transformers. The pre-training and fine-tuning approach has achieved remarkable success in various NLP tasks. Moreover, recent breakthroughs in prompt learning [@liu2023pre] have empowered PLMs to accomplish few-shot or zero-shot learning with minimal labeled data. Notable examples of successful PLMs include ChatGPT, GPT-4, Gemini, Claude, LlaMA-3, Mixtral, etc.

### AutoCompressors

The authors of [@chevalier2023adapting] propose teaching PLMs to compress text into summary vectors [@lester-etal-2021-power], which are significantly shorter than the original text (often 1-2 orders of magnitude shorter). These vectors have a two-pronged function: 1) they allow the LM to handle long documents by extending its context window with minimal computational overhead, and 2) they accelerate inference for pre-computed and cached text.

AutoCompressors, proposed by [@chevalier2023adapting], are trained To distill key information into summary vectors, generated sequentially from extended documents (Figure [7](#fig:autoCompressors){reference-type="ref" reference="fig:autoCompressors"}). The approach builds upon the Recurrent Memory Transformers (RMT) architecture [@bulatov2022recurrent], introducing summary accumulation and training with randomly segmented inputs. This enhances long-range information retention and facilitates reasoning across multiple passages. AutoCompressors can be seeded with PLMs and fine-tuned on long sequences. They improve perplexity for long documents and demonstrate robust compression capabilities across different domains, making them valuable for various downstream applications.

<figure id="fig:autoCompressors" data-latex-placement="ht">
<img src="figures/AutoCompressors.png" />
<figcaption>AutoCompressors recursively generate summary vectors from long documents, using them as soft prompts for subsequent segments <span class="citation" data-cites="chevalier2023adapting"></span></figcaption>
</figure>

### LongNET

Overcoming sequence length limitations in language models has several advantages, including improved interactions with human language, better capture of complex causality and reasoning, and reduced catastrophic forgetting. However, scaling up sequence length poses a challenge in balancing computational complexity and model expressivity. RNN-style models and state space models [@gu2021efficiently], [@smith2022simplified], [@fu2022hungry], [@poli2023hyena] have been proposed, but they have limitations from the perspective of parallelization and model adaptability [@fathi2023block]. An alternative approach is to reduce the complexity of Transformers [@vaswani2017attention], such as using sliding windows or convolution modules for attention, or sparse attention. LongNet [@ding2023longnet], a novel approach, replaces the attention mechanism with \"dilated attention\", which achieves linear computational complexity and logarithmic dependency between tokens. This allows LongNet to efficiently scale sequence lengths to 1 billion tokens, overcoming the constraints of computation and memory.

### In-Context Auto-Encoders

Modeling long-range dependencies is a hurdle for Transformer-based LMs [@vaswani2017attention] due to their self-attention mechanism. Previous research by [@beltagy2020longformer], [@bulatov2022recurrent], and Ding [@ding2023longnet] has attempted to cope with this issue through architectural innovations, but these approaches often struggle to maintain performance in long contexts, as underscored by [@liu2024lost]. A novel approach, \"context compression\", is proposed by [@ge2023context], which recognizes that an LLM can represent the same information in varying lengths. They introduce the In-context Autoencoder (ICAE), which compresses lengthy contexts into a fixed number of memory buffers using a learnable encoder and a fixed decoder (Figure [8](#fig:icae){reference-type="ref" reference="fig:icae"}). The ICAE is pre-trained using auto-encoding and language modeling objectives and fine-tuned using instruction data. The approach achieves 4x context compression while maintaining effective conditioning for the target LLM, enabling faster and more memory-efficient inference.

<figure id="fig:icae" data-latex-placement="ht">
<img src="figures/ICAE.png" />
<figcaption>Condensing an extended context into a compact memory representation, which can be leveraged by the target LLM to respond to diverse prompts. <span class="citation" data-cites="ge2023context"></span></figcaption>
</figure>

### RECOMP

In their work, [@xu2024recomp] introduce RECOMP, an intermediary step for Retrieval-augmented Language Models (RALMs) [@izacard2022atlas], [@borgeaud2022improving]. RECOMP compresses retrieved documents into concise textual summaries before integrating them during inference, reducing computational costs and alleviating the burden on LMs to process lengthy documents. The aim is to produce summaries that balance brevity and fidelity to the original evidence documents, guiding the RALM to produce targeted outputs when the summary is used as a prefix to the input (illustrated in Figure [9](#fig:recomp){reference-type="ref" reference="fig:recomp"}). To achieve this, the authors train two types of compressors:

1.  *Extractive Compressor:* This compressor filters out irrelevant sentences, retaining only the most pertinent ones from the retrieved document set.

2.  *Abstractive Compressor:* This compressor produces a summary by fusing information from multiple retrieved documents.

Both compressors employ a multi-document query-based summarization approach [@xu2020coarse], summarizing evidence documents concerning the input query. The authors develop training strategies that maximize performance on the target task to guarantee accurate output. Contrastive learning is employed to train the extractive compressor enabling it to select key sentences effectively, while the abstractive compressor is distilled [@west2021symbolic] from a large language model (like GPT-3 or GPT-4), achieving strong summarization performance. This approach holds promise for enhancing the efficiency and efficacy of RALMs.

<figure id="fig:recomp" data-latex-placement="ht">
<img src="figures/recomp.png" />
<figcaption>RECOMP’s document compression technique generates a summary that serves as input to a language model, facilitating correct answer generation while minimizing encoding costs. <span class="citation" data-cites="xu2024recomp"></span></figcaption>
</figure>

## Retrievers

The retriever [@langchain] is an interface that processes an unstructured query and returns a curated list of documents in response. Contextual compression aims to address the challenges of retrieval by compressing the retrieved context to only include relevant information. In this context, \"compressing\" encompasses both condensing the content of individual documents and eliminating irrelevant documents altogether. The Contextual Compression Retriever uses a *base retriever* and a *Document Compressor* to process queries. The base retriever retrieves the initial documents, which are then passed through the Document Compressor to shorten the list of documents by either reducing the contents of individual documents or excluding entire documents altogether.

### LLMChainExtractor

In this approach, the base retriever is wrapped with a *ContextualCompressionRetriever*. Additionally, an *LLMChainExtractor* serves as the base compressor. The *LLMChainExtractor* iterates over the initially retrieved documents and extracts only the relevant content for the given query. It achieves this by making an additional LLM call for each retrieved document and summarizing the relevant information

### EmbeddingsFilter

Making an additional LLM call for each retrieved document can be both costly and slow. However, the *EmbeddingsFilter* offers a more economical and faster alternative. By embedding both the documents and the query, it selectively returns only those documents that exhibit sufficiently similar embeddings to the query. This approach optimizes retrieval efficiency while maintaining relevance.

### DocumentCompressorPipeline

The DocumentCompressorPipeline allows a seamless combination of multiple compressors in a sequence. Alongside these compressors, we can incorporate *BaseDocumentTransformers* into our pipeline. Unlike contextual compressors, these transformers don't alter the content significantly but perform specific transformations on a set of documents. For instance, *TextSplitters* can divide documents into smaller segments, while the *EmbeddingsRedundantFilter* identifies and filters out redundant documents based on embedding similarity. This modular approach enhances flexibility and adaptability in document processing. e.g.

- *Splitter:* create small chunks

- *Redundant filter:* remove similar docs --- embedded

- *Relevant filter:* relevant to query

# Metrics and Benchmarks

## Metrics

Evaluating language model inference efficiency involves considering various metrics that capture different performance aspects, including accuracy, zero-shot capabilities, compression ratio, and inference time. Within the framework of RAG-based solutions, the \"Triad of Metrics\" [^3] - Groundedness, Context Relevance, and Answer Relevance - are also employed for evaluation. Achieving satisfactory performance across these metrics helps ensure that the language model application is reliable and free from hallucinations.

<figure id="fig:rag_triad" data-latex-placement="ht">
<img src="figures/rag_triad.png" />
<figcaption>RAG-Triad</figcaption>
</figure>

### Compression Ratio

The compression ratio measures the reduction in size from the original uncompressed context to the compressed context. A higher compression ratio means that the compression is more efficient, as it achieves a greater reduction in size while preserving the context's coherence.

### Inference Time

Inference time, also known as latency, measures how long it takes for a Large Language Model (LLM) to process input data and generate responses. This metric is crucial for real-world applications that require quick handling of user queries or processing of large data volumes in real-time.

### Context Relevance

In RAG applications, the first step is retrieval, and it's crucial to ensure that the retrieved context chunks are relevant to the input query. Irrelevant information in the context can lead to hallucinations in the LLM's answer. To evaluate context relevance, the structure of the serialized record can be analyzed.

### Groundedness

After retrieving the context, an LLM transforms it into an answer. However, LLMs can sometimes stray from the facts and generate responses that are not entirely accurate. To ensure the groundedness of the application, the response can be broken down into individual claims and verified by searching for supporting evidence within the retrieved context.

### Answer Relevance

Furthermore, our response must still effectively address the original question. We can assess this by evaluating the relevance of the final response to the user's input.

### Others

RAG evaluation also encompasses four key abilities that reflect the model's adaptability and efficiency: noise robustness, negative rejection, information integration, and counterfactual robustness [@chen2024benchmarking], [@liu2023recall]. The model's quality scores are heavily influenced by its ability to leverage these capabilities in diverse challenges and complex scenarios:

1.  *Noise Robustness:* This metric gauges a model's capacity to distinguish between relevant and irrelevant documents, even when the latter are tangentially related to the question.

2.  *Negative Rejection:* The metric measures a model's capacity to recognize when the retrieved documents are insufficient to answer a question, and to withhold a response accordingly.

3.  *Information Integration:* Information integration tests a model's proficiency in combining relevant information from multiple documents to provide well-informed answers to challenging questions.

4.  *Counterfactual Robustness:* Counterfactual robustness measures a model's skill in identifying and ignoring flawed or misleading information in documents, regardless of its awareness of potential errors.

In brief, context relevance and noise robustness are crucial for evaluating the retrieval process, while answer groundedness, answer relevance, negative rejection, information integration, and counterfactual robustness are vital for assessing the quality of generated text.

## Benchmarks and Datasets

The primary objective of these benchmarks and datasets is to assess the trade-offs between compressed and uncompressed contexts in terms of effectiveness, efficiency, and accuracy, covering a broad range of NLP tasks and applications.

### Common Benchmarks and Datasets

RAG's primary function revolves around answering questions, encompassing various formats such as single-hop and multi-hop queries, multiple-choice options, and domain-specific inquiries, as well as lengthy scenarios that leverage RAG's capabilities. Moreover, RAG is constantly evolving to tackle additional tasks, including extracting relevant information, generating conversational dialogue, and searching for code snippets, documentations and even interpreting them. For more details, refer to the study by [@gao2023retrieval].

# Challenges and Future Directions

## More advanced Methods

Research on contextual compression for LLMs is still in its early stages. While previous studies have shown compressed contexts, they still lag behind uncompressed contexts in terms of performance. By exploring more advanced compression methods tailored for LLMs, we can potentially bridge this performance gap and enhance the performance of uncompressed contexts.

## Performance-Size Trade-offs

Previous research highlights the importance of balancing LLM performance with context size, considering hardware limitations and practical constraints. Despite its significance, the theoretical and empirical foundations of this trade-off remain poorly understood. Future investigations should focus on conducting exhaustive examinations to drive the creation of sophisticated compression techniques that can meet the demands of increasingly complex data sets, enabling researchers to create tailored methods that effectively navigate the design space and optimize performance.

## Dynamic Contextual Compression

Contemporary compression approaches still utilize manual compressors, such as retrievers, which often require an empirical methodology driven by input data or task specifications. This can be a practical hindrance to adoption, especially in scenarios like context distillation, where finding suitable student templates within computational constraints can be time-consuming and require multiple trials.

## Explainability

Compressing pre-trained language models can make them hard to understand (lacking explainability). To fix this, using explainable compression methods can help make models more interpretable, easier to evaluate, and more reliable in real-life scenarios.

# Conclusion

This in-depth analysis explores the domain of contextual compression techniques, with a focus on their application to LLMs. Our study encompasses a broad range of compression methods, evaluation metrics, and benchmark datasets, providing a comprehensive understanding of the field. By examining the complexities of contextual compression, we identify the key challenges and opportunities that arise in this area. As research in this field continues to advance, the development of specialized methodologies tailored to the needs of LLMs is crucial for unlocking their full potential across various domains. This survey aims to serve as a valuable resource, providing a detailed overview of the current landscape and encouraging further investigation into this vital topic.

# Limitations {#limitations .unnumbered}

While this survey provides a comprehensive overview of contextual compression techniques for large language models, there are several limitations to acknowledge. Firstly, the field of contextual compression is rapidly evolving, and this survey may not capture the very latest advancements in the area. Additionally, the focus on large language models may not be representative of other types of language models or AI systems, which may have different compression requirements. Furthermore, the survey's reliance on existing evaluation metrics and benchmark datasets may not fully capture the complexities and nuances of contextual compression. Moreover, the need for advanced methodologies specifically designed for LLMs highlights the potential limitations of current approaches, which may not be scalable or effective for future LLM architectures. Finally, the survey's scope is limited to contextual compression, and future research may uncover new challenges and opportunities at the intersection of compression and other aspects of LLMs.

# Ethics {#ethics .unnumbered}

As research in contextual compression for large language models continues to advance, it is essential to consider the ethical implications of these developments. One key concern is the potential for biased or unfair compression methods, which could perpetuate existing social inequalities or create new ones. For instance, compression techniques that prioritize certain types of data or language styles over others may disadvantage certain groups or communities. Furthermore, the focus on large language models may exacerbate existing power imbalances, where only those with access to significant computational resources and data can develop and deploy these models.

Additionally, the reliance on existing evaluation metrics and benchmark datasets may perpetuate biases and limitations in the development of compression techniques. It is crucial to ensure that these metrics and datasets are diverse, representative, and regularly updated to reflect the complexities of real-world language use.

The need for advanced methodologies specifically designed for LLMs also raises ethical concerns around the responsible development and deployment of these models. As LLMs become increasingly ubiquitous, it is essential to consider their potential impact on individuals, communities, and society as a whole. This includes ensuring that these models are transparent, explainable, and accountable, and that their development and deployment are guided by ethical principles and values.

Finally, the survey's limited scope to contextual compression highlights the need for a more comprehensive consideration of the ethical implications of LLMs and their applications. Future research should prioritize ethical considerations and ensure that the development of compression techniques and LLMs is guided by a commitment to social responsibility, fairness, and transparency.

[^1]: Resources are available at <https://github.com/SrGrace/Contextual-Compression>

[^2]: procedural learning vs. declarative learning - <https://en.wikipedia.org/wiki/Procedural_knowledge>

[^3]: RAG Triad (Figure [10](#fig:rag_triad){reference-type="ref" reference="fig:rag_triad"}): <https://www.trulens.org/trulens_eval/getting_started/core_concepts/rag_triad/>
