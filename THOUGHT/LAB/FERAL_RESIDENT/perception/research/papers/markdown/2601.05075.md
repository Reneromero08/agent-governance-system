# Introduction

Representing text as a semantically meaningful vector is a fundamental and essential step in natural language processing, supporting various downstream applications such as text classification, clustering, and information retrieval. Seminal works primarily leverage token-level contrastive learning [@gao-etal-2021-simcse; @chuang-etal-2022-diffcse] and derive sentence embeddings by extracting the hidden states of special tokens such as `[CLS]` or the pooled representation of all tokens. The success of such methods relies on powerful pre-trained encoder-only models like BERT [@devlin-etal-2019-bert] or RoBERTa [@liu2019roberta], as shown in Figure [1](#fig:intro){reference-type="ref" reference="fig:intro"}(top).

<figure id="fig:intro" data-latex-placement="t!">
<embed src="figs/intro_SemPA.pdf" style="width:98.0%" />
<figcaption>Comparison of sentence embedding methods. Top: contrastive learning for encoder-only models. Bottom: our semantic preference alignment for LLMs.</figcaption>
</figure>

::: table*
:::

Generative large language models (LLMs; [@brown2020language; @openaichatgpt; @achiam2023gpt]), with massive parameters and trained on extensive data, have demonstrated remarkable capabilities. This has led to the emergence of sentence embedding methods based on either prompting [@jiang-etal-2022-promptbert; @lei-etal-2024-meta] or modifying these LLMs [@behnamghader2024llmvec; @fu-etal-2025-token]. However, prompting-based approaches, which rely solely on prompt engineering, have inherent performance limitations. While model-modification methods can improve sentence representations, they often compromise LLMs' generative ability.

To achieve semantic representation with LLMs while preserving their inherent generative capabilities, we fine-tune LLMs using preference alignment [@NEURIPS2022_b1efde53]. Preference alignment was originally designed to enhance the truthfulness and safety of LLMs by constructing preference data pairs, enabling sentence-level optimization in certain aspects. Inspired by this, we propose [SemPA]{.smallcaps}, a semantic preference alignment method that operates at the sentence level by applying Direct Preference Optimization (DPO; [@NEURIPS2023_a85b405e]) to a paraphrase generation task. This allows LLMs to learn fine-grained semantic distinctions by assigning higher relative probabilities to semantically accurate paraphrases and thus improves the underlying semantic representations with lightweight training, without sacrificing their generative proficiency, as shown in Figure [1](#fig:intro){reference-type="ref" reference="fig:intro"}(bottom).

Theoretically, we provide a derivation that connects contrastive learning and DPO in a unified form, offering a new perspective on why preference alignment serves as an effective paradigm for representation learning. Empirically, results on Semantic Textual Similarity (STS) tasks and various generative benchmarks ([@cobbe2021training], [@hendrycks2021measuring], etc.) using LLaMA models [@touvron2023llama2openfoundation; @grattafiori2024llama3herdmodels] demonstrate that [SemPA]{.smallcaps} outperforms pre-trained and prompt-based baselines and stays competitive with specialized embedding models, while maintaining the core generation capability of LLMs across various tasks.

# Related Work

**Sentence Embedding.** Sentence embedding aims to encode the semantic content of text into numerical vectors for semantic similarity assessment and downstream tasks. Current sentence embedding methods can be categorized into four types:

1\) *Token-Level.* Token-level contrastive learning is widely used in traditional pre-trained models [@gao-etal-2021-simcse; @chuang-etal-2022-diffcse] or generative language models [@neelakantan2022text; @behnamghader2024llmvec] for sentence embedding. It optimizes the token-level representations of models by distinguishing positive and negative samples, thereby extracting general semantic representations from special tokens such as `[CLS]` or the pooling result of all tokens. Contrastive learning is also extensively employed in commercial embedding models such as NV-Embed [@lee2025nvembed], Qwen3-embedding [@zhang2025qwen3embeddingadvancingtext], and Gemini embedding [@lee2025gemini], demonstrating strong performance and scalability.

2\) *Sentence-Level.* To further adapt LLMs to various downstream tasks (e.g., classification, retrieval, and clustering), @su-etal-2023-one and @muennighoff2025generative employ sentence-level instruction tuning to train LLMs to acquire task-specific semantic representations. These types of methods often rely on large-scale and high-quality instruction datasets such as MEDI [@su-etal-2023-one] and E5 [@wang2022text].

3\) *Context-Level.* There have been some studies on context-based modification that extract overall semantic representations of given texts without fine-tuning the model parameters. @jiang-etal-2022-promptbert obtains semantic information based on the `[MASK]` token of BERT through specific templates. @jiang-etal-2024-scaling and @lei-etal-2024-meta design prompts to extract semantic information from generative models such as LLaMA. @cheng-etal-2025-contrastive enhances semantic representation by leveraging multiple contexts with different meanings. The effectiveness of such context-based methods largely depends on the model's inherent capabilities and the quality of specialized prompts.

4\) *Architecture-Level.* Modifying the internal structure of LLMs has emerged recently for sentence embeddings [@fu-etal-2025-token; @ding2025hierarchicaltokenprependingenhancing]. These approaches alter the computational operations within the Transformer decoder architecture to extract specific hidden states as semantic representations. However, such invasive modifications change the model's structure, thus compromising the model's original generative capabilities.

**Preference Alignment of LLMs.** Preference alignment has been shown to be an effective way to improve the safety and truthfulness of LLMs. It can be achieved by training LLMs on datasets of human or AI-generated preferences, using methods such as reinforcement learning from human feedback (RLHF; [@NEURIPS2022_b1efde53]) or direct preference optimization (DPO; [@NEURIPS2023_a85b405e]). Preference alignment optimizes the model beyond token-level prediction and improves the overall response quality, and it has also been applied to tasks such as combinatorial optimization [@pan2025preference], code generation [@zhang-etal-2025-focused], and mathematical reasoning [@ICLR2025_31a57804]. To our knowledge, we are the first to explore preference alignment for improving the semantic representations of LLMs.

We summarize and compare the main related works with our [SemPA]{.smallcaps} in Table [\[tab:relatedwork\]](#tab:relatedwork){reference-type="ref" reference="tab:relatedwork"}. Note that the primary goal of our work is not to compete with state-of-the-art embedding models in terms of performance, but rather to offer insights for improving semantic representations of LLMs through the perspective of lightweight preference alignment, which has not been explored in existing work.

# Preliminary

**PromptEOL.** Given a sentence $\mathcal{S} = s_1, \ldots, s_n$, PromptEOL [@jiang-etal-2024-scaling] extracts the sentence embedding of $\mathcal{S}$ using LLMs by inserting $\mathcal{S}$ in a template $\mathcal{T}$ defined as: $$\text{\textit{This sentence: ``$\mathcal{S}$'' means in one word:``}}$$

The template with filled sentence $\mathcal{T}(\mathcal{S})$ is used as the input for a generative model $\mathcal{M}$, and the corresponding sentence embedding of $\mathcal{S}$ is the hidden state of the last token in the final layer $L$:

$$\begin{equation}
\begin{aligned}
{h^L_1}, {h^L_2}, ..., {h^L_{\rm last}} &= \mathcal{M}(\mathcal{T}(\mathcal{S})),\\
{\rm Emb}(\mathcal{S}) &= {h^L_{\rm last}},
\end{aligned}
\label{eq:prompteol}
\end{equation}$$ where ${\rm Emb}$($\mathcal{S}$) serves as a training-free baseline embedding extracted from generative LLMs.

**Direct Preference Optimization.** To tailor the behavior of model $\mathcal{M}$ more closely to human preferences, DPO [@NEURIPS2023_a85b405e] directly optimizes a policy by leveraging a closed-form mapping from reward functions to optimal policies, thereby avoiding the need for an explicit reward model. Formally, given dataset $\mathcal{D}$ consisting of triplets $(x,y_w,y_l)$ where $y_w$ is preferred over $y_l$ for input $x$, the loss function of DPO is defined as:

$$\begin{equation}
\scalebox{0.65}{$
        \mathcal{L}_{\text{DPO}}(\pi_\theta; \pi_{\text{ref}}) = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}}\Bigg[ \log \sigma \Big( \beta \log \frac{\pi_\theta(y_w \mid x)}{\pi_{\text{ref}}(y_w \mid x)} - \beta \log \frac{\pi_\theta(y_l \mid x)}{\pi_{\text{ref}}(y_l \mid x)} \Big) \Bigg],
    $}
    \label{eq:dpo}
\end{equation}$$ where $\pi_\theta$ is the current policy to be optimized and $\pi_{\text{ref}}$ is the reference policy by the original model $\mathcal{M}$. After identity transformation, the above formula can be expressed as:

$$\begin{equation}
\scalebox{0.7}{$
        \mathcal{L}_{\text{DPO}}(\pi_\theta; \pi_{\text{ref}}) =
         -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}}   
        \Bigg[ \log \frac{e^{\beta \log \frac{\pi_\theta(y_w \mid x)}{\pi_{\text{ref}}(y_w \mid x)}}}{e^{\beta \log \frac{\pi_\theta(y_w \mid x)}{\pi_{\text{ref}}(y_w \mid x)}}+e^{\beta \log \frac{\pi_\theta(y_l \mid x)}{\pi_{\text{ref}}(y_l \mid x)}}} \Bigg],
$}
\label{eq:dpo_transform}
\end{equation}$$ which shares a unified framework with contrastive learning, as discussed in Section [4.4](#subsec:dpo_contrastive_link){reference-type="ref" reference="subsec:dpo_contrastive_link"}.

# Our Method

<figure id="fig:SemPA" data-latex-placement="t!">
<embed src="figs/method_SemPA.pdf" style="width:98.0%" />
<figcaption>The overall pipeline of our proposed <span class="smallcaps">SemPA</span> method. (a) We first construct the paraphrase generation preference pairs using NLI datasets (§<a href="#subsec:paraphrase" data-reference-type="ref" data-reference="subsec:paraphrase">4.1</a>). (b) Then we perform semantic DPO training on LLMs (§<a href="#subsec:dpotraining" data-reference-type="ref" data-reference="subsec:dpotraining">4.2</a>). (c) The final sentence embedding is acquired using the PromptEOL template (§<a href="#subsec:acquisition" data-reference-type="ref" data-reference="subsec:acquisition">4.3</a>).</figcaption>
</figure>

## Paraphrase Generation Task {#subsec:paraphrase}

We integrate the concept of "paraphrase" to improve the semantic representations of models. Traditional paraphrase-related tasks center on paraphrase identification [@yin-schutze-2015-convolutional; @wang-etal-2021-knowledge-guided; @peng-etal-2022-predicate], where an auto-encoding model is trained with an explicit classification head to determine whether two given sentences convey the same meaning. To leverage paraphrase to enhance the semantic representations of generative LLMs, we propose a paraphrase generation task and construct paraphrase-based preference pairs for model optimization.

Preference alignment pairs for training LLMs are typically sourced from user-annotated [@NEURIPS2022_b1efde53] or model-generated [@alpaca] data, which entail significant annotation costs or computational overhead. In this work, we employ existing natural language inference (NLI; [@williams-etal-2018-broad]) datasets as the source of semantic preference pairs, thereby enabling a lightweight and low-cost model fine-tuning approach.

For instance, in NLI datasets, given a premise, another sentence needs to be labeled with one of the following labels: entailment, neutral, or contradiction. Following @gao-etal-2021-simcse, we treat the premise as the anchor sentence $x^*$, regard the entailment-labeled sentence as the positive sample $y^+$, and the contradiction-labeled sentence as the negative sample $y^-$, as shown in Figure [2](#fig:SemPA){reference-type="ref" reference="fig:SemPA"}(a). The paraphrase generation task encourages the model to generate $y^+$ given $x^*$, which requires the generative model to possess fine-grained semantic understanding of the sentences.

::: table*
:::

## Semantic Preference Alignment via DPO {#subsec:dpotraining}

Given the triplet ($x^*$, $y^+$, $y^-$), instead of directly performing sequence-to-sequence generation from input $x^*$ to output $y^+$, we align the model to prefer generating $y^+$ over $y^-$ conditioned on $x^*$, thereby preventing the model from degenerating into a paraphrasing model. We first insert the anchor $x^*$ into a paraphrasing instruction template $\mathcal{T}^{\rm para}$ as follows:

> *Keep the same meaning of this sentence: "$x^*$", while making some changes.*

Then we apply the constructed data to DPO training, as shown in Figure [2](#fig:SemPA){reference-type="ref" reference="fig:SemPA"}(b). In particular, we fill the template $\mathcal{T}^{\rm para}$ with the anchor sentence $x^*$ to obtain $\mathcal{T}^{\rm para}(x^*)$, which is used as the model input $x$ in Eq. [\[eq:dpo\]](#eq:dpo){reference-type="ref" reference="eq:dpo"}; the positive sample $y^+$ serves as the preferred response $y_w$, and the negative sample $y^-$ serves as the rejected response $y_l$. We compare different paraphrasing templates $\mathcal{T}^{\rm para}$ in Section [\[subsec:template\]](#subsec:template){reference-type="ref" reference="subsec:template"}.

We refer to the above alignment method as *semantic preference alignment* (i.e., [SemPA]{.smallcaps}), which aims to: 1) retain the original architecture of the generative LLMs without modification; 2) enhance the semantic understanding capability of LLMs; and 3) preserve the foundational capabilities of the original LLMs through lightweight fine-tuning.

## Acquisition of Sentence Embedding {#subsec:acquisition}

As shown in Figure [2](#fig:SemPA){reference-type="ref" reference="fig:SemPA"}(c), after obtaining the semantic preference aligned model $\mathcal{M}^\textsc{\textsc{SemPA}}$, we follow PromptEOL by using the template $\mathcal{T}$ to extract our sentence embedding of $\mathcal{S}$ from the last-layer hidden state of the LLM similar to Eq. [\[eq:prompteol\]](#eq:prompteol){reference-type="ref" reference="eq:prompteol"}: $$\begin{equation}
\begin{aligned}
{h^L_1}, {h^L_2}, ..., {h^L_{\rm last}} &= \mathcal{M}^\textsc{\textsc{SemPA}}(\mathcal{T}(\mathcal{S})),\\
{\rm Emb}^{\textsc{SemPA}}(\mathcal{S}) &= {h^L_{\rm last}}
\end{aligned}
\label{eq:ours}
\end{equation}$$ We discuss different templates $\mathcal{T}$ to obtain the final sentence embedding ${\rm Emb}^{\textsc{SemPA}}(\mathcal{S})$ in Section [\[subsec:template\]](#subsec:template){reference-type="ref" reference="subsec:template"}.

## Unifying Contrastive Learning and DPO {#subsec:dpo_contrastive_link}

We offer a theoretical connection between contrastive learning and DPO in terms of their objective formulations, which can help us understand that DPO can also enhance semantic representations similar to contrastive learning. Specifically, they can both be formally regarded as specific instantiations of the Plackett-Luce model [@af5079a1-8ca5-3727-a405-0a82390327b7]. For $N$ responses $\{y_1, \dots, y_N\}$, let the observed ranking be $\sigma = (y_{(1)} \succ y_{(2)} \succ \dots \succ y_{(N)})$, where $y_{(1)}$ is the optimal response and $y_{(N)}$ is the worst. The probability of the complete ranking is: $$\begin{equation}
    P(\sigma \mid x) = \prod_{k=1}^{N} \frac{\exp\left(s_\theta(x, y_{(k)})\right)}{\sum_{j=k}^{N} \exp\left(s_\theta(x, y_{(j)})\right)},
\end{equation}$$ where $s_\theta$ is a score function between the input and the candidate. The above formula can be interpreted as a sequential selection process: the top-ranked candidate is chosen first, followed by the second-ranked candidate from the remaining options, and this procedure continues iteratively.

Similarly, both contrastive learning and DPO focus solely on the selection probability of the top-ranked candidate. As a result, the formula can be simplified to: $$\begin{equation}
    P(y_1 \text{ is the best} \mid x) = \frac{\exp\left(s_\theta(x, y_1)\right)}{\sum_{j=1}^{N} \exp\left(s_\theta(x, y_j)\right)},
\end{equation}$$ where the main difference lies in the scoring function $s_\theta$ and the final objective, as shown in Table [\[tab:dpo_infonce_comp\]](#tab:dpo_infonce_comp){reference-type="ref" reference="tab:dpo_infonce_comp"}.

For the scoring function, contrastive learning uses a similarity function with representations (usually from the special token `[CLS]`) given by the encoder $e_{\theta}$, while DPO uses the log ratio of the policy $\pi_\theta$ to a generative reference model $\pi_{\text{ref}}$. For the objective $\mathcal{L}$, contrastive learning uses the log-likelihood InfoNCE [@oord2018representation] similar to Eq. [\[eq:dpo_transform\]](#eq:dpo_transform){reference-type="ref" reference="eq:dpo_transform"} in DPO.

# Experiments

## Settings

**Datasets.** We use the natural language inference dataset for training, which is also used in SimCSE[^1]. This dataset consists of 275K triplets, and we use a subset of 40$\sim$`<!-- -->`{=html}80K samples based on the performance on the development set.

For evaluation, we use seven standard Semantic Textual Similarity (STS) datasets, including STS 2012-2016 [@agirre-etal-2012-semeval; @agirre-etal-2013-sem; @agirre-etal-2014-semeval; @agirre-etal-2015-semeval; @agirre-etal-2016-semeval] , STS-B [@cer-etal-2017-semeval], and SICK-R [@marelli-etal-2014-sick].

**Baselines.** Following @fu-etal-2025-token, we compare our [SemPA]{.smallcaps} with the following baselines: **BERT-avg** [@devlin-etal-2019-bert] averages the last-layer token embeddings from BERT to form a sentence embedding. **Sentence-T5-avg** [@ni-etal-2022-sentence] uses the mean pooling of token representations from the T5 encoder. **PromptBERT** [@jiang-etal-2022-promptbert] represents sentences with the `[MASK]` token using pre-defined prompts. **SBERT** [@reimers-gurevych-2019-sentence] fine-tunes a BERT model with siamese networks and refines its semantic representations. **LLM2Vec** [@behnamghader2024llmvec] converts decoder-only LLMs into bidirectional encoders by modifying the attention pattern and performs contrastive learning. **Echo Embedding** [@springer2025repetition] forms sentence representations by repeating the input twice and pooling only the hidden states from the second occurrence. **PromptEOL** [@jiang-etal-2024-scaling] designs prompts and extracts the last-token hidden state as semantic representation from LLMs. **Contrastive Prompting** [@cheng-etal-2025-contrastive] introduces an auxiliary prompt to contrastively intervene in the original prompt's representations, thereby suppressing redundant information. **Token Prepending** [@fu-etal-2025-token] prepends the per-layer sentence embedding to the next layer's input, allowing early tokens to attend to full-sentence context under causal attention. For the Contrastive Prompting and Token Prepending baselines, PromptEOL is also applied for embedding extraction and fair comparison.

<figure id="fig:SP" data-latex-placement="t!">
<embed src="figs/SP12.pdf" />
<figcaption>Performance of LLaMA models on the STS-B dev set and STS test sets with different training data sizes. STS test results are averaged over all STS tasks.</figcaption>
</figure>

<figure id="fig:total" data-latex-placement="t!">

<figcaption>Qualitative visualization of embedding space using LLaMA2 (left) and LLaMA3 (right). Our method results in a more isotropic embedding space.</figcaption>
</figure>

**Models and Implementation Details.** Considering the reported results of the aforementioned baselines, we use LLaMA2-7B [@touvron2023llama2openfoundation] and LLaMA3-8B [@grattafiori2024llama3herdmodels] as base models. We apply LoRA [@hu2022lora] fine-tuning on four NVIDIA RTX 5090 GPUs to train our models. See Appendix [8](#app:moredetails){reference-type="ref" reference="app:moredetails"} for details.

::: table*
:::

## Main Results

The results are shown in Table [\[tab:sts-results\]](#tab:sts-results){reference-type="ref" reference="tab:sts-results"}. Overall, our method achieves the highest score on 5 out of 7 datasets with LLaMA2-7B, and on 6 out of 7 datasets when leveraging the LLaMA3-8B backbone, indicating that our proposed semantic preference alignment method can indeed enhance the semantic representation capability of generative large language models.

Our method, based on LoRA fine-tuning (with only 0.3% of the 7B model parameters being trainable), outperforms smaller-scale encoder models such as SBERT. This indicates that, in terms of performance, generative semantic representation can effectively replace traditional representation schemes. On the other hand, compared with recent LLM-based approaches such as Token Prepending, our method achieves performance improvements of 0.5$\sim$`<!-- -->`{=html}2.8 points without requiring modifications to the model architecture or computation process.

Across different backbone LLMs, we find that our method performs better with the more advanced LLaMA3-8B than with LLaMA2-7B, showing more substantial improvements over the baselines. This indicates that our approach can further exploit the intrinsic semantic representation capability as the base model's capacity increases.

# Analyses {#section:analysis}

**Impact of Data Size.** Figure [3](#fig:SP){reference-type="ref" reference="fig:SP"} shows the results obtained using different amounts of training data for a simple epoch. We find that both LLaMA2 and LLaMA3 achieve their highest Spearman correlation scores when trained with 40K to 80K NLI samples, and the best scores for both exceed 77. When trained with 100K or more samples, performance declines slightly and then plateaus, indicating some degree of over-alignment in paraphrase generation. Nonetheless, performance remains mostly above 73, outperforming the PromptEOL baseline. Overall, these results demonstrate that our method effectively enhances the semantic representations of generative LLMs in a data-efficient manner, without relying on large-scale, high-quality data for contrastive learning or instruction tuning.

**Visualization of Embedding Space.** []{#section:visualize_embedidng label="section:visualize_embedidng"} Representations of neural models often exhibits an anisotropic distribution, especially for generative LLMs, which limits their semantic expressiveness [@ethayarajh-2019-contextual; @li-etal-2020-sentence]. Figure [4](#fig:total){reference-type="ref" reference="fig:total"} shows the PCA visualization of embeddings from randomly selected 0.2% instances from the NLI dataset.

Results show that sentence embeddings derived via mean pooling suffer from a severe anisotropy issue, with the majority of sentence vectors clustered in a specific region of the embedding space. In comparison, PromptEOL and our [SemPA]{.smallcaps} mitigate this issue to a certain extent and generates embeddings of superior quality, where positive and negative samples are more clearly separated.

We further use two metrics that assess the representation quality for comparison: 1) *uniformity* introduced by @pmlr-v119-wang20k, and 2) *isotropy score* proposed by @mu2018allbutthetop. Details of these two metrics are shown in Appendix [9](#app:metrics){reference-type="ref" reference="app:metrics"}. Results in Table [\[tab:align_uniform\]](#tab:align_uniform){reference-type="ref" reference="tab:align_uniform"} demonstrate that [SemPA]{.smallcaps} outperforms both the original model and the PromptEOL baseline across the two metrics, indicating that [SemPA]{.smallcaps} can enhance semantic expressiveness by improving the model's representation space.

:::: table*
::: tabular
\@lcccccc@ \***Models** & **GSM8K** & **MMLU** & **HellaSwag** & **DROP** & **TruthfulQA** & \***Avg.**\
& Acc. &Acc. & Acc_norm & F1 & MC1 &\
LLaMA2-7B & 14.03 & 45.91 & 76.13 & 40.23 & 24.85 & 40.23\
*w/* Contrastive Learning & 3.79 [($\downarrow$`<!-- -->`{=html}10.24)]{style="color: magenta"} & 46.12 [($\uparrow$`<!-- -->`{=html}0.21)]{style="color: teal"} & 73.50 [($\downarrow$`<!-- -->`{=html}2.63)]{style="color: magenta"} & 15.92 [($\downarrow$`<!-- -->`{=html}24.31)]{style="color: magenta"} & 23.87 [($\downarrow$`<!-- -->`{=html}0.98)]{style="color: magenta"} & 32.64 [($\downarrow$`<!-- -->`{=html}7.59)]{style="color: magenta"}\
***w/* Semantic DPO (Ours)** & 11.68 [($\downarrow$`<!-- -->`{=html}2.35)]{style="color: magenta"} & 47.43 [($\uparrow$`<!-- -->`{=html}1.52)]{style="color: teal"} & 79.99 [($\uparrow$`<!-- -->`{=html}3.86)]{style="color: teal"} & 31.02 [($\downarrow$`<!-- -->`{=html}9.21)]{style="color: magenta"} & 35.62 [($\uparrow$`<!-- -->`{=html}10.77)]{style="color: teal"} & 41.15 [($\uparrow$`<!-- -->`{=html}0.92)]{style="color: teal"}\
LLaMA3-8B & 55.80 & 64.96 & 79.22 & 58.99 & 27.29 & 57.25\
*w/* Contrastive Learning & 9.86 [($\downarrow$`<!-- -->`{=html}45.94)]{style="color: magenta"} & 62.68 [($\downarrow$`<!-- -->`{=html}2.28)]{style="color: magenta"} & 75.44 [($\downarrow$`<!-- -->`{=html}3.78)]{style="color: magenta"} & 25.02 [($\downarrow$`<!-- -->`{=html}33.97)]{style="color: magenta"} & 24.72 [($\downarrow$`<!-- -->`{=html}2.57)]{style="color: magenta"} & 39.54 [($\downarrow$`<!-- -->`{=html}17.71)]{style="color: magenta"}\
***w/* Semantic DPO (Ours)** & 52.01 [($\downarrow$`<!-- -->`{=html}3.79)]{style="color: magenta"} & 64.71 [($\downarrow$`<!-- -->`{=html}0.25)]{style="color: magenta"} & 82.56 [($\uparrow$`<!-- -->`{=html}3.34)]{style="color: teal"} & 60.93 [($\uparrow$`<!-- -->`{=html}1.94)]{style="color: teal"} & 36.47 [($\uparrow$`<!-- -->`{=html}9.18)]{style="color: teal"} & 59.34 [($\uparrow$`<!-- -->`{=html}2.09)]{style="color: teal"}\
:::
::::

**Impact of Templates.** []{#subsec:template label="subsec:template"} We test the performance using different paraphrase generation templates $\mathcal{T}^{\rm para}$ and the embedding extraction templates $\mathcal{T}$. To guide LLMs in generating paraphrases, we design multiple instructions that include phrases like "Generate a paraphrase \...", "Keep the meaning \...", or "Rewrite the sentence \...", supplemented by constraints to promote syntactic diversity. For embedding extraction, besides the PromptEOL baseline, we also use a chain-of-thought enhanced template Pretended CoT [@10.1007/978-981-97-5669-8_5] as follows:

> *After thinking step by step, this sentence: "$\mathcal{S}$" means in one word:"*

The averaged results on seven STS datasets using LLaMA2 and LLaMA3 are shown in Table [\[tab:compare_prompt\]](#tab:compare_prompt){reference-type="ref" reference="tab:compare_prompt"}. We observe that when using PromptEOL as the sentence embedding extraction method, all five paraphrase generation templates we designed lead to significant improvements in model performance, elevating the baseline scores from 70.03/71.90 to a minimum of 74.62/76.73 and a maximum of 77.69/78.10. This demonstrates that our semantic alignment approach is generally stable and effective across different templates.

When Pretended CoT is used as the sentence embedding extraction template, we observe varying degrees of performance decline when fine-tuning the LLaMA2 model. For instance, the impact is minimal with the first two templates, while the latter three templates show a drop of 1$\sim$`<!-- -->`{=html}4 points. On the more advanced LLaMA3 model, however, our method consistently improves performance across all paraphrase generation templates. This discrepancy may stem from differences in the semantic representations elicited by CoT prompting across different base models, which could be a direction worth further exploration in future work.

**Top-10 Aligned Tokens in Vocabulary.** @nie-etal-2025-text finds that the tokens mapped by the embeddings from PromptEOL tend to align with key tokens from the original inputs, which helps explain the effectiveness of sentence embeddings from generative LLMs. Following their analysis, we show examples of the top-10 aligned tokens for three different texts using the LLaMA2-7B backbone in Table [\[tab:aligned_tokens\]](#tab:aligned_tokens){reference-type="ref" reference="tab:aligned_tokens"}. These cases show that the embeddings of PromptEOL still align with some unrelated and meaningless tokens such as `<0x0A>`, which may be due to the lack of semantic refinement. In contrast, after lightweight semantic preference alignment, our method makes the aligned tokens more accurate. The GAR scores (see detailed definition in Appendix [10](#app:gar){reference-type="ref" reference="app:gar"}) proposed by @nie-etal-2025-text also show that our method indeed increases overall coverage of tokens at the dataset level.

**Impact to Generation Capability of LLMs.**

To verify that the model fine-tuned with our proposed method can enhance its semantic representation ability while preserving its inherent generative capability, we adopt a variety of benchmarks across multiple dimensions, including mathematical reasoning (**GSM8K**; [@cobbe2021trainingverifierssolvemath]), multitask language understanding (**MMLU**; [@hendrycks2021measuring]), commonsense reasoning (**HellaSwag**; [@zellers-etal-2019-hellaswag]), reading comprehension (**DROP**; [@dua-etal-2019-drop]), and response reliability (**TruthfulQA**; [@lin-etal-2022-truthfulqa]).

The results are shown in Table [\[tab:generative-capacity\]](#tab:generative-capacity){reference-type="ref" reference="tab:generative-capacity"}. We find that after token-level optimization based on contrastive learning, the model's performance declines across almost all tasks (by 7.59 on LLaMA2-7B and 17.71 on LLaMA3-8B). This indicates that contrastive learning methods tend to cause the model to degenerate into a purely representational model, thereby sacrificing the original generative capabilities of the language model.

In contrast, our preference alignment approach remains a lightweight, sentence-level optimization method that does not significantly affect performance on these general tasks. Moreover, we observe that on the TruthfulQA dataset, our semantically aligned optimization improves the accuracy of the base model by 9.18$\sim$`<!-- -->`{=html}10.77 points. This reflects the inherent impact of semantic representation on certain downstream tasks.

# Conclusion

We introduce [SemPA]{.smallcaps}, a sentence-level semantic preference alignment method to obtain better sentence embeddings from LLMs. We establish a theoretical connection between contrastive learning and our preference alignment method for improving sentence representations. Empirically, we evaluate [SemPA]{.smallcaps} and other approaches on a series of semantic textual similarity tasks and LLM-related benchmarks. The results show that [SemPA]{.smallcaps} enables better sentence embeddings compared with various baselines, without sacrificing the intrinsic generative capabilities of LLMs. Analysis of the embedding space and the aligned tokens of sentences shows that [SemPA]{.smallcaps} can alleviate the embedding space anisotropy and reflect the information in sentences more accurately.

# Limitations {#limitations .unnumbered}

This study preliminarily demonstrates through lightweight experiments that semantic preference optimization enhances the semantic representation of generative LLMs. Further improvements and exploration could be pursued in terms of the quantity and quality of the dataset, or automatic prompt optimization methods to enhance robustness. Additionally, our method employs pairwise DPO for preference optimization, which only considers binary comparisons between two responses. Recent advances such as LiPO [@liu-etal-2025-lipo] demonstrate that listwise preference optimization can better leverage the complete ranking information among multiple candidates through sophisticated weighting mechanisms. Future work could integrate listwise ranking objectives to capture more nuanced semantic relationships.

# More Implementation Details {#app:moredetails}

We implement our DPO training using the SWIFT framework [@Zhao_Huang_Hu_Wang_Mao_Zhang_Jiang_Wu_Ai_Wang_Zhou_Chen_2025]. We use four NVIDIA RTX 5090 GPUs to train LLMs with LoRA fine-tuning, setting LoRA modules to all linear layers except the language model head, with rank $r = 8$ and scaling factor $\alpha = 32$. Additionally, we set the per-step batch size to 8 and the gradient accumulation steps to 8, resulting in an effective batch size of 256. For optimization, we employ the AdamW optimizer with a learning rate schedule that includes a linear warmup phase with a ratio of 0.05, peaks at $1\times10^{-4}$, and is followed by a cosine annealing schedule. A checkpoint is saved every 20K training samples to enable subsequent investigations into the relationship between data size and performance.

# Uniformity and Isotropy Score {#app:metrics}

## Uniformity

The uniformity metric serves as a quantitative measure for evaluating embedding quality. It assesses the extent to which the embedding distribution is roughly uniform on the unit hypersphere. Preserving such uniformity is essential for maintaining maximal information and effectively utilizing the feature space, attributes that have been shown to correlate strongly with performance on downstream tasks.

**Theoretical Basis.** Given the data distribution $p_{\text {data}}$, it is formally defined as: $$\begin{equation}
 \ell_{\text {uniform}} \triangleq \log \mathop{\mathbb{E}}_{\substack{x, y \sim p_{\text {data}} \\ \text{i.i.d.}}} e^{-2 \lVert f(x) - f(y) \rVert^2}.
\end{equation}$$

**Interpretation.** A lower value of $\ell_{\text {uniform}}$ indicates that the embeddings are more spread out and isotropic. This uniformity is desirable because it ensures that the feature space is utilized to its full capacity. When features are uniformly distributed, they are more likely to be linearly separable and contain maximal entropy, which corresponds to preserving the most information from the data.

## Isotropy Score

The isotropy score provides a quantitative measure of how uniformly distributed word embeddings are in the high-dimensional space. It is rigorously defined based on the partition function $Z(c)$, which aggregates the contributions of all sentence vectors in a specific direction $c$.

**Rigorous Definition.** Given a set of sentence representations $\{v(s) : s \in V\}$, where $v(s) \in \mathbb{R}^d$, the partition function for a unit vector $c$ is defined as: $$\begin{equation}
    Z(c) = \sum_{s \in V} \exp(c^\top v(s)).
\end{equation}$$

The isotropy score $I(\{v(s)\})$ is then defined as the ratio of the minimum to the maximum value of this partition function over all possible unit vectors: $$\begin{equation}
I(\{v(s)\}) = \frac{\min_{\lVert c\rVert=1} Z(c)}{\max_{\lVert c\rVert=1} Z(c)}.
\end{equation}$$

Intuitively, if the embeddings are perfectly isotropic, $Z(c)$ should be a constant independent of the direction $c$, implying $I = 1$, while a score closer to 0 indicates a stronger anisotropy.

**Empirical Verification via Eigenvectors.** Since there is no closed-form solution for $\arg\max_{\lVert c\rVert=1} Z(c)$ or $\arg\min_{\lVert c\rVert=1} Z(c)$, we employ an empirical verification strategy based on the eigenvectors of the covariance matrix. We restrict the candidate directions $c$ to the eigenvectors of $V^\top V$, where $V$ is the matrix of all sentence vectors. Let $\{u_1, \dots, u_d\}$ be the eigenvectors of $V^\top V$. The isotropy score is then estimated by computing the partition function for each eigenvector, denoted as $Z(u_j) = \sum_{s \in V} \exp(u_j^\top v(s))$, and taking the ratio of the minimum to maximum values observed among these eigenvectors: $$\begin{equation}
    I_{\text {approx}} = \frac{\min_{j} Z(u_j)}{\max_{j} Z(u_j)}.
\end{equation}$$

# Comparison of GAR Score {#app:gar}

We adopt the global alignment rate (GAR) score proposed by @nie-etal-2025-text to measure, at the dataset level, how well the aligned tokens of the model's embeddings cover the real tokens. Given an input sentence $s_i$, we first tokenize it using the tokenizer of the model and deduplicate the resulting token sequence to obtain the surface token set $T_{s_i}$. We then compute the top-$K_i$ aligned tokens of the sentence embedding and form the aligned token set $\hat{T}^{K_i}_{s_i}$. The intersection $\hat{T}^{K_i}_{s_i}\cap T_{s_i}$ indicates which tokens in the top-$K_i$ aligned tokens also appear in the surface token set. We perform the same procedure for every sentence in the dataset $D$ and take the union of the resulting "hit token sets" $\textstyle\bigcup_{i=1}^{|D|} (\hat{T}^{K_i}_{s_i} \cap T_{s_i})$. The cardinality of this union represents the number of distinct surface token types that are hit by alignment across the dataset. On the other hand, $\textstyle\bigcup_{i=1}^{|D|} T_{s_i}$ gives the set of all surface token types appearing in the dataset. Therefore, GAR is defined as the ratio of the sizes (cardinalities) of the two sets and measures the global coverage of surface tokens by the aligned tokens at the dataset level: $$\begin{equation}
\tag{11}
{\text {GAR}}
=
\frac{\left|\bigcup_{i=1}^{|D|}\left(\hat{T}^{K_i}_{s_i}\cap T_{s_i}\right)\right|}
{\left|\bigcup_{i=1}^{|D|} T_{s_i}\right|}.
\end{equation}$$

We randomly sample 10K of the 1M Wikipedia texts provided by @gao-etal-2021-simcse as the dataset $D$, set $K=10$ for all sentences, and compute GAR for LLaMA2-7B before and after fine-tuning following the above procedure. The GAR score increases from $0.185$ (pre-finetuning) to $0.324$ (post-finetuning). This result indicates that our method substantially improves the dataset-level coverage of surface token types by the model's top-$K$ aligned tokens and thus increases the diversity of aligned surface tokens.

# Use of AI Assistants

We used AI tools solely to assist with translation and language polishing (e.g., wording and grammar suggestions). The tools were not used to generate research conclusions, experimental results, or core technical content.

[^1]: <https://huggingface.co/datasets/princeton-nlp/datasets-for-simcse/blob/main/nli_for_simcse.csv>
