---
uuid: 00000000-0000-0000-0000-000000000000
title: Sonnet 4.5 Lexic OS
section: research
bucket: 2025-12/Week-52
author: System
priority: Medium
created: 2025-12-26 06:39
modified: 2026-01-06 13:09
status: Active
summary: Legacy research document migrated to canon format
tags:
- research
- legacy
hashtags: []
---
<!-- CONTENT_HASH: 5beda3f628832c85a8232c705acdf25fa5a30ae36873928fba447a71c45c8ef8 -->

# LEXIC: Language-Executable Intelligence Coordination

**A governance framework for durable AI collaboration**

Version: 1.0  
Status: Production-Ready Architecture  
Origin: Extracted from real-world static site generation system (5.2.36)

---

## What This Is

LEXIC is an **operating system for AI-native projects** where text is law, code is consequence, and drift is mechanically prevented.

It solves the fundamental problem: **How do you preserve human intent through multiple AI agents, model switches, and years of iteration?**

### Core Innovation

Most systems treat documentation as commentary on code. LEXIC inverts this:

- **Canon** = executable law (enforced, not suggested)
- **Fixtures** = precedents (proof, not tests)
- **Tokens** = stable semantics (symbolic interface)
- **Packs** = portable cognition (context handoff)

### When You Need This

- Projects lasting months/years with multiple AI sessions
- Complex systems where "helpful" AI changes break things
- Work requiring strict governance (compliance, publishing, technical specs)
- Any domain where human intent must survive automated execution
- Contexts where decisions must be explainable months/years later

### Continuity Problem Solved

**Without LEXIC:**
```
Session 1: "Make buttons blue, not green - green feels too playful"
[AI changes buttons to blue]

[Context lost - new session]

Session 2: AI suggests green buttons (no memory of why blue was chosen)
Human: "Didn't we already decide on blue?"
AI: "I don't have that context..."
```

**With LEXIC:**
```
Session 1: "Make buttons blue, not green - green feels too playful"
[AI changes buttons to blue]
[AI creates ADR-001: Button colors (blue chosen, green rejected)]

[Context lost - new session]

Session 2: AI queries CONTEXT before suggesting changes
AI finds: ADR-001 states green was rejected as "too playful"
AI: "I see button colors are locked to blue per ADR-001. Should we reconsider?"
```

**Three types of continuity:**
- **Structural:** System doesn't break (CANON + CONTRACTS)
- **Cognitive:** Decisions compound (CONTEXT)
- **Temporal:** State survives handoff (MEMORY)

All three are required for durable AI collaboration.

### When You Don't

- Simple scripts or one-off tasks
- Exploratory prototyping (before locking patterns)
- Systems with no stability requirements

---

## Architecture

### Problem-Solution Map

| Layer | Solves | Without It |
|-------|--------|------------|
| **CANON** | "What are the rules?" | AI breaks locked patterns, rewrites working code |
| **CONTEXT** | "Why did we decide this?" | Every session re-argues settled decisions |
| **MAPS** | "Where do I make changes?" | AI edits wrong files, creates tangled dependencies |
| **SKILLS** | "What can I do?" | Monolithic agents, unclear boundaries |
| **CONTRACTS** | "Did I break anything?" | Silent regressions, broken builds |
| **MEMORY** | "What's the current state?" | Context loss, starting from scratch |

### The Six Layers

```
/CANON/          Authority (what's true, what never changes)
/CONTEXT/        Wisdom (why decisions were made, what was rejected)
/MAPS/           Navigation (where to change what)
/SKILLS/         Capabilities (modular, versioned agents)
/CONTRACTS/      Enforcement (executable proofs)
/MEMORY/         Snapshots (portable state dumps)
```

**Key Distinction:**
- CANON = rules that **must** be followed
- CONTEXT = wisdom that **should** inform decisions
- MEMORY = current state (what exists now)

---

## /CANON/ â€” Authority Layer

**Purpose:** Define truth and how it changes.

### Required Files

#### `CONTRACT.md`
The "social contract" between human and AI.

```markdown
# System Contract

## Core Rules (Non-Negotiable)
1. Do not redesign X without explicit instruction
2. Source of truth is Y
3. Do not introduce Z
4. Changes must preserve compatibility with W
5. Update CHANGELOG.md for any behavior change

## Authority Gradient
If documents conflict, precedence order is:
1. CONTRACT.md
2. INVARIANTS.md
3. ROADMAP.md
4. Implementation code

## Canon Governance (Locked)
Definition: A "system behavior change" is any alteration to...

Required sequence:
1. Implement + verify with fixtures
2. Run validation (must PASS)
3. Update canon in same commit
4. No behavior change without canon update

Landing Checklist:
- [ ] Behavior change? (yes/no)
- [ ] Fixture added/updated?
- [ ] Canon updated?
- [ ] Validation PASS?
```

#### `INVARIANTS.md`
Locked decisions that must never change.

```markdown
# System Invariants

## Core Approach
- [Decision locked on DATE]
- Rationale: ...
- Never: ...

## Naming Conventions
- [Pattern locked on DATE]
- Rationale: ...
- Never: ...

## File Structure
- [Structure locked on DATE]
- Rationale: ...
- Never: ...

Update rule: Any change to this file requires:
1. Explicit approval
2. Migration plan
3. CHANGELOG entry with rationale
```

#### `CHANGELOG.md`
What changed and why (newest first).

```markdown
# Changelog

## [1.2.3] - 2025-12-20
- Added X capability
- Rationale: Needed for Y use case
- Breaking: Z now requires W

## [1.2.2] - 2025-12-19
- Fixed X behavior
- No breaking changes
```

---

## /CONTEXT/ â€” Wisdom Layer

**Purpose:** Capture why decisions were made, what was rejected, and what's still open.

**Philosophy:** CANON tells you what you can't do. CONTEXT tells you what you learned. Without context, every AI session starts from zero knowledge. With context, decisions compound.

### Structure

```
/CONTEXT/
  /decisions/
    ADR-001-button-colors.md
    ADR-002-navigation-structure.md
    ADR-003-api-versioning.md
  /preferences/
    STYLE-001-tone-and-voice.md
    STYLE-002-error-handling.md
  /rejected/
    REJECT-001-graphql-approach.md
    REJECT-002-microservices.md
  /open/
    OPEN-001-mobile-strategy.md
    OPEN-002-search-implementation.md
  INDEX.md          (queryable index)
  query-context.py  (search tool)
```

### Decision Record Template (ADR)

```markdown
# ADR-XXX: [Decision Title]

**Status:** [Accepted | Superseded | Deprecated | Experimental]  
**Date:** YYYY-MM-DD  
**Review:** YYYY-MM-DD (when to reconsider)  
**Confidence:** [High | Medium | Low]  
**Impact:** [Critical | High | Medium | Low]  
**Tags:** [design, architecture, ux, performance, ...]

## Context

What is the situation forcing this decision?
- Problem statement
- Constraints (time, budget, technical)
- Stakeholders affected

## Decision

What are we doing?
- Specific choice made
- Key parameters/configuration
- Scope and boundaries

## Alternatives Considered

What else did we evaluate?

### Option A: [Name]
- **Pros:** 
- **Cons:**
- **Why rejected:**

### Option B: [Name]
- **Pros:**
- **Cons:**
- **Why rejected:**

## Rationale

Why did we choose this over alternatives?
- Key factors in priority order
- Trade-offs accepted
- Assumptions made

## Consequences

What changes because of this?
- **Positive:**
- **Negative:**
- **Neutral:**

## Implementation Notes

How is this enforced?
- Related canon: `CANON/INVARIANTS.md` section X
- Related fixtures: `CONTRACTS/fixtures/example-*`
- Related code: `src/module.py` lines 123-145

## Evidence

What data informed this?
- User testing results
- Performance benchmarks
- Expert consultation
- Prior art examples

## Review Triggers

When should we reconsider?
- If X metric falls below Y
- When technology Z becomes available
- After N months of usage data
- If team size exceeds M people

---
*Supersedes: ADR-050 (deprecated 2025-01-15)*  
*Superseded by: [none yet]*
```

### Preference Record Template

```markdown
# STYLE-XXX: [Preference Title]

**Type:** [Tone | Format | Pattern | Convention]  
**Date:** YYYY-MM-DD  
**Confidence:** [Strong | Medium | Weak]  
**Tags:** [writing, code-style, ui, ...]

## Preference

[Clear statement of preference]

Example:
> Use active voice in documentation. Prefer "The system processes X" over "X is processed by the system."

## Rationale

Why this preference?
- Aligns with goal X
- Improves metric Y
- Team consensus from discussion Z

## Counter-Examples

What to avoid:

**Don't:**
```
[example of what not to do]
```

**Do:**
```
[example of preferred approach]
```

## Exceptions

When is it okay to violate this?
- Exception case A
- Exception case B

## Strength

How strongly do we hold this?
- **Strong:** Block changes that violate this
- **Medium:** Encourage but don't require
- **Weak:** Slight preference, easily overridden
```

### Rejected Approach Template

```markdown
# REJECT-XXX: [Approach Name]

**Date Rejected:** YYYY-MM-DD  
**Proposed By:** [Person/Session]  
**Tags:** [category, ...]

## What Was Proposed

[Clear description of the rejected approach]

## Why It Seemed Good

- Benefit 1
- Benefit 2
- Benefit 3

## Why We Rejected It

**Primary reason:** [Most important factor]

**Additional reasons:**
- Factor A
- Factor B
- Factor C

## Conditions for Reconsidering

If any of these change, revisit this decision:
- [ ] Condition 1
- [ ] Condition 2
- [ ] Condition 3

## Related Decisions

- **Chosen instead:** ADR-XXX
- **Similar rejection:** REJECT-YYY
```

### Open Question Template

```markdown
# OPEN-XXX: [Question Title]

**Opened:** YYYY-MM-DD  
**Priority:** [Critical | High | Medium | Low]  
**Blocking:** [List of blocked work]  
**Tags:** [category, ...]

## Question

[Clear statement of the unresolved question]

## Why It Matters

- Impact A if answered one way
- Impact B if answered another way

## Options Being Considered

1. **Option A:**
   - Pros:
   - Cons:
   - What we need to know:

2. **Option B:**
   - Pros:
   - Cons:
   - What we need to know:

## Research Needed

- [ ] Task 1 to inform decision
- [ ] Task 2 to inform decision
- [ ] Task 3 to inform decision

## Decision Deadline

Must decide by: [DATE]  
Reason: [Why this deadline]

## Discussion Log

**[DATE]:** Brief note of discussion  
**[DATE]:** Update or new information  
```

### Context Index (INDEX.md)

```markdown
# Context Index

**Last Updated:** 2025-12-20  
**Total Records:** 47

## Quick Reference

### Active Decisions (Status: Accepted)
- ADR-001: Button Colors â†’ Blue (#4A90E2)
- ADR-002: Navigation â†’ Flat, max 2 levels
- ADR-003: API Versioning â†’ Semantic, URL-based

### Critical Preferences
- STYLE-001: Active voice in docs
- STYLE-002: Error messages include solution

### Recently Rejected (Last 90 Days)
- REJECT-001: GraphQL (complexity vs benefit)
- REJECT-002: Microservices (team too small)

### Open Questions (Priority: High)
- OPEN-001: Mobile-first vs responsive?
- OPEN-002: Search: Client-side or server?

## By Category

### Architecture
- ADR-003: API Versioning
- REJECT-002: Microservices

### Design
- ADR-001: Button Colors
- STYLE-003: Icon System

### Performance
- ADR-015: Caching Strategy
- OPEN-003: CDN Provider

## By Status

### Accepted (34)
[list...]

### Experimental (3)
[list...]

### Deprecated (8)
[list...]

### Superseded (2)
[list...]

## Review Schedule

### Due This Month
- ADR-001: Review by 2025-12-25
- ADR-007: Review by 2025-12-30

### Overdue Review
- ADR-004: Review was 2025-11-15 âš ï¸

## Supersession Chain

```
ADR-050 â†’ ADR-045 â†’ ADR-023 (current)
ADR-012 â†’ ADR-018 (current)
```
```

### Context Query Tool

```python
#!/usr/bin/env python3
"""
Context Query Tool - Search and retrieve decision context.

Usage:
  python CONTEXT/query-context.py "button colors"
  python CONTEXT/query-context.py --tag design --status accepted
  python CONTEXT/query-context.py --review-due 30  # due in next 30 days
  python CONTEXT/query-context.py --superseded  # show supersession chains
"""

import sys
import re
import json
from pathlib import Path
from datetime import datetime, timedelta
from dataclasses import dataclass
from typing import List, Optional

@dataclass
class ContextRecord:
    """Represents a single context record."""
    id: str
    type: str  # ADR, STYLE, REJECT, OPEN
    title: str
    status: Optional[str]
    date: datetime
    review: Optional[datetime]
    confidence: Optional[str]
    impact: Optional[str]
    tags: List[str]
    path: Path
    content: str

def parse_record(path: Path) -> Optional[ContextRecord]:
    """Parse a context record file."""
    content = path.read_text()
    
    # Extract metadata from markdown
    metadata = {}
    for line in content.split('\n')[:20]:  # Check first 20 lines
        if match := re.match(r'\*\*(\w+):\*\*\s*(.+)', line):
            key, value = match.groups()
            metadata[key.lower()] = value.strip()
    
    # Extract tags
    tags = []
    if 'tags' in metadata:
        tags = [t.strip() for t in metadata['tags'].strip('[]').split(',')]
    
    # Parse dates
    date = None
    if 'date' in metadata:
        try:
            date = datetime.strptime(metadata['date'], '%Y-%m-%d')
        except ValueError:
            pass
    
    review = None
    if 'review' in metadata:
        try:
            review = datetime.strptime(metadata['review'], '%Y-%m-%d')
        except ValueError:
            pass
    
    # Extract title from first heading
    title_match = re.search(r'^#\s+(.+?)

**Purpose:** Tell agents where to look and what to change.

### Required Files

#### `ENTRYPOINTS.md`
Where to change what.

```markdown
# Entrypoints

## Rules / Governance
- `CANON/CONTRACT.md`
- `CANON/INVARIANTS.md`
- `CANON/CHANGELOG.md`

## Core Logic
- `src/main.py` (primary business logic)
- `src/parser.py` (input processing)

## Output Templates
- `templates/default.html`
- `templates/special.html`

## Configuration
- `config.json` (never hardcode paths)

## Tests / Validation
- `tests/fixtures/**`
- `tests/runner.py`
```

#### `SYSTEM_MAP.md`
High-level mental model of subsystems.

```markdown
# System Map

## Subsystems

### Input Pipeline
- Purpose: Transform user data into canonical format
- Entrypoint: `src/ingest.py`
- Constraints: Must validate against schema X

### Processing Engine
- Purpose: Apply business logic
- Entrypoint: `src/engine.py`
- Constraints: Must be deterministic

### Output Generator
- Purpose: Produce final artifacts
- Entrypoint: `src/output.py`
- Constraints: Must match template structure
```

---

## /SKILLS/ â€” Capability Layer

**Purpose:** Modular, versioned, scoped capabilities.

### Structure

```
/SKILLS/
  /research/
    SKILL.md
    research.py
  /summarize/
    SKILL.md
    summarize.py
  /validate/
    SKILL.md
    validate.py
```

### Skill Contract Template

```markdown
# [Skill Name]

**Version:** 1.0  
**Status:** Stable

## Trigger
When: User requests "research X"
How: Direct invocation or orchestrator routing

## Inputs
- `topic` (string, required)
- `depth` (enum: shallow|medium|deep, default: medium)
- `sources` (list, optional)

## Outputs
- Markdown file in `output/research/YYYY-MM-DD-<topic>.md`
- Citations in `output/research/citations.json`

## Constraints
- Never overwrite existing research files
- Always append timestamp to filename
- Must cite sources inline
- Maximum 5000 tokens per output

## Dependencies
- Requires: web search capability
- Uses: `SKILLS/cite/` for formatting

## Fixtures
- `tests/fixtures/research-basic.json`
- `tests/fixtures/research-deep.json`
```

---

## /CONTRACTS/ â€” Enforcement Layer

**Purpose:** Mechanical proof that behavior matches canon.

### Structure

```
/CONTRACTS/
  /fixtures/
    scenario-1/
      input.json
      expected-output.json
    scenario-2/
      input.md
      expected-output.html
  runner.py
  README.md
```

### Runner Template

```python
#!/usr/bin/env python3
"""
Contract Runner - Enforces canon through deterministic fixtures.

Usage: python contracts/runner.py
Exit: 0 if all fixtures pass, 1 if any fail
"""

import sys
from pathlib import Path

def run_fixture(fixture_path):
    """Run a single fixture and verify output."""
    # Load input
    # Run system
    # Compare output to expected
    # Return pass/fail
    pass

def main():
    fixture_dir = Path(__file__).parent / "fixtures"
    fixtures = sorted(fixture_dir.glob("*/"))
    
    passed = 0
    failed = 0
    
    for fixture in fixtures:
        print(f"Running: {fixture.name}")
        if run_fixture(fixture):
            passed += 1
            print("  âœ“ PASS")
        else:
            failed += 1
            print("  âœ— FAIL")
    
    print(f"\nResults: {passed} passed, {failed} failed")
    return 0 if failed == 0 else 1

if __name__ == "__main__":
    sys.exit(main())
```

### Pre-Commit Hook

```bash
#!/bin/bash
# .git/hooks/pre-commit

echo "Running contract validation..."
python contracts/runner.py

if [ $? -ne 0 ]; then
    echo "âŒ Contract validation failed. Commit rejected."
    exit 1
fi

echo "âœ“ Contract validation passed"
exit 0
```

---

## /MEMORY/ â€” Context Layer

**Purpose:** Portable cognition for AI handoff.

### Packer Template

```python
#!/usr/bin/env python3
"""
Memory Packer - Generate LLM-friendly context snapshot.

Outputs:
- COMBINED/snapshot.md (full verbatim)
- COMBINED/SPLIT/*.md (priority-ordered chunks)
- COMBINED/manifest.json (sources + hashes)
"""

import json
import hashlib
from pathlib import Path
from datetime import datetime

PRIORITY_ORDER = [
    ("01_CANON.md", ["CANON/CONTRACT.md", "CANON/INVARIANTS.md", "CANON/CHANGELOG.md"]),
    ("02_ROADMAP.md", ["ROADMAP.md"]),
    ("03_MAPS.md", ["MAPS/ENTRYPOINTS.md", "MAPS/SYSTEM_MAP.md"]),
    ("04_SKILLS.md", ["SKILLS/*/SKILL.md"]),
    ("05_CONTRACTS.md", ["CONTRACTS/runner.py", "CONTRACTS/fixtures/**"]),
]

def hash_file(path):
    """SHA256 hash of file contents."""
    return hashlib.sha256(Path(path).read_bytes()).hexdigest()

def generate_pack():
    """Generate memory pack with integrity checks."""
    output_dir = Path("COMBINED/SPLIT")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    manifest = {
        "pack_version": datetime.now().isoformat(),
        "canon_version": "1.0",  # Read from CANON/INVARIANTS.md
        "source_hashes": {},
        "split_files": []
    }
    
    for split_file, source_patterns in PRIORITY_ORDER:
        sources = []
        for pattern in source_patterns:
            sources.extend(Path(".").glob(pattern))
        
        content = []
        for src in sorted(sources):
            if src.is_file():
                manifest["source_hashes"][str(src)] = hash_file(src)
                content.append(f"# {src}\n\n{src.read_text()}\n\n---\n")
        
        split_path = output_dir / split_file
        split_path.write_text("".join(content))
        manifest["split_files"].append(str(split_path))
    
    Path("COMBINED/manifest.json").write_text(
        json.dumps(manifest, indent=2)
    )
    
    print(f"âœ“ Generated pack: {manifest['pack_version']}")

if __name__ == "__main__":
    generate_pack()
```

### Pack Load Order

When loading a pack into an AI context:

```
1. Load CANON first (establishes authority)
2. Load ROADMAP (current focus)
3. Load MAPS (navigation)
4. Load SKILLS (available capabilities)
5. Load CONTRACTS (verification examples)
```

---

## /TOOLS/ â€” Governance Tools

### Canon Governance Check

```python
#!/usr/bin/env python3
"""
Canon Governance Check - Verify behavior/canon synchronization.

Run: python tools/check-canon-governance.py
Exit: 0 if synchronized, 1 if drift detected
"""

import sys
from pathlib import Path
from datetime import datetime, timedelta

def check_canon_sync():
    """Verify canon was updated in recent commits."""
    canon_files = [
        "CANON/CONTRACT.md",
        "CANON/INVARIANTS.md", 
        "CANON/CHANGELOG.md"
    ]
    
    behavior_files = [
        "src/**/*.py",
        "SKILLS/**/*.py",
        "templates/**/*"
    ]
    
    # Check if behavior files changed without canon update
    # Implementation depends on git integration
    
    pass

if __name__ == "__main__":
    sys.exit(check_canon_sync())
```

---

## Quick Start

### 1. Initialize Repository

```bash
mkdir my-project
cd my-project
git init

# Create structure
mkdir -p CANON CONTEXT/{decisions,preferences,rejected,open} MAPS SKILLS CONTRACTS/fixtures MEMORY TOOLS
```

### 2. Create Core Canon

```bash
# Copy templates from this document
cp templates/CONTRACT.md CANON/
cp templates/INVARIANTS.md CANON/
touch CANON/CHANGELOG.md
```

### 3. Define Your System

Edit `CANON/CONTRACT.md`:
- What are your non-negotiable rules?
- What is your source of truth?
- What must never change?

Edit `CANON/INVARIANTS.md`:
- What decisions are locked?
- What patterns are canonical?

### 4. Create First Skill

```bash
mkdir SKILLS/example
cat > SKILLS/example/SKILL.md << 'EOF'
# Example Skill

Version: 1.0

## Trigger
When: User requests "example action"

## Inputs
- input_data (string, required)

## Outputs
- result.txt

## Constraints
- Must validate input
- Must not modify source files
EOF
```

### 5. Add First Fixture

```bash
mkdir -p CONTRACTS/fixtures/example-basic
echo '{"input": "test"}' > CONTRACTS/fixtures/example-basic/input.json
echo '{"output": "expected"}' > CONTRACTS/fixtures/example-basic/expected.json
```

### 6. Set Up Validation

```bash
# Create runner (adapt template above)
python CONTRACTS/runner.py

# Add pre-commit hook
cp templates/pre-commit .git/hooks/
chmod +x .git/hooks/pre-commit
```

---

## Best Practices

### Do

- âœ“ Update CHANGELOG.md for every behavior change
- âœ“ Create ADR before implementing significant changes
- âœ“ Add fixtures before implementing new features
- âœ“ Use the pre-commit hook to catch violations early
- âœ“ Query CONTEXT when restarting work: `python CONTEXT/query-context.py --review-due 30`
- âœ“ Document rejected approaches (prevents rearguing)
- âœ“ Version your Skills explicitly
- âœ“ Generate memory packs before long breaks
- âœ“ Keep canon files under 10KB each (split if needed)
- âœ“ Set review dates on ADRs (3-12 months typical)

### AI-Native Workflow

When an AI agent starts work:

```python
# 1. Load authority
ai.load("CANON/CONTRACT.md")  # What are the rules?
ai.load("CANON/INVARIANTS.md")  # What's locked?

# 2. Check for relevant context
results = query_context("button design")
if results:
    ai.consider(results)  # Learn from past decisions

# 3. Navigate to correct files
ai.load("MAPS/ENTRYPOINTS.md")  # Where should I work?

# 4. Execute with skill
ai.use_skill("design/update_buttons")

# 5. Validate before committing
run_contracts()  # Mechanical enforcement

# 6. Document decision
if significant_change:
    ai.create_adr("Button color refinement")
```

### Don't

- âœ— Change behavior without updating canon
- âœ— Skip fixture validation
- âœ— Hardcode paths (use config files)
- âœ— Mix canon and implementation in same file
- âœ— Let secondary docs become authoritative

### Recovery Pattern

If context is lost, restart by stating:

> We have a LEXIC-governed system. Canon is at CANON/CONTRACT.md. Current phase is in ROADMAP.md. Load in priority order.

---

## Adaptation Guide

### For Different Domains

**Novel Writing:**
```
/CANON/
  CONTRACT.md       (style guide, never rules)
  INVARIANTS.md     (character facts, timeline)
  CHANGELOG.md      (plot revisions)
/CONTEXT/
  decisions/
    ADR-001-protagonist-motivation.md
    ADR-002-narrative-pov.md
  rejected/
    REJECT-001-alternate-ending.md
  open/
    OPEN-001-book-two-setup.md
/SKILLS/
  /draft/           (generate chapters)
  /edit/            (consistency checks)
  /research/        (world building)
```

**Business Operations:**
```
/CANON/
  CONTRACT.md       (brand guidelines)
  INVARIANTS.md     (legal requirements)
  CHANGELOG.md      (policy updates)
/CONTEXT/
  decisions/
    ADR-001-pricing-model.md
    ADR-002-target-market.md
  preferences/
    STYLE-001-customer-communication.md
  rejected/
    REJECT-001-freemium-model.md
/SKILLS/
  /analyze/         (data reports)
  /forecast/        (projections)
  /document/        (compliance docs)
```

**Research:**
```
/CANON/
  CONTRACT.md       (methodology)
  INVARIANTS.md     (definitions, scope)
  CHANGELOG.md      (hypothesis revisions)
/CONTEXT/
  decisions/
    ADR-001-statistical-method.md
    ADR-002-sample-size.md
  rejected/
    REJECT-001-alternative-methodology.md
  open/
    OPEN-001-confounding-variables.md
/SKILLS/
  /literature/      (citation management)
  /analyze/         (statistical analysis)
  /synthesize/      (paper writing)
```

---

## Version History

- **1.0 (2025-12-20):** Initial extraction from static site system 5.2.36
  - Production-ready governance patterns
  - Proven through 36+ iterations of real-world usage
  - Added CONTEXT layer for cognitive continuity
  - Future-proofed with decision records, preference tracking, and review mechanisms

---

## License

MIT License - adapt freely, attribution appreciated.

---

## Origin

Extracted from a production static site generation system that solved:
- Drift prevention across 30+ AI sessions
- Context persistence through model switches
- Intent preservation over 6 months of iteration
- Mechanical enforcement of 100+ authoring rules

The patterns emerged from necessity, not theory.

---

## Support

This is a reference architecture, not a framework. Adapt the patterns to your needs. The core insight is:

**Text is law. Law is executable. Execution is validated. Validation gates change. Wisdom compounds.**

Three types of memory:
- **CANON** = rules (structural continuity)
- **CONTEXT** = decisions (cognitive continuity)  
- **MEMORY** = snapshots (state continuity)

That recursive loop is what makes AI collaboration durable., content, re.MULTILINE)

```
    title = title_match.group(1) if title_match else path.stem
    
    # Determine type from path
    record_type = path.parent.name.upper()
    
    return ContextRecord(
        id=path.stem,
        type=record_type,
        title=title,
        status=metadata.get('status'),
        date=date,
        review=review,
        confidence=metadata.get('confidence'),
        impact=metadata.get('impact'),
        tags=tags,
        path=path,
        content=content
    )

def search_records(
    query: Optional[str] = None,
    tags: Optional[List[str]] = None,
    status: Optional[str] = None,
    review_due_days: Optional[int] = None,
    show_superseded: bool = False
) -> List[ContextRecord]:
    """Search context records with filters."""
    context_dir = Path("CONTEXT")
    records = []
    
    # Gather all records
    for subdir in ["decisions", "preferences", "rejected", "open"]:
        dir_path = context_dir / subdir
        if dir_path.exists():
            for md_file in dir_path.glob("*.md"):
                if record := parse_record(md_file):
                    records.append(record)
    
    # Apply filters
    filtered = records
    
    if query:
        query_lower = query.lower()
        filtered = [r for r in filtered if 
                   query_lower in r.title.lower() or 
                   query_lower in r.content.lower()]
    
    if tags:
        filtered = [r for r in filtered if 
                   any(tag in r.tags for tag in tags)]
    
    if status:
        filtered = [r for r in filtered if 
                   r.status and status.lower() in r.status.lower()]
    
    if review_due_days is not None:
        cutoff = datetime.now() + timedelta(days=review_due_days)
        filtered = [r for r in filtered if 
                   r.review and r.review <= cutoff]
    
    if show_superseded:
        # Extract supersession chains
        chains = extract_supersession_chains(records)
        return chains
    
    return filtered

def extract_supersession_chains(records: List[ContextRecord]) -> List[dict]:
    """Extract chains of superseded decisions."""
    chains = []
    
    for record in records:
        # Look for "Supersedes:" and "Superseded by:" in content
        supersedes = re.search(r'\*Supersedes:\s*(.+?)\*', record.content)
        superseded_by = re.search(r'\*Superseded by:\s*(.+?)\*', record.content)
        
        if supersedes or superseded_by:
            chains.append({
                'id': record.id,
                'title': record.title,
                'supersedes': supersedes.group(1) if supersedes else None,
                'superseded_by': superseded_by.group(1) if superseded_by else None
            })
    
    return chains

def format_output(records: List[ContextRecord], verbose: bool = False):
    """Format search results for display."""
    if not records:
        print("No records found.")
        return
    
    print(f"\nFound {len(records)} record(s):\n")
    
    for record in records:
        print(f"{'='*60}")
        print(f"{record.type}: {record.title}")
        print(f"ID: {record.id}")
        
        if record.status:
            print(f"Status: {record.status}")
        if record.date:
            print(f"Date: {record.date.strftime('%Y-%m-%d')}")
        if record.review:
            print(f"Review: {record.review.strftime('%Y-%m-%d')}")
        if record.confidence:
            print(f"Confidence: {record.confidence}")
        if record.impact:
            print(f"Impact: {record.impact}")
        if record.tags:
            print(f"Tags: {', '.join(record.tags)}")
        
        print(f"Path: {record.path}")
        
        if verbose:
            print(f"\n{record.content}\n")
        else:
            # Show first paragraph
            first_para = record.content.split('\n\n')[1] if '\n\n' in record.content else ""
            if first_para:
                print(f"\n{first_para[:200]}...")
        
        print()

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="Query context records")
    parser.add_argument('query', nargs='?', help="Search query")
    parser.add_argument('--tag', action='append', help="Filter by tag")
    parser.add_argument('--status', help="Filter by status")
    parser.add_argument('--review-due', type=int, metavar='DAYS',
                       help="Show records due for review within N days")
    parser.add_argument('--superseded', action='store_true',
                       help="Show supersession chains")
    parser.add_argument('-v', '--verbose', action='store_true',
                       help="Show full content")
    
    args = parser.parse_args()
    
    results = search_records(
        query=args.query,
        tags=args.tag,
        status=args.status,
        review_due_days=args.review_due,
        show_superseded=args.superseded
    )
    
    if args.superseded:
        # Special formatting for chains
        print("\nSupersession Chains:\n")
        for chain in results:
            print(f"{chain['id']}: {chain['title']}")
            if chain['supersedes']:
                print(f"  â† Supersedes: {chain['supersedes']}")
            if chain['superseded_by']:
                print(f"  â†’ Superseded by: {chain['superseded_by']}")
            print()
    else:
        format_output(results, verbose=args.verbose)

if __name__ == "__main__":
    main()
```

### Context Review Tool

```python
#!/usr/bin/env python3
"""
Context Review Tool - Flag stale decisions and enforce review schedule.

Usage:
  python CONTEXT/review-context.py          # Show all overdue reviews
  python CONTEXT/review-context.py --due 30 # Show reviews due in 30 days
  python CONTEXT/review-context.py --fix    # Update INDEX.md
"""

import sys
from pathlib import Path
from datetime import datetime, timedelta
from query_context import parse_record, search_records

def check_reviews(warn_days: int = 30) -> dict:
    """Check for overdue and upcoming reviews."""
    context_dir = Path("CONTEXT")
    now = datetime.now()
    warn_date = now + timedelta(days=warn_days)
    
    overdue = []
    upcoming = []
    no_review = []
    
    for subdir in ["decisions", "preferences"]:
        dir_path = context_dir / subdir
        if not dir_path.exists():
            continue
            
        for md_file in dir_path.glob("*.md"):
            record = parse_record(md_file)
            if not record:
                continue
            
            if not record.review:
                no_review.append(record)
            elif record.review < now:
                overdue.append(record)
            elif record.review <= warn_date:
                upcoming.append(record)
    
    return {
        'overdue': sorted(overdue, key=lambda r: r.review),
        'upcoming': sorted(upcoming, key=lambda r: r.review),
        'no_review': no_review
    }

def update_index():
    """Regenerate INDEX.md from current records."""
    # Implementation: Scan all records and rebuild INDEX.md
    pass

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="Review context records")
    parser.add_argument('--due', type=int, default=30,
                       help="Warn about reviews due within N days")
    parser.add_argument('--fix', action='store_true',
                       help="Regenerate INDEX.md")
    
    args = parser.parse_args()
    
    if args.fix:
        update_index()
        print("âœ“ INDEX.md updated")
        return 0
    
    results = check_reviews(warn_days=args.due)
    
    if results['overdue']:
        print(f"\nâš ï¸  {len(results['overdue'])} OVERDUE REVIEW(S):\n")
        for record in results['overdue']:
            days_overdue = (datetime.now() - record.review).days
            print(f"  {record.id}: {record.title}")
            print(f"    Due: {record.review.strftime('%Y-%m-%d')} ({days_overdue} days ago)")
            print()
    
    if results['upcoming']:
        print(f"\nðŸ“… {len(results['upcoming'])} UPCOMING REVIEW(S):\n")
        for record in results['upcoming']:
            days_until = (record.review - datetime.now()).days
            print(f"  {record.id}: {record.title}")
            print(f"    Due: {record.review.strftime('%Y-%m-%d')} (in {days_until} days)")
            print()
    
    if results['no_review']:
        print(f"\nâš ï¸  {len(results['no_review'])} RECORD(S) WITH NO REVIEW DATE:\n")
        for record in results['no_review']:
            print(f"  {record.id}: {record.title}")
        print()
    
    if not any(results.values()):
        print("âœ“ All reviews current")
    
    return 1 if results['overdue'] else 0

if __name__ == "__main__":
    sys.exit(main())
```

### Integration with Governance Loop

#### Update CANON/CONTRACT.md

Add to "Required sequence":

```markdown
4. Document decision in CONTEXT/ (why this change, what alternatives considered)
5. Only then update canon
```

#### Update Pre-Commit Hook

```bash
#!/bin/bash
# .git/hooks/pre-commit

echo "Running contract validation..."
python CONTRACTS/runner.py
if [ $? -ne 0 ]; then
    echo "âŒ Contract validation failed"
    exit 1
fi

echo "Checking context reviews..."
python CONTEXT/review-context.py --due 7
# Don't block on overdue reviews, just warn

echo "âœ“ All checks passed"
exit 0
```

### Context Best Practices

**Do:**
- âœ“ Create ADR *before* implementing significant changes
- âœ“ Update INDEX.md when adding new records
- âœ“ Set realistic review dates (3-12 months)
- âœ“ Document *why* something was rejected (prevents rearguing)
- âœ“ Link to evidence (test results, benchmarks, user feedback)
- âœ“ Close OPEN questions once decided (convert to ADR)

**Don't:**
- âœ— Skip context docs for "obvious" decisions (they're never obvious later)
- âœ— Copy-paste ADRs without customizing
- âœ— Let review dates slip (use the review tool)
- âœ— Delete rejected approaches (learning compounds)
- âœ— Make context docs authoritative over canon (context informs, canon governs)

### Context Lifecycle

```
1. OPEN Question Created
   â†“
2. Research & Discussion
   â†“
3. Decision Made â†’ ADR Created
   â†“
4. Alternative Options â†’ REJECT Records Created
   â†“
5. Implementation â†’ Linked to Canon/Fixtures
   â†“
6. Review Date Set (3-12 months)
   â†“
7. Review Triggered
   â†“
8. Either: Confirmed (update review date)
   Or: Superseded (new ADR created)
```

### Future-Proofing Features

**1. Confidence Decay**
ADRs marked "Low confidence" automatically flag for review sooner.

**2. Evidence Links**
External links to test results, benchmarks, docs expire after N months (forces revalidation).

**3. Impact Tracking**
High-impact decisions require more thorough documentation and more frequent reviews.

**4. AI Query Interface**
Context records are structured to be AI-queryable:
```
Human: "Why did we choose REST over GraphQL?"
AI: [Searches CONTEXT] "Per REJECT-001, GraphQL was rejected due to team size..."
```

**5. Temporal Queries**
```bash
# What decisions were made in Q4 2024?
python CONTEXT/query-context.py --date-range 2024-10-01 2024-12-31

# What's been superseded in the last year?
python CONTEXT/query-context.py --superseded --since 2024-01-01
```

### AI Agent Integration

**Starting a new AI session:**

```
Prompt: "We have a LEXIC-governed system. Before making changes:
1. Read CANON/CONTRACT.md for rules
2. Query CONTEXT for relevant decisions: [topic]
3. Check MAPS/ENTRYPOINTS.md for where to work
4. Use appropriate SKILL

Current task: [describe task]"
```

**When AI suggests something:**

```
Prompt: "Before implementing that, check:
- Does CONTEXT have a decision about this?
  â†’ python CONTEXT/query-context.py "[topic]"
- Was this approach already rejected?
  â†’ Check CONTEXT/rejected/
- Is there an open question about this?
  â†’ Check CONTEXT/open/"
```

**After making changes:**

```
Prompt: "Now:
1. Run CONTRACTS/runner.py to validate
2. Create ADR if this was a significant decision
3. Update CANON/CHANGELOG.md
4. Set a review date (6 months typical)"
```

**Recovering from context loss:**

```
Prompt: "I lost context. Restart me:
1. What system am I working on?
   â†’ Read CANON/CONTRACT.md introduction
2. What's the current focus?
   â†’ Read ROADMAP.md
3. What decisions are relevant?
   â†’ python CONTEXT/query-context.py --tag [current-area]
4. What am I allowed to change?
   â†’ Read MAPS/ENTRYPOINTS.md"
```

---

## /MAPS/ â€” Navigation Layer

**Purpose:** Tell agents where to look and what to change.

### Required Files

#### `ENTRYPOINTS.md`
Where to change what.

```markdown
# Entrypoints

## Rules / Governance
- `CANON/CONTRACT.md`
- `CANON/INVARIANTS.md`
- `CANON/CHANGELOG.md`

## Core Logic
- `src/main.py` (primary business logic)
- `src/parser.py` (input processing)

## Output Templates
- `templates/default.html`
- `templates/special.html`

## Configuration
- `config.json` (never hardcode paths)

## Tests / Validation
- `tests/fixtures/**`
- `tests/runner.py`
```

#### `SYSTEM_MAP.md`
High-level mental model of subsystems.

```markdown
# System Map

## Subsystems

### Input Pipeline
- Purpose: Transform user data into canonical format
- Entrypoint: `src/ingest.py`
- Constraints: Must validate against schema X

### Processing Engine
- Purpose: Apply business logic
- Entrypoint: `src/engine.py`
- Constraints: Must be deterministic

### Output Generator
- Purpose: Produce final artifacts
- Entrypoint: `src/output.py`
- Constraints: Must match template structure
```

---

## /SKILLS/ â€” Capability Layer

**Purpose:** Modular, versioned, scoped capabilities.

### Structure

```
/SKILLS/
  /research/
    SKILL.md
    research.py
  /summarize/
    SKILL.md
    summarize.py
  /validate/
    SKILL.md
    validate.py
```

### Skill Contract Template

```markdown
# [Skill Name]

**Version:** 1.0  
**Status:** Stable

## Trigger
When: User requests "research X"
How: Direct invocation or orchestrator routing

## Inputs
- `topic` (string, required)
- `depth` (enum: shallow|medium|deep, default: medium)
- `sources` (list, optional)

## Outputs
- Markdown file in `output/research/YYYY-MM-DD-<topic>.md`
- Citations in `output/research/citations.json`

## Constraints
- Never overwrite existing research files
- Always append timestamp to filename
- Must cite sources inline
- Maximum 5000 tokens per output

## Dependencies
- Requires: web search capability
- Uses: `SKILLS/cite/` for formatting

## Fixtures
- `tests/fixtures/research-basic.json`
- `tests/fixtures/research-deep.json`
```

---

## /CONTRACTS/ â€” Enforcement Layer

**Purpose:** Mechanical proof that behavior matches canon.

### Structure

```
/CONTRACTS/
  /fixtures/
    scenario-1/
      input.json
      expected-output.json
    scenario-2/
      input.md
      expected-output.html
  runner.py
  README.md
```

### Runner Template

```python
#!/usr/bin/env python3
"""
Contract Runner - Enforces canon through deterministic fixtures.

Usage: python contracts/runner.py
Exit: 0 if all fixtures pass, 1 if any fail
"""

import sys
from pathlib import Path

def run_fixture(fixture_path):
    """Run a single fixture and verify output."""
    # Load input
    # Run system
    # Compare output to expected
    # Return pass/fail
    pass

def main():
    fixture_dir = Path(__file__).parent / "fixtures"
    fixtures = sorted(fixture_dir.glob("*/"))
    
    passed = 0
    failed = 0
    
    for fixture in fixtures:
        print(f"Running: {fixture.name}")
        if run_fixture(fixture):
            passed += 1
            print("  âœ“ PASS")
        else:
            failed += 1
            print("  âœ— FAIL")
    
    print(f"\nResults: {passed} passed, {failed} failed")
    return 0 if failed == 0 else 1

if __name__ == "__main__":
    sys.exit(main())
```

### Pre-Commit Hook

```bash
#!/bin/bash
# .git/hooks/pre-commit

echo "Running contract validation..."
python contracts/runner.py

if [ $? -ne 0 ]; then
    echo "âŒ Contract validation failed. Commit rejected."
    exit 1
fi

echo "âœ“ Contract validation passed"
exit 0
```

---

## /MEMORY/ â€” Context Layer

**Purpose:** Portable cognition for AI handoff.

### Packer Template

```python
#!/usr/bin/env python3
"""
Memory Packer - Generate LLM-friendly context snapshot.

Outputs:
- COMBINED/snapshot.md (full verbatim)
- COMBINED/SPLIT/*.md (priority-ordered chunks)
- COMBINED/manifest.json (sources + hashes)
"""

import json
import hashlib
from pathlib import Path
from datetime import datetime

PRIORITY_ORDER = [
    ("01_CANON.md", ["CANON/CONTRACT.md", "CANON/INVARIANTS.md", "CANON/CHANGELOG.md"]),
    ("02_ROADMAP.md", ["ROADMAP.md"]),
    ("03_MAPS.md", ["MAPS/ENTRYPOINTS.md", "MAPS/SYSTEM_MAP.md"]),
    ("04_SKILLS.md", ["SKILLS/*/SKILL.md"]),
    ("05_CONTRACTS.md", ["CONTRACTS/runner.py", "CONTRACTS/fixtures/**"]),
]

def hash_file(path):
    """SHA256 hash of file contents."""
    return hashlib.sha256(Path(path).read_bytes()).hexdigest()

def generate_pack():
    """Generate memory pack with integrity checks."""
    output_dir = Path("COMBINED/SPLIT")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    manifest = {
        "pack_version": datetime.now().isoformat(),
        "canon_version": "1.0",  # Read from CANON/INVARIANTS.md
        "source_hashes": {},
        "split_files": []
    }
    
    for split_file, source_patterns in PRIORITY_ORDER:
        sources = []
        for pattern in source_patterns:
            sources.extend(Path(".").glob(pattern))
        
        content = []
        for src in sorted(sources):
            if src.is_file():
                manifest["source_hashes"][str(src)] = hash_file(src)
                content.append(f"# {src}\n\n{src.read_text()}\n\n---\n")
        
        split_path = output_dir / split_file
        split_path.write_text("".join(content))
        manifest["split_files"].append(str(split_path))
    
    Path("COMBINED/manifest.json").write_text(
        json.dumps(manifest, indent=2)
    )
    
    print(f"âœ“ Generated pack: {manifest['pack_version']}")

if __name__ == "__main__":
    generate_pack()
```

### Pack Load Order

When loading a pack into an AI context:

```
1. Load CANON first (establishes authority)
2. Load ROADMAP (current focus)
3. Load MAPS (navigation)
4. Load SKILLS (available capabilities)
5. Load CONTRACTS (verification examples)
```

---

## /TOOLS/ â€” Governance Tools

### Canon Governance Check

```python
#!/usr/bin/env python3
"""
Canon Governance Check - Verify behavior/canon synchronization.

Run: python tools/check-canon-governance.py
Exit: 0 if synchronized, 1 if drift detected
"""

import sys
from pathlib import Path
from datetime import datetime, timedelta

def check_canon_sync():
    """Verify canon was updated in recent commits."""
    canon_files = [
        "CANON/CONTRACT.md",
        "CANON/INVARIANTS.md", 
        "CANON/CHANGELOG.md"
    ]
    
    behavior_files = [
        "src/**/*.py",
        "SKILLS/**/*.py",
        "templates/**/*"
    ]
    
    # Check if behavior files changed without canon update
    # Implementation depends on git integration
    
    pass

if __name__ == "__main__":
    sys.exit(check_canon_sync())
```

---

## Quick Start

### 1. Initialize Repository

```bash
mkdir my-project
cd my-project
git init

# Create structure
mkdir -p CANON MAPS SKILLS CONTRACTS/fixtures MEMORY TOOLS
```

### 2. Create Core Canon

```bash
# Copy templates from this document
cp templates/CONTRACT.md CANON/
cp templates/INVARIANTS.md CANON/
touch CANON/CHANGELOG.md
```

### 3. Define Your System

Edit `CANON/CONTRACT.md`:
- What are your non-negotiable rules?
- What is your source of truth?
- What must never change?

Edit `CANON/INVARIANTS.md`:
- What decisions are locked?
- What patterns are canonical?

### 4. Create First Skill

```bash
mkdir SKILLS/example
cat > SKILLS/example/SKILL.md << 'EOF'
# Example Skill

Version: 1.0

## Trigger
When: User requests "example action"

## Inputs
- input_data (string, required)

## Outputs
- result.txt

## Constraints
- Must validate input
- Must not modify source files
EOF
```

### 5. Add First Fixture

```bash
mkdir -p CONTRACTS/fixtures/example-basic
echo '{"input": "test"}' > CONTRACTS/fixtures/example-basic/input.json
echo '{"output": "expected"}' > CONTRACTS/fixtures/example-basic/expected.json
```

### 6. Set Up Validation

```bash
# Create runner (adapt template above)
python CONTRACTS/runner.py

# Add pre-commit hook
cp templates/pre-commit .git/hooks/
chmod +x .git/hooks/pre-commit
```

---

## Best Practices

### Do

- âœ“ Update CHANGELOG.md for every behavior change
- âœ“ Add fixtures before implementing new features
- âœ“ Use the pre-commit hook to catch violations early
- âœ“ Version your Skills explicitly
- âœ“ Generate memory packs before long breaks
- âœ“ Keep canon files under 10KB each (split if needed)

### Don't

- âœ— Change behavior without updating canon
- âœ— Skip fixture validation
- âœ— Hardcode paths (use config files)
- âœ— Mix canon and implementation in same file
- âœ— Let secondary docs become authoritative

### Recovery Pattern

If context is lost, restart by stating:

> We have a LEXIC-governed system. Canon is at CANON/CONTRACT.md. Current phase is in ROADMAP.md. Load in priority order.

---

## Adaptation Guide

### For Different Domains

**Novel Writing:**
```
/CANON/
  CONTRACT.md       (style guide, never rules)
  INVARIANTS.md     (character facts, timeline)
  CHANGELOG.md      (plot revisions)
/SKILLS/
  /draft/           (generate chapters)
  /edit/            (consistency checks)
  /research/        (world building)
```

**Business Operations:**
```
/CANON/
  CONTRACT.md       (brand guidelines)
  INVARIANTS.md     (legal requirements)
  CHANGELOG.md      (policy updates)
/SKILLS/
  /analyze/         (data reports)
  /forecast/        (projections)
  /document/        (compliance docs)
```

**Research:**
```
/CANON/
  CONTRACT.md       (methodology)
  INVARIANTS.md     (definitions, scope)
  CHANGELOG.md      (hypothesis revisions)
/SKILLS/
  /literature/      (citation management)
  /analyze/         (statistical analysis)
  /synthesize/      (paper writing)
```

---

## Version History

- **1.0 (2025-12-20):** Initial extraction from static site system 5.2.36
- Production-ready governance patterns
- Proven through 36+ iterations of real-world usage

---

## License

MIT License - adapt freely, attribution appreciated.

---

## Origin

Extracted from a production static site generation system that solved:
- Drift prevention across 30+ AI sessions
- Context persistence through model switches
- Intent preservation over 6 months of iteration
- Mechanical enforcement of 100+ authoring rules

The patterns emerged from necessity, not theory.

---

## Support

This is a reference architecture, not a framework. Adapt the patterns to your needs. The core insight is:

**Text is law. Law is executable. Execution is validated. Validation gates change.**

That recursive loop is what makes AI collaboration durable.