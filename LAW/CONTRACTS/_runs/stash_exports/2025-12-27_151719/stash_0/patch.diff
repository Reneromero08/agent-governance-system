diff --git a/AGS_ROADMAP_MASTER.md b/AGS_ROADMAP_MASTER.md
index 00048a5..4dbf6f0 100644
--- a/AGS_ROADMAP_MASTER.md
+++ b/AGS_ROADMAP_MASTER.md
@@ -21,14 +21,14 @@ This roadmap assumes:
 
 # Current state snapshot (what you already have)
 
-From the SPLIT_LITE pack pointers, the system expects these major components to exist in-repo:
+From the LITE pack pointers, the system expects these major components to exist in-repo:
 - Governance: `repo/AGENTS.md`, `repo/CANON/*`
 - Maps: `repo/MAPS/ENTRYPOINTS.md`
 - Contracts: `repo/CONTRACTS/runner.py`
 - Skills: `repo/SKILLS/*/SKILL.md`
 - Tools: `repo/TOOLS/critic.py`
 - Cortex interface: `repo/CORTEX/query.py`
-- Packer engine: `repo/MEMORY/LLM_PACKER/Engine/packer.py`
+- Packer engine: `repo/MEMORY/LLM_PACKER/Engine/packer/`
 - MCP seam: `repo/MCP/server.py`
 
 (Those pointers are present in your LITE pack index and root stubs.)
@@ -87,7 +87,7 @@ Problem:
 - LITE packs are required for navigation-first workflows but are not guaranteed by the packer or CI.
 
 Tasks:
-- [x] [P0] Add `--profile lite` to `MEMORY/LLM_PACKER/Engine/packer.py` with FULL output unchanged.
+- [x] [P0] Add `--profile lite` to `MEMORY/LLM_PACKER/Engine/packer/` with FULL output unchanged.
 - [x] [P0] Emit LITE meta outputs: `meta/LITE_ALLOWLIST.json`, `meta/LITE_OMITTED.json`, `meta/LITE_START_HERE.md`, `meta/SKILL_INDEX.json`, `meta/FIXTURE_INDEX.json`, `meta/CODEBOOK.md`, `meta/CODE_SYMBOLS.json`.
 - [x] [P0] Add smoke/CI validation that asserts required LITE meta outputs exist.
 - [ ] [P1] Optional: add `--profile test` (fixtures-heavy) for debugging, separate from LITE.
diff --git a/CONTEXT/archive/planning/AGS_3.0_ROADMAP.md b/CONTEXT/archive/planning/AGS_3.0_ROADMAP.md
index 3b7720f..12dc931 100644
--- a/CONTEXT/archive/planning/AGS_3.0_ROADMAP.md
+++ b/CONTEXT/archive/planning/AGS_3.0_ROADMAP.md
@@ -1,7 +1,6 @@
-Superseded by AGS_ROADMAP_MASTER.md
+# AGS Roadmap (Superseded)
 
----
-title: AGS Roadmap (Updated)
+**NOTE: This document is superseded by `AGS_ROADMAP_MASTER.md`.**
 version: 0.1
 last_updated: 2025-12-23
 scope: Agent Governance System (repo + packer + cortex)
diff --git a/CONTEXT/archive/planning/AGS_MASTER_TODO-2025-12-21.md b/CONTEXT/archive/planning/AGS_MASTER_TODO-2025-12-21.md
index 4da659b..886fef9 100644
--- a/CONTEXT/archive/planning/AGS_MASTER_TODO-2025-12-21.md
+++ b/CONTEXT/archive/planning/AGS_MASTER_TODO-2025-12-21.md
@@ -1,5 +1,6 @@
-# AGS Master TODO
-_generated: 2025-12-21_
+# AGS Master TODO (Superseded)
+
+**NOTE: This document is superseded by `AGS_ROADMAP_MASTER.md`.**
 
 Legend:
 - **P0** = must-fix (correctness / prevents drift / blocks release)
diff --git a/CONTEXT/archive/planning/REPO_FIXES_TASKS.md b/CONTEXT/archive/planning/REPO_FIXES_TASKS.md
index ead5be0..65eab9e 100644
--- a/CONTEXT/archive/planning/REPO_FIXES_TASKS.md
+++ b/CONTEXT/archive/planning/REPO_FIXES_TASKS.md
@@ -1,6 +1,7 @@
-Superseded by AGS_ROADMAP_MASTER.md
+# Repo Fix Tasks (Superseded)
 
-# Repo Fix Tasks (Post v2.5.5)
+**NOTE: This document is superseded by `AGS_ROADMAP_MASTER.md` and `MEMORY/PACKER_ROADMAP.md`.
+References to `SPLIT_LITE` and `packer.py` are outdated as of v1.3.1 (see `MEMORY/LLM_PACKER/README.md`).**
 
 This is a task list derived from a contract-doc scan + local checks. It is intended to be implemented as minimal, correctness-first changes.
 
diff --git a/MAPS/DATA_FLOW.md b/MAPS/DATA_FLOW.md
index 225a209..c6a58e9 100644
--- a/MAPS/DATA_FLOW.md
+++ b/MAPS/DATA_FLOW.md
@@ -2,6 +2,8 @@
 
 This document explains how information moves through the Agent Governance System during a typical session.
 
+## Core AGS Flow
+
 1. **Load canon** - On startup, agents read the canon files in order (`CONTRACT.md`, invariants, versioning, glossary). This establishes the rules.
 2. **Load context** - Agents then read relevant context records (ADRs, rejections, preferences, open issues). These provide historical and stylistic guidance.
 3. **Query cortex** - Instead of scanning files, agents query the shadow index (`CORTEX/_generated/cortex.db`) via `CORTEX/query.py`. The cortex returns structured metadata about pages, assets and tokens.
@@ -9,3 +11,24 @@ This document explains how information moves through the Agent Governance System
 5. **Execute skill** - The agent invokes a skill (a script under `SKILLS/`) to perform the action. Skills operate on data provided by the cortex and abide by the canon constraints.
 6. **Validate via fixtures** - After the skill runs, fixtures in `CONTRACTS/fixtures/` are executed by the runner. Any failure blocks the merge. Runner artifacts are written under `CONTRACTS/_runs/`.
 7. **Update memory** - If the work is completed, the packer serialises the current state into a pack for future sessions under `MEMORY/LLM_PACKER/_packs/`. The manifest includes file hashes and canon version.
+
+## CATALYTIC-DPT Pipeline Flow
+
+For distributed pipeline execution, CATALYTIC-DPT follows its own data flow:
+
+1. **Load pipeline definition** - Read DAG from `CATALYTIC-DPT/PIPELINES/` specifying nodes and dependencies.
+2. **Validate against schemas** - Pipeline structures are validated against `CATALYTIC-DPT/SCHEMAS/`.
+3. **Schedule nodes** - The DAG scheduler determines execution order based on dependencies.
+4. **Execute primitives** - Core operations from `CATALYTIC-DPT/PRIMITIVES/` are invoked.
+5. **Orchestrate swarm** - For distributed work, the swarm-orchestrator dispatches tasks to ant-workers.
+6. **Generate receipts** - Execution receipts record success/failure and outputs.
+7. **Restore on failure** - The restore runner can replay failed nodes from checkpoints.
+
+## MCP Integration Flow
+
+When accessed via MCP (Model Context Protocol):
+
+1. **Client connects** - IDE or tool connects to `MCP/server.py` via stdio.
+2. **Audit wrapper** - Requests pass through `CONTRACTS/ags_mcp_entrypoint.py` for logging.
+3. **Tool dispatch** - MCP server routes to appropriate AGS tool or skill.
+4. **Response** - Results returned to client with governance metadata.
diff --git a/MAPS/ENTRYPOINTS.md b/MAPS/ENTRYPOINTS.md
index cd224bf..694e718 100644
--- a/MAPS/ENTRYPOINTS.md
+++ b/MAPS/ENTRYPOINTS.md
@@ -32,7 +32,7 @@ This document lists the primary entrypoints where agents and humans are expected
 
 - `CORTEX/` - Modify when adding new index fields or query capabilities.
 - `TOOLS/cortex.py` - Cortex section index CLI (`read`, `resolve`, `search`).
-- `MEMORY/LLM_PACKER/Engine/packer.py` - Modify when updating the pack format or manifest.
+- `MEMORY/LLM_PACKER/Engine/packer/` - Modify when updating the pack format or manifest.
 - `MEMORY/LLM_PACKER/` - Windows wrapper scripts for running the packer.
 - `TOOLS/` - Add critics, linters and migration scripts here.
 
@@ -44,3 +44,18 @@ This document lists the primary entrypoints where agents and humans are expected
 - `CONTRACTS/ags_mcp_entrypoint.py` - Recommended entrypoint wrapper (audit logs under allowed roots).
 - `SKILLS/mcp-smoke/` - CLI smoke test for MCP server.
 - `SKILLS/mcp-extension-verify/` - Extension-agnostic verification checklist + smoke test.
+
+## CATALYTIC-DPT (Distributed Pipeline Toolkit)
+
+- `CATALYTIC-DPT/AGENTS.md` - Agent definitions for pipeline execution.
+- `CATALYTIC-DPT/SKILLS/` - Pipeline-specific skills:
+  - `swarm-orchestrator/` - Coordinates distributed task execution
+  - `ant-worker/` - Individual task execution agents
+  - `mcp-startup/` - MCP integration for pipelines
+- `CATALYTIC-DPT/PIPELINES/` - DAG definitions and pipeline configurations.
+- `CATALYTIC-DPT/PRIMITIVES/` - Core building blocks (nodes, executors).
+- `CATALYTIC-DPT/CONTRACTS/` - Pipeline-specific fixtures and validation.
+- `CATALYTIC-DPT/SCHEMAS/` - JSON schemas for pipeline structures.
+- `CATALYTIC-DPT/SPECTRUM/` - Capability spectrum definitions.
+- `CATALYTIC-DPT/LAB/` - Experimental features and research.
+- `CATALYTIC-DPT/TESTBENCH/` - Testing infrastructure for pipelines.
diff --git a/MAPS/FILE_OWNERSHIP.md b/MAPS/FILE_OWNERSHIP.md
index 2e13542..852bc62 100644
--- a/MAPS/FILE_OWNERSHIP.md
+++ b/MAPS/FILE_OWNERSHIP.md
@@ -1,6 +1,8 @@
 # File Ownership
 
-To minimise conflicts and maintain clarity, this document assigns responsibility for different parts of the repository.  Ownership means that changes to those files should be reviewed by the designated team or individual.
+To minimise conflicts and maintain clarity, this document assigns responsibility for different parts of the repository. Ownership means that changes to those files should be reviewed by the designated team or individual.
+
+## Core AGS
 
 | Path | Owner | Notes |
 |---|---|---|
@@ -15,5 +17,30 @@ To minimise conflicts and maintain clarity, this document assigns responsibility
 | `MEMORY/LLM_PACKER/Engine/*` | Automation leads | Core packer logic and launcher assets |
 | `CORTEX/*` | Index team | Maintain the schema and build scripts for the shadow index |
 | `TOOLS/*` | Tooling team | Implement critics, linters and migration scripts |
+| `MCP/*` | Integration team | MCP server and protocol implementation |
+
+## CATALYTIC-DPT Subsystem
+
+| Path | Owner | Notes |
+|---|---|---|
+| `CATALYTIC-DPT/AGENTS.md` | Pipeline architects | Agent definitions for distributed execution |
+| `CATALYTIC-DPT/SKILLS/*` | Pipeline skill authors | Swarm orchestrator, ant-worker, etc. |
+| `CATALYTIC-DPT/PIPELINES/*` | Pipeline designers | DAG definitions and configurations |
+| `CATALYTIC-DPT/PRIMITIVES/*` | Core pipeline team | Fundamental building blocks |
+| `CATALYTIC-DPT/CONTRACTS/*` | Pipeline QA | Pipeline-specific fixtures |
+| `CATALYTIC-DPT/SCHEMAS/*` | Schema maintainers | JSON schemas for pipeline structures |
+| `CATALYTIC-DPT/SPECTRUM/*` | Capability team | Spectrum definitions |
+| `CATALYTIC-DPT/LAB/*` | Research leads | Experimental features |
+| `CATALYTIC-DPT/TESTBENCH/*` | Pipeline QA | Testing infrastructure |
+
+## Root-Level Files
+
+| Path | Owner | Notes |
+|---|---|---|
+| `AGENTS.md` | Project leads | Top-level agent definitions |
+| `AGS_ROADMAP_MASTER.md` | Project leads | Master roadmap and planning |
+| `README.md` | Documentation team | Project overview |
+| `demos/*` | Demo maintainers | Demonstration scripts |
+| `.github/*` | DevOps team | CI/CD workflows |
 
-Ownership can be shared or transferred, but must always be clearly documented.
\ No newline at end of file
+Ownership can be shared or transferred, but must always be clearly documented.
diff --git a/MAPS/SYSTEM_MAP.md b/MAPS/SYSTEM_MAP.md
index 750f801..cf2bb88 100644
--- a/MAPS/SYSTEM_MAP.md
+++ b/MAPS/SYSTEM_MAP.md
@@ -26,8 +26,48 @@ This map describes the overall architecture of the Agent Governance System. It p
 ┌───────▼──────┐
 │   MEMORY     │ - persist and pack
 └──────────────┘
+```
+
+## Supporting Components
+
+```
+┌──────────────┐
+│   CORTEX     │ - indexing and querying across all layers
+└──────────────┘
+
+┌──────────────┐
+│    TOOLS     │ - critics, linters, validators, runtime utilities
+└──────────────┘
 
-╳ Additional:  CORTEX provides indexing and querying across all layers.
+┌──────────────┐
+│     MCP      │ - Model Context Protocol server for IDE integration
+└──────────────┘
 ```
 
-Each arrow represents a dependency. For example, skills depend on maps to know where to operate, and contracts enforce the correct behaviour of skills.
+## Subsystem: CATALYTIC-DPT
+
+CATALYTIC-DPT is a self-contained distributed pipeline toolkit with its own governance structure:
+
+```
+CATALYTIC-DPT/
+├── AGENTS.md        - agent definitions for pipeline execution
+├── SKILLS/          - pipeline-specific skills (swarm-orchestrator, ant-worker, etc.)
+├── PIPELINES/       - DAG definitions and pipeline configurations
+├── PRIMITIVES/      - core building blocks (nodes, executors)
+├── CONTRACTS/       - pipeline-specific fixtures and validation
+├── SCHEMAS/         - JSON schemas for pipeline structures
+├── SPECTRUM/        - capability spectrum definitions
+├── LAB/             - experimental features and research
+├── TESTBENCH/       - testing infrastructure
+├── FIXTURES/        - test fixtures for pipeline validation
+└── DEMOS/           - demonstration pipelines
+```
+
+## Other Directories
+
+| Directory | Purpose |
+|-----------|---------|
+| `demos/` | Project-level demonstration scripts |
+| `.github/` | GitHub workflows and CI configuration |
+
+Each arrow in the main hierarchy represents a dependency. For example, skills depend on maps to know where to operate, and contracts enforce the correct behaviour of skills.
diff --git a/MEMORY/LLM_PACKER/1-AGS-PACK.lnk b/MEMORY/LLM_PACKER/1-AGS-PACK.lnk
index 4de8089..a56dd1f 100644
Binary files a/MEMORY/LLM_PACKER/1-AGS-PACK.lnk and b/MEMORY/LLM_PACKER/1-AGS-PACK.lnk differ
diff --git a/MEMORY/LLM_PACKER/2-CAT-PACK.lnk b/MEMORY/LLM_PACKER/2-CAT-PACK.lnk
index 1eb0e1c..5307bf6 100644
Binary files a/MEMORY/LLM_PACKER/2-CAT-PACK.lnk and b/MEMORY/LLM_PACKER/2-CAT-PACK.lnk differ
diff --git a/MEMORY/LLM_PACKER/3-LAB-PACK.lnk b/MEMORY/LLM_PACKER/3-LAB-PACK.lnk
index 466f924..d7aa9c1 100644
Binary files a/MEMORY/LLM_PACKER/3-LAB-PACK.lnk and b/MEMORY/LLM_PACKER/3-LAB-PACK.lnk differ
diff --git a/MEMORY/LLM_PACKER/CAT-DPT-PACK.lnk b/MEMORY/LLM_PACKER/CAT-DPT-PACK.lnk
deleted file mode 100644
index 08a7de4..0000000
Binary files a/MEMORY/LLM_PACKER/CAT-DPT-PACK.lnk and /dev/null differ
diff --git a/MEMORY/LLM_PACKER/Engine/CATALYTIC-DPT-PACK.cmd b/MEMORY/LLM_PACKER/Engine/CATALYTIC-DPT-PACK.cmd
index 0a82916..1fcfa3e 100644
--- a/MEMORY/LLM_PACKER/Engine/CATALYTIC-DPT-PACK.cmd
+++ b/MEMORY/LLM_PACKER/Engine/CATALYTIC-DPT-PACK.cmd
@@ -4,14 +4,13 @@ cd /d "%~dp0"
 
 REM One-click packer for CATALYTIC-DPT only.
 REM Output goes to MEMORY/LLM_PACKER/_packs/
+
 if "%~1"=="" (
-  REM Default: build a MAIN pack + LAB sub-pack, then zip the whole bundle.
-  call "%~dp0CAT-DPT1.cmd"
+  powershell -NoProfile -ExecutionPolicy Bypass -File "%~dp0pack.ps1" -Scope catalytic-dpt -Profile full -SplitLite -Combined -Zip
 ) else (
-  echo This launcher does not accept arguments. Use CAT-DPT1.cmd or the Python packers directly.
-  exit /b 1
+  powershell -NoProfile -ExecutionPolicy Bypass -File "%~dp0pack.ps1" -Scope catalytic-dpt %*
 )
 
 echo.
 echo Done.
-set /p _="Type anything then Enter to close: "
+pause
diff --git a/MEMORY/LLM_PACKER/Engine/fix_shortcuts.ps1 b/MEMORY/LLM_PACKER/Engine/fix_shortcuts.ps1
new file mode 100644
index 0000000..d2b008e
--- /dev/null
+++ b/MEMORY/LLM_PACKER/Engine/fix_shortcuts.ps1
@@ -0,0 +1,24 @@
+$WshShell = New-Object -comObject WScript.Shell
+
+# 1-AGS-PACK.lnk -> Engine\1-AGS-PACK.cmd
+$Shortcut = $WshShell.CreateShortcut("$PSScriptRoot\..\1-AGS-PACK.lnk")
+$Shortcut.TargetPath = "$PSScriptRoot\1-AGS-PACK.cmd"
+$Shortcut.WorkingDirectory = "$PSScriptRoot"
+$Shortcut.IconLocation = "$PSScriptRoot\ags_launcher_blue.ico"
+$Shortcut.Save()
+
+# 2-CAT-PACK.lnk -> Engine\2-CAT-PACK.cmd
+$Shortcut = $WshShell.CreateShortcut("$PSScriptRoot\..\2-CAT-PACK.lnk")
+$Shortcut.TargetPath = "$PSScriptRoot\2-CAT-PACK.cmd"
+$Shortcut.WorkingDirectory = "$PSScriptRoot"
+$Shortcut.IconLocation = "$PSScriptRoot\ags_launcher_blue.ico"
+$Shortcut.Save()
+
+# 3-LAB-PACK.lnk -> Engine\3-LAB-PACK.cmd
+$Shortcut = $WshShell.CreateShortcut("$PSScriptRoot\..\3-LAB-PACK.lnk")
+$Shortcut.TargetPath = "$PSScriptRoot\3-LAB-PACK.cmd"
+$Shortcut.WorkingDirectory = "$PSScriptRoot"
+$Shortcut.IconLocation = "$PSScriptRoot\lab.ico"
+$Shortcut.Save()
+
+Write-Host "Shortcuts updated."
diff --git a/MEMORY/LLM_PACKER/Engine/inspect_zip.py b/MEMORY/LLM_PACKER/Engine/inspect_zip.py
new file mode 100644
index 0000000..f6b9130
--- /dev/null
+++ b/MEMORY/LLM_PACKER/Engine/inspect_zip.py
@@ -0,0 +1,16 @@
+import zipfile
+import sys
+from pathlib import Path
+
+def list_zip(path):
+    print(f"Listing {path}...")
+    try:
+        with zipfile.ZipFile(path, 'r') as z:
+            for i in z.infolist():
+                print(f"{i.filename} ({i.file_size} bytes)")
+    except Exception as e:
+        print(f"Error: {e}")
+
+if __name__ == "__main__":
+    if len(sys.argv) > 1:
+        list_zip(sys.argv[1])
diff --git a/MEMORY/LLM_PACKER/Engine/migrate_phase1.py b/MEMORY/LLM_PACKER/Engine/migrate_phase1.py
new file mode 100644
index 0000000..71cbe88
--- /dev/null
+++ b/MEMORY/LLM_PACKER/Engine/migrate_phase1.py
@@ -0,0 +1,6 @@
+
+"""
+Migration script for Phase 1 (Stub).
+Moves files to new modular structure.
+"""
+print("Migration Phase 1 complete (Manual).")
diff --git a/MEMORY/LLM_PACKER/Engine/pack.ps1 b/MEMORY/LLM_PACKER/Engine/pack.ps1
index 0e3f38f..8385404 100644
--- a/MEMORY/LLM_PACKER/Engine/pack.ps1
+++ b/MEMORY/LLM_PACKER/Engine/pack.ps1
@@ -1,5 +1,5 @@
 param(
-  [ValidateSet("ags", "catalytic-dpt", "catalytic-dpt-lab")]
+  [ValidateSet("ags", "catalytic-dpt", "lab")]
   [string]$Scope = "ags",
   [string]$OutDir = "",
   [ValidateSet("full", "delta")]
@@ -22,11 +22,7 @@ function Get-RepoRoot {
 }
 
 $repoRoot = Get-RepoRoot
-$packer = Join-Path $PSScriptRoot "packer.py"
-
-if (-not (Test-Path -LiteralPath $packer)) {
-  throw "Missing Python packer at: $packer"
-}
+$pythonModule = "MEMORY.LLM_PACKER.Engine.packer"
 
 if ($Stamp -eq "") { $Stamp = (Get-Date).ToString("yyyy-MM-dd_HH-mm-ss") }
 
@@ -59,18 +55,17 @@ if ($PSBoundParameters.ContainsKey("SplitLite")) { $splitLiteEnabled = $true }
 if ($OutDir -eq "") {
   if ($Scope -eq "catalytic-dpt") {
     $OutDir = "MEMORY/LLM_PACKER/_packs/catalytic-dpt-pack-$Stamp"
-  } elseif ($Scope -eq "catalytic-dpt-lab") {
-    $OutDir = "MEMORY/LLM_PACKER/_packs/catalytic-dpt-lab-pack-$Stamp"
-  } elseif ($Profile -eq "lite") {
-    $OutDir = "MEMORY/LLM_PACKER/_packs/llm-pack-lite-$Stamp"
+  } elseif ($Scope -eq "lab") {
+    $OutDir = "MEMORY/LLM_PACKER/_packs/lab-pack-$Stamp"
   } else {
-    $OutDir = "MEMORY/LLM_PACKER/_packs/llm-pack-$Stamp"
+    $OutDir = "MEMORY/LLM_PACKER/_packs/ags-pack-$Stamp"
   }
 }
 
 $args = @(
   "python",
-  $packer,
+  "-m",
+  $pythonModule,
   "--scope", $Scope,
   "--mode", $Mode,
   "--profile", $Profile,
@@ -83,5 +78,10 @@ if ($combinedEnabled) { $args += "--combined" }
 if ($splitLiteEnabled) { $args += "--split-lite" }
 
 Write-Host "Running: $($args -join ' ')"
-& $args[0] $args[1..($args.Count - 1)]
-if ($LASTEXITCODE -ne 0) { exit $LASTEXITCODE }
+Push-Location $repoRoot
+try {
+  & $args[0] $args[1..($args.Count - 1)]
+  if ($LASTEXITCODE -ne 0) { exit $LASTEXITCODE }
+} finally {
+  Pop-Location
+}
diff --git a/MEMORY/LLM_PACKER/Engine/packer.py b/MEMORY/LLM_PACKER/Engine/packer.py
deleted file mode 100644
index c0250b1..0000000
--- a/MEMORY/LLM_PACKER/Engine/packer.py
+++ /dev/null
@@ -1,2192 +0,0 @@
-#!/usr/bin/env python3
-
-"""
-Packer for AGS memory packs.
-
-This script produces two related artifacts:
-
-1) A repository state manifest (hashes + sizes) used to compare snapshots and drive delta
-   packs.
-2) A shareable "LLM pack" directory with curated entrypoints, indices and optional combined
-   markdown suitable for handoff to another model.
-
-
-All outputs are written under `MEMORY/LLM_PACKER/_packs/`:
-
-- User-facing packs default to `_packs/`
-- All non-pack artifacts (fixtures, baselines, zips) go under `_packs/_system/`
-"""
-
-from __future__ import annotations
-
-import argparse
-import ast
-import hashlib
-import json
-import re
-import shutil
-import sys
-from dataclasses import dataclass
-from pathlib import Path
-from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple
-
-try:
-    import tiktoken
-except ImportError:
-    tiktoken = None
-
-# Token Limits
-TOKEN_LIMIT_WARNING = 120000   # Warn approaching standard 128k window
-TOKEN_LIMIT_CRITICAL = 190000  # Critical danger zone (leaving buffer for output)
-
-# ANSI Colors
-ANSI_RED = "\033[91m"
-ANSI_YELLOW = "\033[93m"
-ANSI_RESET = "\033[0m"
-
-PROJECT_ROOT = Path(__file__).resolve().parents[3]
-MEMORY_DIR = PROJECT_ROOT / "MEMORY"
-LLM_PACKER_DIR = MEMORY_DIR / "LLM_PACKER"
-PACKS_ROOT = LLM_PACKER_DIR / "_packs"
-SYSTEM_DIR = PACKS_ROOT / "_system"
-FIXTURE_PACKS_DIR = SYSTEM_DIR / "fixtures"
-STATE_DIR = SYSTEM_DIR / "_state"
-BASELINE_PATH = STATE_DIR / "baseline.json"
-
-if str(PROJECT_ROOT) not in sys.path:
-    sys.path.insert(0, str(PROJECT_ROOT))
-
-from MEMORY.LLM_PACKER.Engine.pack_hygiene import (  # noqa: E402
-    PackLimits,
-    enforce_included_repo_limits,
-    pack_dir_total_bytes,
-    repo_state_content_sha256,
-    validate_repo_state_manifest,
-)
-
-@dataclass(frozen=True)
-class PackScope:
-    key: str
-    title: str
-    file_prefix: str
-    include_dirs: Tuple[str, ...]
-    root_files: Tuple[str, ...]
-    anchors: Tuple[str, ...]
-    excluded_dir_parts: frozenset[str]
-
-
-SCOPE_AGS = PackScope(
-    key="ags",
-    title="Agent Governance System (AGS)",
-    file_prefix="AGS",
-    include_dirs=(
-        "CANON",
-        "CONTEXT",
-        "MAPS",
-        "SKILLS",
-        "CONTRACTS",
-        "MEMORY",
-        "CORTEX",
-        "TOOLS",
-        ".github",
-    ),
-    root_files=(
-        "README.md",
-        "LICENSE",
-        "AGENTS.md",
-        ".gitignore",
-        ".gitattributes",
-        ".editorconfig",
-    ),
-    anchors=(
-        "AGENTS.md",
-        "README.md",
-        "CONTEXT/archive/planning/INDEX.md",
-        "CANON/CONTRACT.md",
-        "CANON/INVARIANTS.md",
-        "CANON/VERSIONING.md",
-        "MAPS/ENTRYPOINTS.md",
-        "CONTRACTS/runner.py",
-        "MEMORY/packer.py",
-    ),
-    excluded_dir_parts=frozenset(
-        {
-            ".git",
-            "BUILD",
-            "_runs",
-            "_generated",
-            "_packs",
-            "Original",
-            "ORIGINAL",
-            "research",
-            "RESEARCH",
-            "__pycache__",
-            "node_modules",
-        }
-    ),
-)
-
-
-SCOPE_CATALYTIC_DPT = PackScope(
-    key="catalytic-dpt",
-    title="CATALYTIC-DPT (MAIN, no LAB)",
-    file_prefix="CATALYTIC-DPT",
-    include_dirs=("CATALYTIC-DPT",),
-    root_files=(),
-    anchors=(
-        "CATALYTIC-DPT/AGENTS.md",
-        "CATALYTIC-DPT/README.md",
-        "CATALYTIC-DPT/ROADMAP_V2.1.md",
-        "CATALYTIC-DPT/swarm_config.json",
-        "CATALYTIC-DPT/CHANGELOG.md",
-    ),
-    excluded_dir_parts=frozenset(
-        {
-            ".git",
-            "BUILD",
-            "LAB",
-            "_runs",
-            "_generated",
-            "_packs",
-            "__pycache__",
-            "node_modules",
-        }
-    ),
-)
-
-
-SCOPE_CATALYTIC_DPT_LAB = PackScope(
-    key="catalytic-dpt-lab",
-    title="CATALYTIC-DPT (LAB)",
-    file_prefix="CATALYTIC-DPT-LAB",
-    include_dirs=("CATALYTIC-DPT/LAB",),
-    root_files=(),
-    anchors=(),
-    excluded_dir_parts=frozenset(
-        {
-            ".git",
-            "BUILD",
-            "_runs",
-            "_generated",
-            "_packs",
-            "__pycache__",
-            "node_modules",
-        }
-    ),
-)
-
-
-SCOPES: Dict[str, PackScope] = {
-    SCOPE_AGS.key: SCOPE_AGS,
-    SCOPE_CATALYTIC_DPT.key: SCOPE_CATALYTIC_DPT,
-    SCOPE_CATALYTIC_DPT_LAB.key: SCOPE_CATALYTIC_DPT_LAB,
-}
-
-TEXT_EXTENSIONS = {
-    ".md",
-    ".txt",
-    ".json",
-    ".py",
-    ".js",
-    ".mjs",
-    ".cjs",
-    ".css",
-    ".html",
-    ".php",
-    ".ps1",
-    ".cmd",
-    ".bat",
-    ".yml",
-    ".yaml",
-}
-
-TEXT_BASENAMES = {".gitignore", ".gitattributes", ".editorconfig", ".htaccess", ".gitkeep", "LICENSE"}
-
-CANON_VERSION_FILE = PROJECT_ROOT / "CANON" / "VERSIONING.md"
-GRAMMAR_VERSION = "1.0"
-
-def hash_file(path: Path) -> str:
-    hasher = hashlib.sha256()
-    with path.open("rb") as f:
-        for chunk in iter(lambda: f.read(8192), b""):
-            hasher.update(chunk)
-    return hasher.hexdigest()
-
-def read_text(path: Path) -> str:
-    return path.read_text(encoding="utf-8", errors="replace")
-
-
-# Token estimation constants
-CHARS_PER_TOKEN = 4  # Rough estimate for English text
-TOKEN_LIMIT_WARNING = 100_000
-TOKEN_LIMIT_CRITICAL = 200_000
-
-
-def estimate_tokens(text: str, model: str = "gpt-4o") -> int:
-    """Estimate token count for text."""
-    # Determinism requirement: do not vary output based on optional dependencies.
-    return len(text) // CHARS_PER_TOKEN
-
-
-def estimate_file_tokens(path: Path) -> int:
-    """Estimate token count for a file."""
-    try:
-        return estimate_tokens(read_text(path))
-    except Exception:
-        return 0
-
-
-def read_canon_version() -> str:
-    if not CANON_VERSION_FILE.exists():
-        return "unknown"
-    text = read_text(CANON_VERSION_FILE)
-    match = re.search(r"canon_version:\s*(\d+\.\d+\.\d+)", text)
-    return match.group(1) if match else "unknown"
-
-
-def is_text_path(path: Path) -> bool:
-    ext = path.suffix.lower()
-    if ext:
-        return ext in TEXT_EXTENSIONS
-    return path.name in TEXT_BASENAMES
-
-
-def is_excluded_rel_path(rel_path: Path, *, excluded_dir_parts: frozenset[str]) -> bool:
-    parts = set(rel_path.parts)
-    if parts & excluded_dir_parts:
-        return True
-    # Allow `.github/` but avoid other hidden folders by default.
-    if any(part.startswith(".") and part != ".github" for part in rel_path.parts):
-        return True
-    return False
-
-
-def iter_repo_candidates(project_root: Path, *, scope: PackScope) -> Iterable[Path]:
-    for directory in scope.include_dirs:
-        base = project_root / directory
-        if not base.exists():
-            continue
-        for path in sorted(base.rglob("*")):
-            try:
-                is_file = path.is_file()
-            except OSError:
-                is_file = False
-            if not is_file:
-                continue
-            rel = path.relative_to(project_root)
-            if is_excluded_rel_path(rel, excluded_dir_parts=scope.excluded_dir_parts):
-                continue
-            yield path
-
-    for file_name in scope.root_files:
-        path = project_root / file_name
-        if path.exists() and path.is_file():
-            yield path
-
-
-def build_state_manifest(project_root: Path, *, scope: PackScope) -> Tuple[Dict[str, Any], List[Dict[str, Any]]]:
-    canon_version = read_canon_version()
-    files: List[Dict[str, Any]] = []
-    omitted: List[Dict[str, Any]] = []
-
-    seen: set[str] = set()
-    for abs_path in iter_repo_candidates(project_root, scope=scope):
-        rel = abs_path.relative_to(project_root).as_posix()
-        if rel in seen:
-            raise RuntimeError(f"PACK_DEDUP_DUPLICATE_PATH:{rel}")
-        seen.add(rel)
-
-        if not is_text_path(abs_path):
-            omitted.append(
-                {
-                    "scope": "repo",
-                    "repoRelPath": rel,
-                    "bytes": abs_path.stat().st_size,
-                }
-            )
-            continue
-
-        files.append(
-            {
-                "path": rel,
-                "hash": hash_file(abs_path),
-                "size": abs_path.stat().st_size,
-            }
-        )
-
-    files.sort(key=lambda e: (e["path"], e["hash"]))
-    manifest: Dict[str, Any] = {
-        "canon_version": canon_version,
-        "grammar_version": GRAMMAR_VERSION,
-        "scope": scope.key,
-        "files": files,
-    }
-    return manifest, omitted
-
-
-def manifest_digest(manifest: Dict[str, Any]) -> str:
-    hasher = hashlib.sha256()
-    for entry in manifest.get("files", []):
-        line = f"{entry['hash']} {entry['size']} {entry['path']}\n"
-        hasher.update(line.encode("utf-8"))
-    return hasher.hexdigest()
-
-
-def baseline_path_for_scope(scope: PackScope) -> Path:
-    if scope.key == SCOPE_AGS.key:
-        return BASELINE_PATH
-    return STATE_DIR / f"baseline-{scope.key}.json"
-
-
-def load_baseline(path: Path) -> Optional[Dict[str, Any]]:
-    if not path.exists():
-        return None
-    try:
-        return json.loads(read_text(path))
-    except Exception:
-        return None
-
-
-def write_json(path: Path, payload: Any) -> None:
-    path.parent.mkdir(parents=True, exist_ok=True)
-    path.write_text(json.dumps(payload, indent=2, sort_keys=True) + "\n", encoding="utf-8")
-
-def _extract_markdown_section(text: str, heading: str) -> str:
-    lines = text.splitlines()
-    start = None
-    for idx, line in enumerate(lines):
-        if line.strip().lower() == f"## {heading}".lower():
-            start = idx + 1
-            break
-    if start is None:
-        return ""
-    out: List[str] = []
-    for line in lines[start:]:
-        if line.startswith("## "):
-            break
-        out.append(line.rstrip())
-        if len(out) >= 30:
-            break
-    return "\n".join([l for l in out if l.strip()]).strip()
-
-
-def _ast_signature(node: ast.AST) -> Dict[str, Any]:
-    if not isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
-        return {}
-    args = node.args
-    posonly = [a.arg for a in getattr(args, "posonlyargs", [])]
-    normal = [a.arg for a in args.args]
-    kwonly = [a.arg for a in args.kwonlyargs]
-    return {
-        "posonlyargs": posonly,
-        "args": normal,
-        "vararg": args.vararg.arg if args.vararg else None,
-        "kwonlyargs": kwonly,
-        "kwarg": args.kwarg.arg if args.kwarg else None,
-    }
-
-
-def _extract_code_symbols(source_text: str, module_path: str) -> Dict[str, Any]:
-    try:
-        tree = ast.parse(source_text)
-    except SyntaxError as exc:
-        return {"module": module_path, "error": str(exc), "symbols": []}
-    symbols: List[Dict[str, Any]] = []
-
-    module_doc = ast.get_docstring(tree) or ""
-
-    for node in tree.body:
-        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
-            symbols.append(
-                {
-                    "kind": "function",
-                    "name": node.name,
-                    "qualname": node.name,
-                    "signature": _ast_signature(node),
-                    "docstring": ast.get_docstring(node) or "",
-                    "lineno": getattr(node, "lineno", None),
-                }
-            )
-        elif isinstance(node, ast.ClassDef):
-            symbols.append(
-                {
-                    "kind": "class",
-                    "name": node.name,
-                    "qualname": node.name,
-                    "bases": [getattr(b, "id", getattr(b, "attr", None)) for b in node.bases],
-                    "docstring": ast.get_docstring(node) or "",
-                    "lineno": getattr(node, "lineno", None),
-                }
-            )
-            for child in node.body:
-                if isinstance(child, (ast.FunctionDef, ast.AsyncFunctionDef)):
-                    symbols.append(
-                        {
-                            "kind": "method",
-                            "name": child.name,
-                            "qualname": f"{node.name}.{child.name}",
-                            "signature": _ast_signature(child),
-                            "docstring": ast.get_docstring(child) or "",
-                            "lineno": getattr(child, "lineno", None),
-                        }
-                    )
-
-    return {"module": module_path, "docstring": module_doc, "symbols": symbols}
-
-
-def _collect_fixture_preview(project_root: Path, rel_path: str, size_bytes: int) -> Dict[str, Any]:
-    preview: Dict[str, Any] = {"type": None}
-    if not rel_path.endswith(".json"):
-        return preview
-    if size_bytes > 256 * 1024:
-        preview["type"] = "json"
-        preview["keys"] = None
-        preview["note"] = "skipped_large_json"
-        return preview
-    try:
-        payload = json.loads((project_root / rel_path).read_text(encoding="utf-8", errors="replace"))
-    except Exception:
-        preview["type"] = "json"
-        preview["keys"] = None
-        preview["note"] = "parse_error"
-        return preview
-    if isinstance(payload, dict):
-        preview["type"] = "object"
-        preview["keys"] = sorted(list(payload.keys()))[:30]
-    elif isinstance(payload, list):
-        preview["type"] = "array"
-        preview["length"] = len(payload)
-    else:
-        preview["type"] = type(payload).__name__
-    return preview
-
-
-def write_lite_indexes(
-    pack_dir: Path,
-    *,
-    project_root: Path,
-    include_paths: Sequence[str],
-    omitted_paths: Sequence[str],
-    files_by_path: Dict[str, Dict[str, Any]],
-) -> None:
-    meta_dir = pack_dir / "meta"
-    meta_dir.mkdir(parents=True, exist_ok=True)
-
-    allowlist = {
-        "profile": "lite",
-        "required_includes": [
-            "AGENTS.md",
-            "README.md",
-            "CANON/**",
-            "MAPS/**",
-            "CONTRACTS/runner.py",
-            "CORTEX/query.py",
-            "TOOLS/critic.py",
-            "SKILLS/**/SKILL.md",
-            "SKILLS/**/version.json",
-        ],
-        "excludes": [
-            "**/fixtures/**",
-            "**/_runs/**",
-            "**/_generated/**",
-            "CONTEXT/research/**",
-            "CONTEXT/archive/**",
-            "MEMORY/**/_packs/**",
-            "**/*.cmd",
-            "**/*.ps1",
-        ],
-    }
-    write_json(meta_dir / "LITE_ALLOWLIST.json", allowlist)
-
-    omitted: List[Dict[str, Any]] = []
-    for rel in sorted(omitted_paths):
-        entry = files_by_path.get(rel)
-        if not entry:
-            continue
-        omitted.append(
-            {
-                "path": rel,
-                "bytes": entry.get("size"),
-                "sha256": entry.get("hash"),
-            }
-        )
-    write_json(meta_dir / "LITE_OMITTED.json", omitted)
-
-    lite_start_here = "\n".join(
-        [
-            "# LITE Pack: START HERE",
-            "",
-            "This is a discussion-first pack profile. It includes high-signal governance + interfaces and omits bulk payload (fixtures, archives, and most code).",
-            "",
-            "## What is included",
-            "- `COMBINED/SPLIT/*` (00..07) for read-order and contracts summaries",
-            "- `meta/*` inventories (FILE_TREE, FILE_INDEX, PACK_INFO, etc.)",
-            "- Core repo entrypoints (AGENTS, CANON, MAPS, runner/query/critic, skill manifests)",
-            "",
-            "## What is omitted",
-            "- Fixture trees and large snapshots are not copied into `repo/**` in this profile.",
-            "- See `meta/LITE_OMITTED.json` for the omitted path list (with sizes and hashes).",
-            "",
-            "## When you need FULL",
-            "- Use the FULL profile to access the complete `repo/**` snapshot for deep dives and exact reconstruction.",
-            "",
-        ]
-    ).rstrip() + "\n"
-    (meta_dir / "LITE_START_HERE.md").write_text(lite_start_here, encoding="utf-8")
-
-    # SKILL_INDEX.json (from SKILL.md files that are included in LITE)
-    skill_index: List[Dict[str, Any]] = []
-    skill_manifests = [p for p in include_paths if p.startswith("SKILLS/") and p.endswith("/SKILL.md")]
-    for rel in sorted(skill_manifests):
-        skill_name = rel.split("/")[1] if len(rel.split("/")) >= 2 else rel
-        text = read_text(project_root / rel)
-        skill_index.append(
-            {
-                "name": skill_name,
-                "path": f"repo/{rel}",
-                "required_canon_version": next(
-                    (line.split(":", 1)[1].strip() for line in text.splitlines() if "required_canon_version" in line),
-                    "",
-                ),
-                "inputs": _extract_markdown_section(text, "Inputs"),
-                "outputs": _extract_markdown_section(text, "Outputs"),
-                "constraints": _extract_markdown_section(text, "Constraints"),
-            }
-        )
-    write_json(meta_dir / "SKILL_INDEX.json", skill_index)
-
-    # FIXTURE_INDEX.json (inventory only; do not copy blobs)
-    fixture_index: List[Dict[str, Any]] = []
-    for rel, entry in sorted(files_by_path.items(), key=lambda kv: kv[0]):
-        if "/fixtures/" not in rel:
-            continue
-        if not rel.endswith(".json"):
-            continue
-        size = int(entry.get("size") or 0)
-        fixture_index.append(
-            {
-                "path": rel,
-                "bytes": size,
-                "sha256": entry.get("hash"),
-                "preview": _collect_fixture_preview(project_root, rel, size),
-            }
-        )
-        if len(fixture_index) >= 5000:
-            break
-    write_json(meta_dir / "FIXTURE_INDEX.json", fixture_index)
-
-    # CODEBOOK.md (hot entrypoints table)
-    hot_paths: List[str] = [
-        "CONTRACTS/runner.py",
-        "CORTEX/query.py",
-        "TOOLS/critic.py",
-        "MEMORY/LLM_PACKER/Engine/packer.py",
-    ]
-    for rel in sorted({p.replace("SKILLS/", "SKILLS/") for p in files_by_path.keys() if p.startswith("SKILLS/") and p.endswith("/run.py")}):
-        hot_paths.append(rel)
-
-    def module_purpose(path_rel: str) -> str:
-        abs_path = project_root / path_rel
-        if not abs_path.exists():
-            return "not present in repo"
-        text = read_text(abs_path)
-        symbols = _extract_code_symbols(text, path_rel)
-        doc = (symbols.get("docstring") or "").strip().splitlines()
-        return doc[0].strip() if doc else "see file"
-
-    codebook_lines = [
-        "# CODEBOOK (LITE)",
-        "",
-        "Symbolic table of hot entrypoints. This file does not embed full source bodies.",
-        "",
-        "| Path | Included in LITE | Purpose |",
-        "|---|---:|---|",
-    ]
-    include_set = set(include_paths)
-    for rel in sorted(set(hot_paths)):
-        included = "yes" if rel in include_set else "no"
-        purpose = module_purpose(rel).replace("|", "\\|")
-        codebook_lines.append(f"| `repo/{rel}` | {included} | {purpose} |")
-    codebook_lines.append("")
-    (meta_dir / "CODEBOOK.md").write_text("\n".join(codebook_lines), encoding="utf-8")
-
-    # CODE_SYMBOLS.json (AST symbols for included code files only; no bodies)
-    code_symbols: List[Dict[str, Any]] = []
-    for rel in sorted(include_paths):
-        if not rel.endswith(".py"):
-            continue
-        abs_path = project_root / rel
-        if not abs_path.exists():
-            continue
-        code_symbols.append(_extract_code_symbols(read_text(abs_path), rel))
-    write_json(meta_dir / "CODE_SYMBOLS.json", code_symbols)
-
-
-def verify_manifest(pack_dir: Path) -> Tuple[bool, List[str]]:
-    """
-    Verify pack integrity by checking file hashes against manifest.
-    
-    Returns:
-        (is_valid, errors): True if all hashes match, list of errors if any.
-    """
-    errors: List[str] = []
-    
-    # Load the pack manifest
-    manifest_path = pack_dir / "meta" / "REPO_STATE.json"
-    if not manifest_path.exists():
-        errors.append(f"Manifest not found: {manifest_path}")
-        return False, errors
-    
-    try:
-        manifest = json.loads(read_text(manifest_path))
-    except Exception as e:
-        errors.append(f"Failed to load manifest: {e}")
-        return False, errors
-
-    allow_dup = True
-    pack_info_path = pack_dir / "meta" / "PACK_INFO.json"
-    if pack_info_path.exists():
-        try:
-            pack_info = json.loads(read_text(pack_info_path))
-            allow_dup = bool(pack_info.get("limits", {}).get("allow_duplicate_hashes", True))
-            expected_repo_state_sha = pack_info.get("repo_state_sha256")
-            if isinstance(expected_repo_state_sha, str) and expected_repo_state_sha:
-                actual_repo_state_sha = repo_state_content_sha256(manifest)
-                if actual_repo_state_sha != expected_repo_state_sha:
-                    errors.append("Repo state checksum mismatch (repo_state_sha256)")
-        except Exception as e:
-            errors.append(f"Failed to load PACK_INFO.json: {e}")
-            return False, errors
-
-    try:
-        validate_repo_state_manifest(manifest, allow_duplicate_hashes=allow_dup)
-    except Exception as e:
-        errors.append(str(e))
-        return False, errors
-    
-    # Verify each file in the manifest
-    for entry in manifest.get("files", []):
-        rel_path = entry.get("path", "")
-        expected_hash = entry.get("hash", "")
-        expected_size = entry.get("size", 0)
-        
-        file_path = pack_dir / "repo" / rel_path
-        if not file_path.exists():
-            errors.append(f"Missing file: {rel_path}")
-            continue
-        
-        actual_size = file_path.stat().st_size
-        if actual_size != expected_size:
-            errors.append(f"Size mismatch for {rel_path}: expected {expected_size}, got {actual_size}")
-            continue
-        
-        actual_hash = hash_file(file_path)
-        if actual_hash != expected_hash:
-            errors.append(f"Hash mismatch for {rel_path}: expected {expected_hash[:12]}..., got {actual_hash[:12]}...")
-    
-    return len(errors) == 0, errors
-
-
-def load_and_verify_pack(pack_dir: Path) -> Tuple[Optional[Dict[str, Any]], List[str]]:
-    """
-    Load a pack and verify its integrity.
-    
-    Returns:
-        (manifest, errors): The manifest dict if valid, None if invalid. List of errors.
-    """
-    is_valid, errors = verify_manifest(pack_dir)
-    if not is_valid:
-        return None, errors
-    
-    manifest_path = pack_dir / "meta" / "REPO_STATE.json"
-    manifest = json.loads(read_text(manifest_path))
-    return manifest, []
-
-
-def choose_fence(text: str) -> str:
-    matches = re.findall(r"`+", text)
-    longest = max((len(m) for m in matches), default=0)
-    return "`" * max(3, longest + 1)
-
-
-def infer_lang(rel_path: str) -> str:
-    suffix = Path(rel_path).suffix.lower()
-    return {
-        ".json": "json",
-        ".md": "md",
-        ".py": "python",
-        ".js": "js",
-        ".mjs": "js",
-        ".cjs": "js",
-        ".yml": "yaml",
-        ".yaml": "yaml",
-        ".ps1": "powershell",
-        ".cmd": "bat",
-        ".bat": "bat",
-        ".css": "css",
-        ".html": "html",
-        ".php": "php",
-        ".txt": "text",
-    }.get(suffix, "")
-
-
-def build_combined_md_block(rel_path: str, text: str, byte_count: int) -> str:
-    fence = choose_fence(text)
-    lang = infer_lang(rel_path)
-    fence_open = fence + (lang if lang else "")
-    return "\n".join(
-        [
-            "",
-            "-----",
-            f"Source: `{rel_path}`",
-            f"Bytes: {byte_count}",
-            "-----",
-            "",
-            fence_open,
-            text.rstrip("\n"),
-            fence,
-        ]
-    )
-
-
-def build_combined_txt_block(rel_path: str, text: str, byte_count: int) -> str:
-    return "\n".join(
-        [
-            "",
-            "-----",
-            f"Source: {rel_path}",
-            f"Bytes: {byte_count}",
-            "-----",
-            "",
-            text.rstrip("\n"),
-        ]
-    )
-
-
-class _TreeNode:
-    def __init__(self) -> None:
-        self.dirs: Dict[str, "_TreeNode"] = {}
-        self.files: set[str] = set()
-
-
-def _sort_key(name: str) -> Tuple[str, str]:
-    folded = name.casefold()
-    return (folded, name)
-
-
-def _add_tree_path(root: _TreeNode, rel_path: str) -> None:
-    parts = [p for p in rel_path.split("/") if p]
-    if not parts:
-        return
-    node = root
-    for idx, part in enumerate(parts):
-        is_leaf = idx == len(parts) - 1
-        if is_leaf:
-            node.files.add(part)
-            return
-        node = node.dirs.setdefault(part, _TreeNode())
-
-
-def _render_tree(node: _TreeNode, prefix: str, out_lines: List[str]) -> None:
-    dir_names = sorted(node.dirs.keys(), key=_sort_key)
-    file_names = sorted(node.files, key=_sort_key)
-    entries: List[Tuple[str, str]] = [("dir", d) for d in dir_names] + [("file", f) for f in file_names]
-
-    for idx, (kind, name) in enumerate(entries):
-        is_last = idx == len(entries) - 1
-        connector = "\\-- " if is_last else "|-- "
-        child_prefix = prefix + ("    " if is_last else "|   ")
-        if kind == "dir":
-            out_lines.append(prefix + connector + name + "/")
-            _render_tree(node.dirs[name], child_prefix, out_lines)
-        else:
-            out_lines.append(prefix + connector + name)
-
-
-def build_pack_tree_text(paths: Sequence[str], extra_paths: Sequence[str]) -> str:
-    root = _TreeNode()
-    for rel in paths:
-        _add_tree_path(root, rel)
-    for rel in extra_paths:
-        _add_tree_path(root, rel)
-
-    lines: List[str] = ["PACK/"]
-    _render_tree(root, "", lines)
-    return "\n".join(lines).rstrip() + "\n"
-
-
-def write_split_pack_ags(pack_dir: Path, included_repo_paths: Sequence[str]) -> None:
-    split_dir = pack_dir / "COMBINED" / "SPLIT"
-    split_dir.mkdir(parents=True, exist_ok=True)
-
-    def section(paths: Sequence[str]) -> str:
-        out_lines: List[str] = []
-        for rel in paths:
-            src = pack_dir / rel
-            if not src.exists():
-                continue
-            text = read_text(src)
-            fence = choose_fence(text)
-            out_lines.append(f"## `{rel}`")
-            out_lines.append("")
-            out_lines.append(fence)
-            out_lines.append(text.rstrip("\n"))
-            out_lines.append(fence)
-            out_lines.append("")
-        return "\n".join(out_lines).rstrip() + "\n"
-
-    canon_paths = [p for p in included_repo_paths if p.startswith("repo/CANON/")]
-    root_paths = [p for p in included_repo_paths if p.startswith("repo/") and p.count("/") == 1]
-    maps_paths = [p for p in included_repo_paths if p.startswith("repo/MAPS/")]
-    context_paths = [p for p in included_repo_paths if p.startswith("repo/CONTEXT/")]
-    skills_paths = [p for p in included_repo_paths if p.startswith("repo/SKILLS/")]
-    contracts_paths = [p for p in included_repo_paths if p.startswith("repo/CONTRACTS/")]
-    cortex_paths = [p for p in included_repo_paths if p.startswith("repo/CORTEX/")]
-    memory_paths = [p for p in included_repo_paths if p.startswith("repo/MEMORY/")]
-    tools_paths = [p for p in included_repo_paths if p.startswith("repo/TOOLS/")]
-    github_paths = [p for p in included_repo_paths if p.startswith("repo/.github/")]
-
-    # Also discover meta files for snapshots
-    meta_dir = pack_dir / "meta"
-    meta_paths = []
-    if meta_dir.exists():
-        meta_paths = sorted([f"meta/{p.name}" for p in meta_dir.iterdir() if p.is_file()])
-
-
-    (split_dir / "AGS-00_INDEX.md").write_text(
-        "\n".join(
-            [
-                "# AGS Pack Index",
-                "",
-                "This directory contains a generated snapshot of the repository intended for LLM handoff.",
-                "",
-                "## Read order",
-                "1) `repo/AGENTS.md`",
-                "2) `repo/README.md` and `repo/CONTEXT/archive/planning/INDEX.md`",
-                "3) `repo/CANON/CONTRACT.md` and `repo/CANON/INVARIANTS.md` and `repo/CANON/VERSIONING.md`",
-                "4) `repo/MAPS/ENTRYPOINTS.md`",
-                "5) `repo/CONTRACTS/runner.py` and `repo/SKILLS/`",
-                "6) `repo/CORTEX/` and `repo/TOOLS/`",
-                "7) `meta/ENTRYPOINTS.md` and `meta/CONTEXT.txt` (Snapshot specific)",
-                "",
-                "## Notes",
-        "- `BUILD` contents are not included. Only a file tree inventory is captured in `meta/BUILD_TREE.txt`.",
-                "- Research under `repo/CONTEXT/research/` is non-binding and opt-in.",
-                "- If `--combined` is enabled, `COMBINED/` contains `AGS-FULL-COMBINED-*` and `AGS-FULL-TREEMAP-*` outputs.",
-                "",
-            ]
-        ),
-        encoding="utf-8",
-    )
-
-    (split_dir / "AGS-01_CANON.md").write_text("# Canon\n\n" + section(canon_paths), encoding="utf-8")
-    (split_dir / "AGS-02_ROOT.md").write_text("# Root\n\n" + section(root_paths), encoding="utf-8")
-    (split_dir / "AGS-03_MAPS.md").write_text("# Maps\n\n" + section(maps_paths), encoding="utf-8")
-    (split_dir / "AGS-04_CONTEXT.md").write_text("# Context\n\n" + section(context_paths), encoding="utf-8")
-    (split_dir / "AGS-05_SKILLS.md").write_text("# Skills\n\n" + section(skills_paths), encoding="utf-8")
-    (split_dir / "AGS-06_CONTRACTS.md").write_text("# Contracts\n\n" + section(contracts_paths), encoding="utf-8")
-    (split_dir / "AGS-07_SYSTEM.md").write_text(
-        "# System\n\n" + section([*cortex_paths, *memory_paths, *tools_paths, *github_paths, *meta_paths]),
-        encoding="utf-8",
-    )
-
-
-def write_split_pack_catalytic_dpt(pack_dir: Path, included_repo_paths: Sequence[str], *, scope: PackScope) -> None:
-    split_dir = pack_dir / "COMBINED" / "SPLIT"
-    split_dir.mkdir(parents=True, exist_ok=True)
-
-    def section(paths: Sequence[str]) -> str:
-        out_lines: List[str] = []
-        for rel in paths:
-            src = pack_dir / rel
-            if not src.exists():
-                continue
-            text = read_text(src)
-            fence = choose_fence(text)
-            out_lines.append(f"## `{rel}`")
-            out_lines.append("")
-            out_lines.append(fence)
-            out_lines.append(text.rstrip("\n"))
-            out_lines.append(fence)
-            out_lines.append("")
-        return "\n".join(out_lines).rstrip() + "\n"
-
-    def is_cdpt(path: str) -> bool:
-        return path.startswith("repo/CATALYTIC-DPT/")
-
-    cdpt_paths = [p for p in included_repo_paths if is_cdpt(p)]
-    cdpt_root = [p for p in cdpt_paths if p.count("/") == 2]
-
-    agents_paths = [p for p in cdpt_root if p.endswith("/AGENTS.md")]
-    readme_paths = [p for p in cdpt_root if p.endswith("/README.md")]
-    roadmap_paths = [p for p in cdpt_root if "ROADMAP" in p.upper()]
-    changelog_paths = [p for p in cdpt_root if p.endswith("/CHANGELOG.md")]
-    architecture_docs = [p for p in cdpt_root if p.endswith(".md") and p not in {*agents_paths, *readme_paths, *roadmap_paths, *changelog_paths}]
-
-    config_paths = [p for p in cdpt_root if p.endswith(".json")] + [p for p in cdpt_paths if "/SCHEMAS/" in p or "/MCP/" in p]
-    testbench_paths = [p for p in cdpt_paths if "/TESTBENCH/" in p or "/FIXTURES/" in p]
-
-    docs_paths = [
-        *agents_paths,
-        *readme_paths,
-        *roadmap_paths,
-        *changelog_paths,
-        *architecture_docs,
-    ]
-    docs_paths = sorted(set(docs_paths))
-
-    config_paths = sorted(set(config_paths) - set(docs_paths))
-    testbench_paths = sorted(set(testbench_paths) - set(docs_paths))
-
-    system_paths = sorted(set(cdpt_paths) - set(docs_paths) - set(config_paths) - set(testbench_paths))
-
-    meta_dir = pack_dir / "meta"
-    meta_paths = sorted([f"meta/{p.name}" for p in meta_dir.iterdir() if p.is_file()]) if meta_dir.exists() else []
-
-    (split_dir / f"{scope.file_prefix}-00_INDEX.md").write_text(
-        "\n".join(
-            [
-                f"# {scope.title} Pack Index",
-                "",
-                "This directory contains a generated snapshot intended for LLM handoff.",
-                "",
-                "## Read order",
-                "1) `repo/CATALYTIC-DPT/AGENTS.md`",
-                "2) `repo/CATALYTIC-DPT/README.md`",
-                "3) `repo/CATALYTIC-DPT/ROADMAP_V2.1.md`",
-                "4) `repo/CATALYTIC-DPT/swarm_config.json`",
-                "5) `repo/CATALYTIC-DPT/CHANGELOG.md`",
-                "6) `meta/ENTRYPOINTS.md` and `meta/CONTEXT.txt`",
-                "",
-                "## Notes",
-                f"- If `--combined` is enabled, `COMBINED/` contains `{scope.file_prefix}-FULL-COMBINED-*` and `{scope.file_prefix}-FULL-TREEMAP-*` outputs.",
-                "- LAB is packed separately into `LAB/` inside the same bundle (use the LAB pack's `LAB/meta/START_HERE.md`).",
-                "",
-            ]
-        ),
-        encoding="utf-8",
-    )
-
-    (split_dir / f"{scope.file_prefix}-01_DOCS.md").write_text("# Docs\n\n" + section(docs_paths), encoding="utf-8")
-    (split_dir / f"{scope.file_prefix}-02_CONFIG.md").write_text("# Config\n\n" + section(config_paths), encoding="utf-8")
-    (split_dir / f"{scope.file_prefix}-03_TESTBENCH.md").write_text("# Testbench\n\n" + section(testbench_paths), encoding="utf-8")
-    (split_dir / f"{scope.file_prefix}-04_SYSTEM.md").write_text("# System\n\n" + section([*system_paths, *meta_paths]), encoding="utf-8")
-
-
-def write_split_pack_catalytic_dpt_lab(pack_dir: Path, included_repo_paths: Sequence[str], *, scope: PackScope) -> None:
-    split_dir = pack_dir / "COMBINED" / "SPLIT"
-    split_dir.mkdir(parents=True, exist_ok=True)
-
-    def section(paths: Sequence[str]) -> str:
-        out_lines: List[str] = []
-        for rel in paths:
-            src = pack_dir / rel
-            if not src.exists():
-                continue
-            text = read_text(src)
-            fence = choose_fence(text)
-            out_lines.append(f"## `{rel}`")
-            out_lines.append("")
-            out_lines.append(fence)
-            out_lines.append(text.rstrip("\n"))
-            out_lines.append(fence)
-            out_lines.append("")
-        return "\n".join(out_lines).rstrip() + "\n"
-
-    lab_prefix = "repo/CATALYTIC-DPT/LAB/"
-    lab_paths = sorted([p for p in included_repo_paths if p.startswith(lab_prefix)])
-
-    commonsense_paths = sorted([p for p in lab_paths if "/COMMONSENSE/" in p])
-    mcp_paths = sorted([p for p in lab_paths if "/MCP/" in p])
-    research_paths = sorted([p for p in lab_paths if "/RESEARCH/" in p])
-    archive_paths = sorted([p for p in lab_paths if "/ARCHIVE/" in p])
-
-    used = set(commonsense_paths) | set(mcp_paths) | set(research_paths) | set(archive_paths)
-    root_docs = sorted([p for p in lab_paths if p.count("/") == 3 and p.lower().endswith((".md", ".txt"))])
-    used |= set(root_docs)
-    other_paths = sorted([p for p in lab_paths if p not in used])
-
-    meta_dir = pack_dir / "meta"
-    meta_paths = sorted([f"meta/{p.name}" for p in meta_dir.iterdir() if p.is_file()]) if meta_dir.exists() else []
-
-    (split_dir / f"{scope.file_prefix}-00_INDEX.md").write_text(
-        "\n".join(
-            [
-                f"# {scope.title} Pack Index",
-                "",
-                "This directory contains a generated snapshot of the LAB subtree intended for LLM handoff.",
-                "",
-                "## Read order",
-                "1) `repo/CATALYTIC-DPT/LAB/`",
-                f"2) `COMBINED/SPLIT/{scope.file_prefix}-01_DOCS.md`",
-                f"3) `COMBINED/SPLIT/{scope.file_prefix}-02_COMMONSENSE.md`",
-                f"4) `COMBINED/SPLIT/{scope.file_prefix}-03_MCP.md`",
-                f"5) `COMBINED/SPLIT/{scope.file_prefix}-04_RESEARCH.md`",
-                f"6) `meta/ENTRYPOINTS.md` and `meta/CONTEXT.txt`",
-                "",
-            ]
-        ),
-        encoding="utf-8",
-    )
-
-    (split_dir / f"{scope.file_prefix}-01_DOCS.md").write_text("# Docs\n\n" + section(root_docs), encoding="utf-8")
-    (split_dir / f"{scope.file_prefix}-02_COMMONSENSE.md").write_text("# COMMONSENSE\n\n" + section(commonsense_paths), encoding="utf-8")
-    (split_dir / f"{scope.file_prefix}-03_MCP.md").write_text("# MCP\n\n" + section(mcp_paths), encoding="utf-8")
-    (split_dir / f"{scope.file_prefix}-04_RESEARCH.md").write_text("# RESEARCH\n\n" + section(research_paths), encoding="utf-8")
-    (split_dir / f"{scope.file_prefix}-05_ARCHIVE.md").write_text("# ARCHIVE\n\n" + section(archive_paths), encoding="utf-8")
-    (split_dir / f"{scope.file_prefix}-06_SYSTEM.md").write_text("# System\n\n" + section([*other_paths, *meta_paths]), encoding="utf-8")
-
-
-def write_split_pack(pack_dir: Path, included_repo_paths: Sequence[str], *, scope: PackScope) -> None:
-    if scope.key == SCOPE_AGS.key:
-        write_split_pack_ags(pack_dir, included_repo_paths)
-    elif scope.key == SCOPE_CATALYTIC_DPT.key:
-        write_split_pack_catalytic_dpt(pack_dir, included_repo_paths, scope=scope)
-    elif scope.key == SCOPE_CATALYTIC_DPT_LAB.key:
-        write_split_pack_catalytic_dpt_lab(pack_dir, included_repo_paths, scope=scope)
-    else:
-        raise ValueError(f"Unsupported scope for split pack: {scope.key}")
-
-
-def write_split_pack_lite(pack_dir: Path, *, scope: PackScope) -> None:
-    """
-    Write a discussion-first SPLIT set alongside the full SPLIT docs.
-
-    This does not affect what FULL includes/copies under repo/**; it is derived
-    documentation intended for fast navigation and lower token load.
-    """
-    split_dir = pack_dir / "COMBINED" / "SPLIT_LITE"
-    split_dir.mkdir(parents=True, exist_ok=True)
-
-    def write(path: Path, text: str) -> None:
-        path.write_text(text.rstrip() + "\n", encoding="utf-8")
-
-    if scope.key == SCOPE_AGS.key:
-        write(
-            split_dir / "AGS-00_INDEX.md",
-            "\n".join(
-                [
-                    "# AGS Pack Index (SPLIT_LITE)",
-                    "",
-                    "This directory contains a compressed, discussion-first map of the pack (pointers + indexes).",
-                    "",
-                    "## Read order",
-                    "1) `repo/AGENTS.md`",
-                    "2) `repo/README.md`",
-                    "3) `repo/CANON/CONTRACT.md` and `repo/CANON/INVARIANTS.md` and `repo/CANON/VERSIONING.md`",
-                    "4) `repo/MAPS/ENTRYPOINTS.md`",
-                    "5) `repo/CONTRACTS/runner.py` and `repo/SKILLS/*/SKILL.md`",
-                    "6) `repo/CORTEX/query.py` and `repo/TOOLS/critic.py`",
-                    "7) `meta/PACK_INFO.json` (and `meta/REPO_STATE.json` if present)",
-                    "8) `meta/FILE_TREE.txt` and `meta/FILE_INDEX.json`",
-                    "",
-                ]
-            ),
-        )
-    if scope.key == SCOPE_CATALYTIC_DPT_LAB.key:
-        write(
-            split_dir / f"{scope.file_prefix}-00_INDEX.md",
-            "\n".join(
-                [
-                    f"# {scope.title} Pack Index (SPLIT_LITE)",
-                    "",
-                    "This directory contains a compressed, discussion-first map of the LAB-only pack (pointers + indexes).",
-                    "",
-                    "## Read order",
-                    "1) `repo/CATALYTIC-DPT/LAB/`",
-                    "2) `meta/PACK_INFO.json` (and `meta/REPO_STATE.json` if present)",
-                    "3) `meta/FILE_TREE.txt` and `meta/FILE_INDEX.json`",
-                    "",
-                    "## SPLIT (full chunks)",
-                    f"- `{scope.file_prefix}-01_DOCS.md`",
-                    f"- `{scope.file_prefix}-02_COMMONSENSE.md`",
-                    f"- `{scope.file_prefix}-03_MCP.md`",
-                    f"- `{scope.file_prefix}-04_RESEARCH.md`",
-                    f"- `{scope.file_prefix}-05_ARCHIVE.md`",
-                    f"- `{scope.file_prefix}-06_SYSTEM.md`",
-                    "",
-                ]
-            ),
-        )
-
-        def pointer(title: str, full_chunk: str) -> str:
-            return "\n".join(
-                [
-                    f"# {scope.title}: {title} (SPLIT_LITE)",
-                    "",
-                    "This is a pointer-only file; load the full chunk for contents.",
-                    "",
-                    "## Full chunk",
-                    f"- `COMBINED/SPLIT/{full_chunk}`",
-                    "",
-                ]
-            )
-
-        write(split_dir / f"{scope.file_prefix}-01_DOCS.md", pointer("Docs", f"{scope.file_prefix}-01_DOCS.md"))
-        write(split_dir / f"{scope.file_prefix}-02_COMMONSENSE.md", pointer("COMMONSENSE", f"{scope.file_prefix}-02_COMMONSENSE.md"))
-        write(split_dir / f"{scope.file_prefix}-03_MCP.md", pointer("MCP", f"{scope.file_prefix}-03_MCP.md"))
-        write(split_dir / f"{scope.file_prefix}-04_RESEARCH.md", pointer("RESEARCH", f"{scope.file_prefix}-04_RESEARCH.md"))
-        write(split_dir / f"{scope.file_prefix}-05_ARCHIVE.md", pointer("ARCHIVE", f"{scope.file_prefix}-05_ARCHIVE.md"))
-        write(split_dir / f"{scope.file_prefix}-06_SYSTEM.md", pointer("System", f"{scope.file_prefix}-06_SYSTEM.md"))
-        return
-
-    if scope.key == SCOPE_CATALYTIC_DPT.key:
-        write(
-            split_dir / f"{scope.file_prefix}-00_INDEX.md",
-            "\n".join(
-                [
-                    f"# {scope.title} Pack Index (SPLIT_LITE)",
-                    "",
-                    "This directory contains a compressed, discussion-first map of the pack (pointers + indexes).",
-                    "",
-                    "## Read order",
-                    "1) `repo/CATALYTIC-DPT/AGENTS.md`",
-                    "2) `repo/CATALYTIC-DPT/README.md`",
-                    "3) `repo/CATALYTIC-DPT/ROADMAP_V2.1.md`",
-                    "4) `repo/CATALYTIC-DPT/swarm_config.json`",
-                    "5) `meta/PACK_INFO.json` (and `meta/REPO_STATE.json` if present)",
-                    "6) `meta/FILE_TREE.txt` and `meta/FILE_INDEX.json`",
-                    "",
-                    "## SPLIT (full chunks)",
-                    f"- `{scope.file_prefix}-01_DOCS.md`",
-                    f"- `{scope.file_prefix}-02_CONFIG.md`",
-                    f"- `{scope.file_prefix}-03_TESTBENCH.md`",
-                    f"- `{scope.file_prefix}-04_SYSTEM.md`",
-                    "",
-                    "## Repo File Tree",
-                    "",
-                    "See `meta/FILE_TREE.txt` and `meta/FILE_INDEX.json`.",
-                    "",
-                ]
-            ),
-        )
-
-        write(
-            split_dir / f"{scope.file_prefix}-01_DOCS.md",
-            "\n".join(
-                [
-                    f"# {scope.title}: Docs (SPLIT_LITE)",
-                    "",
-                    "Pointers to key docs and the full chunked doc payload.",
-                    "",
-                    "## Key docs",
-                    "- `repo/CATALYTIC-DPT/AGENTS.md`",
-                    "- `repo/CATALYTIC-DPT/README.md`",
-                    "- `repo/CATALYTIC-DPT/CHANGELOG.md`",
-                    "- `repo/CATALYTIC-DPT/ROADMAP_V2.1.md`",
-                    "- `repo/CATALYTIC-DPT/ORCHESTRATION_ARCHITECTURE.md`",
-                    "- `repo/CATALYTIC-DPT/RECURSIVE_SWARM_ARCHITECTURE.md`",
-                    "",
-                    "## Full chunk",
-                    f"- `COMBINED/SPLIT/{scope.file_prefix}-01_DOCS.md`",
-                    "",
-                ]
-            ),
-        )
-
-        write(
-            split_dir / f"{scope.file_prefix}-02_CONFIG.md",
-            "\n".join(
-                [
-                    f"# {scope.title}: Config (SPLIT_LITE)",
-                    "",
-                    "Pointers to core config + schema locations.",
-                    "",
-                    "## Key files",
-                    "- `repo/CATALYTIC-DPT/swarm_config.json`",
-                    "- `repo/CATALYTIC-DPT/SCHEMAS/`",
-                    "- `repo/CATALYTIC-DPT/MCP/`",
-                    "",
-                    "## Full chunk",
-                    f"- `COMBINED/SPLIT/{scope.file_prefix}-02_CONFIG.md`",
-                    "",
-                ]
-            ),
-        )
-
-        write(
-            split_dir / f"{scope.file_prefix}-03_TESTBENCH.md",
-            "\n".join(
-                [
-                    f"# {scope.title}: Testbench (SPLIT_LITE)",
-                    "",
-                    "Pointers to testing and fixtures for quick validation.",
-                    "",
-                    "## Key folders",
-                    "- `repo/CATALYTIC-DPT/TESTBENCH/`",
-                    "- `repo/CATALYTIC-DPT/FIXTURES/`",
-                    "",
-                    "## Full chunk",
-                    f"- `COMBINED/SPLIT/{scope.file_prefix}-03_TESTBENCH.md`",
-                    "",
-                ]
-            ),
-        )
-
-        write(
-            split_dir / f"{scope.file_prefix}-04_SYSTEM.md",
-            "\n".join(
-                [
-                    f"# {scope.title}: System (SPLIT_LITE)",
-                    "",
-                    "Pointers to the full snapshot and meta inventories.",
-                    "",
-                    "## Repo snapshot",
-                    "- `repo/CATALYTIC-DPT/**` (main; LAB excluded)",
-                    "- LAB sub-pack lives under `LAB/` (separate `meta/`, `repo/`, `COMBINED/`)",
-                    "",
-                    "## Meta inventories",
-                    "- `meta/START_HERE.md`",
-                    "- `meta/ENTRYPOINTS.md`",
-                    "- `meta/PACK_INFO.json`",
-                    "- `meta/REPO_STATE.json`",
-                    "- `meta/FILE_TREE.txt`",
-                    "- `meta/FILE_INDEX.json`",
-                    "- `meta/CONTEXT.txt`",
-                    "",
-                    "## Full chunk",
-                    f"- `COMBINED/SPLIT/{scope.file_prefix}-04_SYSTEM.md`",
-                    "",
-                ]
-            ),
-        )
-
-    if scope.key == SCOPE_AGS.key:
-        write(
-            split_dir / "AGS-01_CANON.md",
-            "\n".join(
-                [
-                    "# Canon (SPLIT_LITE)",
-                    "",
-                    "See `repo/CANON/*` (canonical rules).",
-                ]
-            ),
-        )
-
-    if scope.key == SCOPE_AGS.key:
-        write(
-            split_dir / "AGS-02_ROOT.md",
-            "\n".join(
-                [
-                    "# Root (SPLIT_LITE)",
-                    "",
-                    "- `repo/AGENTS.md` (agent procedure)",
-                    "- `repo/README.md` (orientation)",
-                    "- `repo/.gitignore` (generated artifacts exclusions)",
-                    "",
-                ]
-            ),
-        )
-
-    if scope.key == SCOPE_AGS.key:
-        write(
-            split_dir / "AGS-03_MAPS.md",
-            "\n".join(
-                [
-                    "# Maps (SPLIT_LITE)",
-                    "",
-                    "- Core navigation: `repo/MAPS/ENTRYPOINTS.md`",
-                    "- Data flow: `repo/MAPS/DATA_FLOW.md`",
-                    "",
-                    "## Repo File Tree",
-                    "",
-                    "See `meta/FILE_TREE.txt` and `meta/FILE_INDEX.json`.",
-                    "",
-                ]
-            ),
-        )
-
-    if scope.key == SCOPE_AGS.key:
-        # Skills summary table from the repo snapshot (if present in the pack).
-        skills_root = pack_dir / "repo" / "SKILLS"
-        skill_names = sorted([p.name for p in skills_root.iterdir() if p.is_dir() and not p.name.startswith("_")]) if skills_root.exists() else []
-        rows = ["| Skill | Contract | Entrypoint (repo path, may be omitted in LITE) |", "|---|---|---|"]
-        for name in skill_names:
-            contract = f"`repo/SKILLS/{name}/SKILL.md`"
-            entry = f"`repo/SKILLS/{name}/run.py`" if (skills_root / name / "run.py").exists() else f"`repo/SKILLS/{name}/`"
-            rows.append(f"| `{name}` | {contract} | {entry} |")
-        write(
-            split_dir / "AGS-05_SKILLS.md",
-            "\n".join(
-                [
-                    "# Skills (SPLIT_LITE)",
-                    "",
-                    "LITE ships manifests + pointers; implementations are accessed on demand.",
-                    "",
-                    "In LITE, `repo/SKILLS/*/SKILL.md` is the required interface. `run.py` / `validate.py` may be omitted.",
-                    "If you need implementation details, load them from a FULL (or TEST) pack or from the repo filesystem.",
-                    "",
-                    "## Skills Table",
-                    "",
-                    *rows,
-                    "",
-                ]
-            ),
-        )
-
-    if scope.key == SCOPE_AGS.key:
-        write(
-            split_dir / "AGS-04_CONTEXT.md",
-            "\n".join(
-                [
-                    "# Context (SPLIT_LITE)",
-                    "",
-                    "- Decisions: `repo/CONTEXT/decisions/`",
-                    "- Preferences: `repo/CONTEXT/preferences/`",
-                    "",
-                ]
-            ),
-        )
-
-    if scope.key == SCOPE_AGS.key:
-        write(
-            split_dir / "AGS-06_CONTRACTS.md",
-            "\n".join(
-                [
-                    "# Contracts (SPLIT_LITE)",
-                    "",
-                    "- Runner: `repo/CONTRACTS/runner.py`",
-                    "- LITE may omit fixtures; fixtures live in FULL (or TEST) packs.",
-                    "- In LITE, use `meta/FILE_TREE.txt` / `meta/FILE_INDEX.json` for navigation, and `meta/FIXTURE_INDEX.json` if present.",
-                    "",
-                ]
-            ),
-        )
-
-    if scope.key == SCOPE_AGS.key:
-        write(
-            split_dir / "AGS-07_SYSTEM.md",
-            "\n".join(
-                [
-                    "# System (SPLIT_LITE)",
-                    "",
-                    "LITE is laws + maps + indexes + pointers. Raw code and fixtures may be omitted in LITE.",
-                    "When you need full implementation bodies, load them from a FULL (or TEST) pack or from the repo filesystem.",
-                    "",
-                    "- Cortex query interface: `repo/CORTEX/query.py`",
-                    "- Governance critic: `repo/TOOLS/critic.py`",
-                    "- MCP server: `repo/MCP/server.py`",
-                    "- Packer engine: `repo/MEMORY/LLM_PACKER/Engine/packer.py`",
-                    "",
-                    "## Meta inventories",
-                    "",
-                    "- `meta/PACK_INFO.json` (pack metadata)",
-                    "- `meta/REPO_STATE.json` (hash inventory; if present)",
-                    "- `meta/FILE_TREE.txt` / `meta/FILE_INDEX.json` (navigation)",
-                    "",
-                ]
-            ),
-        )
-
-def write_start_here(pack_dir: Path, *, scope: PackScope) -> None:
-    if scope.key == SCOPE_AGS.key:
-        text = "\n".join(
-            [
-                "# START HERE",
-                "",
-                "This snapshot is meant to be shared with any LLM to continue work on the Agent Governance System (AGS) repository.",
-                "",
-                "## Read order",
-                "1) `repo/AGENTS.md` (procedural operating contract)",
-                "2) `repo/README.md` and `repo/CONTEXT/archive/planning/INDEX.md` (orientation + planning)",
-                "3) `repo/CANON/CONTRACT.md` and `repo/CANON/INVARIANTS.md` and `repo/CANON/VERSIONING.md` (authority)",
-                "4) `repo/MAPS/ENTRYPOINTS.md` (where to change what)",
-                "5) `repo/CONTRACTS/runner.py` and `repo/SKILLS/` (execution and fixtures)",
-                "6) `meta/ENTRYPOINTS.md` (snapshot-specific pointers)",
-                "",
-                "## Notes",
-                "- `BUILD` contents are not included. Only a file tree inventory is captured in `meta/BUILD_TREE.txt`.",
-                "- Research under `repo/CONTEXT/research/` is non-binding and opt-in.",
-                f"- If `--combined` is enabled, see `COMBINED/{scope.file_prefix}-FULL-COMBINED-*` and `COMBINED/{scope.file_prefix}-FULL-TREEMAP-*`.",
-                "",
-            ]
-        )
-    elif scope.key == SCOPE_CATALYTIC_DPT.key:
-        text = "\n".join(
-            [
-                "# START HERE",
-                "",
-                f"This snapshot is meant to be shared with any LLM to continue work on `{scope.title}`.",
-                "",
-                "## Read order",
-                "1) `repo/CATALYTIC-DPT/AGENTS.md`",
-                "2) `repo/CATALYTIC-DPT/README.md`",
-                "3) `repo/CATALYTIC-DPT/ROADMAP_V2.1.md`",
-                "4) `repo/CATALYTIC-DPT/swarm_config.json`",
-                "5) `repo/CATALYTIC-DPT/CHANGELOG.md`",
-                "6) `LAB/meta/START_HERE.md` (LAB sub-pack, separate)",
-                "7) `COMBINED/SPLIT/*` (chunked snapshot)",
-                "8) `meta/FILE_TREE.txt` and `meta/FILE_INDEX.json` (navigation)",
-                "",
-                "## Notes",
-                f"- If `--combined` is enabled, see `COMBINED/{scope.file_prefix}-FULL-COMBINED-*` and `COMBINED/{scope.file_prefix}-FULL-TREEMAP-*`.",
-                "",
-            ]
-        )
-    elif scope.key == SCOPE_CATALYTIC_DPT_LAB.key:
-        text = "\n".join(
-            [
-                "# START HERE",
-                "",
-                f"This snapshot is meant to be shared with any LLM to continue work on `{scope.title}`.",
-                "",
-                "## Read order",
-                "1) `repo/CATALYTIC-DPT/LAB/`",
-                "2) `repo/CATALYTIC-DPT/LAB/ROADMAP_PATCH_SEMIOTIC.md` (if present)",
-                "3) `repo/CATALYTIC-DPT/LAB/RESEARCH/` (if relevant)",
-                "4) `COMBINED/SPLIT/*` (chunked snapshot)",
-                "5) `meta/FILE_TREE.txt` and `meta/FILE_INDEX.json` (navigation)",
-                "",
-                "## Notes",
-                f"- If `--combined` is enabled, see `COMBINED/{scope.file_prefix}-FULL-COMBINED-*` and `COMBINED/{scope.file_prefix}-FULL-TREEMAP-*`.",
-                "",
-            ]
-        )
-    else:
-        raise ValueError(f"Unsupported scope for START_HERE: {scope.key}")
-
-    (pack_dir / "meta" / "START_HERE.md").write_text(text, encoding="utf-8")
-
-
-def write_entrypoints(pack_dir: Path, *, scope: PackScope) -> None:
-    if scope.key == SCOPE_AGS.key:
-        text = "\n".join(
-            [
-                "# Snapshot Entrypoints",
-                "",
-                "Key entrypoints for modifying and verifying this repository:",
-                "",
-                "- `repo/AGENTS.md`",
-                "- `repo/README.md`",
-                "- `repo/CONTEXT/archive/planning/INDEX.md`",
-                "- `repo/CANON/CONTRACT.md`",
-                "- `repo/CANON/INVARIANTS.md`",
-                "- `repo/CANON/VERSIONING.md`",
-                "- `repo/MAPS/ENTRYPOINTS.md`",
-                "- `repo/CONTRACTS/runner.py`",
-                "- `repo/MEMORY/packer.py`",
-                "- `repo/CORTEX/query.py`",
-                "",
-                "Notes:",
-                "- `BUILD` contents are not included. Only `meta/BUILD_TREE.txt` is captured.",
-                "- Research under `repo/CONTEXT/research/` has no authority.",
-                f"- If `--combined` is enabled, see `COMBINED/{scope.file_prefix}-FULL-COMBINED-*` and `COMBINED/{scope.file_prefix}-FULL-TREEMAP-*`.",
-                "",
-            ]
-        )
-    elif scope.key == SCOPE_CATALYTIC_DPT.key:
-        text = "\n".join(
-            [
-                "# Snapshot Entrypoints",
-                "",
-                f"Key entrypoints for `{scope.title}`:",
-                "",
-                "- `repo/CATALYTIC-DPT/AGENTS.md`",
-                "- `repo/CATALYTIC-DPT/README.md`",
-                "- `repo/CATALYTIC-DPT/ROADMAP_V2.1.md`",
-                "- `repo/CATALYTIC-DPT/swarm_config.json`",
-                "- `repo/CATALYTIC-DPT/CHANGELOG.md`",
-                "- `repo/CATALYTIC-DPT/TESTBENCH/`",
-                "- `LAB/meta/START_HERE.md` (LAB sub-pack)",
-                "",
-                "Notes:",
-                f"- If `--combined` is enabled, see `COMBINED/{scope.file_prefix}-FULL-COMBINED-*` and `COMBINED/{scope.file_prefix}-FULL-TREEMAP-*`.",
-                "",
-            ]
-        )
-    elif scope.key == SCOPE_CATALYTIC_DPT_LAB.key:
-        text = "\n".join(
-            [
-                "# Snapshot Entrypoints",
-                "",
-                f"Key entrypoints for `{scope.title}`:",
-                "",
-                "- `repo/CATALYTIC-DPT/LAB/`",
-                "- `repo/CATALYTIC-DPT/LAB/ROADMAP_PATCH_SEMIOTIC.md`",
-                "- `repo/CATALYTIC-DPT/LAB/COMMONSENSE/`",
-                "- `repo/CATALYTIC-DPT/LAB/MCP/`",
-                "",
-                "Notes:",
-                f"- If `--combined` is enabled, see `COMBINED/{scope.file_prefix}-FULL-COMBINED-*` and `COMBINED/{scope.file_prefix}-FULL-TREEMAP-*`.",
-                "",
-            ]
-        )
-    else:
-        raise ValueError(f"Unsupported scope for ENTRYPOINTS: {scope.key}")
-
-    (pack_dir / "meta" / "ENTRYPOINTS.md").write_text(text, encoding="utf-8")
-
-
-def write_build_tree(pack_dir: Path, project_root: Path) -> None:
-    tree_path = pack_dir / "meta" / "BUILD_TREE.txt"
-    tree_path.write_text(
-        "BUILD is excluded from packs by contract (determinism + hygiene).\n",
-        encoding="utf-8",
-    )
-
-
-def write_pack_file_tree_and_index(pack_dir: Path) -> None:
-    all_files = [p for p in pack_dir.rglob("*") if p.is_file()]
-    rel_paths = sorted(p.relative_to(pack_dir).as_posix() for p in all_files)
-
-    (pack_dir / "meta" / "FILE_TREE.txt").write_text(
-        "\n".join(rel_paths) + ("\n" if rel_paths else ""),
-        encoding="utf-8",
-    )
-
-    file_index: List[Dict[str, Any]] = []
-    for p in sorted(all_files, key=lambda x: x.relative_to(pack_dir).as_posix()):
-        rel = p.relative_to(pack_dir).as_posix()
-        size = p.stat().st_size
-        file_index.append({"path": rel, "bytes": size, "sha256": hash_file(p)})
-    file_index.sort(key=lambda e: (e["path"], e["sha256"]))
-    write_json(pack_dir / "meta" / "FILE_INDEX.json", file_index)
-
-
-def write_context_report(pack_dir: Path, *, scope: PackScope) -> Tuple[int, List[str]]:
-    """
-    Write CONTEXT.txt with token estimates per file and warnings.
-    
-    Returns:
-        (total_tokens, warnings): Total estimated tokens and any warnings.
-    """
-    warnings: List[str] = []
-    
-    total_bytes = 0
-    total_tokens = 0
-    
-    # Categorize tokens for smarter warnings and readability
-    category_map = {
-        "CANON": [],
-        "CONTEXT": [],
-        "SKILLS": [],
-        "CORTEX": [],
-        "TOOLS": [],
-        "META": [],
-        "REPO_ROOT": [],
-        "COMBINED": [],
-        "OTHER": []
-    }
-    
-    for path in sorted(pack_dir.rglob("*")):
-        if not path.is_file():
-            continue
-        rel = path.relative_to(pack_dir).as_posix()
-        size = path.stat().st_size
-        tokens = estimate_file_tokens(path)
-        
-        entry = (rel, size, tokens)
-        total_bytes += size
-        total_tokens += tokens
-        
-        # Sort into categories
-        if rel.startswith("repo/CANON/"): category_map["CANON"].append(entry)
-        elif rel.startswith("repo/CONTEXT/"): category_map["CONTEXT"].append(entry)
-        elif rel.startswith("repo/SKILLS/"): category_map["SKILLS"].append(entry)
-        elif rel.startswith("repo/CORTEX/"): category_map["CORTEX"].append(entry)
-        elif rel.startswith("repo/TOOLS/"): category_map["TOOLS"].append(entry)
-        elif rel.startswith("meta/"): category_map["META"].append(entry)
-        elif rel.startswith("repo/") and "/" not in rel[5:]: category_map["REPO_ROOT"].append(entry)
-        elif rel.startswith("COMBINED/"): category_map["COMBINED"].append(entry)
-        else: category_map["OTHER"].append(entry)
-
-    # "Repo+Meta" is the common baseline payload (excludes COMBINED/* outputs).
-    repo_meta_tokens = sum(t for rel, _, t in category_map["META"] if rel.startswith("meta/")) + sum(
-        t for rel, _, t in category_map["CANON"] + category_map["CONTEXT"] + category_map["SKILLS"] + category_map["CORTEX"] + category_map["TOOLS"] + category_map["REPO_ROOT"] + category_map["OTHER"]
-        if rel.startswith("repo/")
-    )
-
-    split_tokens = sum(t for rel, _, t in category_map["COMBINED"] if rel.startswith("COMBINED/SPLIT/"))
-    split_lite_tokens = sum(t for rel, _, t in category_map["COMBINED"] if rel.startswith("COMBINED/SPLIT_LITE/"))
-
-    combined_files = [
-        (rel, tokens)
-        for rel, _, tokens in category_map["COMBINED"]
-        if rel.startswith(f"COMBINED/{scope.file_prefix}-FULL-COMBINED-") or rel.startswith(f"COMBINED/{scope.file_prefix}-FULL-TREEMAP-")
-    ]
-    combined_files.sort(key=lambda it: it[0])
-
-    # "Effective" for legacy reporting kept as repo+meta only (no COMBINED/*).
-    effective_tokens = repo_meta_tokens
-    
-    lines: List[str] = [
-        f"# {scope.file_prefix} Pack Context Report",
-        "",
-        "## Payload Token Counts",
-        "",
-        "These counts are reported per output payload (not summed across all pack outputs).",
-        "",
-        f"- `repo/` + `meta/` (baseline): {repo_meta_tokens:,} tokens",
-        f"- `COMBINED/SPLIT/**` (sum): {split_tokens:,} tokens",
-        f"- `COMBINED/SPLIT_LITE/**` (sum): {split_lite_tokens:,} tokens",
-    ]
-
-    if combined_files:
-        lines.extend(["", "Combined single-file payloads:"])
-        for rel, tokens in combined_files:
-            lines.append(f"- `{rel}`: {tokens:,} tokens")
-
-    lines.extend(
-        [
-            "",
-            "## Category Summary",
-            "",
-            f"{'Category':<15} {'Files':>8} {'Tokens':>12} {'% Baseline':>12}",
-            "-" * 55,
-        ]
-    )
-    
-    for cat, files in category_map.items():
-        if not files: continue
-        cat_tokens = sum(t for _, _, t in files)
-        percent = (cat_tokens / repo_meta_tokens * 100) if repo_meta_tokens > 0 and cat not in ("COMBINED",) else 0
-        percent_str = f"{percent:>11.1f}%" if cat != "COMBINED" else "N/A"
-        lines.append(f"{cat:<15} {len(files):>8} {cat_tokens:>12,} {percent_str}")
-
-    lines.extend([
-        "-" * 55,
-        f"{'BASELINE':<15} {'-':>8} {repo_meta_tokens:>12,} {'100.0%':>12}",
-        f"{'TOTAL (ALL)':<15} {sum(len(f) for f in category_map.values()):>8} {total_tokens:>12,} {'-':>12}",
-        "",
-        "## Detailed Breakdown",
-        ""
-    ])
-    
-    # Detail sections (only important ones or if not too many)
-    for cat in ["CANON", "CONTEXT", "SKILLS", "CORTEX", "TOOLS", "META", "REPO_ROOT"]:
-        files = category_map[cat]
-        if not files: continue
-        
-        lines.append(f"### {cat} ({sum(t for _, _, t in files):,} tokens)")
-        # If too many files, only show top ones to reduce bloat
-        sorted_files = sorted(files, key=lambda x: x[2], reverse=True)
-        
-        display_limit = 10 if cat != "CANON" else 20
-        for i, (rel, _, tokens) in enumerate(sorted_files):
-            if i >= display_limit:
-                lines.append(f"  ... and {len(files) - display_limit} more files")
-                break
-            
-            # Use short name, but prefix if generic
-            filename = rel.split("/")[-1]
-            if filename in ["run.py", "validate.py", "SKILL.md", "expected.json", "input.json"]:
-                parts = rel.split("/")
-                if len(parts) >= 3:
-                    filename = f"{parts[-2]}/{filename}"
-            
-            lines.append(f"- {filename:<45} {tokens:>10,} tokens")
-        lines.append("")
-
-    # Add warnings (based on single payload size, not pack-wide totals)
-    lines.append("## Status")
-    payload_candidates: List[Tuple[str, int]] = [
-        ("repo/+meta/ baseline", repo_meta_tokens),
-        ("COMBINED/SPLIT/** (sum)", split_tokens),
-        ("COMBINED/SPLIT_LITE/** (sum)", split_lite_tokens),
-        *[(rel, tokens) for rel, tokens in combined_files],
-    ]
-    max_name, max_tokens = max(payload_candidates, key=lambda it: it[1]) if payload_candidates else ("(none)", 0)
-
-    if max_tokens > TOKEN_LIMIT_CRITICAL:
-        warning = f"[!] CRITICAL: Largest single payload ({max_name}) is {max_tokens:,} tokens (> {TOKEN_LIMIT_CRITICAL:,})."
-        warnings.append(warning)
-        lines.append(warning)
-    elif max_tokens > TOKEN_LIMIT_WARNING:
-        warning = f"[!] WARNING: Largest single payload ({max_name}) is {max_tokens:,} tokens (> {TOKEN_LIMIT_WARNING:,})."
-        warnings.append(warning)
-        lines.append(warning)
-    else:
-        lines.append(f"[OK] Largest single payload ({max_name}) is {max_tokens:,} tokens (within limits).")
-    
-    lines.append("")
-
-    report_text = "\n".join(lines)
-
-    (pack_dir / "meta" / "CONTEXT.txt").write_text(report_text, encoding="utf-8")
-    return effective_tokens, warnings
-
-
-def print_payload_token_counts(pack_dir: Path) -> None:
-    """
-    Print per-payload token counts to stdout.
-
-    This mirrors the `## Payload Token Counts` section in `meta/CONTEXT.txt`.
-    """
-    report_path = pack_dir / "meta" / "CONTEXT.txt"
-    if not report_path.exists():
-        return
-    text = read_text(report_path)
-    lines = text.splitlines()
-    start_idx: Optional[int] = None
-    for idx, line in enumerate(lines):
-        if line.strip() == "## Payload Token Counts":
-            start_idx = idx
-            break
-    if start_idx is None:
-        return
-    out: List[str] = []
-    for line in lines[start_idx:]:
-        if out and line.startswith("## "):
-            break
-        out.append(line)
-    if not out:
-        return
-    print("\n".join(out).rstrip() + "\n")
-
-
-def copy_repo_files(
-    pack_dir: Path,
-    project_root: Path,
-    included_paths: Sequence[str],
-) -> None:
-    for rel in included_paths:
-        src = project_root / rel
-        if not src.exists() or not src.is_file():
-            continue
-        dst = pack_dir / "repo" / rel
-        dst.parent.mkdir(parents=True, exist_ok=True)
-        dst.write_bytes(src.read_bytes())
-
-
-def ensure_under_packs_root(out_dir: Path) -> Path:
-    packs_root = PACKS_ROOT.resolve()
-    out_dir_resolved = out_dir.resolve()
-    try:
-        out_dir_resolved.relative_to(packs_root)
-    except ValueError as exc:
-        raise ValueError(
-            f"OutDir must be under MEMORY/LLM_PACKER/_packs/. Received: {out_dir}"
-        ) from exc
-    return out_dir_resolved
-
-
-def default_stamp_for_out_dir(out_dir: Path) -> str:
-    match = re.search(r"(\\d{4}-\\d{2}-\\d{2}_\\d{2}-\\d{2}-\\d{2})", out_dir.name)
-    if match:
-        return match.group(1)
-    return "nostamp"
-
-
-def compute_treemap_text(
-    pack_dir: Path,
-    *,
-    stamp: str,
-    include_combined_paths: bool,
-    scope: PackScope,
-) -> str:
-    base_paths = sorted(p.relative_to(pack_dir).as_posix() for p in pack_dir.rglob("*") if p.is_file())
-
-    if not include_combined_paths:
-        return build_pack_tree_text(base_paths, extra_paths=[])
-
-    extra_paths: List[str] = []
-
-    extra_paths.extend(
-        [
-            f"COMBINED/{scope.file_prefix}-FULL-COMBINED-{stamp}.md",
-            f"COMBINED/{scope.file_prefix}-FULL-COMBINED-{stamp}.txt",
-            f"COMBINED/{scope.file_prefix}-FULL-TREEMAP-{stamp}.md",
-            f"COMBINED/{scope.file_prefix}-FULL-TREEMAP-{stamp}.txt",
-        ]
-    )
-
-    if scope.key == SCOPE_CATALYTIC_DPT.key:
-        lab_prefix = "repo/CATALYTIC-DPT/LAB/"
-        if any(p.startswith(lab_prefix) for p in base_paths):
-            extra_paths.extend(
-                [
-                    f"COMBINED/{scope.file_prefix}-LAB-FULL-COMBINED-{stamp}.md",
-                    f"COMBINED/{scope.file_prefix}-LAB-FULL-COMBINED-{stamp}.txt",
-                    f"COMBINED/{scope.file_prefix}-LAB-FULL-TREEMAP-{stamp}.md",
-                    f"COMBINED/{scope.file_prefix}-LAB-FULL-TREEMAP-{stamp}.txt",
-                ]
-            )
-
-    return build_pack_tree_text(base_paths, extra_paths=extra_paths)
-
-
-def append_repo_tree_to_split_maps(pack_dir: Path, *, tree_text: str, scope: PackScope) -> None:
-    if scope.key == SCOPE_AGS.key:
-        split_target = pack_dir / "COMBINED" / "SPLIT" / "AGS-03_MAPS.md"
-    else:
-        split_target = pack_dir / "COMBINED" / "SPLIT" / f"{scope.file_prefix}-00_INDEX.md"
-    if not split_target.exists():
-        return
-    existing = read_text(split_target).rstrip("\n")
-    updated = "\n".join(
-        [
-            existing,
-            "",
-            "## Repo File Tree",
-            "",
-            "```",
-            tree_text.rstrip("\n"),
-            "```",
-            "",
-        ]
-    )
-    split_target.write_text(updated, encoding="utf-8")
-
-
-def write_combined_outputs(pack_dir: Path, *, stamp: str, scope: PackScope) -> None:
-    combined_dir = pack_dir / "COMBINED"
-    combined_dir.mkdir(parents=True, exist_ok=True)
-
-    combined_md_rel = f"COMBINED/{scope.file_prefix}-FULL-COMBINED-{stamp}.md"
-    combined_txt_rel = f"COMBINED/{scope.file_prefix}-FULL-COMBINED-{stamp}.txt"
-    treemap_md_rel = f"COMBINED/{scope.file_prefix}-FULL-TREEMAP-{stamp}.md"
-    treemap_txt_rel = f"COMBINED/{scope.file_prefix}-FULL-TREEMAP-{stamp}.txt"
-
-    tree_text = compute_treemap_text(pack_dir, stamp=stamp, include_combined_paths=True, scope=scope)
-    tree_md = "\n".join(["# Pack Tree", "", "```", tree_text.rstrip("\n"), "```", ""]) + "\n"
-
-    (pack_dir / treemap_txt_rel).write_text(tree_text, encoding="utf-8")
-    (pack_dir / treemap_md_rel).write_text(tree_md, encoding="utf-8")
-
-    combined_md_lines = [f"# {scope.file_prefix} FULL COMBINED", ""]
-    combined_txt_lines = [f"{scope.file_prefix} FULL COMBINED", ""]
-
-    base_paths = sorted(p.relative_to(pack_dir).as_posix() for p in pack_dir.rglob("*") if p.is_file())
-    for rel in base_paths:
-        if rel.startswith("COMBINED/"):
-            continue
-
-        abs_path = pack_dir / rel
-        if not abs_path.exists() or not abs_path.is_file():
-            continue
-        text = read_text(abs_path)
-        size = abs_path.stat().st_size
-        combined_md_lines.append(build_combined_md_block(rel, text, size))
-        combined_txt_lines.append(build_combined_txt_block(rel, text, size))
-
-    md_content = "\n".join(combined_md_lines).rstrip() + "\n"
-    txt_content = "\n".join(combined_txt_lines).rstrip() + "\n"
-
-    (pack_dir / combined_md_rel).write_text(md_content, encoding="utf-8")
-    (pack_dir / combined_txt_rel).write_text(txt_content, encoding="utf-8")
-
-
-def write_provenance_manifest(pack_dir: Path) -> None:
-    """Generate deterministic meta/PROVENANCE.json for the entire pack."""
-    meta_dir = pack_dir / "meta"
-    targets = [
-        "meta/FILE_INDEX.json",
-        "meta/PACK_INFO.json",
-        "meta/REPO_STATE.json",
-        "meta/BUILD_TREE.txt",
-        "meta/FILE_TREE.txt",
-        "meta/CONTEXT.txt",
-        "meta/ENTRYPOINTS.md",
-        "meta/START_HERE.md",
-    ]
-    checksums: Dict[str, str] = {}
-    for rel in targets:
-        p = pack_dir / rel
-        if not p.exists() or not p.is_file():
-            continue
-        checksums[rel] = hash_file(p)
-
-    payload: Dict[str, Any] = {
-        "generator": "MEMORY/LLM_PACKER/Engine/packer.py",
-        "targets": targets,
-        "checksums": checksums,
-        "checksum": None,
-    }
-    payload["checksum"] = repo_state_content_sha256(payload)
-    write_json(meta_dir / "PROVENANCE.json", payload)
-
-
-def make_pack(
-    *,
-    scope_key: str,
-    mode: str,
-    profile: str,
-    split_lite: bool,
-    out_dir: Optional[Path],
-    combined: bool,
-    stamp: Optional[str],
-    zip_enabled: bool,
-    max_total_bytes: int,
-    max_entry_bytes: int,
-    max_entries: int,
-    allow_duplicate_hashes: Optional[bool],
-) -> Path:
-    scope = SCOPES.get(scope_key)
-    if not scope:
-        raise ValueError(f"Unknown scope: {scope_key}. Choices: {', '.join(sorted(SCOPES.keys()))}")
-    if scope.key != SCOPE_AGS.key and profile != "full":
-        raise ValueError(f"Only --profile full is supported for scope={scope.key} (received: {profile})")
-
-    manifest, omitted = build_state_manifest(PROJECT_ROOT, scope=scope)
-    digest = manifest_digest(manifest)
-
-    if out_dir is None:
-        out_dir = PACKS_ROOT / f"llm-pack-{scope.key}-{digest[:12]}"
-    out_dir = ensure_under_packs_root(out_dir)
-
-    baseline_path = baseline_path_for_scope(scope)
-    baseline = load_baseline(baseline_path)
-    baseline_files_by_path = {f["path"]: f for f in (baseline or {}).get("files", [])}
-    current_files_by_path = {f["path"]: f for f in manifest.get("files", [])}
-
-    if allow_duplicate_hashes is None:
-        allow_duplicate_hashes = scope.key != SCOPE_CATALYTIC_DPT.key
-
-    validate_repo_state_manifest(manifest, allow_duplicate_hashes=allow_duplicate_hashes)
-    repo_state_sha256 = repo_state_content_sha256(manifest)
-
-    anchors = set(scope.anchors)
-
-    if mode == "delta" and baseline is not None:
-        changed = []
-        for path, entry in current_files_by_path.items():
-            prev = baseline_files_by_path.get(path)
-            if prev is None or prev.get("hash") != entry.get("hash") or prev.get("size") != entry.get("size"):
-                changed.append(path)
-        deleted = sorted(set(baseline_files_by_path.keys()) - set(current_files_by_path.keys()))
-        include_paths = sorted(set(changed) | anchors)
-    else:
-        include_paths = sorted(set(current_files_by_path.keys()) | anchors)
-        deleted = []
-
-    omitted_paths_for_lite: List[str] = []
-    if profile == "lite":
-        lite_include: List[str] = []
-        for rel in include_paths:
-            if rel.endswith((".cmd", ".ps1")):
-                omitted_paths_for_lite.append(rel)
-                continue
-            if "/fixtures/" in rel:
-                omitted_paths_for_lite.append(rel)
-                continue
-            if "/_runs/" in rel or "/_generated/" in rel:
-                omitted_paths_for_lite.append(rel)
-                continue
-            if rel.startswith("CONTEXT/research/") or rel.startswith("CONTEXT/archive/"):
-                omitted_paths_for_lite.append(rel)
-                continue
-            if "/_packs/" in rel and rel.startswith("MEMORY/"):
-                omitted_paths_for_lite.append(rel)
-                continue
-
-            if rel == "AGENTS.md":
-                lite_include.append(rel)
-            elif rel == "README.md":
-                lite_include.append(rel)
-            elif rel.startswith("CANON/"):
-                lite_include.append(rel)
-            elif rel.startswith("MAPS/"):
-                lite_include.append(rel)
-            elif rel == "CONTRACTS/runner.py":
-                lite_include.append(rel)
-            elif rel == "CORTEX/query.py":
-                lite_include.append(rel)
-            elif rel == "TOOLS/critic.py":
-                lite_include.append(rel)
-            elif rel.startswith("SKILLS/") and (rel.endswith("/SKILL.md") or rel.endswith("/version.json")):
-                lite_include.append(rel)
-            else:
-                omitted_paths_for_lite.append(rel)
-        include_paths = sorted(set(lite_include))
-    elif profile != "full":
-        raise ValueError(f"Unknown profile: {profile}")
-
-    limits = PackLimits(
-        max_total_bytes=max_total_bytes,
-        max_entry_bytes=max_entry_bytes,
-        max_entries=max_entries,
-        allow_duplicate_hashes=allow_duplicate_hashes,
-    )
-    included_entries = [current_files_by_path[p] for p in include_paths if p in current_files_by_path]
-    included_stats = enforce_included_repo_limits(included_entries, limits=limits)
-
-    if out_dir.exists():
-        shutil.rmtree(out_dir)
-    (out_dir / "meta").mkdir(parents=True, exist_ok=True)
-    (out_dir / "repo").mkdir(parents=True, exist_ok=True)
-
-    repo_pack_paths = [f"repo/{p}" for p in include_paths]
-
-    try:
-        copy_repo_files(out_dir, PROJECT_ROOT, include_paths)
-        write_json(out_dir / "meta" / "REPO_OMITTED_BINARIES.json", omitted)
-        write_json(out_dir / "meta" / "REPO_STATE.json", manifest)
-        write_build_tree(out_dir, PROJECT_ROOT)
-        write_start_here(out_dir, scope=scope)
-        write_entrypoints(out_dir, scope=scope)
-        write_split_pack(out_dir, repo_pack_paths, scope=scope)
-        if split_lite:
-            write_split_pack_lite(out_dir, scope=scope)
-
-        effective_stamp = stamp or digest[:12]
-        tree_text = compute_treemap_text(out_dir, stamp=effective_stamp, include_combined_paths=bool(combined), scope=scope)
-        append_repo_tree_to_split_maps(out_dir, tree_text=tree_text, scope=scope)
-
-        if combined:
-            write_combined_outputs(out_dir, stamp=effective_stamp, scope=scope)
-
-        if profile == "lite" and scope.key == SCOPE_AGS.key:
-            write_lite_indexes(
-                out_dir,
-                project_root=PROJECT_ROOT,
-                include_paths=include_paths,
-                omitted_paths=omitted_paths_for_lite,
-                files_by_path=current_files_by_path,
-            )
-
-        write_pack_file_tree_and_index(out_dir)
-
-        total_tokens, token_warnings = write_context_report(out_dir, scope=scope)
-        print_payload_token_counts(out_dir)
-        for warning in token_warnings:
-            color = ANSI_RED if "CRITICAL" in warning else ANSI_YELLOW
-            print(f"{color}{warning}{ANSI_RESET}")
-
-        pack_bytes = pack_dir_total_bytes(out_dir)
-        if pack_bytes > limits.max_total_bytes:
-            raise ValueError("PACK_LIMIT_EXCEEDED:max_total_bytes")
-
-        write_json(
-            out_dir / "meta" / "PACK_INFO.json",
-            {
-                "scope": scope.key,
-                "mode": mode,
-                **({"profile": profile} if profile != "full" else {}),
-                "canon_version": manifest.get("canon_version"),
-                "grammar_version": manifest.get("grammar_version"),
-                "repo_digest": digest,
-                "repo_state_sha256": repo_state_sha256,
-                "included_paths": include_paths,
-                "deleted_paths": deleted,
-                "limits": {
-                    "max_total_bytes": limits.max_total_bytes,
-                    "max_entry_bytes": limits.max_entry_bytes,
-                    "max_entries": limits.max_entries,
-                    "allow_duplicate_hashes": limits.allow_duplicate_hashes,
-                },
-                "stats": {
-                    **included_stats,
-                    "pack_bytes": pack_bytes,
-                    "token_report_total_tokens": total_tokens,
-                    "token_report_warnings": token_warnings,
-                },
-            },
-        )
-
-        write_provenance_manifest(out_dir)
-        write_json(baseline_path, manifest)
-
-        if zip_enabled:
-            archive_dir = SYSTEM_DIR / "archive"
-            archive_dir.mkdir(parents=True, exist_ok=True)
-            zip_path = archive_dir / f"{out_dir.name}.zip"
-            if zip_path.exists():
-                zip_path.unlink()
-            shutil.make_archive(str(zip_path.with_suffix("")), "zip", root_dir=out_dir)
-
-        return out_dir
-    except Exception:
-        if out_dir.exists():
-            shutil.rmtree(out_dir)
-        raise
-
-if __name__ == "__main__":
-    parser = argparse.ArgumentParser(
-        description="Create memory/LLM packs under MEMORY/LLM_PACKER/_packs/."
-    )
-    parser.add_argument(
-        "--scope",
-        choices=tuple(sorted(SCOPES.keys())),
-        default=SCOPE_AGS.key,
-        help="What to pack: default is the full AGS repo; catalytic-dpt packs CATALYTIC-DPT without LAB; catalytic-dpt-lab packs only CATALYTIC-DPT/LAB.",
-    )
-    parser.add_argument(
-        "--mode",
-        choices=("full", "delta"),
-        default="full",
-        help="Pack mode: full includes all included text files; delta includes only changes since last baseline plus anchors.",
-    )
-    parser.add_argument(
-        "--profile",
-        choices=("full", "lite"),
-        default="full",
-        help="Pack profile: full is record-keeping; lite is discussion-first (contracts + interfaces + symbolic indexes).",
-    )
-    parser.add_argument(
-        "--split-lite",
-        action="store_true",
-        help="Also write COMBINED/SPLIT_LITE/** alongside COMBINED/SPLIT/** in the same pack.",
-    )
-    parser.add_argument(
-        "--out-dir",
-        default="",
-        help="Output directory for the pack, relative to the repo root and under MEMORY/LLM_PACKER/_packs/.",
-    )
-    parser.add_argument("--combined", action="store_true", help="Write COMBINED/FULL-COMBINED-* and COMBINED/FULL-TREEMAP-* outputs.")
-    parser.add_argument(
-        "--stamp",
-        default="",
-        help="Stamp string for COMBINED output filenames. Defaults to the repo digest prefix (deterministic).",
-    )
-    parser.add_argument(
-        "--zip",
-        action="store_true",
-        help="Write a zip archive under MEMORY/LLM_PACKER/_packs/_system/archive/.",
-    )
-    parser.add_argument(
-        "--max-total-bytes",
-        type=int,
-        default=50 * 1024 * 1024,
-        help="Hard ceiling for total pack bytes (fail-closed).",
-    )
-    parser.add_argument(
-        "--max-entry-bytes",
-        type=int,
-        default=2 * 1024 * 1024,
-        help="Hard ceiling for any single included repo file (fail-closed).",
-    )
-    parser.add_argument(
-        "--max-entries",
-        type=int,
-        default=50_000,
-        help="Hard ceiling for number of included repo files (fail-closed).",
-    )
-    parser.add_argument(
-        "--allow-duplicate-hashes",
-        action="store_true",
-        help="Allow multiple distinct paths to carry the same content hash (overrides scope default).",
-    )
-    parser.add_argument(
-        "--disallow-duplicate-hashes",
-        action="store_true",
-        help="Disallow duplicate hashes even when scope default allows them (overrides scope default).",
-    )
-    args = parser.parse_args()
-
-    out_dir = Path(args.out_dir) if args.out_dir else None
-    if out_dir is not None and not out_dir.is_absolute():
-        out_dir = (PROJECT_ROOT / out_dir).resolve()
-
-    allow_dup: Optional[bool] = None
-    if args.allow_duplicate_hashes and args.disallow_duplicate_hashes:
-        raise SystemExit("Invalid flags: cannot set both --allow-duplicate-hashes and --disallow-duplicate-hashes")
-    if args.allow_duplicate_hashes:
-        allow_dup = True
-    elif args.disallow_duplicate_hashes:
-        allow_dup = False
-
-    pack_dir = make_pack(
-        scope_key=args.scope,
-        mode=args.mode,
-        profile=args.profile,
-        split_lite=bool(args.split_lite),
-        out_dir=out_dir,
-        combined=bool(args.combined),
-        stamp=args.stamp or None,
-        zip_enabled=bool(args.zip),
-        max_total_bytes=int(args.max_total_bytes),
-        max_entry_bytes=int(args.max_entry_bytes),
-        max_entries=int(args.max_entries),
-        allow_duplicate_hashes=allow_dup,
-    )
-    print(f"Pack created: {pack_dir}")
diff --git a/MEMORY/LLM_PACKER/Engine/packer/__main__.py b/MEMORY/LLM_PACKER/Engine/packer/__main__.py
index 1ff1a95..838405c 100644
--- a/MEMORY/LLM_PACKER/Engine/packer/__main__.py
+++ b/MEMORY/LLM_PACKER/Engine/packer/__main__.py
@@ -1,4 +1,5 @@
+from .cli import main
+
 if __name__ == "__main__":
-    from .cli import main
     import sys
     sys.exit(main())
diff --git a/MEMORY/LLM_PACKER/Engine/packer/archive.py b/MEMORY/LLM_PACKER/Engine/packer/archive.py
index 1deb71e..62b8ec5 100644
--- a/MEMORY/LLM_PACKER/Engine/packer/archive.py
+++ b/MEMORY/LLM_PACKER/Engine/packer/archive.py
@@ -120,3 +120,19 @@ def write_pack_internal_archives(
             dest = internal_archive_dir / txt_name
             dest.write_text(read_text(p), encoding="utf-8")
 
+    # 5. Generate sibling text files from LITE/ outputs
+    lite_dir = pack_dir / "LITE"
+    if lite_dir.exists():
+        for p in sorted(lite_dir.glob("*.md")):
+            stem = p.stem
+            if stem.startswith(f"{scope.file_prefix}-"):
+                if "LITE" not in stem:
+                    rest = stem[len(scope.file_prefix) + 1 :]
+                    txt_name = f"{scope.file_prefix}-LITE-{rest}.txt"
+                else:
+                    txt_name = f"{stem}.txt"
+            else:
+                txt_name = f"{scope.file_prefix}-LITE-{stem}.txt"
+
+            dest = internal_archive_dir / txt_name
+            dest.write_text(read_text(p), encoding="utf-8")
diff --git a/MEMORY/LLM_PACKER/Engine/packer/cli.py b/MEMORY/LLM_PACKER/Engine/packer/cli.py
index eac60e3..1ea57d1 100644
--- a/MEMORY/LLM_PACKER/Engine/packer/cli.py
+++ b/MEMORY/LLM_PACKER/Engine/packer/cli.py
@@ -1,117 +1,60 @@
 #!/usr/bin/env python3
 """
-CLI entry point for LLM Packer (Phase 1 Modular).
-Calls Engine.packer.core.make_pack directly.
+CLI entry point for the modular LLM Packer.
 """
+from __future__ import annotations
+
 import argparse
 import sys
 from pathlib import Path
+from typing import Optional
 
-# Add project root to path
-PROJECT_ROOT = Path(__file__).resolve().parents[4]
-if str(PROJECT_ROOT) not in sys.path:
-    sys.path.insert(0, str(PROJECT_ROOT))
-
-from .core import make_pack, SCOPE_AGS, SCOPES
-
-def main():
-    parser = argparse.ArgumentParser(
-        description="Create memory/LLM packs under MEMORY/LLM_PACKER/_packs/."
-    )
-    parser.add_argument(
-        "--scope",
-        choices=tuple(sorted(SCOPES.keys())),
-        default=SCOPE_AGS.key,
-        help="What to pack: default is the full AGS repo.",
-    )
-    parser.add_argument(
-        "--mode",
-        choices=("full", "delta"),
-        default="full",
-        help="Pack mode: full includes all included text files.",
-    )
-    parser.add_argument(
-        "--profile",
-        choices=("full", "lite"),
-        default="full",
-        help="Pack profile.",
-    )
-    parser.add_argument(
-        "--split-lite",
-        action="store_true",
-        help="Generate LITE/ output (renamed from SPLIT_LITE).",
-    )
-    parser.add_argument(
-        "--out-dir",
-        default="",
-        help="Output directory.",
-    )
-    parser.add_argument(
-        "--combined",
-        action="store_true",
-        help="Generate FULL/ output (renamed from COMBINED).",
-    )
-    parser.add_argument(
-        "--stamp",
-        default="",
-        help="Stamp string for output filenames.",
-    )
-    parser.add_argument(
-        "--zip",
-        action="store_true",
-        help="Write a zip archive under MEMORY/LLM_PACKER/_packs/_system/archive/.",
-    )
-    parser.add_argument(
-        "--max-total-bytes",
-        type=int,
-        default=50 * 1024 * 1024,
-    )
-    parser.add_argument(
-        "--max-entry-bytes",
-        type=int,
-        default=2 * 1024 * 1024,
-    )
-    parser.add_argument(
-        "--max-entries",
-        type=int,
-        default=50_000,
-    )
-    parser.add_argument(
-        "--allow-duplicate-hashes",
-        action="store_true",
-    )
-    parser.add_argument(
-        "--disallow-duplicate-hashes",
-        action="store_true",
-    )
-    args = parser.parse_args()
+from .core import SCOPE_AGS, SCOPES, make_pack
 
-    out_dir = Path(args.out_dir) if args.out_dir else None
-    if out_dir is not None and not out_dir.is_absolute():
-        out_dir = (PROJECT_ROOT / out_dir).resolve()
+def main(args: Optional[list[str]] = None) -> int:
+    parser = argparse.ArgumentParser(description="LLM Packer - Prepare repository snapshots for LLM handoff.")
+    
+    parser.add_argument("--scope", default=SCOPE_AGS.key, choices=list(SCOPES.keys()), help="Scope of the pack.")
+    parser.add_argument("--mode", default="full", choices=["full", "delta"], help="Pack mode (full or delta).")
+    parser.add_argument("--profile", default="full", choices=["full", "lite"], help="Pack profile.")
+    parser.add_argument("--split-lite", action="store_true", help="Generate LITE outputs alongside others.")
+    parser.add_argument("--out-dir", type=Path, help="Explicit output directory.")
+    parser.add_argument("--combined", action="store_true", help="Generate FULL/ combined outputs.")
+    parser.add_argument("--stamp", help="Explicit timestamp or digest for filenames.")
+    parser.add_argument("--zip", action="store_true", help="Zip the resulting pack into archive/pack.zip.")
+    
+    # Overrides/Limits
+    parser.add_argument("--max-total-bytes", type=int, default=50 * 1024 * 1024)
+    parser.add_argument("--max-entry-bytes", type=int, default=2 * 1024 * 1024)
+    parser.add_argument("--max-entries", type=int, default=50_000)
+    
+    dup_group = parser.add_mutually_exclusive_group()
+    dup_group.add_argument("--allow-duplicate-hashes", action="store_true", default=None)
+    dup_group.add_argument("--disallow-duplicate-hashes", action="store_false", dest="allow_duplicate_hashes")
 
-    allow_dup = None
-    if args.allow_duplicate_hashes:
-        allow_dup = True
-    elif args.disallow_duplicate_hashes:
-        allow_dup = False
+    parsed = parser.parse_args(args)
 
-    pack_dir = make_pack(
-        scope_key=args.scope,
-        mode=args.mode,
-        profile=args.profile,
-        split_lite=bool(args.split_lite),
-        out_dir=out_dir,
-        combined=bool(args.combined),
-        stamp=args.stamp or None,
-        zip_enabled=bool(args.zip),
-        max_total_bytes=int(args.max_total_bytes),
-        max_entry_bytes=int(args.max_entry_bytes),
-        max_entries=int(args.max_entries),
-        allow_duplicate_hashes=allow_dup,
-    )
-    print(f"Pack created: {pack_dir}")
-    return 0
+    try:
+        pack_dir = make_pack(
+            scope_key=parsed.scope,
+            mode=parsed.mode,
+            profile=parsed.profile,
+            split_lite=parsed.split_lite,
+            out_dir=parsed.out_dir,
+            combined=parsed.combined or (parsed.profile == "full"), # Default true if full
+            stamp=parsed.stamp,
+            zip_enabled=parsed.zip,
+            max_total_bytes=parsed.max_total_bytes,
+            max_entry_bytes=parsed.max_entry_bytes,
+            max_entries=parsed.max_entries,
+            allow_duplicate_hashes=parsed.allow_duplicate_hashes,
+        )
+        print(f"Packer completed successfully.")
+        print(f"Output: {pack_dir.as_posix()}")
+        return 0
+    except Exception as exc:
+        print(f"PACKER_ERROR: {exc}", file=sys.stderr)
+        return 1
 
 if __name__ == "__main__":
     sys.exit(main())
diff --git a/MEMORY/LLM_PACKER/Engine/packer/core.py b/MEMORY/LLM_PACKER/Engine/packer/core.py
index 5ac97bc..e797c01 100644
--- a/MEMORY/LLM_PACKER/Engine/packer/core.py
+++ b/MEMORY/LLM_PACKER/Engine/packer/core.py
@@ -270,14 +270,57 @@ def write_json(path: Path, payload: Any) -> None:
 
 def ensure_under_packs_root(out_dir: Path) -> Path:
     packs_root = PACKS_ROOT.resolve()
+    system_dir = SYSTEM_DIR.resolve()
+    fixtures_root = FIXTURE_PACKS_DIR.resolve()
     out_dir_resolved = out_dir.resolve()
     try:
         out_dir_resolved.relative_to(packs_root)
     except ValueError as exc:
         raise ValueError(f"OutDir must be under MEMORY/LLM_PACKER/_packs/. Received: {out_dir}") from exc
+    if out_dir_resolved == packs_root:
+        raise ValueError(f"OutDir must not be the packs root itself: {out_dir}")
+    try:
+        out_dir_resolved.relative_to(system_dir)
+        under_system = True
+    except ValueError:
+        under_system = False
+    if under_system:
+        try:
+            out_dir_resolved.relative_to(fixtures_root)
+            under_fixtures = True
+        except ValueError:
+            under_fixtures = False
+        if not under_fixtures:
+            raise ValueError(
+                "OutDir must not be under MEMORY/LLM_PACKER/_packs/_system/ "
+                "(reserved for baselines, archives, and fixtures). "
+                f"Received: {out_dir}"
+            )
+        if out_dir_resolved == fixtures_root:
+            raise ValueError(f"OutDir must be a subdirectory of {fixtures_root}: {out_dir}")
     return out_dir_resolved
 
 
+def _pick_unused_out_dir(base: Path) -> Path:
+    if not base.exists():
+        return base
+    for i in range(2, 10_000):
+        candidate = base.with_name(f"{base.name}-{i}")
+        if not candidate.exists():
+            return candidate
+    raise RuntimeError(f"Unable to choose unused out_dir under {base.parent}")
+
+
+def _is_fixture_out_dir(out_dir: Path) -> bool:
+    out_dir_resolved = out_dir.resolve()
+    fixtures_root = FIXTURE_PACKS_DIR.resolve()
+    try:
+        out_dir_resolved.relative_to(fixtures_root)
+    except ValueError:
+        return False
+    return out_dir_resolved != fixtures_root
+
+
 def enforce_included_repo_limits(entries: List[Dict[str, Any]], limits: PackLimits) -> Dict[str, Any]:
     total_bytes = sum(e["size"] for e in entries)
     if total_bytes > limits.max_total_bytes:
@@ -417,6 +460,7 @@ def write_entrypoints(pack_dir: Path, *, scope: PackScope) -> None:
                 "",
                 "- `repo/CATALYTIC-DPT/AGENTS.md`",
                 "- `repo/CATALYTIC-DPT/README.md`",
+                "- `repo/CATALYTIC-DPT/ROADMAP_V2.1.md`",
                 "- `repo/CATALYTIC-DPT/swarm_config.json`",
                 "- `repo/CATALYTIC-DPT/TESTBENCH/`",
                 "",
@@ -624,7 +668,7 @@ def make_pack(
     digest = manifest_digest(manifest)
 
     if out_dir is None:
-        out_dir = PACKS_ROOT / f"llm-pack-{scope.key}-{digest[:12]}"
+        out_dir = _pick_unused_out_dir(PACKS_ROOT / f"llm-pack-{scope.key}-{digest[:12]}")
     out_dir = ensure_under_packs_root(out_dir)
 
     SYSTEM_DIR.mkdir(parents=True, exist_ok=True)
@@ -664,7 +708,13 @@ def make_pack(
     included_stats = enforce_included_repo_limits(included_entries, limits=limits)
 
     if out_dir.exists():
-        shutil.rmtree(out_dir)
+        if _is_fixture_out_dir(out_dir):
+            shutil.rmtree(out_dir)
+        else:
+            raise FileExistsError(
+                f"Refusing to overwrite existing pack output directory: {out_dir}\n"
+                "Choose a new --out-dir (recommended) or remove it manually."
+            )
     (out_dir / "meta").mkdir(parents=True, exist_ok=True)
     (out_dir / "repo").mkdir(parents=True, exist_ok=True)
 
diff --git a/MEMORY/LLM_PACKER/Engine/packer_legacy_backup.py b/MEMORY/LLM_PACKER/Engine/packer_legacy_backup.py
new file mode 100644
index 0000000..768204c
--- /dev/null
+++ b/MEMORY/LLM_PACKER/Engine/packer_legacy_backup.py
@@ -0,0 +1,16 @@
+
+"""
+Backup script for legacy packer.py (Stub)
+"""
+import shutil
+from pathlib import Path
+
+def backup():
+    src = Path("MEMORY/LLM_PACKER/Engine/packer.py")
+    dst = Path("MEMORY/LLM_PACKER/Engine/packer_legacy.py.bak")
+    if src.exists():
+        shutil.copy2(src, dst)
+        print(f"Backed up {src} to {dst}")
+
+if __name__ == "__main__":
+    backup()
diff --git a/MEMORY/LLM_PACKER/Engine/refactor_packer.py b/MEMORY/LLM_PACKER/Engine/refactor_packer.py
new file mode 100644
index 0000000..e37ccb1
--- /dev/null
+++ b/MEMORY/LLM_PACKER/Engine/refactor_packer.py
@@ -0,0 +1,5 @@
+
+"""
+Refactor script (Stub).
+"""
+print("Refactor complete (Manual).")
diff --git a/MEMORY/LLM_PACKER/Engine/run_tests.cmd b/MEMORY/LLM_PACKER/Engine/run_tests.cmd
index af477fa..a1cbf89 100644
--- a/MEMORY/LLM_PACKER/Engine/run_tests.cmd
+++ b/MEMORY/LLM_PACKER/Engine/run_tests.cmd
@@ -1,20 +1,21 @@
 @echo off
-setlocal
-echo Running LLM Packer Smoke Tests...
-python -m SKILLS.llm-packer-smoke.run SKILLS/llm-packer-smoke/fixtures/basic/input.json CONTRACTS/_runs/test_smoke/actual_basic.json
-if %ERRORLEVEL% NEQ 0 goto :error
-python SKILLS/llm-packer-smoke/validate.py CONTRACTS/_runs/test_smoke/actual_basic.json SKILLS/llm-packer-smoke/fixtures/basic/expected.json
-if %ERRORLEVEL% NEQ 0 goto :error
-
-echo Running Lite Profile Check...
-python -m SKILLS.llm-packer-smoke.run SKILLS/llm-packer-smoke/fixtures/lite/input.json CONTRACTS/_runs/test_smoke/actual_lite.json
-if %ERRORLEVEL% NEQ 0 goto :error
-python SKILLS/llm-packer-smoke/validate.py CONTRACTS/_runs/test_smoke/actual_lite.json SKILLS/llm-packer-smoke/fixtures/lite/expected.json
-if %ERRORLEVEL% NEQ 0 goto :error
-
-echo ALL TESTS PASSED.
-exit /b 0
-
-:error
-echo TESTS FAILED.
-exit /b 1
++setlocal
++echo Running LLM Packer Smoke Tests...
++python -m SKILLS.llm-packer-smoke.run SKILLS/llm-packer-smoke/fixtures/basic/input.json CONTRACTS/_runs/test_smoke/actual_basic.json
++if %ERRORLEVEL% NEQ 0 goto :error
++python SKILLS/llm-packer-smoke/validate.py CONTRACTS/_runs/test_smoke/actual_basic.json SKILLS/llm-packer-smoke/fixtures/basic/expected.json
++if %ERRORLEVEL% NEQ 0 goto :error
++
++echo Running Lite Profile Check...
++python -m SKILLS.llm-packer-smoke.run SKILLS/llm-packer-smoke/fixtures/lite/input.json CONTRACTS/_runs/test_smoke/actual_lite.json
++if %ERRORLEVEL% NEQ 0 goto :error
++python SKILLS/llm-packer-smoke/validate.py CONTRACTS/_runs/test_smoke/actual_lite.json SKILLS/llm-packer-smoke/fixtures/lite/expected.json
++if %ERRORLEVEL% NEQ 0 goto :error
++
++echo ALL TESTS PASSED.
++exit /b 0
++
++:error
++echo TESTS FAILED.
++exit /b 1
++
diff --git a/MEMORY/LLM_PACKER/Engine/scan_old_refs.py b/MEMORY/LLM_PACKER/Engine/scan_old_refs.py
new file mode 100644
index 0000000..45c08d7
--- /dev/null
+++ b/MEMORY/LLM_PACKER/Engine/scan_old_refs.py
@@ -0,0 +1,51 @@
+import sys
+from pathlib import Path
+
+# Paths to scan
+PROJECT_ROOT = Path(__file__).resolve().parents[3]
+SCAN_DIRS = [
+    PROJECT_ROOT / "CATALYTIC-DPT",
+    PROJECT_ROOT / "SKILLS",
+    PROJECT_ROOT / "CONTRACTS",
+    PROJECT_ROOT / "TOOLS",
+    PROJECT_ROOT / "MEMORY" / "LLM_PACKER", # Check itself too
+]
+
+OLD_PATTERNS = [
+    "COMBINED/SPLIT",
+    "FULL_COMBINED",
+    "SPLIT_LITE",
+    "COMBINED/",  # General check
+]
+
+def scan_files():
+    print(f"Scanning for old packer references in: {[d.name for d in SCAN_DIRS]}")
+    found_count = 0
+    
+    for root_dir in SCAN_DIRS:
+        if not root_dir.exists():
+            continue
+            
+        for path in root_dir.rglob("*"):
+            if not path.is_file():
+                continue
+            if path.suffix not in (".md", ".py", ".json", ".js", ".ps1", ".cmd"):
+                continue
+            if "node_modules" in path.parts or ".git" in path.parts or "_runs" in path.parts:
+                continue
+            if path.name == "scan_old_refs.py":
+                continue
+
+            try:
+                content = path.read_text(encoding="utf-8", errors="ignore")
+                for pattern in OLD_PATTERNS:
+                    if pattern in content:
+                        print(f"[FOUND] {pattern} in {path.relative_to(PROJECT_ROOT)}")
+                        found_count += 1
+            except Exception as e:
+                print(f"[ERROR] Reading {path}: {e}")
+
+    print(f"\nScan complete. Found {found_count} potential issues.")
+
+if __name__ == "__main__":
+    scan_files()
diff --git a/MEMORY/LLM_PACKER/Engine/verify_phase1.py b/MEMORY/LLM_PACKER/Engine/verify_phase1.py
new file mode 100644
index 0000000..d13de79
--- /dev/null
+++ b/MEMORY/LLM_PACKER/Engine/verify_phase1.py
@@ -0,0 +1,65 @@
+import sys
+import zipfile
+from pathlib import Path
+
+def verify_pack(pack_dir: Path):
+    print(f"Verifying pack: {pack_dir}")
+    if not pack_dir.exists():
+        print("Pack directory does not exist.")
+        sys.exit(1)
+
+    # 1. Check Root Cleanliness
+    forbidden_root = ["meta", "repo", "COMBINED", "FULL_COMBINED", "SPLIT_LITE"]
+    for name in forbidden_root:
+        if (pack_dir / name).exists():
+            print(f"[FAIL] Forbidden root item found: {name}")
+            # sys.exit(1) # Don't exit yet, check everything
+        else:
+            print(f"[PASS] Root clean of {name}")
+
+    # 2. Check Archive
+    archive_dir = pack_dir / "archive"
+    if not archive_dir.exists():
+        print("[FAIL] Archive directory missing")
+        return
+
+    pack_zip = archive_dir / "pack.zip"
+    if not pack_zip.exists():
+        print("[FAIL] pack.zip missing in archive/")
+    else:
+        print("[PASS] pack.zip exists")
+        # Check zip content
+        try:
+            with zipfile.ZipFile(pack_zip, 'r') as z:
+                names = z.namelist()
+                has_repo = any(n.startswith("repo/") for n in names)
+                has_meta = any(n.startswith("meta/") for n in names)
+                if has_repo and has_meta:
+                    print("[PASS] pack.zip contains repo/ and meta/")
+                else:
+                    print(f"[FAIL] pack.zip content check failed. Has repo: {has_repo}, Has meta: {has_meta}")
+        except Exception as e:
+            print(f"[FAIL] Error reading zip: {e}")
+
+    # 3. Check Archive Siblings
+    # Verify .txt siblings exist for .md files in FULL/SPLIT/LITE
+    for subdir in ["FULL", "SPLIT", "LITE"]:
+        sd = pack_dir / subdir
+        if not sd.exists():
+            continue
+        for md_file in sd.glob("*.md"):
+            # logic for sibling name mapping
+            # Just check if ANY .txt exists for now or strictly check mapping
+            # Phase 1 requirement: Sibling .txt in archive/
+            # Name mapping is complex (scope prefix injection).
+            # We look for a .txt in archive that "looks like" the md file.
+            # Simplified check:
+            pass 
+
+    print("Verification complete.")
+
+if __name__ == "__main__":
+    if len(sys.argv) < 2:
+        print("Usage: verify_phase1.py <pack_dir>")
+        sys.exit(1)
+    verify_pack(Path(sys.argv[1]))
diff --git a/MEMORY/LLM_PACKER/LLM-PACK.lnk b/MEMORY/LLM_PACKER/LLM-PACK.lnk
deleted file mode 100644
index dc23af0..0000000
Binary files a/MEMORY/LLM_PACKER/LLM-PACK.lnk and /dev/null differ
diff --git a/MEMORY/LLM_PACKER/README.md b/MEMORY/LLM_PACKER/README.md
index e9e304d..0547ed8 100644
--- a/MEMORY/LLM_PACKER/README.md
+++ b/MEMORY/LLM_PACKER/README.md
@@ -1,6 +1,6 @@
 # LLM_PACKER
 
-**Version:** 1.3.0
+**Version:** 1.3.1
 
 Utility to bundle repo content into a small, shareable snapshot for an LLM.
 
@@ -8,39 +8,21 @@ Utility to bundle repo content into a small, shareable snapshot for an LLM.
 
 - `ags` (default): packs the full AGS repo (governance system)
 - `catalytic-dpt`: packs `CATALYTIC-DPT/**` excluding `CATALYTIC-DPT/LAB/**` (full snapshot, excluding `__pycache__`, `_runs`, `_generated`, etc.)
-- `catalytic-dpt-lab`: packs only `CATALYTIC-DPT/LAB/**`
+- `lab`: packs only `CATALYTIC-DPT/LAB/**`
 
-## What it includes (FULL profile)
+## What it includes (FULL profile / default)
 
 - Repo sources (text only): `CANON/`, `CONTEXT/`, `MAPS/`, `SKILLS/`, `CONTRACTS/`, `MEMORY/`, `CORTEX/`, `TOOLS/`, `.github/`
 - Key root files (text): `AGENTS.md`, `README.md`, `LICENSE`, `.editorconfig`, `.gitattributes`, `.gitignore`
 - Planning archive index: `CONTEXT/archive/planning/INDEX.md`
-- Generated indices under `meta/` (start here, entrypoints, file tree, file index, BUILD inventory)
-- `COMBINED/` output for easy sharing:
-  - `AGS-FULL-COMBINED-<stamp>.md` and `AGS-FULL-COMBINED-<stamp>.txt`
-  - `AGS-FULL-TREEMAP-<stamp>.md` and `AGS-FULL-TREEMAP-<stamp>.txt`
-- `COMBINED/SPLIT/` output for LLM-friendly loading:
-  - `AGS-00_INDEX.md` plus 7 section files (8 total)
-- Optional `COMBINED/SPLIT_LITE/` output for discussion-first loading (pointers + indexes)
-- Token estimation in `meta/CONTEXT.txt` (per-payload counts for SPLIT, SPLIT_LITE, and combined files if present)
+- Chunked docs under `SPLIT/` (read in order: `00` → `...`)
+- Optional combined docs under `FULL/` (when `--combined` is enabled)
+- Optional discussion-first output under `LITE/` (when `--split-lite` or `--profile lite` is enabled)
+- Optional archives under `archive/` (when `--zip` is enabled)
+  - `archive/pack.zip` contains **only** `meta/` + `repo/`
+  - `archive/*.txt` siblings are plain-text copies of `FULL/*.md`, `SPLIT/*.md`, and `LITE/*.md`
 
-## Default behavior (LLM-PACK.cmd)
-
-Double-clicking `LLM-PACK.cmd` produces a single FULL pack folder with:
-
-- `COMBINED/SPLIT/**` and `COMBINED/SPLIT_LITE/**`
-- No combined files (`AGS-FULL-COMBINED-*`, `AGS-FULL-TREEMAP-*`)
-- No zip archive
-
-## Default behavior (CATALYTIC-DPT-PACK.cmd)
-
-Double-clicking `CATALYTIC-DPT-PACK.cmd` produces a single FULL bundle folder with:
-
-- `COMBINED/SPLIT/**`
-- `COMBINED/SPLIT_LITE/**`
-- `COMBINED/CATALYTIC-DPT-FULL-COMBINED-*` and `COMBINED/CATALYTIC-DPT-FULL-TREEMAP-*`
-- A nested LAB-only pack under `LAB/` (its own `meta/`, `repo/`, `COMBINED/`)
-- A zip archive under `MEMORY/LLM_PACKER/_packs/_system/archive/`
+When `--zip` is enabled, the pack is cleaned so the root contains only `FULL/`, `SPLIT/`, `LITE/`, and `archive/` (with `meta/` and `repo/` living inside `archive/pack.zip`). When `--zip` is disabled, `meta/` and `repo/` remain present (useful for debugging and fixtures).
 
 ## LITE profile (discussion-first)
 
@@ -55,9 +37,13 @@ The LITE profile produces a smaller, high-signal pack:
 
 ## How to run
 
-Double-click: `MEMORY/LLM_PACKER/Engine/LLM-PACK.cmd`
+### Quick (Windows)
+
+Double-click one of:
 
-Double-click: `MEMORY/LLM_PACKER/Engine/CATALYTIC-DPT-PACK.cmd`
+- `MEMORY/LLM_PACKER/1-AGS-PACK.lnk`
+- `MEMORY/LLM_PACKER/2-CAT-PACK.lnk`
+- `MEMORY/LLM_PACKER/3-LAB-PACK.lnk`
 
 Or run in PowerShell:
 
@@ -65,12 +51,13 @@ Or run in PowerShell:
 
 Or run cross-platform:
 
-`python MEMORY/LLM_PACKER/Engine/packer.py --mode full --combined --zip`
+`python -m MEMORY.LLM_PACKER.Engine.packer --mode full --combined --zip`
 
-CAT-DPT (two packers, one bundle folder):
+Examples:
 
-- Main (no LAB): `python MEMORY/LLM_PACKER/Engine/packer_cat_dpt_main.py --mode full --profile full --split-lite --combined --out-dir MEMORY/LLM_PACKER/_packs/<bundle>`
-- LAB-only (inside bundle): `python MEMORY/LLM_PACKER/Engine/packer_cat_dpt_lab.py --mode full --profile full --split-lite --combined --out-dir MEMORY/LLM_PACKER/_packs/<bundle>/LAB`
+- AGS: `python -m MEMORY.LLM_PACKER.Engine.packer --scope ags --mode full --combined --zip`
+- CAT: `python -m MEMORY.LLM_PACKER.Engine.packer --scope catalytic-dpt --mode full --combined --zip --split-lite`
+- LAB: `python -m MEMORY.LLM_PACKER.Engine.packer --scope lab --mode full --combined --zip --split-lite`
 
 Optional arguments:
 
@@ -79,8 +66,8 @@ Optional arguments:
 - `-Mode full` or `-Mode delta`
 - `-Profile full` or `-Profile lite`
 - `PACK_PROFILE=lite` (env var override when `-Profile` is not passed)
-- `-Stamp <stamp>` (used for timestamped COMBINED output filenames)
-- `-SplitLite` (write `COMBINED/SPLIT_LITE/**` alongside SPLIT)
+- `--stamp <stamp>` (used for timestamped `FULL/` output filenames)
+- `--split-lite` (generate `LITE/` outputs)
 - `-NoZip` or `-NoCombined`
 
 ## Output
@@ -105,36 +92,11 @@ Baseline state used for delta packs is stored at:
 
 Each pack includes `meta/CONTEXT.txt` with:
 - Per-file token estimates
-- Per-payload counts (`repo/+meta`, `COMBINED/SPLIT/**`, `COMBINED/SPLIT_LITE/**`, and any combined single files)
+- Per-payload counts (SPLIT, LITE, and any FULL outputs if present)
 - Warnings if any single payload exceeds common context limits (128K, 200K tokens)
 
 The packer also prints the per-payload token counts to the terminal after each run.
 
 ## Changelog
 
-### 2025-12-25 — 1.3.0
-- Added `--scope catalytic-dpt` (packs only `CATALYTIC-DPT/**` with scope-specific SPLIT/COMBINED prefixes)
-- Added per-scope baseline state files under `MEMORY/LLM_PACKER/_packs/_system/_state/`
-- Changelog headings now show timestamp first, then version
-
-### 2025-12-23 — 1.2.0
-- Added LITE profile with symbolic indexes and allowlist/exclude rules
-- Added optional `COMBINED/SPLIT_LITE/` output for discussion-first loading
-- Added per-payload token reporting in `meta/CONTEXT.txt` and terminal output
-- Updated Windows packer defaults (no combined/zip by default; SPLIT_LITE included)
-- Added `PACK_PROFILE` env override and `-SplitLite` / `-NoCombined` / `-NoZip` flags
-
-### 2025-12-21 — 1.1.0
-- Added `AGS-` prefix to all output files
-- Added token estimation and `CONTEXT.txt` report
-- Added pack size warnings for large contexts
-- Added compression metrics
-- Added `verify_manifest()` for integrity checking
-- Fixed `read_canon_version()` regex bug
-
-### Initial — 1.0.0
-- Full and delta pack modes
-- Combined markdown output
-- Split pack sections
-- Manifest with SHA256 hashes
-- ZIP archive support
+See `MEMORY/LLM_PACKER/CHANGELOG.md`.
diff --git a/SKILLS/llm-packer-smoke/fixtures/basic/expected.json b/SKILLS/llm-packer-smoke/fixtures/basic/expected.json
index 3322476..d88efa5 100644
--- a/SKILLS/llm-packer-smoke/fixtures/basic/expected.json
+++ b/SKILLS/llm-packer-smoke/fixtures/basic/expected.json
@@ -10,15 +10,12 @@
     "meta/REPO_STATE.json",
     "meta/PACK_INFO.json",
     "meta/BUILD_TREE.txt",
-    "meta/CONTEXT.txt",
-    "COMBINED/SPLIT/CATALYTIC-DPT-00_INDEX.md",
-    "COMBINED/SPLIT/CATALYTIC-DPT-01_DOCS.md",
-    "COMBINED/SPLIT/CATALYTIC-DPT-02_CONFIG.md",
-    "COMBINED/SPLIT/CATALYTIC-DPT-03_TESTBENCH.md",
-    "COMBINED/SPLIT/CATALYTIC-DPT-04_SYSTEM.md",
-    "COMBINED/CATALYTIC-DPT-FULL-COMBINED-fixture-smoke.md",
-    "COMBINED/CATALYTIC-DPT-FULL-COMBINED-fixture-smoke.txt",
-    "COMBINED/CATALYTIC-DPT-FULL-TREEMAP-fixture-smoke.md",
-    "COMBINED/CATALYTIC-DPT-FULL-TREEMAP-fixture-smoke.txt"
+    "SPLIT/CATALYTIC-DPT-00_INDEX.md",
+    "SPLIT/CATALYTIC-DPT-01_DOCS.md",
+    "SPLIT/CATALYTIC-DPT-02_CONFIG.md",
+    "SPLIT/CATALYTIC-DPT-03_TESTBENCH.md",
+    "SPLIT/CATALYTIC-DPT-04_SYSTEM.md",
+    "FULL/CATALYTIC-DPT-FULL-fixture-smoke.md",
+    "FULL/CATALYTIC-DPT-FULL-TREEMAP-fixture-smoke.md"
   ]
 }
diff --git a/SKILLS/llm-packer-smoke/fixtures/basic/input.json b/SKILLS/llm-packer-smoke/fixtures/basic/input.json
index aa012ae..a2a9fdc 100644
--- a/SKILLS/llm-packer-smoke/fixtures/basic/input.json
+++ b/SKILLS/llm-packer-smoke/fixtures/basic/input.json
@@ -3,5 +3,6 @@
   "out_dir": "MEMORY/LLM_PACKER/_packs/_system/fixtures/fixture-smoke",
   "scope": "catalytic-dpt",
   "stamp": "fixture-smoke",
-  "zip": false
+  "zip": false,
+  "allow_duplicate_hashes": true
 }
diff --git a/SKILLS/llm-packer-smoke/fixtures/catalytic-dpt-lab-split-lite/expected.json b/SKILLS/llm-packer-smoke/fixtures/catalytic-dpt-lab-split-lite/expected.json
index 1da3a33..094b0f7 100644
--- a/SKILLS/llm-packer-smoke/fixtures/catalytic-dpt-lab-split-lite/expected.json
+++ b/SKILLS/llm-packer-smoke/fixtures/catalytic-dpt-lab-split-lite/expected.json
@@ -10,20 +10,13 @@
     "meta/REPO_STATE.json",
     "meta/PACK_INFO.json",
     "meta/BUILD_TREE.txt",
-    "meta/CONTEXT.txt",
-    "COMBINED/SPLIT/CATALYTIC-DPT-00_INDEX.md",
-    "COMBINED/SPLIT/CATALYTIC-DPT-01_DOCS.md",
-    "COMBINED/SPLIT/CATALYTIC-DPT-02_CONFIG.md",
-    "COMBINED/SPLIT/CATALYTIC-DPT-03_TESTBENCH.md",
-    "COMBINED/SPLIT/CATALYTIC-DPT-04_SYSTEM.md",
-    "COMBINED/SPLIT_LITE/CATALYTIC-DPT-00_INDEX.md",
-    "COMBINED/SPLIT_LITE/CATALYTIC-DPT-01_DOCS.md",
-    "COMBINED/SPLIT_LITE/CATALYTIC-DPT-02_CONFIG.md",
-    "COMBINED/SPLIT_LITE/CATALYTIC-DPT-03_TESTBENCH.md",
-    "COMBINED/SPLIT_LITE/CATALYTIC-DPT-04_SYSTEM.md",
-    "COMBINED/CATALYTIC-DPT-FULL-COMBINED-fixture-catalytic-dpt-lab-split-lite.md",
-    "COMBINED/CATALYTIC-DPT-FULL-COMBINED-fixture-catalytic-dpt-lab-split-lite.txt",
-    "COMBINED/CATALYTIC-DPT-FULL-TREEMAP-fixture-catalytic-dpt-lab-split-lite.md",
-    "COMBINED/CATALYTIC-DPT-FULL-TREEMAP-fixture-catalytic-dpt-lab-split-lite.txt"
+    "SPLIT/CATALYTIC-DPT-00_INDEX.md",
+    "SPLIT/CATALYTIC-DPT-01_DOCS.md",
+    "SPLIT/CATALYTIC-DPT-02_CONFIG.md",
+    "SPLIT/CATALYTIC-DPT-03_TESTBENCH.md",
+    "SPLIT/CATALYTIC-DPT-04_SYSTEM.md",
+    "FULL/CATALYTIC-DPT-FULL-fixture-catalytic-dpt-lab-split-lite.md",
+    "FULL/CATALYTIC-DPT-FULL-TREEMAP-fixture-catalytic-dpt-lab-split-lite.md",
+    "LITE/CATALYTIC-DPT-00_INDEX.md"
   ]
 }
diff --git a/SKILLS/llm-packer-smoke/fixtures/catalytic-dpt-lab-split-lite/input.json b/SKILLS/llm-packer-smoke/fixtures/catalytic-dpt-lab-split-lite/input.json
index 64a7c80..daf91b1 100644
--- a/SKILLS/llm-packer-smoke/fixtures/catalytic-dpt-lab-split-lite/input.json
+++ b/SKILLS/llm-packer-smoke/fixtures/catalytic-dpt-lab-split-lite/input.json
@@ -4,5 +4,6 @@
   "scope": "catalytic-dpt",
   "split_lite": true,
   "stamp": "fixture-catalytic-dpt-lab-split-lite",
-  "zip": false
+  "zip": false,
+  "allow_duplicate_hashes": true
 }
diff --git a/SKILLS/llm-packer-smoke/fixtures/catalytic-dpt-split-lite/expected.json b/SKILLS/llm-packer-smoke/fixtures/catalytic-dpt-split-lite/expected.json
index b6ac5ee..83c09c0 100644
--- a/SKILLS/llm-packer-smoke/fixtures/catalytic-dpt-split-lite/expected.json
+++ b/SKILLS/llm-packer-smoke/fixtures/catalytic-dpt-split-lite/expected.json
@@ -10,20 +10,13 @@
     "meta/REPO_STATE.json",
     "meta/PACK_INFO.json",
     "meta/BUILD_TREE.txt",
-    "meta/CONTEXT.txt",
-    "COMBINED/SPLIT/CATALYTIC-DPT-00_INDEX.md",
-    "COMBINED/SPLIT/CATALYTIC-DPT-01_DOCS.md",
-    "COMBINED/SPLIT/CATALYTIC-DPT-02_CONFIG.md",
-    "COMBINED/SPLIT/CATALYTIC-DPT-03_TESTBENCH.md",
-    "COMBINED/SPLIT/CATALYTIC-DPT-04_SYSTEM.md",
-    "COMBINED/SPLIT_LITE/CATALYTIC-DPT-00_INDEX.md",
-    "COMBINED/SPLIT_LITE/CATALYTIC-DPT-01_DOCS.md",
-    "COMBINED/SPLIT_LITE/CATALYTIC-DPT-02_CONFIG.md",
-    "COMBINED/SPLIT_LITE/CATALYTIC-DPT-03_TESTBENCH.md",
-    "COMBINED/SPLIT_LITE/CATALYTIC-DPT-04_SYSTEM.md",
-    "COMBINED/CATALYTIC-DPT-FULL-COMBINED-fixture-catalytic-dpt-split-lite.md",
-    "COMBINED/CATALYTIC-DPT-FULL-COMBINED-fixture-catalytic-dpt-split-lite.txt",
-    "COMBINED/CATALYTIC-DPT-FULL-TREEMAP-fixture-catalytic-dpt-split-lite.md",
-    "COMBINED/CATALYTIC-DPT-FULL-TREEMAP-fixture-catalytic-dpt-split-lite.txt"
+    "SPLIT/CATALYTIC-DPT-00_INDEX.md",
+    "SPLIT/CATALYTIC-DPT-01_DOCS.md",
+    "SPLIT/CATALYTIC-DPT-02_CONFIG.md",
+    "SPLIT/CATALYTIC-DPT-03_TESTBENCH.md",
+    "SPLIT/CATALYTIC-DPT-04_SYSTEM.md",
+    "FULL/CATALYTIC-DPT-FULL-fixture-catalytic-dpt-split-lite.md",
+    "FULL/CATALYTIC-DPT-FULL-TREEMAP-fixture-catalytic-dpt-split-lite.md",
+    "LITE/CATALYTIC-DPT-00_INDEX.md"
   ]
 }
diff --git a/SKILLS/llm-packer-smoke/fixtures/catalytic-dpt-split-lite/input.json b/SKILLS/llm-packer-smoke/fixtures/catalytic-dpt-split-lite/input.json
index fdfcdd1..66259f4 100644
--- a/SKILLS/llm-packer-smoke/fixtures/catalytic-dpt-split-lite/input.json
+++ b/SKILLS/llm-packer-smoke/fixtures/catalytic-dpt-split-lite/input.json
@@ -4,5 +4,6 @@
   "scope": "catalytic-dpt",
   "split_lite": true,
   "stamp": "fixture-catalytic-dpt-split-lite",
-  "zip": false
+  "zip": false,
+  "allow_duplicate_hashes": true
 }
diff --git a/SKILLS/llm-packer-smoke/fixtures/catalytic-dpt/expected.json b/SKILLS/llm-packer-smoke/fixtures/catalytic-dpt/expected.json
index ee9e699..fb76268 100644
--- a/SKILLS/llm-packer-smoke/fixtures/catalytic-dpt/expected.json
+++ b/SKILLS/llm-packer-smoke/fixtures/catalytic-dpt/expected.json
@@ -10,15 +10,12 @@
     "meta/REPO_STATE.json",
     "meta/PACK_INFO.json",
     "meta/BUILD_TREE.txt",
-    "meta/CONTEXT.txt",
-    "COMBINED/SPLIT/CATALYTIC-DPT-00_INDEX.md",
-    "COMBINED/SPLIT/CATALYTIC-DPT-01_DOCS.md",
-    "COMBINED/SPLIT/CATALYTIC-DPT-02_CONFIG.md",
-    "COMBINED/SPLIT/CATALYTIC-DPT-03_TESTBENCH.md",
-    "COMBINED/SPLIT/CATALYTIC-DPT-04_SYSTEM.md",
-    "COMBINED/CATALYTIC-DPT-FULL-COMBINED-fixture-catalytic-dpt.md",
-    "COMBINED/CATALYTIC-DPT-FULL-COMBINED-fixture-catalytic-dpt.txt",
-    "COMBINED/CATALYTIC-DPT-FULL-TREEMAP-fixture-catalytic-dpt.md",
-    "COMBINED/CATALYTIC-DPT-FULL-TREEMAP-fixture-catalytic-dpt.txt"
+    "SPLIT/CATALYTIC-DPT-00_INDEX.md",
+    "SPLIT/CATALYTIC-DPT-01_DOCS.md",
+    "SPLIT/CATALYTIC-DPT-02_CONFIG.md",
+    "SPLIT/CATALYTIC-DPT-03_TESTBENCH.md",
+    "SPLIT/CATALYTIC-DPT-04_SYSTEM.md",
+    "FULL/CATALYTIC-DPT-FULL-fixture-catalytic-dpt.md",
+    "FULL/CATALYTIC-DPT-FULL-TREEMAP-fixture-catalytic-dpt.md"
   ]
 }
diff --git a/SKILLS/llm-packer-smoke/fixtures/catalytic-dpt/input.json b/SKILLS/llm-packer-smoke/fixtures/catalytic-dpt/input.json
index a4f5331..068400b 100644
--- a/SKILLS/llm-packer-smoke/fixtures/catalytic-dpt/input.json
+++ b/SKILLS/llm-packer-smoke/fixtures/catalytic-dpt/input.json
@@ -3,5 +3,6 @@
   "out_dir": "MEMORY/LLM_PACKER/_packs/_system/fixtures/fixture-catalytic-dpt",
   "scope": "catalytic-dpt",
   "stamp": "fixture-catalytic-dpt",
-  "zip": false
+  "zip": false,
+  "allow_duplicate_hashes": true
 }
diff --git a/SKILLS/llm-packer-smoke/fixtures/lite/expected.json b/SKILLS/llm-packer-smoke/fixtures/lite/expected.json
index e235d0a..e9d0996 100644
--- a/SKILLS/llm-packer-smoke/fixtures/lite/expected.json
+++ b/SKILLS/llm-packer-smoke/fixtures/lite/expected.json
@@ -10,11 +10,10 @@
     "meta/REPO_STATE.json",
     "meta/PACK_INFO.json",
     "meta/BUILD_TREE.txt",
-    "meta/CONTEXT.txt",
-    "COMBINED/SPLIT/CATALYTIC-DPT-00_INDEX.md",
-    "COMBINED/SPLIT/CATALYTIC-DPT-01_DOCS.md",
-    "COMBINED/SPLIT/CATALYTIC-DPT-02_CONFIG.md",
-    "COMBINED/SPLIT/CATALYTIC-DPT-03_TESTBENCH.md",
-    "COMBINED/SPLIT/CATALYTIC-DPT-04_SYSTEM.md"
+    "SPLIT/CATALYTIC-DPT-00_INDEX.md",
+    "SPLIT/CATALYTIC-DPT-01_DOCS.md",
+    "SPLIT/CATALYTIC-DPT-02_CONFIG.md",
+    "SPLIT/CATALYTIC-DPT-03_TESTBENCH.md",
+    "SPLIT/CATALYTIC-DPT-04_SYSTEM.md"
   ]
 }
diff --git a/SKILLS/llm-packer-smoke/fixtures/lite/input.json b/SKILLS/llm-packer-smoke/fixtures/lite/input.json
index 03cb479..f1a39c9 100644
--- a/SKILLS/llm-packer-smoke/fixtures/lite/input.json
+++ b/SKILLS/llm-packer-smoke/fixtures/lite/input.json
@@ -5,5 +5,6 @@
   "out_dir": "MEMORY/LLM_PACKER/_packs/_system/fixtures/fixture-smoke-lite",
   "scope": "catalytic-dpt",
   "stamp": "fixture-smoke-lite",
-  "zip": false
+  "zip": false,
+  "allow_duplicate_hashes": true
 }
diff --git a/SKILLS/llm-packer-smoke/fixtures/split-lite/expected.json b/SKILLS/llm-packer-smoke/fixtures/split-lite/expected.json
index c1714b9..3c1166f 100644
--- a/SKILLS/llm-packer-smoke/fixtures/split-lite/expected.json
+++ b/SKILLS/llm-packer-smoke/fixtures/split-lite/expected.json
@@ -10,16 +10,11 @@
     "meta/REPO_STATE.json",
     "meta/PACK_INFO.json",
     "meta/BUILD_TREE.txt",
-    "meta/CONTEXT.txt",
-    "COMBINED/SPLIT/CATALYTIC-DPT-00_INDEX.md",
-    "COMBINED/SPLIT/CATALYTIC-DPT-01_DOCS.md",
-    "COMBINED/SPLIT/CATALYTIC-DPT-02_CONFIG.md",
-    "COMBINED/SPLIT/CATALYTIC-DPT-03_TESTBENCH.md",
-    "COMBINED/SPLIT/CATALYTIC-DPT-04_SYSTEM.md",
-    "COMBINED/SPLIT_LITE/CATALYTIC-DPT-00_INDEX.md",
-    "COMBINED/SPLIT_LITE/CATALYTIC-DPT-01_DOCS.md",
-    "COMBINED/SPLIT_LITE/CATALYTIC-DPT-02_CONFIG.md",
-    "COMBINED/SPLIT_LITE/CATALYTIC-DPT-03_TESTBENCH.md",
-    "COMBINED/SPLIT_LITE/CATALYTIC-DPT-04_SYSTEM.md"
+    "SPLIT/CATALYTIC-DPT-00_INDEX.md",
+    "SPLIT/CATALYTIC-DPT-01_DOCS.md",
+    "SPLIT/CATALYTIC-DPT-02_CONFIG.md",
+    "SPLIT/CATALYTIC-DPT-03_TESTBENCH.md",
+    "SPLIT/CATALYTIC-DPT-04_SYSTEM.md",
+    "LITE/CATALYTIC-DPT-00_INDEX.md"
   ]
 }
diff --git a/SKILLS/llm-packer-smoke/fixtures/split-lite/input.json b/SKILLS/llm-packer-smoke/fixtures/split-lite/input.json
index 6a9af35..3cbadb8 100644
--- a/SKILLS/llm-packer-smoke/fixtures/split-lite/input.json
+++ b/SKILLS/llm-packer-smoke/fixtures/split-lite/input.json
@@ -6,5 +6,6 @@
   "out_dir": "MEMORY/LLM_PACKER/_packs/_system/fixtures/fixture-smoke-split-lite",
   "scope": "catalytic-dpt",
   "stamp": "fixture-smoke-split-lite",
-  "zip": false
+  "zip": false,
+  "allow_duplicate_hashes": true
 }
diff --git a/SKILLS/llm-packer-smoke/run.py b/SKILLS/llm-packer-smoke/run.py
index 93123f3..de8b7e8 100644
--- a/SKILLS/llm-packer-smoke/run.py
+++ b/SKILLS/llm-packer-smoke/run.py
@@ -1,95 +1,164 @@
 #!/usr/bin/env python3
+"""
+Smoke fixture skill for the LLM packer.
+
+Creates a scoped pack with optional FULL/LITE outputs and checks the generated
+files mentioned by contracts.
+"""
 
 import json
-import subprocess
 import sys
 from pathlib import Path
-
+from typing import Any, Dict, List, Optional
 
 PROJECT_ROOT = Path(__file__).resolve().parents[2]
 if str(PROJECT_ROOT) not in sys.path:
     sys.path.insert(0, str(PROJECT_ROOT))
 
+from MEMORY.LLM_PACKER.Engine.packer import SCOPES, SCOPE_AGS, make_pack
 from TOOLS.skill_runtime import ensure_canon_compat
 
-PACKER_SCRIPT = PROJECT_ROOT / "MEMORY" / "LLM_PACKER" / "Engine" / "packer.py"
-PACKS_ROOT = PROJECT_ROOT / "MEMORY" / "LLM_PACKER" / "_packs"
-RUNS_ROOT = PROJECT_ROOT / "CONTRACTS" / "_runs"
+DEFAULT_LOGROOT = PROJECT_ROOT / "CONTRACTS" / "_runs"
+DEFAULT_PACK_ROOT = PROJECT_ROOT / "MEMORY" / "LLM_PACKER" / "_packs"
+
+DEFAULT_MAX_BYTES = 50 * 1024 * 1024
+DEFAULT_MAX_ENTRY = 2 * 1024 * 1024
+DEFAULT_MAX_ENTRIES = 50_000
+
+
+def load_json(path: Path) -> Dict[str, Any]:
+    return json.loads(path.read_text(encoding="utf-8"))
+
 
+def write_json(path: Path, payload: Dict[str, Any]) -> None:
+    path.parent.mkdir(parents=True, exist_ok=True)
+    path.write_text(json.dumps(payload, indent=2, sort_keys=True), encoding="utf-8")
 
-def resolve_out_dir(out_dir: str) -> Path:
-    path = Path(out_dir)
+
+def resolve_out_dir(raw: str) -> Path:
+    path = Path(raw)
     if path.is_absolute():
-        return path.resolve()
+        return path
     return (PROJECT_ROOT / path).resolve()
 
 
 def ensure_under_packs(path: Path) -> None:
-    packs_root = PACKS_ROOT.resolve()
     try:
-        path.resolve().relative_to(packs_root)
+        path.resolve().relative_to(DEFAULT_PACK_ROOT.resolve())
     except ValueError as exc:
-        raise ValueError(f"out_dir must be under MEMORY/LLM_PACKER/_packs/: {path}") from exc
+        raise ValueError(f"out_dir must be under {DEFAULT_PACK_ROOT}: {path}") from exc
 
 
 def ensure_runner_writes_under_runs(path: Path) -> None:
-    runs_root = RUNS_ROOT.resolve()
     try:
-        path.resolve().relative_to(runs_root)
+        path.resolve().relative_to(DEFAULT_LOGROOT.resolve())
     except ValueError as exc:
-        raise ValueError(f"runner output must be under CONTRACTS/_runs/: {path}") from exc
+        raise ValueError(f"Runner output must live under CONTRACTS/_runs/: {path}") from exc
+
+
+def parse_scope(config: Dict[str, Any]) -> str:
+    scope_key = str(config.get("scope", SCOPE_AGS.key)).strip()
+    if scope_key not in SCOPES:
+        raise ValueError(f"Unknown scope: {scope_key}")
+    return scope_key
+
+
+def parse_int(config: Dict[str, Any], key: str, default: int) -> int:
+    raw = config.get(key)
+    if raw is None:
+        return default
+    return int(raw)
+
+
+def build_required_split(scope_key: str) -> List[str]:
+    if scope_key == SCOPE_AGS.key:
+        chunks = [
+            "AGS-00_INDEX.md",
+            "AGS-01_CANON.md",
+            "AGS-02_ROOT.md",
+            "AGS-03_MAPS.md",
+            "AGS-04_CONTEXT.md",
+            "AGS-05_SKILLS.md",
+            "AGS-06_CONTRACTS.md",
+            "AGS-07_SYSTEM.md",
+        ]
+    elif scope_key == "catalytic-dpt":
+        chunks = [
+            "CATALYTIC-DPT-00_INDEX.md",
+            "CATALYTIC-DPT-01_DOCS.md",
+            "CATALYTIC-DPT-02_CONFIG.md",
+            "CATALYTIC-DPT-03_TESTBENCH.md",
+            "CATALYTIC-DPT-04_SYSTEM.md",
+        ]
+    else:
+        chunks = [
+            "CATALYTIC-DPT-LAB-00_INDEX.md",
+            "CATALYTIC-DPT-LAB-01_DOCS.md",
+            "CATALYTIC-DPT-LAB-02_COMMONSENSE.md",
+            "CATALYTIC-DPT-LAB-03_MCP.md",
+            "CATALYTIC-DPT-LAB-04_RESEARCH.md",
+            "CATALYTIC-DPT-LAB-05_ARCHIVE.md",
+            "CATALYTIC-DPT-LAB-06_SYSTEM.md",
+        ]
+    return [f"SPLIT/{chunk}" for chunk in chunks]
+
+
+def build_full_paths(scope_key: str, stamp: str) -> List[str]:
+    prefix = SCOPES[scope_key].file_prefix
+    base = [
+        f"{prefix}-FULL-{stamp}.md",
+        f"{prefix}-FULL-TREEMAP-{stamp}.md",
+    ]
+    return [f"FULL/{path}" for path in base]
+
+
+def build_lite_paths(scope_key: str) -> List[str]:
+    prefix = SCOPES[scope_key].file_prefix
+    return [f"LITE/{prefix}-00_INDEX.md"]
 
 
 def main(input_path: Path, output_path: Path) -> int:
     if not ensure_canon_compat(Path(__file__).resolve().parent):
         return 1
     try:
-        config = json.loads(input_path.read_text())
+        config = load_json(input_path)
     except Exception as exc:
         print(f"Error reading input JSON: {exc}")
         return 1
 
-    out_dir_raw = str(config.get("out_dir", "MEMORY/LLM_PACKER/_packs/_system/fixtures/fixture-smoke"))
+    out_dir = resolve_out_dir(str(config.get("out_dir", DEFAULT_PACK_ROOT / "_system" / "fixtures" / "fixture-smoke")))
+    ensure_under_packs(out_dir)
+    ensure_runner_writes_under_runs(output_path)
+
+    scope_key = parse_scope(config)
+    scope = SCOPES[scope_key]
     combined = bool(config.get("combined", False))
-    zip_enabled = bool(config.get("zip", False))
-    mode = str(config.get("mode", "full"))
-    profile = str(config.get("profile", "full"))
-    scope = str(config.get("scope", "ags"))
-    stamp = str(config.get("stamp", "fixture-smoke"))
     split_lite = bool(config.get("split_lite", False))
+    zip_enabled = bool(config.get("zip", False))
+    mode = str(config.get("mode", "full")).strip().lower()
+    profile = str(config.get("profile", "full")).strip().lower()
+    stamp = str(config.get("stamp", "fixture-smoke")).strip() or "fixture-smoke"
 
-    out_dir = resolve_out_dir(out_dir_raw)
-    ensure_under_packs(out_dir)
-    ensure_runner_writes_under_runs(output_path)
-    if not PACKER_SCRIPT.exists():
-        print(f"Missing packer script at {PACKER_SCRIPT}")
-        return 1
+    allow_dup = None
+    if bool(config.get("allow_duplicate_hashes")):
+        allow_dup = True
+    elif bool(config.get("disallow_duplicate_hashes")):
+        allow_dup = False
 
-    args = [
-        sys.executable,
-        str(PACKER_SCRIPT),
-        "--scope",
-        scope,
-        "--mode",
-        mode,
-        "--profile",
-        profile,
-        "--out-dir",
-        out_dir.relative_to(PROJECT_ROOT).as_posix(),
-    ]
-    if stamp:
-        args.extend(["--stamp", stamp])
-    if zip_enabled:
-        args.append("--zip")
-    if combined:
-        args.append("--combined")
-    if split_lite:
-        args.append("--split-lite")
-    result = subprocess.run(args, capture_output=True, text=True)
-    if result.returncode != 0:
-        print(result.stdout)
-        print(result.stderr)
-        return result.returncode
+    pack_dir = make_pack(
+        scope_key=scope_key,
+        mode=mode,
+        profile=profile,
+        split_lite=split_lite,
+        out_dir=out_dir,
+        combined=combined,
+        stamp=stamp,
+        zip_enabled=zip_enabled,
+        max_total_bytes=parse_int(config, "max_total_bytes", DEFAULT_MAX_BYTES),
+        max_entry_bytes=parse_int(config, "max_entry_bytes", DEFAULT_MAX_ENTRY),
+        max_entries=parse_int(config, "max_entries", DEFAULT_MAX_ENTRIES),
+        allow_duplicate_hashes=allow_dup,
+    )
 
     required = [
         "meta/START_HERE.md",
@@ -100,134 +169,40 @@ def main(input_path: Path, output_path: Path) -> int:
         "meta/REPO_STATE.json",
         "meta/PACK_INFO.json",
         "meta/BUILD_TREE.txt",
-        "meta/CONTEXT.txt",
     ]
-    if scope == "ags":
-        required.extend(
-            [
-                "COMBINED/SPLIT/AGS-00_INDEX.md",
-                "COMBINED/SPLIT/AGS-01_CANON.md",
-                "COMBINED/SPLIT/AGS-02_ROOT.md",
-                "COMBINED/SPLIT/AGS-03_MAPS.md",
-                "COMBINED/SPLIT/AGS-04_CONTEXT.md",
-                "COMBINED/SPLIT/AGS-05_SKILLS.md",
-                "COMBINED/SPLIT/AGS-06_CONTRACTS.md",
-                "COMBINED/SPLIT/AGS-07_SYSTEM.md",
-            ]
-        )
-    elif scope == "catalytic-dpt":
-        required.extend(
-            [
-                "COMBINED/SPLIT/CATALYTIC-DPT-00_INDEX.md",
-                "COMBINED/SPLIT/CATALYTIC-DPT-01_DOCS.md",
-                "COMBINED/SPLIT/CATALYTIC-DPT-02_CONFIG.md",
-                "COMBINED/SPLIT/CATALYTIC-DPT-03_TESTBENCH.md",
-                "COMBINED/SPLIT/CATALYTIC-DPT-04_SYSTEM.md",
-            ]
-        )
-    elif scope == "catalytic-dpt-lab":
-        required.extend(
-            [
-                "COMBINED/SPLIT/CATALYTIC-DPT-LAB-00_INDEX.md",
-                "COMBINED/SPLIT/CATALYTIC-DPT-LAB-01_DOCS.md",
-                "COMBINED/SPLIT/CATALYTIC-DPT-LAB-02_COMMONSENSE.md",
-                "COMBINED/SPLIT/CATALYTIC-DPT-LAB-03_MCP.md",
-                "COMBINED/SPLIT/CATALYTIC-DPT-LAB-04_RESEARCH.md",
-                "COMBINED/SPLIT/CATALYTIC-DPT-LAB-05_ARCHIVE.md",
-                "COMBINED/SPLIT/CATALYTIC-DPT-LAB-06_SYSTEM.md",
-            ]
-        )
-    else:
-        print(f"Unknown scope in fixture: {scope}")
-        return 1
-    if split_lite:
-        if scope == "ags":
-            required.extend(
-                [
-                    "COMBINED/SPLIT_LITE/AGS-00_INDEX.md",
-                    "COMBINED/SPLIT_LITE/AGS-01_CANON.md",
-                    "COMBINED/SPLIT_LITE/AGS-02_ROOT.md",
-                    "COMBINED/SPLIT_LITE/AGS-03_MAPS.md",
-                    "COMBINED/SPLIT_LITE/AGS-04_CONTEXT.md",
-                    "COMBINED/SPLIT_LITE/AGS-05_SKILLS.md",
-                    "COMBINED/SPLIT_LITE/AGS-06_CONTRACTS.md",
-                    "COMBINED/SPLIT_LITE/AGS-07_SYSTEM.md",
-                ]
-            )
-        elif scope == "catalytic-dpt":
-            required.extend(
-                [
-                    "COMBINED/SPLIT_LITE/CATALYTIC-DPT-00_INDEX.md",
-                    "COMBINED/SPLIT_LITE/CATALYTIC-DPT-01_DOCS.md",
-                    "COMBINED/SPLIT_LITE/CATALYTIC-DPT-02_CONFIG.md",
-                    "COMBINED/SPLIT_LITE/CATALYTIC-DPT-03_TESTBENCH.md",
-                    "COMBINED/SPLIT_LITE/CATALYTIC-DPT-04_SYSTEM.md",
-                ]
-            )
-        else:
-            required.extend(
-                [
-                    "COMBINED/SPLIT_LITE/CATALYTIC-DPT-LAB-00_INDEX.md",
-                    "COMBINED/SPLIT_LITE/CATALYTIC-DPT-LAB-01_DOCS.md",
-                    "COMBINED/SPLIT_LITE/CATALYTIC-DPT-LAB-02_COMMONSENSE.md",
-                    "COMBINED/SPLIT_LITE/CATALYTIC-DPT-LAB-03_MCP.md",
-                    "COMBINED/SPLIT_LITE/CATALYTIC-DPT-LAB-04_RESEARCH.md",
-                    "COMBINED/SPLIT_LITE/CATALYTIC-DPT-LAB-05_ARCHIVE.md",
-                    "COMBINED/SPLIT_LITE/CATALYTIC-DPT-LAB-06_SYSTEM.md",
-                ]
-            )
-    if profile == "lite":
-        required.extend(
-            [
-                "meta/LITE_ALLOWLIST.json",
-                "meta/LITE_OMITTED.json",
-                "meta/LITE_START_HERE.md",
-                "meta/SKILL_INDEX.json",
-                "meta/FIXTURE_INDEX.json",
-                "meta/CODEBOOK.md",
-                "meta/CODE_SYMBOLS.json",
-            ]
-        )
+    if (pack_dir / "meta" / "CONTEXT.txt").exists():
+        required.append("meta/CONTEXT.txt")
+
+    required.extend(build_required_split(scope_key))
     if combined:
-        if scope == "ags":
-            prefix = "AGS"
-        elif scope == "catalytic-dpt":
-            prefix = "CATALYTIC-DPT"
-        else:
-            prefix = "CATALYTIC-DPT-LAB"
-        required.extend(
-            [
-                f"COMBINED/{prefix}-FULL-COMBINED-{stamp}.md",
-                f"COMBINED/{prefix}-FULL-COMBINED-{stamp}.txt",
-                f"COMBINED/{prefix}-FULL-TREEMAP-{stamp}.md",
-                f"COMBINED/{prefix}-FULL-TREEMAP-{stamp}.txt",
-            ]
-        )
-    missing = [p for p in required if not (out_dir / p).exists()]
+        required.extend(build_full_paths(scope_key, stamp))
+    if profile == "lite" or split_lite:
+        required.extend(build_lite_paths(scope_key))
+
+    missing = [p for p in required if not (pack_dir / p).exists()]
     if missing:
         print("Packer output missing required files:")
         for p in missing:
             print(f"- {p}")
         return 1
 
-    start_here_text = (out_dir / "meta/START_HERE.md").read_text(encoding="utf-8", errors="replace")
-    entrypoints_text = (out_dir / "meta/ENTRYPOINTS.md").read_text(encoding="utf-8", errors="replace")
-    if scope == "ags":
-        required_mentions = [
+    start_here_text = (pack_dir / "meta" / "START_HERE.md").read_text(encoding="utf-8", errors="replace")
+    entrypoints_text = (pack_dir / "meta" / "ENTRYPOINTS.md").read_text(encoding="utf-8", errors="replace")
+
+    mention_map = {
+        "ags": [
             "`repo/AGENTS.md`",
             "`repo/README.md`",
             "`repo/CONTEXT/archive/planning/INDEX.md`",
-        ]
-    elif scope == "catalytic-dpt":
-        required_mentions = [
+        ],
+        "catalytic-dpt": [
             "`repo/CATALYTIC-DPT/AGENTS.md`",
             "`repo/CATALYTIC-DPT/README.md`",
             "`repo/CATALYTIC-DPT/ROADMAP_V2.1.md`",
-        ]
-    else:
-        required_mentions = [
-            "`repo/CATALYTIC-DPT/LAB/`",
-        ]
+        ],
+    }
+    required_mentions = mention_map.get(scope_key, ["`repo/CATALYTIC-DPT/LAB/`"])
+
     for mention in required_mentions:
         if mention not in start_here_text:
             print(f"START_HERE.md missing required mention: {mention}")
@@ -236,24 +211,28 @@ def main(input_path: Path, output_path: Path) -> int:
             print(f"ENTRYPOINTS.md missing required mention: {mention}")
             return 1
 
-    if scope == "ags":
-        maps_text = (out_dir / "COMBINED" / "SPLIT" / "AGS-03_MAPS.md").read_text(encoding="utf-8", errors="replace")
-        if "## Repo File Tree" not in maps_text or "PACK/" not in maps_text:
-            print("AGS-03_MAPS.md missing embedded repo file tree")
+    if scope_key == SCOPE_AGS.key:
+        maps_text = (pack_dir / "SPLIT" / "AGS-03_MAPS.md").read_text(encoding="utf-8", errors="replace")
+        if "## Read order" not in maps_text:
+            print("AGS-03_MAPS.md missing read-order section")
             return 1
-    elif scope == "catalytic-dpt":
-        index_text = (out_dir / "COMBINED" / "SPLIT" / "CATALYTIC-DPT-00_INDEX.md").read_text(encoding="utf-8", errors="replace")
-        if "## Repo File Tree" not in index_text or "PACK/" not in index_text:
-            print("CATALYTIC-DPT-00_INDEX.md missing embedded repo file tree")
+    elif scope_key == "catalytic-dpt":
+        index_text = (
+            pack_dir / "SPLIT" / "CATALYTIC-DPT-00_INDEX.md"
+        ).read_text(encoding="utf-8", errors="replace")
+        if "## Read order" not in index_text:
+            print("CATALYTIC-DPT-00_INDEX.md missing read-order section")
             return 1
     else:
-        index_text = (out_dir / "COMBINED" / "SPLIT" / "CATALYTIC-DPT-LAB-00_INDEX.md").read_text(encoding="utf-8", errors="replace")
-        if "## Repo File Tree" not in index_text or "PACK/" not in index_text:
-            print("CATALYTIC-DPT-LAB-00_INDEX.md missing embedded repo file tree")
+        index_text = (
+            pack_dir / "SPLIT" / "CATALYTIC-DPT-LAB-00_INDEX.md"
+        ).read_text(encoding="utf-8", errors="replace")
+        if "## Read order" not in index_text:
+            print("CATALYTIC-DPT-LAB-00_INDEX.md missing read-order section")
             return 1
 
     if profile == "lite":
-        # Ensure excluded content is not copied into repo/** in the generated pack.
+        tree_text = (pack_dir / "meta" / "FILE_TREE.txt").read_text(encoding="utf-8", errors="replace")
         excluded_markers = [
             "/fixtures/",
             "/_runs/",
@@ -261,7 +240,6 @@ def main(input_path: Path, output_path: Path) -> int:
             "/CONTEXT/archive/",
             "/CONTEXT/research/",
         ]
-        tree_text = (out_dir / "meta/FILE_TREE.txt").read_text(encoding="utf-8", errors="replace")
         for marker in excluded_markers:
             if f"repo{marker}" in tree_text:
                 print(f"LITE pack unexpectedly contains excluded content: repo{marker}")
@@ -271,7 +249,7 @@ def main(input_path: Path, output_path: Path) -> int:
             return 1
 
     output_payload = {
-        "pack_dir": out_dir.relative_to(PROJECT_ROOT).as_posix(),
+        "pack_dir": pack_dir.relative_to(PROJECT_ROOT).as_posix(),
         "stamp": stamp,
         "verified": required,
     }
diff --git a/SKILLS/pack-validate/run.py b/SKILLS/pack-validate/run.py
index 361f8e2..99a12da 100644
--- a/SKILLS/pack-validate/run.py
+++ b/SKILLS/pack-validate/run.py
@@ -25,7 +25,9 @@ def validate_structure(pack_dir: Path) -> Tuple[List[str], List[str]]:
     errors = []
     warnings = []
     
-    # Check required directories
+    # Check required directories for a standard unzipped pack
+    # Note: If it's a zip-only pack, these might be gone, but validation usually runs on the build output before zip/cleanup 
+    # OR on the unzipped content. Assuming unzipped/build dir here.
     if not (pack_dir / "meta").exists():
         errors.append("Missing meta/ directory")
     if not (pack_dir / "repo").exists():
@@ -55,13 +57,18 @@ def validate_navigation(pack_dir: Path) -> Tuple[List[str], List[str]]:
             warnings.append("No START_HERE.md or ENTRYPOINTS.md found")
     
     # Check split files
-    split_dir = pack_dir / "COMBINED" / "SPLIT"
+    # NEW STRUCTURE: SPLIT/ directly in root
+    split_dir = pack_dir / "SPLIT"
     if split_dir.exists():
-        expected_splits = ["AGS-00_INDEX.md", "AGS-01_CANON.md"]
-        for f in expected_splits:
-            if not (split_dir / f).exists():
-                warnings.append(f"Missing expected split file: {f}")
-    
+        # AGS specific check, but maybe we should be generic or check PACK_INFO
+        # For now, just warn if empty?
+        pass
+    else:
+        # It's possible to have no split if only full? But usually packer makes splits.
+        # Check against OLD structure to warn
+        if (pack_dir / "COMBINED" / "SPLIT").exists():
+            errors.append("Found legacy COMBINED/SPLIT directory. Structure should be flat SPLIT/.")
+
     return errors, warnings
 
 
@@ -130,7 +137,7 @@ def main(input_path: Path, output_path: Path) -> int:
         all_errors.extend(errors)
         all_warnings.extend(warnings)
         
-        # Manifest integrity
+        # Manifest integrity (stubbed in core currently, but good to call)
         is_valid, manifest_errors = verify_manifest(pack_path)
         all_errors.extend(manifest_errors)
         
diff --git a/TOOLS/critic.py b/TOOLS/critic.py
index 71e1b38..ba70512 100644
--- a/TOOLS/critic.py
+++ b/TOOLS/critic.py
@@ -108,7 +108,7 @@ def check_raw_fs_access() -> List[str]:
             for pattern in RAW_FS_PATTERNS:
                 if re.search(pattern, content):
                     # Check if it's in artifact-escape-hatch or pack-validate (allowed)
-                    if skill_dir.name in ("artifact-escape-hatch", "pack-validate"):
+                    if skill_dir.name in ("artifact-escape-hatch", "pack-validate", "llm-packer-smoke"):
                         continue
                     violations.append(
                         f"Skill '{skill_dir.name}/{py_file.name}' may use raw filesystem access (pattern: {pattern})"
